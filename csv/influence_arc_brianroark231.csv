2020.lrec-1.294,W14-3901,0,0.0185532,"ne et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arabic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for gloss"
2020.lrec-1.294,W16-4807,0,0.0224852,"ion particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arabic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for glossing of linguistic texts (Aroonmanakun, 2004; Kim and Shin, 2013). Others have simply tried t"
2020.lrec-1.294,N16-1055,0,0.0667864,"Missing"
2020.lrec-1.294,W11-3501,0,0.0282273,"2004). Advances have been made in transliteration modeling in a range of scenarios – see Wolf-Sonkin et al. (2019) for further citations – including the use of transliteration in increasingly challenging settings, such as information retrieval with mixed scripts (Gupta et al., 2014) or for text entry on mobile devices (Hellsten et al., 2017). As SMS messaging and social media have become more and more ubiquitous, Latin script input method editors (IMEs) have become increasingly common for languages with other native scripts, leading to increasing amounts of romanized text in these languages (Ahmed et al., 2011). The spelling variation that arises from the above-mentioned lack of standard orthography in the Latin script in South Asian languages is similar to issues found in the writing of dialects of Arabic (Habash et al., 2012), and in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for"
2020.lrec-1.294,W14-1604,0,0.0228361,"easing amounts of romanized text in these languages (Ahmed et al., 2011). The spelling variation that arises from the above-mentioned lack of standard orthography in the Latin script in South Asian languages is similar to issues found in the writing of dialects of Arabic (Habash et al., 2012), and in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in bot"
2020.lrec-1.294,C14-1111,0,0.0121944,"frequency greater than one in the training section of the Wikipedia text for that language, as described in § 3.1.. We have 30,000 entries for all languages except Sindi, where we have 20,000. For convenience, we have partitioned the data, with 5,000 being considered validation examples (split evenly between development and test sections), and the rest in a training section. To improve the training/validation separation, we ensured that no validation set word shares a lemma with any training set word. To obtain lemmata (of which there may be multiple) for each word, we used Morfessor FlatCat (Grönroos et al., 2014) to train a model for each language on the first 77853 lines5 of its shuffled Wikipedia text. Using standard hyperparameters yields slightly over-segmented (and thus more conservatively separated) lemmata for each word in training and validation sets. 3.3. Romanized Wikipedia Finally, we provide romanizations of full Wikipedia sentences, randomly selected from the validation set of the native script Wikipedia collections described in § 3.1.. For each language in the dataset, we had 10,000 sentences romanized by native speakers. We split long sentences (greater than 30 whitespace delimited toke"
2020.lrec-1.294,Y04-1021,0,0.125394,"abic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for glossing of linguistic texts (Aroonmanakun, 2004; Kim and Shin, 2013). Others have simply tried to perform text normalization directly, e.g., based on clustering (Rafae et al., 2015). Language modeling of romanized text for these languages, with their lack of orthography, has been relatively underexplored. There are important use scenarios for such models, including the potential for language identification of romanized text – a task that has been studied though without the benefit of advanced language modeling methods – but also for models used in mobile keyboards for autocorrection, gesture-based text entry and/or word prediction and comp"
2020.lrec-1.294,habash-etal-2012-conventional,0,0.0342546,"as information retrieval with mixed scripts (Gupta et al., 2014) or for text entry on mobile devices (Hellsten et al., 2017). As SMS messaging and social media have become more and more ubiquitous, Latin script input method editors (IMEs) have become increasingly common for languages with other native scripts, leading to increasing amounts of romanized text in these languages (Ahmed et al., 2011). The spelling variation that arises from the above-mentioned lack of standard orthography in the Latin script in South Asian languages is similar to issues found in the writing of dialects of Arabic (Habash et al., 2012), and in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and U"
2020.lrec-1.294,W17-4002,1,0.882987,"e of reference. and each character on the output side is from the Latin script (or _ ). Note that diacritics such as virama () are separate characters in this string, and can align to the empty string or to silent characters in the English orthography. Given such a string of “pair” symbols, we can train an ngram language model, which provides a joint distribution over input/output string relations. Converted into a finitestate transducer, this can be used to find the most likely transliterations for a given input string in either direction (native to Latin or vice versa). See Hellsten et al. (2017) for more details on such approaches. For these experiments, we trained 6-gram models with Witten-Bell (1991) smoothing. LSTM Sequence-to-sequence. This baseline treats transliteration as a standard sequence-to-sequence problem, and applies an encoder-decoder architecture to it, similar to the type used for machine translation in Bahdanau et al. (2014). The architecture consists of a deep bidirectional encoder network7 , itself made up of layers containing a forward LSTM and a backward LSTM, connected to a forward decoder LSTM by an attention mechanism. Character input was embedded at each inp"
2020.lrec-1.294,W12-6204,0,0.0248807,".g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or b"
2020.lrec-1.294,W12-2109,0,0.0267339,"in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al."
2020.lrec-1.294,W12-4808,0,0.029458,"ilar to issues found in the writing of dialects of Arabic (Habash et al., 2012), and in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagat"
2020.lrec-1.294,I13-1017,0,0.0291448,"lish (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for glossing of linguistic texts (Aroonmanakun, 2004; Kim and Shin, 2013). Others have simply tried to perform text normalization directly, e.g., based on clustering (Rafae et al., 2015). Language modeling of romanized text for these languages, with their lack of orthography, has been relatively underexplored. There are important use scenarios for such models, including the potential for language identification of romanized text – a task that has been studied though without the benefit of advanced language modeling methods – but also for models used in mobile keyboards for autocorrection, gesture-based text entry and/or word prediction and completion (Wolf-Sonkin e"
2020.lrec-1.294,P98-1036,0,0.214053,"pically romanized as ‘sam’ when the following aksara begins with labial closure (संपूरण, ‘sampurn’) versus not (संसकृत, ‘sanskrit’). Also of note is that those romanizing South Asian languages from Perso-Arabic scripts romanize with vowels, despite not typically being included in the native writing system. 2.2. Transliteration & Romanized text processing Early NLP work on automatic transliteration between writing systems was driven by the needs of machine translation or information retrieval systems, and hence was generally focused on proper names and/or loanwords (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). Pronunciation modeling did play a role in early approaches (Knight and Graehl, 1998), though directly modeling script-to-script correspondences was eventually shown to be effective (Li et al., 2004). Advances have been made in transliteration modeling in a range of scenarios – see Wolf-Sonkin et al. (2019) for further citations – including the use of transliteration in increasingly challenging settings, such as information retrieval with mixed scripts (Gupta et al., 2014) or for text entry on mobile devices (Hellsten et al., 2017). As SMS messagin"
2020.lrec-1.294,P18-1008,0,0.0498243,"Missing"
2020.lrec-1.294,P04-1021,0,0.147817,"aksara begins with labial closure (संपूरण, ‘sampurn’) versus not (संसकृत, ‘sanskrit’). Also of note is that those romanizing South Asian languages from Perso-Arabic scripts romanize with vowels, despite not typically being included in the native writing system. 2.2. Transliteration & Romanized text processing Early NLP work on automatic transliteration between writing systems was driven by the needs of machine translation or information retrieval systems, and hence was generally focused on proper names and/or loanwords (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). Pronunciation modeling did play a role in early approaches (Knight and Graehl, 1998), though directly modeling script-to-script correspondences was eventually shown to be effective (Li et al., 2004). Advances have been made in transliteration modeling in a range of scenarios – see Wolf-Sonkin et al. (2019) for further citations – including the use of transliteration in increasingly challenging settings, such as information retrieval with mixed scripts (Gupta et al., 2014) or for text entry on mobile devices (Hellsten et al., 2017). As SMS messaging and social media have become more and more"
2020.lrec-1.294,N18-2085,1,0.758426,"standard deviation over 5 trials. For language modeling results, we are evaluating openvocabulary models (as motivated in § 4.4.), hence we report bits-per-character (BPC). This is a standard measure (related to perplexity) typically applied to character-level language models. Per sample, it is calculated as the total negative log base 2 probability of the correct output character sequence, divided by the number of characters in the output string. Additionally, since within each language we have parallel native script and romanized corpora that we are training and evaluating on, we can follow Cotterell et al. (2018) and Mielke et al. (2019) in comparing language modeling results across parallel data samples by normalizing with a common factor. We call this bits-per-nativecharacter (BPNC): total negative log base 2 probability divided by the number of characters in the native script strings (rather than the romanized strings). 4.2. Single word transliteration For single word transliteration, we train on the training section of the romanization lexicon for each language and validate on the dev section of that language’s lexicon. The test section remains in reserve. For each romanization in the dev set (ign"
2020.lrec-1.294,D15-1166,0,0.0175326,"Missing"
2020.lrec-1.294,maleki-ahrenberg-2008-converting,0,0.0286441,"s of Arabic (Habash et al., 2012), and in the processing (e.g., optical character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern st"
2020.lrec-1.294,D18-1030,0,0.0278965,"sed XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arabic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for glossing of linguistic texts (Aroonmanakun, 2004; Kim and Shin, 2013). Others have simply tried to perform text normalization directly, e.g., based on clustering (Rafae et al., 2015). Language modeling of romanized text for these"
2020.lrec-1.294,P19-1491,1,0.836027,"ials. For language modeling results, we are evaluating openvocabulary models (as motivated in § 4.4.), hence we report bits-per-character (BPC). This is a standard measure (related to perplexity) typically applied to character-level language models. Per sample, it is calculated as the total negative log base 2 probability of the correct output character sequence, divided by the number of characters in the output string. Additionally, since within each language we have parallel native script and romanized corpora that we are training and evaluating on, we can follow Cotterell et al. (2018) and Mielke et al. (2019) in comparing language modeling results across parallel data samples by normalizing with a common factor. We call this bits-per-nativecharacter (BPNC): total negative log base 2 probability divided by the number of characters in the native script strings (rather than the romanized strings). 4.2. Single word transliteration For single word transliteration, we train on the training section of the romanization lexicon for each language and validate on the dev section of that language’s lexicon. The test section remains in reserve. For each romanization in the dev set (ignoring the number of attes"
2020.lrec-1.294,W08-0904,0,0.0247829,"2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arabic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other text"
2020.lrec-1.294,D15-1097,0,0.122377,"character recognition) of historical documents from prior to spelling normalization (Garrette and AlpertAbrams, 2016). In fact, dialectal Arabic is also frequently romanized, compounding the problem by lacking a standard orthography on either side of transliteration (Al-Badrashiny et al., 2014). Automatic processing of romanized text for languages using Perso-Arabic scripts is relatively common in the NLP literature, including, of course, Arabic – e.g., the above citations and Chalabi and Gerges (2012) – but also Persian (Maleki and Ahrenberg, 2008) and Urdu (Irvine et al., 2012; Bögel, 2012; Rafae et al., 2015). Romanization can make language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arab"
2020.lrec-1.294,W03-1508,0,0.0832383,"s ‘sam’ when the following aksara begins with labial closure (संपूरण, ‘sampurn’) versus not (संसकृत, ‘sanskrit’). Also of note is that those romanizing South Asian languages from Perso-Arabic scripts romanize with vowels, despite not typically being included in the native writing system. 2.2. Transliteration & Romanized text processing Early NLP work on automatic transliteration between writing systems was driven by the needs of machine translation or information retrieval systems, and hence was generally focused on proper names and/or loanwords (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). Pronunciation modeling did play a role in early approaches (Knight and Graehl, 1998), though directly modeling script-to-script correspondences was eventually shown to be effective (Li et al., 2004). Advances have been made in transliteration modeling in a range of scenarios – see Wolf-Sonkin et al. (2019) for further citations – including the use of transliteration in increasingly challenging settings, such as information retrieval with mixed scripts (Gupta et al., 2014) or for text entry on mobile devices (Hellsten et al., 2017). As SMS messaging and social media have bec"
2020.lrec-1.294,voss-etal-2014-finding,0,0.0257744,"language identification particularly difficult, as multiple languages may share similarities in their romanized strings. In fact, Bögel (2012) used rule-based XFST-encoded transliteration between romanized Urdu and its native Perso-Arabic script in both directions, to allow the romanized text to be used as an intermediate representation between Hindi and Urdu. In addition, code switching is common, e.g., between Japanese and English in language learners (Nagata et al., 2008), between dialectal and modern standard Arabic (Eskander et al., 2014), or between dialectal Arabic, French and English (Voss et al., 2014). Adouane et al. (2016) looked at a language identification scenario that included both romanized dialectal Arabic and romanized Berber. Zhang et al. (2018) looked at large-scale language identification that included some evaluation of Hindi written in the Latin script. While most research looks at transforming romanized text into a native script, some has looked at automatic romanization, as the means of, for example, normalizing SMS messages or other texts into some formal romanization standard, such as those used for glossing of linguistic texts (Aroonmanakun, 2004; Kim and Shin, 2013). Oth"
2020.lrec-1.294,W19-3114,1,0.758611,"on standard romanization systems (such as pinyin for Chinese). In other words, when individuals use the Latin script to write in South Asian languages, they do not adhere to a system, i.e., there is no commonly used standard Latin script orthography in these languages. Rather, individuals generally use the Latin script to provide a rough phonetic transcription of the intended word, which can vary from individual to individual due to any number of factors, including regional or dialectal differences in pronunciations, differing conventions of transcription, or simple idiosyncrasy. For example, Wolf-Sonkin et al. (2019) present an example from the comment thread to a blog entry, which includes Hindi comments in both the native script Devanagari and in the Latin script, where the word भरषटाचार is romanized variously as bhrastachar, bhrashtachar, barashtachaar and bharastachar, among others. The prevalence of the Latin script for writing these languages has led to, among other things, work on text-entry systems for those languages in the Latin script, including systems that transliterate from Latin script input to the native script of the language (Hellsten et al., 2017), as well as systems that produce text"
2020.tacl-1.1,J92-1002,0,0.824584,"Missing"
2020.tacl-1.1,C65-1001,0,0.259592,"lasi, 2014). To the extent that this is interpreted as being a compensatory relation, this would indicate that word length is being taken as an implicit measure of complexity. Alternatively, word length has a natural interpretation in terms of information rate, 2.3 Phonotactics Beyond characterizing the complexity of phonemes in isolation or the number of syllables, one can also look at the system determining how phonemes combine to form longer sequences in order to create words. The study of which sequences of phonemes constitute natural-sounding words is called phonotactics. For example, as Chomsky and Halle (1965) point out in their oftcited example, brick is an actual word in English;5 3 Note that by examining negative correlations between word length and inventory size within the context of complexity compensation, word length is also being taken implicitly as a complexity measure, as we shortly make explicit. 4 McWhorter (2001) was one of the first to offer a quantitative treatment of linguistic complexity at all levels. Note, however, he rejects the equal complexity hypothesis, arguing that creoles are simpler than other languages. As our data contain no creole languages, we cannot address this hyp"
2020.tacl-1.1,P17-1109,1,0.80852,"Missing"
2020.tacl-1.1,N01-1021,0,0.682633,"g language, which Miestamo (2006) points out may vary depending on the individual (hence, is relative to the individual being considered). For example, vowel harmony, which we will touch upon later in the paper, may make vowels more predictable for a native speaker, hence less difficult to process; for a second language learner, however, vowel harmony may increase difficulty of learning and speaking. Absolute complexity measures, in contrast, assess the number of parts of a linguistic (sub-)system (e.g., number of phonemes or licit syllables). In the sentence processing literature, surprisal (Hale, 2001; Levy, 2008) is a widely used measure of processing difficulty, defined as the negative log probability of a word given the preceding words. Words that are highly predictable from the preceding context have low surprisal, and those that are not predictable have high surprisal. The phonotactic measure we advocate for in §3 is related to surprisal, though at the phoneme level rather than the word level, and over words rather than sentences. Measures related to phonotactic probability have been used in a range of psycholinguistic studies—see §2.4—though generally to characterize single words wit"
2020.tacl-1.1,Q17-1006,0,0.053281,"Missing"
2020.tacl-1.1,P82-1020,0,0.84283,"Missing"
2020.tacl-1.1,J86-2003,0,0.156571,"et al., 1992). 1 Introduction One prevailing view on system wide phonological complexity is that as one aspect increases in complexity (e.g., size of phonemic inventory), another reduces in complexity (e.g., degree of phonotactic interactions). Underlying this claim— the so-called compensation hypothesis (Martinet, 1955; Moran and Blasi, 2014)—is the conjecture that languages are, generally speaking, of roughly equivalent complexity, that is, no language is overall inherently more complex than another. This conjecture is widely accepted in the literature and dates back at least to the work of Hockett (1958). Because along any one axis, a language may be more complex than another, this conjecture has a corollary that compensatory relationships between different types of complexity must exist. Such compensation has been hypothesized to be the result of natural processes of historical change, 1 Transactions of the Association for Computational Linguistics, vol. 8, pp. 1–18, 2020. https://doi.org/10.1162/tacl a 00296 Action Editor: Eric Fosler-Lussier. Submission batch: 5/2019; Revision batch: 9/2019; Published 1/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 l"
2020.tacl-1.1,J94-3001,0,0.469028,"lobstruent devoicing reduces phonological complexity under our information-theoretic metric. The reason is simple: There are fewer valid syllables as all those with voiced final obstruents are ruled out. Indeed, this point is also true of the syllable counting metric discussed in §2.2. One computational notion of complexity might say that the complexity of the phonology is equal to the number of states required to encode the transduction from an underlying form to a surface form in a minimal finite-state transduction. Note that all Sound Pattern of English (SPE)-style rules may be so encoded (Kaplan and Kay, 1994). Thus, the complexity of the phonotactics could be said to be related to the number of SPE-style rules that operate. In contrast, under our metric, any process that constrains the number of possibilities will, inherently, reduce complexity. The studies in §5.3 allow us to examine the magnitude of such a reduction, and validate our models with respect to this expected behavior. We create two artificial datasets without finalobstruent devoicing based on the German and Dutch portions of NorthEuraLex. We reverse the 8 Most of the concepts in the dataset do not contain function words and verbs are"
2020.tacl-1.1,P18-1027,0,0.0172519,"th black immediately portuguese finnish north karelian veps northern sami hill mari olho korva antua hambaz c˘ a´ hppes t¨op¨ok /oLu/ /kOrVA/ /AntUA/ /hAmbAz/ &gt; /Ùaahppes/ /tørøk/ corresponding embedding representation z (k) . A phoneme embedding will, then, be composed by the element-wise average of each of its features lookup embedding Recurrent Neural LM. Recurrent neural networks excel in language modeling, being able to capture complex distributions p(xi |x<i ) (Mikolov et al., 2010; Sundermeyer et al., 2012). Empirically, recent work has observed dependencies on up to around 200 tokens (Khandelwal et al., 2018). We use a characterlevel Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) language model, which is the state of the art for character-level language modeling (Merity et al., 2018). Our architecture receives a sequence of tokens x ∈ Σ∗ and embeds each token xi ∈ Σ using a dictionary-lookup embedding table. This results in vectors zi ∈ Rd which are fed into an LSTM. This LSTM produces a high-dimensional representation of the sequence, often termed hidden states zi = P (k ) (k ) k ai z P (k ) k ai (11) (j ) where ai is 1 if phoneme i presents attribute j and z (j ) is the lookup e"
2020.tacl-1.1,P19-1491,1,0.847512,"ther than a /d/. We conclude that /x/ is in the consonant 3.3 A Variational Upper Bound If we want to compute Equation (1), we are immediately faced with two problems. First, we do not know plex : we simply assume the existence of such a distribution from which the words of the lexicon were drawn. Second, even if we did know plex , computation of the H (plex ) would 7 3.4 A Note on Types and Tokens be woefully intractable, as it involves an infinite sum. Following Brown et al. (1992), we tackle both of these issues together. Note that this line of reasoning follows Cotterell et al. (2018) and Mielke et al. (2019), who use a similar technique for measuring language complexity at the sentence level. We start with a basic inequality from information theory. For any distribution qlex with the same support as plex , the cross-entropy provides an upper bound on the entropy, that is H (plex ) ≤ H (plex , qlex ) To make the implicit explicit, in this work we will exclusively be modeling types, rather than tokens. We briefly justify this discussion from both theoretical and practical concerns. From a theoretical side, a token-based model is unlikely to correctly model an out of vocabulary distribution as very"
2020.tacl-1.1,L18-1293,1,0.863836,"Missing"
2020.tacl-1.1,N18-2085,1,\N,Missing
2021.eacl-demos.3,W16-2409,0,0.444009,"tion transducer has the prefix FROM_ for the native-to-Latin direction, and TO_ for the inverse. The numbers of states (?? ) and arcs (?? ) of the resulting transliteration (ℐ), NFC (?), visual normalization (?) transducers and well-formedness acceptors (?) for each script and alphabet type are shown in Table 4. Figure 5: Software dependency diagrams for the three modes of operation: compile stage (left), Python runtime (center) and C++ run-time (right). automata and transducers we employ Pynini7 , a Python library for constructing finite-state grammars and for performing operations on WFSTs (Gorman, 2016; Gorman and Sproat, in press). In addition, the library depends on Thrax8 , an older relative of Pynini, that provides a custom grammar manipulation language for WFSTs (Tai et al., 2011; Roark et al., 2012). Although Thrax has been mostly superseded by Pynini, we still rely on some of its utilities for unit testing and its C++ runtime components. At their core, both Pynini and Thrax depend on the OpenFst library9 for the implementation of most WFST algorithms (Allauzen et al., 2007; Riley et al., 2009). The overall dependency diagram is shown on the left-hand side of Figure 5 (the minimal dep"
2021.eacl-demos.3,2020.findings-emnlp.223,0,0.0179613,"achine learning and the increased availability of relevant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistical machine translation-based Brahmi-Net (Kunchukuttan et al., 2015), are model-based, while others, such as URoman (Hermjakob et al., 2018), IndicNLP (Kunchukuttan, 2020), and Aksharmukha (Rajan, 2020), employ rules. The main focus of these libraries is script conversion and romanization. In this capacity they were successfully employed in diverse downstream multilingual NLP tasks such as neural machine translation (Zhang et al., 2020; Amrhein and Sennrich, 2020), morphological analysis (Hauer et al., 2019; 3 Brahmic Scripts: An Overview The scripts of interest have evolved from the ancient Brāhmī writing system that was recorded 15 Name Bengali Devanagari Gujarati Gurmukhi Kannada Malayalam Oriya Sinhala Tamil Telugu Id BENG DEVA GUJR GURU KNDA MLYM ORYA SINH TAML TELU iv 16 19 16 12 17 16 14 18 12 16 dv 13 17 15 9 15 16 13 17 11 15 c 43 45 39 39 39 38 38 41 27 38 co Visual 5 4 5 8 3 10 5 2 1 5 ऩ क Legacy sequence NFC normalized NA NUKTA (U+0928 U+093C) QA (U+0958) NNNA (U+0929) KA NUKTA (U+0915 U+093C) Table 2: NFC examples for Devanagari. bols, li"
2021.eacl-demos.3,W19-4202,0,0.0222423,"evant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistical machine translation-based Brahmi-Net (Kunchukuttan et al., 2015), are model-based, while others, such as URoman (Hermjakob et al., 2018), IndicNLP (Kunchukuttan, 2020), and Aksharmukha (Rajan, 2020), employ rules. The main focus of these libraries is script conversion and romanization. In this capacity they were successfully employed in diverse downstream multilingual NLP tasks such as neural machine translation (Zhang et al., 2020; Amrhein and Sennrich, 2020), morphological analysis (Hauer et al., 2019; 3 Brahmic Scripts: An Overview The scripts of interest have evolved from the ancient Brāhmī writing system that was recorded 15 Name Bengali Devanagari Gujarati Gurmukhi Kannada Malayalam Oriya Sinhala Tamil Telugu Id BENG DEVA GUJR GURU KNDA MLYM ORYA SINH TAML TELU iv 16 19 16 12 17 16 14 18 12 16 dv 13 17 15 9 15 16 13 17 11 15 c 43 45 39 39 39 38 38 41 27 38 co Visual 5 4 5 8 3 10 5 2 1 5 ऩ क Legacy sequence NFC normalized NA NUKTA (U+0928 U+093C) QA (U+0958) NNNA (U+0929) KA NUKTA (U+0915 U+093C) Table 2: NFC examples for Devanagari. bols, like anusvara, chandrabindu, and visarga, whic"
2021.eacl-demos.3,W17-4002,1,0.762773,"early formal syntactic work by Datta (1984) and is the first such library designed based on an observation by Sproat (2003) that the fundamental organizing principles of the Brahmic scripts can be algebraically formalized. In particular, all the core components of our library (inverse romanization, normalization and well-formedness) are compactly and efficiently represented as finite state transducers. Such formalization lends itself particularly well to run-time or offline integration with any finite state processing pipeline, such as decoder components of input methods (Ouyang et al., 2017; Hellsten et al., 2017), text normalization for automatic speech recognition and text-to-speech synthesis (Zhang et al., 2019), among other natural language and speech applications. Related Work The computational processing of Brahmic scripts is not a new topic, with the first applications dating back to the early formal syntactic work by Datta (1984). With an increased focus on the South Asian languages within the NLP community, facilitated by advances in machine learning and the increased availability of relevant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistica"
2021.eacl-demos.3,P18-4003,0,0.0203497,"r natural language and speech applications. Related Work The computational processing of Brahmic scripts is not a new topic, with the first applications dating back to the early formal syntactic work by Datta (1984). With an increased focus on the South Asian languages within the NLP community, facilitated by advances in machine learning and the increased availability of relevant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistical machine translation-based Brahmi-Net (Kunchukuttan et al., 2015), are model-based, while others, such as URoman (Hermjakob et al., 2018), IndicNLP (Kunchukuttan, 2020), and Aksharmukha (Rajan, 2020), employ rules. The main focus of these libraries is script conversion and romanization. In this capacity they were successfully employed in diverse downstream multilingual NLP tasks such as neural machine translation (Zhang et al., 2020; Amrhein and Sennrich, 2020), morphological analysis (Hauer et al., 2019; 3 Brahmic Scripts: An Overview The scripts of interest have evolved from the ancient Brāhmī writing system that was recorded 15 Name Bengali Devanagari Gujarati Gurmukhi Kannada Malayalam Oriya Sinhala Tamil Telugu Id BENG DEV"
2021.eacl-demos.3,N19-1252,0,0.012206,"e considerable superficial differences between these scripts, they follow the same encoding model in Unicode, and maintain a very similar character repertoire having evolved from the same source — the Brāhmī script (Salomon, 1996; Fedorova, 2012). This observation lends itself to the script-agnostic design (outlined in §4) that, unlike other approaches reviewed in §2, is based on the weighted finite state transducer (WFST) formalism (Mohri, 2004). The details of our system are provided in §5. 2 Murikinati et al., 2020), named entity recognition (Huang et al., 2019) and part-of-speech tagging (Cardenas et al., 2019). Similar to the software mentioned above, our library does provide romanization, but unlike some of the packages, such as URoman, we guarantee reversibility from Latin back to the native script. Similar to others we do not focus on faithful invertible transliteration of named entities which typically requires model-based approaches (Sequiera et al., 2014). Unlike the IndicNLP package, our software does not provide morphological analysis, but instead offers significantly richer script normalization capabilities than other packages. These capabilities are functionally separated into normalizati"
2021.eacl-demos.3,D19-1672,0,0.0220274,"Missing"
2021.eacl-demos.3,2020.findings-emnlp.445,0,0.0507757,"Missing"
2021.eacl-demos.3,J94-3001,0,0.853186,"tial independent vowels (line 9). The two remaining operations on aksara, namely NFC and visual normalization, are represented in our library using the context-dependent rewrite rules from the formal approach popularized by Chomsky and Halle (1968). The normalization rules are represented as a sequence {? → ?/? __ ?}, where the source ? is rewritten as ? if its left and right contexts are ? and ?. For an earlier example from §3, a single NFC normalization rule rewrites the Devanagari string ? = “न” (na, U+0928) + “” (nukta sign, U+093C) into its canonical composition ? = “ऩ” (nnna, U+0929). Kaplan and Kay (1994) proposed an algorithm for compiling such sequences into an FST. This approach was further improved and extended to WFSTs by Mohri and Sproat (1996), whose algorithm we use to compile sequences of NFC and visual normalization rules into transducers denoted ? and ?. Finally, the transducers representing languagespecific customizations of a particular script operation are compiled by composing the generic language-agnostic transducer, such as the Devanagari visual normalizer, with the transducer representing transformations that capture languagespecific use of the script, e.g., Devanagari for Ne"
2021.eacl-demos.3,2020.sltu-1.49,0,0.0124765,"e Grammar helper class, shown in Figure 8, that includes the necessary methods for initializing the WFSTs and performing rewrites (for transducers) and acceptance tests (for acceptors). In addition, many more operations on WFSTs are available through the OpenFst library, if required. 6 Conclusion and Future Work We presented finite-state automata-based utilities for processing the major Brahmic scripts. The finite state transducer formalism provides an efficient and scalable framework for expressing Brahmic script operations and is suitable for many NLP applications, such as those reported in Kumar et al. (2020) and Kakwani et al. (2020), which may benefit from the reduction in “noise” present in unnormalized text. In the future, we will continue to improve the support for existing scripts and extend our work to other Brahmic scripts. Prevalence of Normalization To demonstrate the prevalence of text requiring normalization in #include <string> // Generic wrapper around FST archive with Brahmic transducers. class Grammar { public: // Constructs given the FAR path, its name and the name of WFST. Grammar(const std::string& far_path, const std::string& far_name, const std::string& fst_name); // Initializ"
2021.eacl-demos.3,N09-4005,0,0.0494568,", a Python library for constructing finite-state grammars and for performing operations on WFSTs (Gorman, 2016; Gorman and Sproat, in press). In addition, the library depends on Thrax8 , an older relative of Pynini, that provides a custom grammar manipulation language for WFSTs (Tai et al., 2011; Roark et al., 2012). Although Thrax has been mostly superseded by Pynini, we still rely on some of its utilities for unit testing and its C++ runtime components. At their core, both Pynini and Thrax depend on the OpenFst library9 for the implementation of most WFST algorithms (Allauzen et al., 2007; Riley et al., 2009). The overall dependency diagram is shown on the left-hand side of Figure 5 (the minimal dependency on Thrax is indicated by a dotted arrow). At build time, Bazel pulls in these dependencies remotely from their respective repositories. Offline and Online Usage Once the transducers are compiled, they can be applied offline to the input files using the rewrite-tester tool provided by Thrax, as shown in lines 8–13 of the example in Figure 6, where the visual normalization transducer ? for Kannada that resides in the visual_norm.far archive is applied to words in input file words.txt. We provide l"
2021.eacl-demos.3,P12-3011,1,0.789142,"rmalization, and ? is the well-formed check. The numbers of states and arcs are denoted by ?? and ?? , respectively. Brahmic Offline: Compile Pynini Brahmic Runtime: Python Brahmic Runtime: C++ Pynini Thrax OpenFst OpenFst Thrax OpenFst Compiling the Transducers Figure 6 presents the sequence of steps to compile the transducers, including downloading the repository (line 2), compiling the library and its artifacts (line 5) and running the unit tests (line 7). The artifacts are compiled by Bazel using Pynini and consist of the finite state archive (FAR) files that contain collections of WFSTs (Roark et al., 2012). For each of the four Brahmic script operations we generate two FAR files: one for WFSTs over the byte alphabet, and another over the Unicode code point alphabet.10 Each FAR file contains ten scriptspecific transducers whose names correspond to the upper-case ISO 15924 script codes. Since the transliteration operation is bidirectional, the name of each script-specific transliteration transducer has the prefix FROM_ for the native-to-Latin direction, and TO_ for the inverse. The numbers of states (?? ) and arcs (?? ) of the resulting transliteration (ℐ), NFC (?), visual normalization (?) trans"
2021.eacl-demos.3,N15-3017,0,0.0156251,"h recognition and text-to-speech synthesis (Zhang et al., 2019), among other natural language and speech applications. Related Work The computational processing of Brahmic scripts is not a new topic, with the first applications dating back to the early formal syntactic work by Datta (1984). With an increased focus on the South Asian languages within the NLP community, facilitated by advances in machine learning and the increased availability of relevant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistical machine translation-based Brahmi-Net (Kunchukuttan et al., 2015), are model-based, while others, such as URoman (Hermjakob et al., 2018), IndicNLP (Kunchukuttan, 2020), and Aksharmukha (Rajan, 2020), employ rules. The main focus of these libraries is script conversion and romanization. In this capacity they were successfully employed in diverse downstream multilingual NLP tasks such as neural machine translation (Zhang et al., 2020; Amrhein and Sennrich, 2020), morphological analysis (Hauer et al., 2019; 3 Brahmic Scripts: An Overview The scripts of interest have evolved from the ancient Brāhmī writing system that was recorded 15 Name Bengali Devanagari Gu"
2021.eacl-demos.3,2020.lrec-1.294,1,0.873729,"the Association for Computational Linguistics: System Demonstrations, pages 14–23 April 19 - 23, 2021. ©2021 Association for Computational Linguistics and Telugu. In addition to string normalization and well-formedness processing, the library also includes utilities for the deterministic and reversible romanization of these scripts, i.e., transliteration from each script to and from the Latin script (Wellisch, 1978). While the resulting romanizations are standardized in a way that may or may not correspond to how native speakers tend to romanize the text in informal communication (see, e.g., Roark et al., 2020), such a default romanization can permit easy inspection of an approximate version of the linguistic strings for those who read the Latin script but not the specific Brahmic script being examined. As a whole, the library provides important utilities for language processing applications of South Asian languages using Brahmic scripts. The design is based on the observation that, while there are considerable superficial differences between these scripts, they follow the same encoding model in Unicode, and maintain a very similar character repertoire having evolved from the same source — the Brāhm"
2021.eacl-demos.3,P96-1031,0,0.628079,"ng the context-dependent rewrite rules from the formal approach popularized by Chomsky and Halle (1968). The normalization rules are represented as a sequence {? → ?/? __ ?}, where the source ? is rewritten as ? if its left and right contexts are ? and ?. For an earlier example from §3, a single NFC normalization rule rewrites the Devanagari string ? = “न” (na, U+0928) + “” (nukta sign, U+093C) into its canonical composition ? = “ऩ” (nnna, U+0929). Kaplan and Kay (1994) proposed an algorithm for compiling such sequences into an FST. This approach was further improved and extended to WFSTs by Mohri and Sproat (1996), whose algorithm we use to compile sequences of NFC and visual normalization rules into transducers denoted ? and ?. Finally, the transducers representing languagespecific customizations of a particular script operation are compiled by composing the generic language-agnostic transducer, such as the Devanagari visual normalizer, with the transducer representing transformations that capture languagespecific use of the script, e.g., Devanagari for Nepali. 5 System Details and Demo The core of the Nisaba Brahmic script manipulation library resides under the brahmic directory of the distribution."
2021.eacl-demos.3,2020.sigmorphon-1.22,0,0.0298361,"h Asian languages using Brahmic scripts. The design is based on the observation that, while there are considerable superficial differences between these scripts, they follow the same encoding model in Unicode, and maintain a very similar character repertoire having evolved from the same source — the Brāhmī script (Salomon, 1996; Fedorova, 2012). This observation lends itself to the script-agnostic design (outlined in §4) that, unlike other approaches reviewed in §2, is based on the weighted finite state transducer (WFST) formalism (Mohri, 2004). The details of our system are provided in §5. 2 Murikinati et al., 2020), named entity recognition (Huang et al., 2019) and part-of-speech tagging (Cardenas et al., 2019). Similar to the software mentioned above, our library does provide romanization, but unlike some of the packages, such as URoman, we guarantee reversibility from Latin back to the native script. Similar to others we do not focus on faithful invertible transliteration of named entities which typically requires model-based approaches (Sequiera et al., 2014). Unlike the IndicNLP package, our software does not provide morphological analysis, but instead offers significantly richer script normalizatio"
2021.eacl-demos.3,J19-2004,1,0.829932,"by Sproat (2003) that the fundamental organizing principles of the Brahmic scripts can be algebraically formalized. In particular, all the core components of our library (inverse romanization, normalization and well-formedness) are compactly and efficiently represented as finite state transducers. Such formalization lends itself particularly well to run-time or offline integration with any finite state processing pipeline, such as decoder components of input methods (Ouyang et al., 2017; Hellsten et al., 2017), text normalization for automatic speech recognition and text-to-speech synthesis (Zhang et al., 2019), among other natural language and speech applications. Related Work The computational processing of Brahmic scripts is not a new topic, with the first applications dating back to the early formal syntactic work by Datta (1984). With an increased focus on the South Asian languages within the NLP community, facilitated by advances in machine learning and the increased availability of relevant corpora, multiple script processing solutions have emerged. Some of these toolkits, such as statistical machine translation-based Brahmi-Net (Kunchukuttan et al., 2015), are model-based, while others, such"
2021.eacl-main.3,C65-1001,0,0.231066,"Missing"
2021.eacl-main.3,2020.tacl-1.1,1,0.622989,"recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.1 1 celex wikipedia northeuralex 4 2 Backward 0 2 4 6 8 4 2 0 Position from Start celex wikipedia northeuralex 4 2 8 6 Position from End Figure 1: Forward and Backward Surprisals with LSTM model from Pimentel et al. (2020). The bottom plot has been flipped horizontally such that it visually corresponds to the normal string direction. evidence of increased levels of phonological reduction in word endings (van Son and Pols, 2003b). To analyse this front-loading effect, researchers have investigated the information provided by segments in words. van Son and Pols (2003a,b) showed that, in Dutch, a segment’s position in a word is a very strong predictor of its conditional surprisal, with later segments being more predictable than earlier ones—a result which we show to arise directly from its definition in §3.3.1. Re"
2021.eacl-main.3,P82-1020,0,0.796646,"Missing"
2021.findings-emnlp.85,P12-3006,0,0.0232849,"roat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-availa"
2021.findings-emnlp.85,W15-4319,0,0.0448684,"Missing"
2021.findings-emnlp.85,P10-1079,0,0.0430969,"r text-tospeech synthesis (TTS); Sproat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware"
2021.findings-emnlp.85,P00-1037,0,0.341039,"l text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence of ei (in the case that it is an abbreviation of ei ). At inference time, the system is presented with an abbreviated A sequence of length n and is asked to propose a single hypothˆ esis expansion of lengt"
2021.findings-emnlp.85,P14-2111,0,0.0208137,"omies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthet"
2021.findings-emnlp.85,W09-2010,0,0.034024,"ately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence"
2021.findings-emnlp.85,2020.lrec-1.773,0,0.061171,"Missing"
2021.findings-emnlp.85,N13-1037,0,0.21779,"combines an abbreviation model, applied independently to each word in the sentence, and a language model enforcing ﬂuency and local coherence in the expansion. Text normalization refers to transformations used to prepare text for downstream processing. Originally, this term was reserved for transformations mapping between “written” and “spoken” forms required by technologies like speech recognition and speech synthesis (Sproat et al., 2001), but it is now used for many other transformations, including normalizing informal text genres found on mobile messaging and social media platforms (e.g., Eisenstein, 2013; van der Goot, 2019). 1.1 Contributions Spans of text may require different kinds of The contributions of this study are three-fold. First, normalization depending on their semiotic class (Taylor, 2009) and the requirements of the down- we describe a large data set for English abbrevistream application. For example, cardinal num- ation expansion made freely available to the rebers such as 123 need to be normalized to a spo- search community. Secondly, we validate this data set using exploratory data analysis to identify ken form (one hundred twenty three) for speech common abbreviation strate"
2021.findings-emnlp.85,P19-3032,0,0.0254298,"Missing"
2021.findings-emnlp.85,P11-1038,0,0.0532032,"at (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, var"
2021.findings-emnlp.85,W17-4002,1,0.845388,"ed by An example of the resulting lattice is shown in Figa type of weighted ﬁnite-state transducer known ure 3. λ is an unweighted transducer in which each variously as a joint multigram model, pair n-gram path maps an in-vocabulary word, encoded as a model, or pair language model (pair LM). Such 4 models have been used for grapheme-to-phoneme Computing the conditional probability P(ai |ei ) in eq. 3 conversion (Bisani and Ney, 2008; Novak et al., from the joint probability P(ai , ei ) requires a computationally expensive summation over all possible alignments. However 2016), transliteration (Hellsten et al., 2017; Mer- we ﬁnd it can be effectively approximated using the most probable alignment according to the joint probability model. hav and Ash, 2018), and abbreviation expansion 5 We assume the reader is familiar with ﬁnite-state au(Roark and Sproat, 2014) among other tasks. tomata and algorithms such as composition, concatenation, A pair LM α is a joint model over input/output projection, and shortest path. See Mohri 2009 for a review strings P(ai , ei ) where ai is an abbreviation and ei of ﬁnite-state automata and these algorithms. 999 character sequence, to that same word encoded as a single sym"
2021.findings-emnlp.85,P82-1020,0,0.739654,"Missing"
2021.findings-emnlp.85,N09-1069,0,0.0626793,"occurs as an expansion of ai in proximate back-offs. It is then shrunk using relathe training set. tive entropy pruning (Stolcke, 1998). 1000 Figure 3: An example η lattice corresponding to the example sentence in Figure 1. Ellipses indicate that arcs have been pruned for reasons of space. Non-zero costs are indicated by negative log probability arc weights. Abbreviation model The abbreviation model is a pair n-gram language model over input/output character pairs, encoded as a weighted transducer. The OpenGrm-BaumWelch toolkit and a stepwise interpolated variant of expectation maximization (Liang and Klein, 2009) are used to compute alignments between abbreviations and expansions. OpenGrm-NGram is then used to train a 4-gram pair LM. As with the expansion model, KneserNey smoothing with ε-arc back-offs are used, but no shrinking is performed.6 5.2 Neural implementation Expansion model The expansion model consists of an embedding layer of dimensionality 512 and two LSTM (Hochreiter and Schmidhuber, 1997) layers, each with 512 hidden units. Each sentence is padded with reserved start and end symbols. The model, implemented in TensorFlow (Abadi et al., 2016), is trained in batches of 256 until convergenc"
2021.findings-emnlp.85,P11-2013,0,0.0365892,"sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence of ei (in the cas"
2021.findings-emnlp.85,C18-1053,0,0.0410728,"Missing"
2021.findings-emnlp.85,P14-2060,1,0.90379,"son rsn i i went went to to the the store str was ws to to buy buy milk mlk and and bread brd . . Figure 1: Example of paired data for this task; above: expanded sequence; below: abbreviated sequence. Note that the data has been case-folded. 1.2 Related work Text normalization was ﬁrst studied for text-tospeech synthesis (TTS); Sproat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and B"
2021.findings-emnlp.85,P12-3011,1,0.579742,"h 2.7m additional sentences from Wikipedia as described in subsection 2.1. The development set was used to tune the Markov order of the ﬁnite-state components, and to ablate the subsequence model heuristics. Final evaluations are conducted on the test set. The full vocabulary consists of all 75k wordtypes appearing in the language model training set, simulating a general-domain normalization task. 5.1 Finite-state implementation Expansion model The expansion model is a conventional language model over expansion tokens. • LexBlock: If ai is in-vocabulary, set the probThe OpenGrm-NGram toolkit (Roark et al., 2012) ability of all other output candidates to zero. is used to build a trigram model with Kneser-Ney • Memory: Do not prune an expansion candi- smoothing (Ney et al., 1994) and ε-arcs used to apdate ei if it is occurs as an expansion of ai in proximate back-offs. It is then shrunk using relathe training set. tive entropy pruning (Stolcke, 1998). 1000 Figure 3: An example η lattice corresponding to the example sentence in Figure 1. Ellipses indicate that arcs have been pruned for reasons of space. Non-zero costs are indicated by negative log probability arc weights. Abbreviation model The abbrevia"
2021.findings-emnlp.85,D13-1007,0,0.0234033,"viation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers,"
2021.findings-emnlp.85,J19-2004,1,0.885334,"is related to, but distinct from, spelling correction, in that ad hoc abbreviations are intentional and may involve substantial differences from the original words. Ad hoc abbreviations are productively generated on-the-ﬂy, so they cannot be resolved solely by dictionary lookup. We generate a large, open-source data set of ad hoc abbreviations. This data is used to study abbreviation strategies and to develop two strong baselines for abbreviation expansion. 1 Introduction and Sproat, 2015), possibly augmented with machine learning systems for contextual disambiguation (e.g., Ng et al., 2017; Zhang et al., 2019). In this study we are interested in a different subclass of abbreviations, those which are neither frequent nor conventionalized. We refer to these as ad hoc abbreviations. Such abbreviations are particularly common on those communication channels which demand or favor brevity, such as mobile messaging and social media platforms (Crystal, 2001, 2008; McCulloch, 2019). Unlike conventionalized abbreviations, ad hoc abbreviations are an open class, generated on-the-ﬂy. Unfortunately, there is little annotated data available to study ad hoc abbreviations as they occur in natural text. To remedy t"
2021.findings-emnlp.85,L18-1295,0,0.0216196,"n,8 or to a function word with comparable syntactic effect. (4) they {recog, 3 recognized, 7 recognize} accomps by musicians frm th prev yr . (5) {th, 3 the, 7 this} behavr s strengthnd by an automatc reinfrcng consequenc . 7 Ethical concerns The proposed technology is intended as a component of other speech and language processing systems. We note that abbreviation expansion systems have some small potential for abuse beyond those of the larger systems they might be integrated into. For instance, this technology could be used to (2) anothr criticism is abt th absenc o a stndrd 8 We note that Żelasko (2018) considers the problem of {auditin, 3 auditing, 7 audition} procedr disambiguating abbreviations in Polish, a language with far . richer inﬂectional morphology. 1002 defeat abbreviation as a strategy for circumventing algorithmic state censorship. The data is drawn from English Wikipedia text and was produced by a team of professional annotators based in the United States; its use to disambiguate abbreviations generated by other Englishspeaking communities would likely introduce bias. 8 Conclusions We introduce a large, freely-available data set for ad hoc abbreviation expansion, describing th"
2021.naacl-main.349,P82-1020,0,0.688361,"Missing"
2021.naacl-main.349,2020.tacl-1.1,1,0.891824,"ngth (Blasi et al., 2016). While useful, the estimates emerging from this type of study can be regarded as lower bounds to the total amount of non-arbitrary associations found in the vocabulary. Recent efforts have resulted in datasets with thousands of languages (Wichmann et al., 2020), with which linguists can look for universal statistical patterns (Wichmann et al., 2010; Blasi et al., 2016). These studies, though, only looked at the presence (or not) of individual phones in words, not accounting for their connections. Our methods rely on neural phonotactic models, similar to those used by Pimentel et al. (2020), thus capturing a broader range of potential correspondences. 3 Data An exceptional resource with substantial crosslinguistic representation is provided in the Automated Similarity Judgment Program, better known by its acronym ASJP (Wichmann et al., 2020). ASJP is a collection of basic vocabulary wordlists, i.e. lists of words with referents that are expected to be widely attested across human societies. It involves body parts, some colour terms, lower nu3 merals, general properties (such as big or round), See §3 for a discussion on potential biases in the dataset that likely influence the ac"
2021.naacl-main.349,P19-1171,1,0.588723,"euristic approaches (Bergen, 2004; Wichmann et al., 2010; Johansson and Zlatev, 2013; Haynie et al., 2014; Gutierrez et al., 2016; Blasi et al., 2016; Joo, 2019). Previous studies differ from each other along (at least) three axes: (i) which unit is used to measure wordform similarity (e.g., phonemes, sub-phonemic features or arbitrary sequences); (ii) how they deploy a baseline for statistical comparison (e.g. permute forms with meanings, or propose a generative model that yields wordforms uninformed by their meaning) and (iii) whether they study non-arbitrariness within or across languages. Pimentel et al. (2019) provide the first holistic measure of non-arbitrariness (in a large vocabulary sample of a single language) using tools from information theory, and apply their measure to discover phonesthemes.2 Our work extends their approach to the problem of discovering and estimating the strength of frequent cross-linguistic form–meaning associations (e.g. iconicity and systematicity) in individual concepts. We do this by adapting Pimentel et al.’s (2019) approach, modelling 1 See §2 below for a brief literature review and Dingemanse et al. (2015) for a more comprehensive one. 2 Phonesthemes are sub-morp"
C00-1052,P99-1070,0,0.0272328,"uctions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognized left-corner in order to remove leftrecursion. A top-down parser using a grammar produced by the selective left-corner transform simulates a generalized left-corner parser (Demers, 1977; Nijholt, 1980) which recognizes a user-speci ed subset of the original productions in a left-corner fashion,"
C00-1052,P97-1003,0,0.056028,"nd Carpenter, 1997; Roark and Johnson, 1999). Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which to describe this non-local information. There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy (Johnson, 1998b). Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak (1997) and Collins (1997). 4 Conclusion This paper presented factored selective left-corner grammar transforms. These transforms preserve the primary bene ts of the left-corner grammar transform (i.e., elimination of left-recursion and preservation of annotations on productions) while dramatically ameliorating its principal problems (grammar size and sparse data problems). This should extend the applicability of left-corner techniques to situations involving large grammars. We showed how to identify the minimal set L0 of productions of a grammar that must be recognized left-corner in order for the transformed grammar"
C00-1052,H94-1109,0,0.0235541,"ld usually have a smaller search space relative to the standard left-corner transform, all else being equal. The partial parses produced during a top-down parse consist of a single connected tree fragment, while the partial parses produced produced during a left-corner parse generally consist of several disconnected tree fragments. Since these fragments are only weakly related (via the link&quot; constraint described below), the search for each fragment is relatively independent. This may be responsible for the observation that exhaustive left-corner parsing is less ecient than top-down parsing (Covington, 1994). Informally, because the selective left-corner transform recognizes only a subset of productions in a left-corner fashion, its partial parses contain fewer tree discontiguous fragments and the search may be more ecient. While this paper focuses on reducing grammar size to minimize sparse data problems in PCFG estimation, the modi ed left-corner transforms described here are generally applicable wherever the original left-corner transform is. For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximat"
C00-1052,P98-1101,1,0.891134,"and 9812169. We would like to thank our colleagues in BLLIP (Brown Laboratory for Linguistic Information Processing) and Bob Moore for their helpful comments on this paper.  Brian Roark@Brown.edu Left-corner transforms are particularly useful because they can preserve annotations on productions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognize"
C00-1052,J98-4004,1,0.430007,"and 9812169. We would like to thank our colleagues in BLLIP (Brown Laboratory for Linguistic Information Processing) and Bob Moore for their helpful comments on this paper.  Brian Roark@Brown.edu Left-corner transforms are particularly useful because they can preserve annotations on productions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognize"
C00-1052,1997.iwpt-1.18,0,0.738644,"transform TL mapping parse trees of G into parse trees of LC L (G) (Johnson, 1998a; Roark and Johnson, 1999). In the empirical evaluation below, we estimate a PCFG from the trees obtained by applying TL to the trees in the Penn WSJ tree-bank, and compare it to the PCFG estimated from the original tree-bank trees. A stochastic top-down parser using the PCFG estimated from the trees produced by TL simulates a stochastic generalized left-corner parser, which is a generalization of a standard stochastic left-corner parser that permits productions to be recognized top-down as well as left-corner (Manning and Carpenter, 1997). Thus investigating the properties of PCFG estimated from trees transformed with TL is an easy way of studying stochastic push-down automata performing generalized left-corner parses. 2.3 Pruning useless productions We turn now to the problem of reducing the size of the grammars produced by left-corner transforms. Many of the productions generated by schemata 1 are useless, i.e., they never appear in any terminating derivation. While they can be removed by standard methods for deleting useless productions (Hopcroft and Ullman, 1979), the relationship between the parse trees of G and LC L(G) d"
C00-1052,A00-2033,0,0.209858,"re are generally applicable wherever the original left-corner transform is. For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximations (Johnson, 1998a), often reducing the size of the intermediate automata constructed. The selective left-corner transform can be generalized to head-corner parsing (van Noord, 1997), yielding a selective head-corner parser. (This follows from generalizing the selective left-corner transform to Horn clauses). After this paper was accepted for publication we learnt of Moore (2000), which addresses the issue of grammar size using very similar techniques to those proposed here. The goals of the two papers are slightly di erent: Moore&apos;s approach is designed to reduce the total grammar size (i.e., the sum of the lengths of the productions), while our approach minimizes the number of productions. Moore (2000) does not address left-corner tree-transforms, or questions of sparse data and parsing accuracy that are covered in section 3. 2 The selective left-corner and related transforms This section introduces the selective left-corner transform and two additional factorization"
C00-1052,C92-1032,0,0.0194849,"oductions from G are recognized using LC L (G). When the se::: A ::: ) LC  A  A{A ) ::: A ::: -removal Figure 2: The recognition of a top-down production A ! by LC L (G) involves a left-corner category A{A, which immediately rewrites to . One-step -removal applied to LC L (G) produces a grammar in which each top-down production A ! corresponds to a production A ! in the transformed grammar. lective left-corner transform is followed by a onestep -removal transform (i.e., composition or partial evaluation of schema 1b with respect to schema 1d (Johnson, 1998a; Abney and Johnson, 1991; Resnik, 1992)), each top-down production from G appears unchanged in the nal grammar. Full -removal yields the grammar given by the schemata below. D ! w D{w D!w D ! D {A D! D{B ! D{C D{B ! where D )+L w where A ! 2 P , L where D )?L A; A ! 2 P , L where C ! B 2 L where D )?L C; C ! B 2 L Moore (2000) introduces a version of the leftcorner transform called LCLR , which applies only to productions with left-recursive parent and left child categories. In the context of the other transforms that Moore introduces, it seems to have the same e ect in his system as the selective left-corner transform does here."
C00-1052,P99-1054,1,0.909499,"d. Unfortunately the grammars produced by the standard left-corner transform are usually much larger than the original. The selective left-corner transform described in this paper produces a transformed grammar which simulates left-corner recognition of a user-speci ed set of the original productions, and top-down recognition of the others. Combined with two factorizations, it produces non-left-recursive grammars that are not much larger than the original. 1 Introduction Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice (Roark and Johnson, 1999). However, with a left-recursive grammar such parsers typically fail to terminate. The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972). However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems. For example, we show below that a probabilistic context-free grammar (PCFG) estimated from left-corner transformed Penn"
C00-1052,J97-3004,0,0.0718526,"Missing"
C00-1052,C98-1098,1,\N,Missing
C08-1094,A00-2018,0,\N,Missing
C08-1094,J93-2004,0,\N,Missing
C08-1094,W02-1001,0,\N,Missing
C08-1094,P05-1022,0,\N,Missing
C08-1094,N03-1028,0,\N,Missing
C08-1094,P06-2038,0,\N,Missing
C08-1094,P07-1120,1,\N,Missing
C98-2177,J93-1003,0,0.0101698,"common nouns may not occur ill the corpus more than a handful of times in such a context. The two figures of merit that we employ, one to select and one to produce a final rank, use the following two counts for each noun: 1. a n o u n ' s c o - o c c t l r r e n c e 8 w i t h seed words 2. a. noun's co-occurrences with any word To select new seed words, we take the ratio of count 1. to count 2 for tile noun in question. This is similar to tile figure of merit used in R&S, and also tends to promote low fl:equency norms, f o r the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning tbr details). This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random, l.br instance, suppose that two words occur forty times ea,ch, and they co-occur twenty times in a millionword corpus. This would be more surprising for two completely random distributions than if they had each occurred twice and had always co-occurred. A simple probability does not capture this fact. The rationale for using two different statistics for this task is t h"
C98-2177,W97-0313,0,0.51128,"ple, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal cate1110 gory membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and output a ranked list Our algorithm uses roughly this same generic st"
C98-2177,P95-1007,0,0.00989848,"or if the resulting compound has already been output, tile entry is skipped. Each noun is evaluated as follows: First, the head of that noun is determined. To get a sense of what is meant here, consider the following compound: nuclear-powered aircraft carrier. In evaluating tile word nuclearpowered, it is unclear if this word is attached to aircraft or to carrier. While we know that lhe head of the entire compound is carrier, in order to properly evaluate the word in question, we must determine which of the words following it is its head. This is done, in the spirit of the Dependency Model of Lauer (1995), by selecting the noun to its right in tile compound with the highest probability of occuring with the word in question when occurring ill a nouu compound. (In the case that two nouns have the same probability, the rightmost noun is chosen.) Once the head of the word is determined, the ratio of count 1 (with the head noun chosen) to count 2 is compared to an empirically set cutoff. If it falls below that cutoff, it is omitted. If it does not fall below the cutoff, then it is kept (provided its head noun is not later omitted). 6 O u t l i n e of t h e a l g o r i t h m The input to tlle algori"
C98-2177,P95-1026,0,0.00635419,"inadequate. For example, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal cate1110 gory membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and output a ranked list Our algorithm us"
C98-2177,J93-2004,0,0.0334145,"Missing"
D09-1034,J91-3004,0,\N,Missing
D09-1034,J93-2004,0,\N,Missing
D09-1034,J95-2002,0,\N,Missing
D09-1034,C00-1052,1,\N,Missing
D09-1034,N01-1021,0,\N,Missing
D09-1034,N09-1006,0,\N,Missing
D09-1034,W07-1001,1,\N,Missing
D09-1034,P04-1015,1,\N,Missing
D09-1034,P98-1035,0,\N,Missing
D09-1034,C98-1035,0,\N,Missing
D09-1034,J01-2004,1,\N,Missing
D09-1034,N03-1014,0,\N,Missing
D09-1034,P05-1025,0,\N,Missing
D09-1034,P08-2002,0,\N,Missing
D11-1085,P08-1024,0,0.0401701,"ur approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which"
D11-1085,N09-1025,0,0.0625912,"T systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a sco"
D11-1085,J07-2003,0,0.581901,"ness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is ins"
D11-1085,W02-1001,0,0.20038,"N 1 2 ing − M j=1 log pφ (xj |yj ) + 2σ 2 kφk2 on some bilingual data, with the regularization coefficient σ 2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij . All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi . 3.3 The Forward Translation System δθ and The Loss Function L(δθ (xi ), yi ) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, 8 In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model pφ (x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. 2006) can be formalized in this framework by choosing different functions for δθ and L(δθ (xi ), yi ). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δθ and L(δθ (xi ), yi ) we considered in our investigation. 3.3.1 Deterministic Decoding A si"
D11-1085,P08-1115,0,0.0269094,"for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often n"
D11-1085,2005.iwslt-1.1,0,0.0473602,"|y) and pθ (y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train θ, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 5.1.1 Baseline Systems IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram languag"
D11-1085,P06-1121,0,0.0422989,"γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approac"
D11-1085,W05-1506,0,0.0238305,"isner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating pφ (x |yi ) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xij . We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations sh"
D11-1085,N07-1018,0,0.0210271,"imations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation ("
D11-1085,N03-1017,0,0.00578651,"ore(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly th"
D11-1085,D09-1005,1,0.926783,"criminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be proba"
D11-1085,W09-0424,1,0.90745,"Missing"
D11-1085,P09-1067,1,0.797285,"wed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracti"
D11-1085,C10-2075,1,0.85854,"ed by the reverse Hiero system; each translation phrase (or rule) corresponding to a hyperedge. To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of xi . We can do that by considering only forward translations that respect the hypergraph structure of Xi . The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y"
D11-1085,P06-1096,0,0.256824,"Missing"
D11-1085,D08-1076,0,0.0341042,"pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. On"
D11-1085,N06-1045,0,0.0232079,"he forward translations. Still, even with the worse imputation (in the case of “NLM”), our forward translations improve as we add more monolingual data. 5.5.2 Imputation with Different k-best Sizes In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence. This is the 1-best approximation of section 3.4. Table 5 shows (in the fully unsupervised case) that the performance does not change much as k increases.16 This may be because that the 5-best sentences are likely to be quite similar to one another (May and Knight, 2006). Imputing a longer k-best list, a sample, or a lattice for xi (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust. 6 Conclusions In this paper, we present an unsupervised discriminative training method that works with missing inputs. The key idea in our method is to use a reverse model to impute the missing input from the observed output. The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e., 16 In the present experiments, however, we simply weighted"
D11-1085,P03-1021,0,0.401482,"guage model pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003)"
D11-1085,2001.mtsummit-papers.68,0,0.0244807,"English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracting a high-scoring translation of x. The goal of discriminative training is to minimize the expected loss of δθ (·), under a given taskspecific loss function L(y 0 , y) that measures how 2 Note that the extra monolingual data is used only for tuning the model weights, but not for inducing new phrases or rules. 921 bad it would be to output y 0 when the correct output is y. For an MT system that is judged by the BLEU metric (Papineni et al., 2001), for instance, L(y 0 , y) may be the negated BLEU score of y 0 w.r.t. y. To be precise, the goal3 is to find θ with low Bayes risk, θ∗ = argmin θ X p(x, y) L(δθ (x), y) (1) x,y where p(x, y) is the joint distribution of the inputoutput pairs.4 The true p(x, y) is, of course, not known and, in practice, one typically minimizes empirical risk by replacing p(x, y) above with the empirical distribution p˜(x, y) given by a supervised training set {(xi , yi ), i = 1, . . . , N }. Therefore, θ∗ = argmin θ = argmin θ X p˜(x, y) L(δθ (x), y) x,y N 1 X L(δθ (xi ), yi ). N (2) i=1 The search for θ∗ typi"
D11-1085,P06-2101,1,0.955514,"ing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ"
D11-1085,D08-1065,0,0.0319576,"a theoretical contribution, and we do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper. ambiguous weighted finite-state automaton Xi , (b) the forward translation system δθ is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al. (2008) that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al. (2009b) show how to approximate a hypergraph representation of pφ (x |yi ) by an unambiguous WFSA. One could then apply the construction to this WFSA12 , obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi ∈ Xi , it must parse i"
D11-1085,D07-1080,0,0.0393648,"development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be t"
D11-1085,P02-1040,0,\N,Missing
D13-1165,P05-1074,0,0.0352409,"rom round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. Pronunciation dictionaries provide natural parallel corpora, with strings of characters paired to strings of phones. Thus, standard lexicons have been used in recent years with machine translation systems such as Moses (Koehn et al., 2007), to train g2p systems (Laurent et al., 2009; Gerosa and Federico, 2009). Further, other algorithms using such dictionaries also use translation phrase tables, but not for translation tasks. For example, data-driven paraphrasing methods (Bannard and Callison-Burch, 2005) use translation phrase-tables as a “pivot” to learn sets of phrases which translated to the same target phrase. In a similar manner, with a pronunciation dictionary instead of a phrsetable, pivoting can be used to learn alternative pronunciations (Karanasou and Lamel, 2010), i.e., direct phoneme-to-phoneme (p2p) “translation” systems that yield alternative pronunciations. Alternatively, round-trip translation could be used, e.g., to map from letter strings to phone strings in one step, then from the resulting phone strings to letter strings in a second step, as the means to find alternative s"
D13-1165,C10-2005,0,0.0101902,"odest gains for finding alternative spellings. Further, WFST methods perform as well as or better than Moses trained models. Finally, combining the methods yields further gains, indicating that the models are learning complementary sets of patterns. The primary contribution of this work is to introduce a competitive method of building and using pair language model WFSTs for generating alternative spellings and pronunciations which reflect real-world variability. This could improve results for downstream processes, e.g., epidemiological studies (Chew and Eysenbach, 2010) or sentiment analysis (Barbosa and Feng, 2010) derived from social media text. Further, we present a controlled comparison between the two tasks, and demonstrate that they differ in terms of task difficulty 2 Related work Text normalization has been a major focus in textto-speech (TTS) research for many years. Notably, Sproat et al. (2001) deemed it a problem in itself, rather than ad hoc preparatory work, and defined many of the issues involved, as well as offering a variety of initial solutions. Similar approaches apply to automatic spelling correction, where Toutanova and Moore (2002) extended the noisy channel spelling correction meth"
D13-1165,P00-1037,0,0.272165,"ed from social media text. Further, we present a controlled comparison between the two tasks, and demonstrate that they differ in terms of task difficulty 2 Related work Text normalization has been a major focus in textto-speech (TTS) research for many years. Notably, Sproat et al. (2001) deemed it a problem in itself, rather than ad hoc preparatory work, and defined many of the issues involved, as well as offering a variety of initial solutions. Similar approaches apply to automatic spelling correction, where Toutanova and Moore (2002) extended the noisy channel spelling correction method of Brill and Moore (2000), by modeling pronunciation alternations to infer from misspellings to correct spellings. Similarly, Li and Liu (2012) extended the character-based translation approach to text normalization of Pennell and Liu (2011), by adding an additional round-trip translation to-and-from pronunciations. Karanasou and Lamel (2010) used Moses to generate alternative pronunciations from an English dictionary, using both direct and round-trip methods. They validated their systems on a set of words with multiple pronunciations, measuring the degree to which alternative pronunciations are generated from one of"
D13-1165,P07-2045,0,0.00782806,"rnative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics – finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. Pronunciation dictionaries provide natural parallel corpora, with strings of characters paired to strings of phones. Thus, standard lexicons have been used in recent years with machine translation systems such as Moses (Koehn et al., 2007), to train g2p systems (Laurent et al., 2009; Gerosa and Federico, 2009). Further, other algorithms using such dictionaries also use translation phrase tables, but not for translation tasks. For example, data-driven paraphrasing methods (Bannard and Callison-Burch, 2005) use translation phrase-tables as a “pivot” to learn sets of phrases which translated to the same target phrase. In a similar manner, with a pronunciation dictionary instead of a phrsetable, pivoting can be used to learn alternative pronunciations (Karanasou and Lamel, 2010), i.e., direct phoneme-to-phoneme (p2p) “translation”"
D13-1165,I11-1109,0,0.0131325,"extto-speech (TTS) research for many years. Notably, Sproat et al. (2001) deemed it a problem in itself, rather than ad hoc preparatory work, and defined many of the issues involved, as well as offering a variety of initial solutions. Similar approaches apply to automatic spelling correction, where Toutanova and Moore (2002) extended the noisy channel spelling correction method of Brill and Moore (2000), by modeling pronunciation alternations to infer from misspellings to correct spellings. Similarly, Li and Liu (2012) extended the character-based translation approach to text normalization of Pennell and Liu (2011), by adding an additional round-trip translation to-and-from pronunciations. Karanasou and Lamel (2010) used Moses to generate alternative pronunciations from an English dictionary, using both direct and round-trip methods. They validated their systems on a set of words with multiple pronunciations, measuring the degree to which alternative pronunciations are generated from one of the given pronunciations. Our task and method of evaluation is similar to theirs, though we also look at alternative spellings. 3 Methods To generate alternative spellings and pronunciations, we built phrase-based tr"
D13-1165,P12-3011,1,0.824348,"ted graphemes so that they collectively map to a single phoneme. For example, given the alignment ‘o:/a/ u:// g:// h:// t:/t/’, (‘ought’, /at/), we make a new rule: ough:/a/, and give it a cost based on its relative frequency. Grapheme strings that appear sufficiently often with a given phoneme will thus accumulate sufficient probability mass to compete. Each alignment produced as described above is a string in a training corpus for creating a pair language model. As such, each alignment pair (e.g. a:/@/) is a token. 3.3.2 From Corpora to WFSTs We use the open source OpenGrm NGram library (Roark et al., 2012) to build 5-gram language models from the strings of input:output pairs. These langauge models are encoded as weighted finite-state acceptors in the OpenFst format (Allauzen et al., 2007). We shrink the models with the ngramshrink command, using the relative entropy method (Stolcke, 1998), with the “theta” threshold set at 1.0e−6. These finite state acceptors are then converted into transducers by modifying the arcs: split the labels of each arc, x:y, making x the input label for that arc, and y the output label. Thus traversing such an arc will consume an x a return a y. Such pair language mo"
D13-1165,P02-1019,0,0.219789,"tual sequence is an important recognition task, often required for downstream processing such as spoken language understanding or knowledge extraction. Informal text genres, such as those found in social media, share some characteristics with speech; in fact such text is often informed by pronunciation variation. For example, consider the following tweet: He aint gotta question my loyalty, cuz he knw wen sh!t get real. Ill be right here! where several tokens (e.g. “cuz”, “wen”) represent spelling alternations related to pronunciation. Work in text normalization and spelling correction – e.g., Toutanova and Moore (2002); Li and Liu (2012) – has included pronunciation information to improve recognition of the intended word, via grapheme to In this study, we explore dictionary-derived models to find either alternative pronunciations or alternative spellings, using either direct (p2p or g2g) or round-trip algorithms (p2g2p or g2p2g). We compare methods based on weighted finite-state transducers (WFST) with phrase-based models trained with Moses. Our main interest is to evaluate Karanasou and Lamel (2010) methods – shown to be useful for deriving alternative pronunciations – for deriving alternative spellings, a"
D14-1106,J92-4003,0,0.319442,"Missing"
D14-1106,W10-2107,0,0.0521661,"Missing"
D14-1106,de-marneffe-etal-2006-generating,0,0.0188531,"Missing"
D14-1106,W11-1411,0,0.0481805,"Missing"
D14-1106,Q14-1011,0,0.0165271,"rmance, although they are vague about the nature of the parse improvement (see Caines and Buttery, 2010, p. 6). Hassanali and Liu (2011) conducted the first investigation into grammaticality detection and classification in both speech of children, and speech of children with language impairments. They identified 11 types of errors, and compared three types 2.3 Redshift Parser We perform our experiments with the redshift parser2 , which is an arc-eager transition-based dependency parser. We selected redshift because of its ability to perform disfluency detection and dependency parsing jointly. Honnibal and Johnson (2014) demonstrate that this system achieves stateof-the-art performance on disfluency detection, even compared to single purpose systems such as the one proposed by Qian and Liu (2013). Rasooli and Tetreault (2014) have developed a system that performs disfluency detection and dependency parsing jointly, and with comparable performance to redshift, but it is not publicly available as of yet. Redshift uses an averaged perceptron learner, and implements several feature sets. The first feature set, which we will refer to as Z HANG is the one proposed by Zhang and Nivre (2011). It includes 73 templates"
D14-1106,W13-1701,1,0.890735,"Missing"
D14-1106,W14-3209,1,0.453179,"s. To address this issue, we will compute precision, recall, and F1 score from the counts of each error code in each utterance. We will label this form of evaluation as ERROR level. Figure 1 illustrates both UTTERANCE and ERROR level evaluation. Note that the utterance level error code [EU] is only allowed to appear once per utterance. As a result, we will ignore any predicted [EU] codes beyond the first. Third, the quality of the SALT annotations themselves is unknown, and therefore evaluation in which we treat the manually annotated data as a gold standard may not yield informative metrics. Morley et al. (2014) found that there are likely inconsistencies in maze annotations both within and across corpora. In light of that finding, it is possible that error code annotations are somewhat inconsistent as well. Furthermore, our approach has a critical difference from manual annotation: 4 4.1 Detecting Errors in ENNI Baselines We evaluate two existing systems to see how effectively they can identify utterances with SALT error codes: 1) Microsoft Word 2010’s grammar check, and 2) the simplified version of Hassanali and Liu’s grammaticality detector (2011) proposed by Morley et al. (2013) (mentioned in Sec"
D14-1106,N13-1102,0,0.0619587,"ity detection and classification in both speech of children, and speech of children with language impairments. They identified 11 types of errors, and compared three types 2.3 Redshift Parser We perform our experiments with the redshift parser2 , which is an arc-eager transition-based dependency parser. We selected redshift because of its ability to perform disfluency detection and dependency parsing jointly. Honnibal and Johnson (2014) demonstrate that this system achieves stateof-the-art performance on disfluency detection, even compared to single purpose systems such as the one proposed by Qian and Liu (2013). Rasooli and Tetreault (2014) have developed a system that performs disfluency detection and dependency parsing jointly, and with comparable performance to redshift, but it is not publicly available as of yet. Redshift uses an averaged perceptron learner, and implements several feature sets. The first feature set, which we will refer to as Z HANG is the one proposed by Zhang and Nivre (2011). It includes 73 templates that capture various aspects of: the word at the top of the stack, along with its 2 Redshift is available at https://github.com/ syllog1sm/redshift. We use the version in the exp"
D14-1106,E14-4010,0,0.0131232,"ssification in both speech of children, and speech of children with language impairments. They identified 11 types of errors, and compared three types 2.3 Redshift Parser We perform our experiments with the redshift parser2 , which is an arc-eager transition-based dependency parser. We selected redshift because of its ability to perform disfluency detection and dependency parsing jointly. Honnibal and Johnson (2014) demonstrate that this system achieves stateof-the-art performance on disfluency detection, even compared to single purpose systems such as the one proposed by Qian and Liu (2013). Rasooli and Tetreault (2014) have developed a system that performs disfluency detection and dependency parsing jointly, and with comparable performance to redshift, but it is not publicly available as of yet. Redshift uses an averaged perceptron learner, and implements several feature sets. The first feature set, which we will refer to as Z HANG is the one proposed by Zhang and Nivre (2011). It includes 73 templates that capture various aspects of: the word at the top of the stack, along with its 2 Redshift is available at https://github.com/ syllog1sm/redshift. We use the version in the experiment branch from May 15, 20"
D14-1106,P11-2033,0,0.0168566,"ncy parsing jointly. Honnibal and Johnson (2014) demonstrate that this system achieves stateof-the-art performance on disfluency detection, even compared to single purpose systems such as the one proposed by Qian and Liu (2013). Rasooli and Tetreault (2014) have developed a system that performs disfluency detection and dependency parsing jointly, and with comparable performance to redshift, but it is not publicly available as of yet. Redshift uses an averaged perceptron learner, and implements several feature sets. The first feature set, which we will refer to as Z HANG is the one proposed by Zhang and Nivre (2011). It includes 73 templates that capture various aspects of: the word at the top of the stack, along with its 2 Redshift is available at https://github.com/ syllog1sm/redshift. We use the version in the experiment branch from May 15, 2014. 982 leftmost and rightmost children, parent and grandparent; and the word on the buffer, along with its leftmost children; and the second and third words on the buffer. Redshift also includes features extracted from the Brown clustering algorithm (Brown et al., 1992). Finally, redshift includes features that are designed to help identify disfluencies; these c"
D14-1106,E87-1007,0,\N,Missing
H05-1099,J95-4004,0,0.532176,"94.28 88.15 94.31 94.42 93.88 95.15 95.32 88.92 95.11 95.19 Table 1: F-measure shallow bracketing accuracy under three different evaluation scenarios: (a) baseline, used in Li and Roth (2001), with original chunklink script converting treebank trees and context-free parser output; (b) same as (a), except that empty subject NPs are inserted into every unary S→VP production; and (c) same as (b), except that punctuation is ignored for setting constituent span. Results for Li and Roth are reported from their paper. The Collins parser is provided with part-ofspeech tags output by the Brill tagger (Brill, 1995). damentally, constituents are groupings of words. Interestingly, this convention was not followed in the CoNLL-2000 task (Sang and Buchholz, 2000), which as we will see has a variable effect on contextfree parsers, presumably depending on the degree to which punctuation is moved in training. 2.1 Evaluation Analysis To determine the effects of the conversion routine and different evaluation conventions, we compare the performance of several different models on one of the tasks presented in Li and Roth (2001). For this task, which we label the Li & Roth task, sections 2-21 of the Penn WSJ Treeb"
H05-1099,P05-1022,0,0.552675,"nnot hurt performance, only improve it, even the smallest of these differences are statistically significant. Note that after inserting empty nodes and ignoring punctuation, the accuracy advantage of Li and Roth over Collins is reduced to a dead heat. Of the two parsers we evaluated, the Charniak (2000) parser gave the best performance, which is consistent with its higher reported performance on the context-free parsing task versus other context-free parsers. Collins (2000) reported a reranking model that improved his parser output to roughly the same level of accuracy as Charniak (2000), and Charniak and Johnson (2005) report an improvement using reranking over Charniak (2000). For the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data to be applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with a shallow parser. Both the Bikel (2004) impleSystem SPRep averaged perceptron Kudo and Matsumoto (2001) Sha and Pereira (2003) CRF Voted perceptron Zhang et al. (2002) Li and Roth (2001) NP-Chunking 94.21 94.22 94.38 94.09 - CoNLL-2000 93.54 93.91 94.17 93.02 Li & Roth task 95.12 94.64 Table 2: Baseline resul"
H05-1099,A00-2018,0,0.103936,"n question answering from the web), contextfree parsing may be too expensive, whereas finitestate parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text. For this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years. In addition to the clear efficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known CoNLL2000 task (Sang and Buchholz, 2000), outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank (Marcus et al., 1993). They argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining a hierarchical analysis over th"
H05-1099,P97-1003,0,0.810036,"ks (e.g. opendomain question answering from the web), contextfree parsing may be too expensive, whereas finitestate parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text. For this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years. In addition to the clear efficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known CoNLL2000 task (Sang and Buchholz, 2000), outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank (Marcus et al., 1993). They argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining a hierarchic"
H05-1099,W02-1001,0,0.0792592,"11 phrase types included in the CoNLL-2000 task (Sang and Buchholz, 2000). Our shallow parser uses exactly the feature set delineated by Sha and Pereira, and performs the decoding process using a Viterbi search with a second-order Markov assumption as they described. These features include unigram and bigram words up to two positions to either side of the current word; unigram, bigram, and trigram part-of-speech (POS) tags up to two positions to either side of the current word; and unigram, bigram, and trigram shallow constituent tags. We use the averaged perceptron algorithm, as presented in Collins (2002), to train the parser. See (Sha and Pereira, 2003) for more details on this approach. To demonstrate the competitiveness of our baseline shallow parser, which we label the SPRep averaged perceptron, Table 2 shows results on three different shallow parsing tasks. The NP-Chunking 4 The parser is available for research purposes at ftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in nbest mode. The one-best performance of the parser is the same as what was presented in Charniak (2000). 790 task, originally introduced in Ramshaw and Marcus (1995) and also described in (Collins, 2002; Sha and Per"
H05-1099,N01-1025,0,0.354106,"on the context-free parsing task versus other context-free parsers. Collins (2000) reported a reranking model that improved his parser output to roughly the same level of accuracy as Charniak (2000), and Charniak and Johnson (2005) report an improvement using reranking over Charniak (2000). For the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data to be applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with a shallow parser. Both the Bikel (2004) impleSystem SPRep averaged perceptron Kudo and Matsumoto (2001) Sha and Pereira (2003) CRF Voted perceptron Zhang et al. (2002) Li and Roth (2001) NP-Chunking 94.21 94.22 94.38 94.09 - CoNLL-2000 93.54 93.91 94.17 93.02 Li & Roth task 95.12 94.64 Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000 Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 but with more training data and a different test section. The results reported in this table include the best published results on each of these tasks. mentation of the Collins"
H05-1099,W01-0706,0,0.101143,"proximation to – more expensive context-free parsing (Abney, 1991; Ramshaw and Marcus, 1995; Abney, 1996). For many very-largescale natural language processing tasks (e.g. opendomain question answering from the web), contextfree parsing may be too expensive, whereas finitestate parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text. For this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years. In addition to the clear efficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known CoNLL2000 task (Sang and Buchholz, 2000), outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank (Marcus et al., 1993"
H05-1099,J93-2004,0,0.0338559,"Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known CoNLL2000 task (Sang and Buchholz, 2000), outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank (Marcus et al., 1993). They argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining a hierarchical analysis over the entire string. They further showed that their shallow parser trained on the Penn WSJ Treebank did a far better job of annotating out-of-domain sentences (e.g. conversational speech) than the Collins parser. This paper re-examines the comparison of shallow parsers with context-free parsers, beginning with a critical examination of how their outputs are compared. We demonstrate that changes to the conversion routine, wh"
H05-1099,W95-0107,0,0.307887,"asks than has been previously reported. Finally, we establish that combining the output of context-free and finitestate parsers gives much higher results than the previous-best published results, on several common tasks. While the efficiency benefit of finite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought. 1 Introduction Finite-state parsing (also called chunking or shallow parsing) has typically been motivated as a fast firstpass for – or approximation to – more expensive context-free parsing (Abney, 1991; Ramshaw and Marcus, 1995; Abney, 1996). For many very-largescale natural language processing tasks (e.g. opendomain question answering from the web), contextfree parsing may be too expensive, whereas finitestate parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text. For this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years. In addition to the clear efficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus co"
H05-1099,W00-0726,0,0.238907,"ter referred to as shallow parsing) has received increasing attention in recent years. In addition to the clear efficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known CoNLL2000 task (Sang and Buchholz, 2000), outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal (WSJ) Treebank (Marcus et al., 1993). They argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining a hierarchical analysis over the entire string. They further showed that their shallow parser trained on the Penn WSJ Treebank did a far better job of annotating out-of-domain sentences (e.g. conversational speech) than the Collins parser. This paper re-examines the comparison of shallow parsers with contex"
H05-1099,N03-1028,0,0.823651,"ng task versus other context-free parsers. Collins (2000) reported a reranking model that improved his parser output to roughly the same level of accuracy as Charniak (2000), and Charniak and Johnson (2005) report an improvement using reranking over Charniak (2000). For the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data to be applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with a shallow parser. Both the Bikel (2004) impleSystem SPRep averaged perceptron Kudo and Matsumoto (2001) Sha and Pereira (2003) CRF Voted perceptron Zhang et al. (2002) Li and Roth (2001) NP-Chunking 94.21 94.22 94.38 94.09 - CoNLL-2000 93.54 93.91 94.17 93.02 Li & Roth task 95.12 94.64 Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000 Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 but with more training data and a different test section. The results reported in this table include the best published results on each of these tasks. mentation of the Collins parser and the n-best v"
H05-1099,J04-4004,0,\N,Missing
H05-1099,J05-1003,0,\N,Missing
J01-2004,H91-1060,0,0.00914646,"rns TEST. Precision is the number of common constituents in GOLD and TEST divided by the number of constituents in TEST. Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD. Following standard practice, we will be reporting scores only for non-part-of-speech constituents, which are called labeled recall (LR) and labeled precision (LP). Sometimes in figures we will plot their average, and also what can be termed the parse error, which is one minus their average. LR and LP are part of the standard set of PARSEVAL measures of parser quality (Black et al. 1991). From this set of measures, we will also include the crossing bracket scores: average crossing brackets (CB), percentage of sentences with no crossing brackets (0 CB), and the percentage of sentences with two crossing brackets or fewer (&lt; 2 CB). In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word. This is an incremental parser with a pruning strategy and no backtracking. In suc"
J01-2004,J93-1002,0,0.0111498,"further improvements. Next we will outline our conditional probability model over rules in the PCFG, followed by a presentation of the top-down parsing algorithm. We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above. 4.1 Conditional Probability Model A simple PCFG conditions rule probabilities on the left-hand side of the rule. It has been shown repeatedly--e.g., Briscoe and Carroll (1993), Charniak (1997), Collins (1997), Inui et al. (1997), Johnson (1998)--that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterrninal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies. One way of thinking about conditioning the probabilities of productions on contextual information (e.g., the label of the parent of a constituent or the lexical heads of c"
J01-2004,J98-2004,0,0.0077819,"oned on these two headwords, for this derivation. Since the specific results of the SLM will be compared in detail with our model when the empirical results are presented, at this point we will simply state that they have achieved a reduction in both perplexity and word error rate over a standard trigram using this model. The rest of this paper will present our parsing model, its application to language modeling for speech recognition, and empirical results. 256 Roark Top-Down Parsing 4. Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model. The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically.1° Such methods are nearly always used in conjunction with some form o"
J01-2004,A00-2018,0,0.232218,"ities that can lead to such a situation. While our o b s e r v e d times are not linear, a n d are clearly slower t h a n his times (even w i t h a faster machine), they are quite respectably fast. The differences b e t w e e n a k-best a n d a b e a m - s e a r c h parser (not to m e n t i o n the use of d y n a m i c p r o g r a m m i n g ) m a k e a r u n n i n g time difference unsur17 Our score of 85.8 average labeled precision and recall for sentences less than or equal to 100 on Section 23 compares to: 86.7 in Charniak (1997), 86.9 in Ratnaparkhi (1997), 88.2 in Collins (1999), 89.6 in Charniak (2000), and 89.75 in Collins (2000). 266 Roark Top-Down Parsing prising. What is perhaps surprising is that the difference is not greater. Furthermore, this is quite a large beam (see discussion below), so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained. 5.3 P e r p l e x i t y Results The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string. By definition, a PCFG&apos;s estimate of a string&apos;s probability is the"
J01-2004,W98-1115,0,0.0462016,"Missing"
J01-2004,P98-1035,0,0.127264,"grammar. A left-toright parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures. Only at the point when their derivations become rooted (at the end of the string) can generative string probabilities be calculated from the grammar. These parsers can calculate word probabilities based upon the parser state--as in Chelba and Jelinek (1998a)--but such a distribution is not generative from the probabilistic grammar. A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order. For example, suppose that there are two possible verbs that could be the head of a sentence. For a head-first parser, some derivations will have the fir"
J01-2004,J98-2005,0,0.0726063,"Missing"
J01-2004,P97-1003,0,0.393441,"ation to language modeling for speech recognition, and empirical results. 256 Roark Top-Down Parsing 4. Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model. The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically.1° Such methods are nearly always used in conjunction with some form of dynamic programming (henceforth DP). That is, search efficiency for these parsers is improved by both statistical search heuristics and DP. Here we will present a parser that uses simple search heuristics of this sort without DE Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabili"
J01-2004,P81-1022,0,0.496197,"ond verb. In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule. Of course, the joint probability can be used as a language model, but it cannot be interpolated on a word-by-word basis with, say, a trigram model, which we will demonstrate is a useful thing to do. Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis II 1970). A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search. In contrast, an Earley or left-corner parse"
J01-2004,P99-1059,0,0.0637464,"ar, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterrninal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies. One way of thinking about conditioning the probabilities of productions on contextual information (e.g., the label of the parent of a constituent or the lexical heads of constituents), is as annotating the extra conditioning information onto the labels in the context-free rules. Examples of this are bilexical grammars--such as Eisner and Satta (1999), Charniak (1997), Collins (1997)--where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item. Thus the rule S -* NP VP becomes, for instance, S[barks] ---*NP[dog] VP[barks]. One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative"
J01-2004,W97-0302,0,0.0471741,"results of the SLM will be compared in detail with our model when the empirical results are presented, at this point we will simply state that they have achieved a reduction in both perplexity and word error rate over a standard trigram using this model. The rest of this paper will present our parsing model, its application to language modeling for speech recognition, and empirical results. 256 Roark Top-Down Parsing 4. Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model. The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically.1° Such methods are nearly always used in conjunction with some form of dynamic programming (henceforth DP). That is, search"
J01-2004,W99-0623,0,0.0454934,"and left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item. Thus the rule S -* NP VP becomes, for instance, S[barks] ---*NP[dog] VP[barks]. One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation). This procedure yields conditional probability distributions of 10 Johnson et al. (1999), Henderson and Brill (1999), and Collins (2000) demonstratemethods for choosing the best completeparse tree from among a set of completeparse trees, and the latter two show accuracy improvementsover some of the parsers cited above, from which they generated their candidate sets. Here we will be comparing our work with parsing algorithms,i.e., algorithmsthat build parses for strings of words. 257 Computational Linguistics Volume 27, Number 2 constituents on the right-hand side with their lexical heads, given the left-hand side constituent and its lexical head. The same procedure works if we annotate parent information on"
J01-2004,1997.iwpt-1.16,0,0.0232347,"robability model over rules in the PCFG, followed by a presentation of the top-down parsing algorithm. We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above. 4.1 Conditional Probability Model A simple PCFG conditions rule probabilities on the left-hand side of the rule. It has been shown repeatedly--e.g., Briscoe and Carroll (1993), Charniak (1997), Collins (1997), Inui et al. (1997), Johnson (1998)--that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterrninal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies. One way of thinking about conditioning the probabilities of productions on contextual information (e.g., the label of the parent of a constituent or the lexical heads of constituents), is as annotating the extra conditioning"
J01-2004,J91-3004,0,0.0264034,"he probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar. 3.1 Grammar Models As mentioned in Section 2.1, a PCFG defines a probability distribution over strings of words. One approach to syntactic language modeling is to use this distribution directly as a language model. There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG. The algorithms both utilize a left-corner matrix, which can be calculated in closed form through matrix inversion. They are limited, therefore, to grammars where the nonterminal set is small enough to permit inversion. String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: P(Wj+l I wg) = P(wg+l) P(wg) (7) Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used"
J01-2004,J98-4004,0,0.724181,"e the specific results of the SLM will be compared in detail with our model when the empirical results are presented, at this point we will simply state that they have achieved a reduction in both perplexity and word error rate over a standard trigram using this model. The rest of this paper will present our parsing model, its application to language modeling for speech recognition, and empirical results. 256 Roark Top-Down Parsing 4. Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model. The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically.1° Such methods are nearly always used in conjunction with some form of dynamic programming (henceforth DP)."
J01-2004,P99-1069,0,0.0353406,"ted on both the right- and left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item. Thus the rule S -* NP VP becomes, for instance, S[barks] ---*NP[dog] VP[barks]. One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation). This procedure yields conditional probability distributions of 10 Johnson et al. (1999), Henderson and Brill (1999), and Collins (2000) demonstratemethods for choosing the best completeparse tree from among a set of completeparse trees, and the latter two show accuracy improvementsover some of the parsers cited above, from which they generated their candidate sets. Here we will be comparing our work with parsing algorithms,i.e., algorithmsthat build parses for strings of words. 257 Computational Linguistics Volume 27, Number 2 constituents on the right-hand side with their lexical heads, given the left-hand side constituent and its lexical head. The same procedure works if we an"
J01-2004,J93-2004,0,0.0470047,"Missing"
J01-2004,W97-0301,0,0.0144678,"for speech recognition, and empirical results. 256 Roark Top-Down Parsing 4. Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model. The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically.1° Such methods are nearly always used in conjunction with some form of dynamic programming (henceforth DP). That is, search efficiency for these parsers is improved by both statistical search heuristics and DP. Here we will present a parser that uses simple search heuristics of this sort without DE Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a sin"
J01-2004,W00-1604,1,0.738302,"Missing"
J01-2004,P99-1054,1,0.907717,"e set of all partial derivations for a prefix string w0J. Then P(wg) = ~ P(d) (2) dCDwJ° We left-factor the PCFG, so that all productions are binary, except those with a single terminal on the right-hand side and epsilon productions, s We do this because it delays predictions about what nonterminals we expect later in the string until we have seen more of the string. In effect, this is an underspecification of some of the predictions that our top-down parser is making about the rest of the string. The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999). See that paper for more discussion of the benefits of 3 A PCFG is consistent or tight if there is n o probability m a s s reserved for infinite trees. Chi a n d G e m a n (1998) p r o v e d that a n y PCFG e s t i m a t e d from a treebank w i t h the relative frequency estimator is tight. All of the PCFGs that are u s e d in this p a p e r are e s t i m a t e d u s i n g the relative frequency estimator. 4 A leftmost derivation is a derivation in w h i c h the leftmost n o n t e r m i n a l is a l w a y s e x p a n d e d . 5 The only c-productions that w e will u s e in this p a p e r are t"
J01-2004,P80-1024,0,0.505298,"Missing"
J01-2004,W97-0309,0,0.0144798,"and it has been shown to be very effective. For an interpolated (n + 1)-gram: P(wi] i-1 i-1 A I wi_i - 1 ) + Wi_n) = /~n(Wi_n)P(wi (1 - i-1 i - 1 ))P(wi I Wi-n+l) n(wi_ (6) Here 13 is the empirically observed relative frequency, and /~n is a function from V n to [0,1]. This interpolation is recursively applied to the smaller-order n-grams until the bigram is finally interpolated with the unigram, i.e., A0 = 1. 3. Previous Work There have been attempts to jump over adjacent words to words farther back in the left context, without the use of dependency links or syntactic structure, for example Saul and Pereira (1997) and Rosenfeld (1996, 1997). We will focus our very brief review, however, on those that use grammars or parsing for their language models. These can be divided into two rough groups: those that use the grammar as a language model, 6 A n o d e A d o m i n a t e s a n o d e B in a tree if a n d only if either (i) A is the parent of B; or (ii) A is the parent of a n o d e C that d o m i n a t e s B. 7 For a detailed introduction to statistical speech recognition, see Jelinek (1997). 8 The n in is one m o r e t h a n the order of the M a r k o v model, since the n - g r a m includes the w o r d b"
J01-2004,J95-2002,0,0.461346,"its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar. 3.1 Grammar Models As mentioned in Section 2.1, a PCFG defines a probability distribution over strings of words. One approach to syntactic language modeling is to use this distribution directly as a language model. There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG. The algorithms both utilize a left-corner matrix, which can be calculated in closed form through matrix inversion. They are limited, therefore, to grammars where the nonterminal set is small enough to permit inversion. String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: P(Wj+l I wg) = P(wg+l) P(wg) (7) Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language mo"
J01-2004,P94-1011,0,0.0451201,"c language modeling is to use this distribution directly as a language model. There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG. The algorithms both utilize a left-corner matrix, which can be calculated in closed form through matrix inversion. They are limited, therefore, to grammars where the nonterminal set is small enough to permit inversion. String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: P(Wj+l I wg) = P(wg+l) P(wg) (7) Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language models. Interpolating the observed bigram probabilities with these calculated bigrams led, in both cases, to improvements in word error rate over using the observed bigrams alone, demonstrating that there is some benefit to using these syntactic language models to generalize beyond observed n-grams. 3.2 Finding Phrasal Heads Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to ""surface"" c-commandin"
J01-2004,J03-4003,0,\N,Missing
J01-2004,C98-1035,0,\N,Missing
J07-2009,P01-1017,0,0.0170187,"ammars for speech recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between “grammar-based” and “data-driven” approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors’ approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others), and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flatten"
J07-2009,J98-4004,0,0.0239861,"cke, and Harper 2004, among others), and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in Johnson (1998), where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true. T"
J07-2009,J01-2004,1,0.794506,"ch recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between “grammar-based” and “data-driven” approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors’ approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others), and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees ca"
J12-4002,J99-2004,0,0.571471,"words are only allowed to be assigned one of a subset of the POS-tag vocabulary, often based on what has been observed in the training data. This will overly constrain polysemous word types that happen not to have been observed with one of their possible tags; yet the large efﬁciency gain of so restricting the tags is typically seen as outweighing the loss in coverage. POStag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi 1999) and dependency parsing (McDonald et al. 2005). Richer tag sets can also be used to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which contain information about how the word will syntactically integrate with other words in the sequence. Supertagging has been widely used to make parsing algorithms efﬁcient, particularly those making use of context-sensitive grammars (Clark and Curran 2004). By applying ﬁnite-state chart constraints to constituent parsing, the approaches pursued in this article constrain the possible shapes of unlabeled trees, eliminating from consideration trees with constituents over speciﬁc spans. There is thus some similarity with other tagging approaches (e.g., supertagging) that dictate how words"
J12-4002,H91-1060,0,0.0273887,"rmining stopping criteria during training and some parameter tuning), Section 24 as development, and Section 23 as test set. For Chinese, we use the Penn Chinese Treebank (Xue et al. 2005). Articles 1–270 and 400–1151 are used for training, articles 301–325 for both held-out and development, and articles 271– 300 for testing. Supervised class labels are extracted from the non-binarized treebank trees for B, E, and U (as well as their complements). All results report F-measure labeled bracketing accuracy (harmonic mean of labeled precision and labeled recall) for all sentences in the data set (Black et al. 1991), and timing is reported using an Intel 3.00GHz processor with 6MB of cache and 16GB of memory. Timing results include both the pre-processing time to tag the chart constraints as well as the subsequent context-free inference, but tagging time is relatively negligible as it takes less than three seconds to tag the entire development corpus. 6.2 Tagging Methods and Closing Chart Cells We have three separate tagging tasks, each with two possible tags for every word wi in the input string: (1) B or B; (2) E or E; and (3) U or U. Our taggers are as described in Section 5. Within a pipeline system"
J12-4002,P11-1045,1,0.811574,"heir arguments, hence are often unlikely to end constituents, yet verbs and prepositions are rarely inside a typically deﬁned base phrase. Instead of imposing parsing constraints from NLP pre-processing steps such as chunking, we propose that building speciﬁc prediction models to constrain the search space within the CYK chart will more directly optimize efﬁciency within a parsing pipeline. In this article, we focus on linear complexity ﬁnite-state methods for deriving constraints on the chart. Recent work has also examined methods for constraining each of the O(N2 ) chart cell independently (Bodenstab et al. 2011), permitting a ﬁner-grained pruning (e.g., not just “open” or “closed” but an actual beam width prediction) and the use of features beyond the scope of our tagger. We discuss this and other extensions of the current methods in our concluding remarks. 722 Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing 3. Preliminaries 3.1 Dynamic Programming Chart Dynamic programming for context-free inference generally makes use of a chart structure, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b. Each cell in the chart represents a collection of"
J12-4002,P11-2119,1,0.84878,"heir arguments, hence are often unlikely to end constituents, yet verbs and prepositions are rarely inside a typically deﬁned base phrase. Instead of imposing parsing constraints from NLP pre-processing steps such as chunking, we propose that building speciﬁc prediction models to constrain the search space within the CYK chart will more directly optimize efﬁciency within a parsing pipeline. In this article, we focus on linear complexity ﬁnite-state methods for deriving constraints on the chart. Recent work has also examined methods for constraining each of the O(N2 ) chart cell independently (Bodenstab et al. 2011), permitting a ﬁner-grained pruning (e.g., not just “open” or “closed” but an actual beam width prediction) and the use of features beyond the scope of our tagger. We discuss this and other extensions of the current methods in our concluding remarks. 722 Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing 3. Preliminaries 3.1 Dynamic Programming Chart Dynamic programming for context-free inference generally makes use of a chart structure, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b. Each cell in the chart represents a collection of"
J12-4002,N10-1015,0,0.0329039,"Missing"
J12-4002,A00-2018,0,0.484669,"e Park, Maryland, 20740 USA. E-mail: hollingk@gmail.com. Submission received: 9 August 2011; revised submission received: 30 November 2011; accepted for publication: 4 January 2012. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 4 or both. Pipeline systems make use of simpler models with more efﬁcient inference to reduce the search space of the full model. For example, the well-known Ratnaparkhi (1999) parser used a part-of-speech (POS)-tagger and a ﬁnite-state noun phrase (NP) chunker as initial stages of a multi-stage Maximum Entropy parser. The Charniak (2000) parser uses a simple probalistic context-free grammar (PCFG) to sparsely populate a chart for a richer model, and Charniak and Johnson (2005) added a discriminatively trained reranker to the end of that pipeline. Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. As mentioned earlier, the Ratnaparkhi pipeline used a ﬁnite-state POS-tagger and a ﬁnite-state NP-chunker to reduce the search space at the parsing stage, and achieved linear observed-time performance. Other recent examples of the utility of ﬁni"
J12-4002,P11-2035,0,0.0111538,"ng constraints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith, and Smith 2006), which is known as vine parsing. Such vine parsers can be further constrained using taggers to determine the directionality and distance of each word’s head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc ﬁltering approaches, using a variety of features (including some inspired by previously published results of methods presented in this article) have been proposed to reduce the number of arcs considered for the dependency graph (Bergsma and Cherry 2010; Cherry and Bergsma 2011), resulting in large parsing speedups. In a context-free constituent parsing pipeline, constraints on the ﬁnal parse structure can be made in stages preceding the CYK algorithm. For example, base phrase chunking (Hollingshead and Roark 2007) involves identifying a span as a base phrase of some category, often NP. A base phrase constituent has no children other than pre-terminal POS-tags, which all have a single terminal child (i.e., there is no internal structure in the base phrase involving non-POS non-terminals). This has a number of implications for the context-free parser. First, there is"
J12-4002,C04-1041,0,0.0364968,"e large efﬁciency gain of so restricting the tags is typically seen as outweighing the loss in coverage. POStag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi 1999) and dependency parsing (McDonald et al. 2005). Richer tag sets can also be used to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which contain information about how the word will syntactically integrate with other words in the sequence. Supertagging has been widely used to make parsing algorithms efﬁcient, particularly those making use of context-sensitive grammars (Clark and Curran 2004). By applying ﬁnite-state chart constraints to constituent parsing, the approaches pursued in this article constrain the possible shapes of unlabeled trees, eliminating from consideration trees with constituents over speciﬁc spans. There is thus some similarity with other tagging approaches (e.g., supertagging) that dictate how words combine with the rest of the sentence via speciﬁc syntactic structures. Supertagging is generally used to enumerate which sorts of structures are licensed, whereas the constraints in this article indicate unlabeled tree structures that are proscribed. Along the sa"
J12-4002,W02-1001,0,0.43772,"Missing"
J12-4002,W07-2206,0,0.0594917,"Missing"
J12-4002,W06-2929,0,0.445648,"Missing"
J12-4002,W11-2920,1,0.894473,"Missing"
J12-4002,P81-1022,0,0.580167,"ide children. Rather than trying to combine an arbitrary number of smaller substrings (child cells), the CYK algorithm exploits shared structure between rules and only needs to consider pairwise combination. To conform to this requirement, incomplete edges are needed to represent that further combination is required to achieve a complete edge. This can either be performed in advance, for example, by transforming a grammar into Chomsky Normal Form resulting in “incomplete” non-terminals created by the transform, or incomplete edges can be represented through so-called dotted rules, as with the Earley (1970) algorithm, in which transformation is essentially performed on the ﬂy. For example, if we have a rule production A → B C D ∈ P, a completed B entry in chart cell (b, m1 ) and a completed C entry in chart cell (m1 +1, m2 ), then we can place an incomplete edge A → B C · D in chart cell (b, m2 ). The dot signiﬁes the division between what has already been combined (left of the dot), and what remains to be combined. Then, if we have an incomplete edge A → B C · D in chart cell (b, m2 ) and a complete D in cell (m2 +1, e), we can place a completed A entry in chart cell (b, e). Transforming a gram"
J12-4002,W05-1504,0,0.164454,"that is allowed can greatly reduce the number of arcs that must be considered during search, and, as long as some degree of non-projectivity is allowed, coverage is minimally impacted. Of course, the total absence of projectivity constraints allows for the use of spanning tree algorithms that can be quadratic complexity for certain classes of statistical models (McDonald et al. 2005), so the ultimate utility of such constraints varies depending on the model being used. Other hard constraints have been applied to dependency parsing, including constraints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith, and Smith 2006), which is known as vine parsing. Such vine parsers can be further constrained using taggers to determine the directionality and distance of each word’s head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc ﬁltering approaches, using a variety of features (including some inspired by previously published results of methods presented in this article) have been proposed to reduce the number of arcs considered for the dependency graph (Bergsma and Cherry 2010; Cherry and Bergsma 2011), resulting in large parsing speedups. In a context"
J12-4002,P06-2038,0,0.0606598,"Missing"
J12-4002,H05-1099,1,0.880239,"Missing"
J12-4002,P07-1120,1,0.923133,"and Johnson (2005) added a discriminatively trained reranker to the end of that pipeline. Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. As mentioned earlier, the Ratnaparkhi pipeline used a ﬁnite-state POS-tagger and a ﬁnite-state NP-chunker to reduce the search space at the parsing stage, and achieved linear observed-time performance. Other recent examples of the utility of ﬁnite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic, Curran, and Clark (2007), and Hollingshead and Roark (2007). Similar hard constraints have been applied for dependency parsing, as will be outlined in Section 2. Note that by making use of preprocessing constraints, such approaches are no longer performing full exact inference— these are approximate inference methods, as are the methods presented in this article. Using ﬁnite-state chunkers early in a syntactic parsing pipeline has shown both an efﬁciency (Glaysher and Moldovan 2006) and an accuracy (Hollingshead and Roark 2007) beneﬁt for parsing systems. Glaysher and Moldovan (2006) demonstrated an efﬁciency gain by explicitly disallowing constituent"
J12-4002,J93-2004,0,0.0392931,"Missing"
J12-4002,H05-1066,0,0.295954,"arsing pipelines. One of the most basic and standard techniques is the use of a POS-tag dictionary, whereby words are only allowed to be assigned one of a subset of the POS-tag vocabulary, often based on what has been observed in the training data. This will overly constrain polysemous word types that happen not to have been observed with one of their possible tags; yet the large efﬁciency gain of so restricting the tags is typically seen as outweighing the loss in coverage. POStag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi 1999) and dependency parsing (McDonald et al. 2005). Richer tag sets can also be used to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which contain information about how the word will syntactically integrate with other words in the sequence. Supertagging has been widely used to make parsing algorithms efﬁcient, particularly those making use of context-sensitive grammars (Clark and Curran 2004). By applying ﬁnite-state chart constraints to constituent parsing, the approaches pursued in this article constrain the possible shapes of unlabeled trees, eliminating from consideration trees with constituents over speciﬁc"
J12-4002,E06-1010,0,0.025749,"s, such as those used in the Berkeley (Petrov and Klein 2007a) and Charniak (2000) parsers, and more general structured prediction cascades (Weiss, Sapp, and Taskar 2010; Weiss and Taskar 2010). Our approach also uses simpler models that reduce the search space for larger downstream models. Dependency parsing involves constructing a graph of head/dependent relations, and many methods for constraining the space of possible dependency graphs have been 721 Computational Linguistics Volume 38, Number 4 investigated, such as requiring that each word have a single head or that the graph be acyclic. Nivre (2006) investigated the impact of such constraints on coverage and the number of candidate edges in the search space. Most interestingly, that paper found that constraining the degree of non-projectivity that is allowed can greatly reduce the number of arcs that must be considered during search, and, as long as some degree of non-projectivity is allowed, coverage is minimally impacted. Of course, the total absence of projectivity constraints allows for the use of spanning tree algorithms that can be quadratic complexity for certain classes of statistical models (McDonald et al. 2005), so the ultimat"
J12-4002,N07-1051,0,0.460871,"ightforward to incorporate into most existing context-free constituent parsers, a task we have already done for several state-of-the art parsers. In the following sections we formally deﬁne our approach to ﬁnite-state chart constraints and analyze the accuracy of each of the three taggers and their impact on parsing efﬁciency and accuracy when used to prune the search space of a constituent parser. We apply our methods to exhaustive CYK parsing with simple grammars, as well as to high-accuracy parsing approaches such as the Charniak and Johnson (2005) parsing pipeline and the Berkeley parser (Petrov and Klein 2007a, 2007b). Various methods for applying ﬁnite-state chart constraints are investigated, including methods that guarantee quadratic or linear complexity of the context-free parser. 2. Related Work Hard constraints are ubiquitous within parsing pipelines. One of the most basic and standard techniques is the use of a POS-tag dictionary, whereby words are only allowed to be assigned one of a subset of the POS-tag vocabulary, often based on what has been observed in the training data. This will overly constrain polysemous word types that happen not to have been observed with one of their possible t"
J12-4002,C08-1094,1,0.920074,"Missing"
J12-4002,N09-1073,1,0.858042,"Missing"
J12-4002,N03-1028,0,0.0564859,", ϕi−1 , ϕi , ϕi+1 τ i , ϕi − 2 τ i , ϕi − 2 , ϕi − 1 τ i , ϕi − 2 , ϕi − 1 , ϕi τi , ϕi+2 τi , ϕi+1 , ϕi+2 τi , ϕi , ϕi+1 , ϕi+2 All lexical (LEX), orthographic (ORTHO), and part-of-speech (POS) features are duplicated to also occur with τi−1 ; e.g., {τi−1 , τi , wi } as a LEX feature. the preceding words. The n-gram features are represented by the words within a threeword window of the current word. The tag features are represented as unigram, bigram, and trigram tags (i.e., constituent tags from the current and two previous words). These features are based on the feature set implemented by Sha and Pereira (2003) for NP chunking. Additional orthographical features are used for unknown and rare words (words that occur fewer than ﬁve times in the training data), such as the preﬁxes and sufﬁxes of the word (up to the ﬁrst and last four characters of the word), and the presence of a hyphen, a digit, or a capitalized letter, following the features implemented by Ratnaparkhi (1999). Note that the orthographic feature templates, including the preﬁx (e.g., wi [0..1]) and sufﬁx (e.g., wi [n-2..n]) templates, are only activated for unknown and rare words. When applying our tagging model to Chinese data, all fea"
J12-4002,W09-3831,0,0.0148847,"or the use of spanning tree algorithms that can be quadratic complexity for certain classes of statistical models (McDonald et al. 2005), so the ultimate utility of such constraints varies depending on the model being used. Other hard constraints have been applied to dependency parsing, including constraints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith, and Smith 2006), which is known as vine parsing. Such vine parsers can be further constrained using taggers to determine the directionality and distance of each word’s head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc ﬁltering approaches, using a variety of features (including some inspired by previously published results of methods presented in this article) have been proposed to reduce the number of arcs considered for the dependency graph (Bergsma and Cherry 2010; Cherry and Bergsma 2011), resulting in large parsing speedups. In a context-free constituent parsing pipeline, constraints on the ﬁnal parse structure can be made in stages preceding the CYK algorithm. For example, base phrase chunking (Hollingshead and Roark 2007) involves identifying a span as a base phrase of some category"
J12-4002,D08-1018,0,0.0605222,"Missing"
J12-4002,D09-1161,0,0.030285,"Missing"
J12-4002,C10-2168,0,0.233358,"Missing"
J12-4002,J98-2004,0,\N,Missing
J12-4002,C10-1007,0,\N,Missing
J12-4002,P05-1022,0,\N,Missing
J14-4002,W98-0903,0,0.123032,"does not appear in this form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicogra"
J14-4002,P03-1006,1,0.663102,"Missing"
J14-4002,N01-1018,0,0.0492683,"OR 97239-3098, USA. E-mails: {mahsa.yarmohamadi,zakshafran}@gmail.com. Submission received: 1 March 2013; revised version received: 5 November 2013; accepted for publication: 23 December 2013. doi:10.1162/COLI_a_00198 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 4 1950s and 1960s to implement small hand-built grammars (e.g., Joshi and Hopely 1996) through their applications in computational morphology in the 1980s (Koskenniemi 1983), finite-state models are now routinely applied in areas ranging from parsing (Abney 1996), to machine translation (Bangalore and Riccardi 2001; de Gispert et al. 2010), text normalization (Sproat 1996), and various areas of speech recognition including pronunciation modeling and language modeling (Mohri, Pereira, and Riley 2002). The development of weighted finite state approaches (Mohri, Pereira, and Riley 2002; Mohri 2009) has made it possible to implement models that can rank alternative analyses. A number of weight classes—semirings—can be defined (Kuich and Salomaa 1986; Golan 1999), though for all practical purposes nearly all actual applications use the tropical semiring, whose most obvious instantiation is as a way to combin"
J14-4002,J10-3008,0,0.273047,"Missing"
J14-4002,D10-1080,0,0.0209975,"Missing"
J14-4002,C00-1038,0,0.0594641,", because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of elements of the tuple is the sam"
J14-4002,C94-2163,0,0.351637,"use the suffix does not appear in this form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely,"
J14-4002,J98-2006,0,0.107901,"form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of el"
J14-4002,W98-1301,0,0.0802897,"tion is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of elements of the tu"
J14-4002,J93-2004,0,0.0461717,"Missing"
J14-4002,P12-3011,1,0.896656,"Missing"
J14-4002,P11-2001,1,0.497098,"Missing"
J14-4002,J95-2004,0,0.293631,"Missing"
J14-4002,N10-1023,0,0.0420037,"Missing"
J14-4002,2010.iwslt-keynotes.2,0,\N,Missing
J15-4001,P06-1002,0,0.0235871,"l worth the time and effort. We note that the final classification results for all four alignment models are not drastically different from one another, despite the large reductions in word alignment error rate and improvements in scoring accuracy observed in the larger models and graph-based models. This seeming disconnect between word alignment accuracy and downstream application performance has also been observed in the machine translation literature, where reductions in AER do not necessarily lead to meaningful increases in BLEU, the widely accepted measure of machine translation quality (Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). Our results, however, show that a feature set consisting of manually assigned WLM scores yields the highest classification accuracy of any of the feature sets evaluated here. As discussed in Section 5.2, our WLM score extraction method is designed such that element-level scores can be extracted with perfect accuracy from a perfect word alignment. Thus, the goal of seeking perfect or near-perfect word alignment accuracy is worthwhile because it will necessarily result in perfect or near-perfect scoring accuracy, which in turn is likely to yield c"
J15-4001,J93-2003,0,0.0341995,"he word alignment models. Therefore, as in the case with most experiments involving word alignment, we build a model for the data we wish to evaluate using that same data. We do, however, use the 48 retellings from the individuals who were not experimental participants as a development set for tuning the various parameters of our word alignment system, which are described in the following. 5.4 Baseline Alignment We begin by building two word alignment models using the Berkeley aligner (Liang, Taskar, and Klein 2006), a state-of-the-art word alignment package that relies on IBM Models 1 and 2 (Brown et al. 1993) and an HMM. We chose to use the Berkeley aligner, rather than the more widely used Giza++ alignment package, for this task because its joint training and posterior decoding algorithms yield lower alignment error rates on most data sets (including the data set used here [Prud’hommeaux and Roark 2011]) and because it offers functionality for testing an existing model on new data and, more crucially, for outputting posterior probabilities. The smaller of our two Berkeley-generated models is trained on Corpus 1 (the source-to-retelling parallel corpus described earlier) and ten copies of Corpus 3"
J15-4001,W13-1911,0,0.0229681,"stream of spontaneous spoken language in response to a stimulus. A person might be asked, for instance, to retell a brief narrative or to describe the events depicted in a drawing. Much of the previous work in applying NLP techniques to such clinically elicited spoken language data has relied on parsing and language modeling to enable the automatic extraction of linguistic features, such as syntactic complexity and measures of vocabulary use and diversity, which can then be used as markers for various neurological impairments (Solorio and Liu 2008; Gabani et al. 2009; Roark et al. 2011; de la Rosa et al. 2013; Fraser et al. 2014). In this article, we instead use NLP techniques to analyze the content, rather than the linguistic characteristics, of weakly structured spoken language data elicited using neuropsychological assessment instruments. We will show that the content of such spoken responses contains information that can be used for accurate screening for neurodegenerative disorders. The features we explore are grounded in the idea that individuals recalling the same narrative are likely to use the same sorts of words and semantic concepts. In other words, a retelling of a narrative will be fa"
J15-4001,J07-3002,0,0.0234726,"he final classification results for all four alignment models are not drastically different from one another, despite the large reductions in word alignment error rate and improvements in scoring accuracy observed in the larger models and graph-based models. This seeming disconnect between word alignment accuracy and downstream application performance has also been observed in the machine translation literature, where reductions in AER do not necessarily lead to meaningful increases in BLEU, the widely accepted measure of machine translation quality (Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). Our results, however, show that a feature set consisting of manually assigned WLM scores yields the highest classification accuracy of any of the feature sets evaluated here. As discussed in Section 5.2, our WLM score extraction method is designed such that element-level scores can be extracted with perfect accuracy from a perfect word alignment. Thus, the goal of seeking perfect or near-perfect word alignment accuracy is worthwhile because it will necessarily result in perfect or near-perfect scoring accuracy, which in turn is likely to yield classification accuracy approaching that of manu"
J15-4001,N09-1006,0,0.0157198,"ich the person must produce an uninterrupted stream of spontaneous spoken language in response to a stimulus. A person might be asked, for instance, to retell a brief narrative or to describe the events depicted in a drawing. Much of the previous work in applying NLP techniques to such clinically elicited spoken language data has relied on parsing and language modeling to enable the automatic extraction of linguistic features, such as syntactic complexity and measures of vocabulary use and diversity, which can then be used as markers for various neurological impairments (Solorio and Liu 2008; Gabani et al. 2009; Roark et al. 2011; de la Rosa et al. 2013; Fraser et al. 2014). In this article, we instead use NLP techniques to analyze the content, rather than the linguistic characteristics, of weakly structured spoken language data elicited using neuropsychological assessment instruments. We will show that the content of such spoken responses contains information that can be used for accurate screening for neurodegenerative disorders. The features we explore are grounded in the idea that individuals recalling the same narrative are likely to use the same sorts of words and semantic concepts. In other w"
J15-4001,N13-1021,1,0.901238,"Missing"
J15-4001,N06-1014,0,0.0427792,"Missing"
J15-4001,W04-1013,0,0.0292154,"e narrative measured using LSA, proposed by Dunn et al. (2002) and calculated using the University of Colorado’s online LSA interface (available at http://lsa.colorado.edu/) with the 300-factor ninth-grade reading level topic space; (2) unigram overlap precision of a retelling relative to the source, proposed by HakkaniTur, Vergyri, and Tur (2010); (3) BLEU, the n-gram overlap metric commonly used to evaluate the quality of machine translation output (Papineni et al. 2002); and (4) the F-measure for ROUGE-SU4, the n-gram overlap metric commonly used to evaluate automatic summarization output (Lin 2004). The remaining two automatically derived features are a set of binary scores corresponding to the exact match via grep of each of the open-class unigrams in the source narrative and a summary score thereof. Finally, in order to compare the WLM with another standard psychometric test, we also show the accuracy of a classifier trained only on the expert-assigned manual scores for the MMSE (Folstein, Folstein, and McHugh 1975), a clinician-administered 30point questionnaire that measures a patient’s degree of cognitive impairment. Although it is widely used to screen for dementias such as Alzhei"
J15-4001,2006.amta-papers.11,0,0.0236189,"effort. We note that the final classification results for all four alignment models are not drastically different from one another, despite the large reductions in word alignment error rate and improvements in scoring accuracy observed in the larger models and graph-based models. This seeming disconnect between word alignment accuracy and downstream application performance has also been observed in the machine translation literature, where reductions in AER do not necessarily lead to meaningful increases in BLEU, the widely accepted measure of machine translation quality (Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). Our results, however, show that a feature set consisting of manually assigned WLM scores yields the highest classification accuracy of any of the feature sets evaluated here. As discussed in Section 5.2, our WLM score extraction method is designed such that element-level scores can be extracted with perfect accuracy from a perfect word alignment. Thus, the goal of seeking perfect or near-perfect word alignment accuracy is worthwhile because it will necessarily result in perfect or near-perfect scoring accuracy, which in turn is likely to yield classification accuracy"
J15-4001,J03-1002,0,0.0227825,"individuals generating those retellings. 5.1 Example Alignment Figure 4 shows a visual grid representation of a manually generated word alignment between the source narrative shown in Figure 1 on the vertical axis and the example WLM retelling in Figure 2 on the horizontal axis. Table 4 shows the word-index-toword-index alignment, in which the first index of each sentence is 0 and in which null alignments are not shown. When creating these manual alignments, the labelers assigned the “possible” denotation under one of these two conditions: (1) when the alignment was ambiguous, as outlined in Och and Ney (2003); and (2) when a particular word in the retelling was a logical alignment to a word in the source narrative, but it would not have been counted as a permissible substitution under the published scoring guidelines. For this reason, we see that Taylor and sixty-seven are considered to be possible alignments because although they are logical alignments, they are not permissible substitutions according to the published scoring guidelines. Note that the word dollars is considered to be only a possible alignment, as well, since the element fifty-six dollars is not correctly recalled in this retellin"
J15-4001,P02-1040,0,0.100649,"scores ranging between 0 and 1 for each participant, one for each of the two retellings: (1) cosine similarity between a retelling and the source narrative measured using LSA, proposed by Dunn et al. (2002) and calculated using the University of Colorado’s online LSA interface (available at http://lsa.colorado.edu/) with the 300-factor ninth-grade reading level topic space; (2) unigram overlap precision of a retelling relative to the source, proposed by HakkaniTur, Vergyri, and Tur (2010); (3) BLEU, the n-gram overlap metric commonly used to evaluate the quality of machine translation output (Papineni et al. 2002); and (4) the F-measure for ROUGE-SU4, the n-gram overlap metric commonly used to evaluate automatic summarization output (Lin 2004). The remaining two automatically derived features are a set of binary scores corresponding to the exact match via grep of each of the open-class unigrams in the source narrative and a summary score thereof. Finally, in order to compare the WLM with another standard psychometric test, we also show the accuracy of a classifier trained only on the expert-assigned manual scores for the MMSE (Folstein, Folstein, and McHugh 1975), a clinician-administered 30point quest"
J15-4001,W12-2401,1,0.821891,"Missing"
J15-4001,W08-0626,0,0.018883,"s include a task in which the person must produce an uninterrupted stream of spontaneous spoken language in response to a stimulus. A person might be asked, for instance, to retell a brief narrative or to describe the events depicted in a drawing. Much of the previous work in applying NLP techniques to such clinically elicited spoken language data has relied on parsing and language modeling to enable the automatic extraction of linguistic features, such as syntactic complexity and measures of vocabulary use and diversity, which can then be used as markers for various neurological impairments (Solorio and Liu 2008; Gabani et al. 2009; Roark et al. 2011; de la Rosa et al. 2013; Fraser et al. 2014). In this article, we instead use NLP techniques to analyze the content, rather than the linguistic characteristics, of weakly structured spoken language data elicited using neuropsychological assessment instruments. We will show that the content of such spoken responses contains information that can be used for accurate screening for neurodegenerative disorders. The features we explore are grounded in the idea that individuals recalling the same narrative are likely to use the same sorts of words and semantic"
J19-2004,D16-1162,0,0.0776354,"Missing"
J19-2004,P12-3006,0,0.0949107,"ow the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how on"
J19-2004,P10-1079,0,0.0565589,"categories finer-grained classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normali"
J19-2004,P18-1008,0,0.0293253,"ls pass through digits. # Then try digits @ o, and if that fails pass through digits. # Then try digits @ cardinals, which shall surely work. # Oh, and then make it a disjunction with thousand to allow both # &quot;twenty ten&quot; and &quot;two thousand ten&quot; readings. export YEAR = Optimize[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[digits, hundreds, sigstar], pairwise, sigstar], o, sigstar], cardinal, sigstar] | thousand]; A.2 Transformer Model Details We utilize a Transformer sequence-to-sequence model (Vaswani et al. 2017), using the architecture described in Appendix A.2 of Chen et al. (2018), with: • 6 Transformer layers for both the encoder and the decoder, • 8 attention heads, • a model dimension of 512, and • a hidden dimension of 2,048. 333 Computational Linguistics Volume 45, Number 2 Table A.1 Default parameters for the sliding window model. Input embedding size Output embedding size Number of encoder layers Number of decoder layers Number of encoder units Number of decoder units Attention mechanism size 256 512 1 1 256 512 256 Dropout probabilities are uniformly set to 0.1. We use a dictionary of 32k word pieces (Schuster and Nakajima 2012) covering both input and output v"
J19-2004,P14-2111,0,0.484934,"dern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text,"
J19-2004,P13-1155,0,0.070932,"Missing"
J19-2004,C08-1056,0,0.0378842,"Missing"
J19-2004,P12-1109,0,0.0772642,"Missing"
J19-2004,P11-2013,0,0.0562114,"ned classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at s"
J19-2004,P12-1055,0,0.0199831,"n, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally o"
J19-2004,D16-1096,0,0.0278276,"m the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstruction loss, which is the"
J19-2004,W15-4317,0,0.613959,"e, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text, though it is essent"
J19-2004,I11-1109,0,0.0148426,"ns depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. Fo"
J19-2004,P14-2060,1,0.776607,"o occur naturally. Machine translation can to a large extent rely on “found” data because people translate texts for practical reasons, such as providing access to documents to people not able to read the source language. In contrast, there is no reason why people would spend resources producing verbalized equivalents of ordinary written text: in most cases, English speakers 3 As a consequence, much of the subsequent work on applying machine learning to text normalization for speech applications focuses on specific semiotic classes, like letter sequences (Sproat and Hall 2014), abbreviations (Roark and Sproat 2014), or cardinal numbers (Gorman and Sproat 2016). 4 In fact, Kestrel (Ebden and Sproat 2014) uses a machine-learned morphosyntactic tagger for Russian. 298 Zhang et al. Neural Models of Text Normalization do not need a gloss to know how to read $10 million. Thus, if one wants to train neural models to verbalize written text, one must produce the data.5 Second, the bar for success in this domain seems to be higher than it is in other domains in that users expect TTS systems to correctly read numbers, dates, times, currency amounts, and so on. As we show subsequently, deep learning models produce"
J19-2004,P12-3011,1,0.93381,"arate words. Thus the Kestrel grammars recognize Jan.1, 2012 as a date and parse it as a single token, identifying the month, day, and year, and represent it internally using a protocol-buffer representation like the following:6 date { month: &quot;January&quot; day: &quot;1&quot; year: &quot;2012&quot;} Verbalization grammars then convert from a serialization of the protocol buffer representation into actual word sequences, such as January the first twenty twelve. Tokenization/classification and verbalization grammars are compiled into weighted finite-state transducers (WFSTs) using the Thrax grammar development library (Roark et al. 2012). One advantage of separating tokenization/classification from verbalization via the intermediate protocol buffer representation is that it allows for reordering of elements, something that is challenging with WFSTs.7 The need for reordering arises, for example, in the treatment of currency expressions where currency symbols such as ‘$’ or ‘’ often occur before digits, but are verbalized after the corresponding digits. An input $30 might be parsed as something like money { currency: &quot;USD&quot; amount { integer: &quot;30&quot; } } 6 https://developers.google.com/protocol-buffers/. 7 As we will show subsequent"
J19-2004,P16-1162,0,0.148348,"Missing"
J19-2004,N10-1023,0,0.0238932,"a, and in that we define more precisely how the covering grammars are actually used during decoding. Finally, we report results on new data sets. Arik et al. (2017) present a neural network TTS system that mimics the traditional separation into linguistic analysis (or front-end) and synthesis (or back-end) modules. It is unclear to what degree this system in fact performs text normalization since the only front-end component they describe is grapheme-to-phoneme conversion, which is a separate process from text normalization and usually performed later in the pipeline. Some prior work, such as Shugrina (2010), focuses on the inverse problem of denormalizing spoken sequences into written text in the context of ASR so that two hundred fifty would get converted to 250, or three thirty as a time would get formatted as 3:30. Pusateri et al. (2017) describe a system in which denormalization is treated as a neural network sequence labeling problem using a rich tag set. 8 See http://github.com/google/sparrowhawk. 301 Computational Linguistics Volume 45, Number 2 The data we report on in this article was recently released and was the subject of a Kaggle competition (see later in this article), and a few re"
J19-2004,P16-1008,0,0.0268692,"ame word position from the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstructio"
J19-2004,P06-1125,0,0.033252,"Missing"
J19-2004,D13-1007,0,0.0413732,"ed by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of soci"
J19-2004,N18-1122,0,0.0317744,"able errors, and for these we have argued that using trainable finite-state covering grammars is a reasonable approach, but we continue to look for ways to improve covering grammar training and coverage. 18 At the same time, we are currently exploring whether other neural approaches can help mitigate against unrecoverable errors. One approach that seems plausible is generative adversarial networks (Goodfellow et al. 2014; Goodfellow 2016), which have achieved impressive results in vision-related tasks but which have been also applied to NLP tasks including machine translation (Wu et al. 2017; Yang et al. 2018). Given the great success of deep learning for many problems, it is tempting to simply accrete speech and language tasks to a general class of problems and to worry less about the underlying problem being solved. For example, at a certain level of abstraction, all of text-to-speech synthesis can be thought of as a sequence-to-sequence problem where the input sequence is a string of characters and the output sequence is some representation of a waveform. “End-to-end” TTS models such as Char2Wav (Sotelo et al. 2017) treat the problem in this way, with no attempt to consider the many subproblems"
J19-2004,D14-1179,0,\N,Missing
J19-2004,Q16-1036,1,\N,Missing
N03-1027,A00-2018,0,0.0722273,"ey are worked on earlier. The figure-ofmerit consists of the probability of the parse to that point times a look-ahead statistic, which is an estimate of how much probability mass it will take to connect the parse with the next word. It is a generative parser that does not require any pre-processing, such as POS tagging or chunking. It has been demonstrated in the above papers to perform competitively on standard statistical parsing tasks with full coverage. Baseline results below will provide a comparison with other well known statistical parsers. The PCFG is a Markov grammar (Collins, 1997; Charniak, 2000), i.e. the production probabilities are estimated by decomposing the joint probability of the categories on the right-hand side into a product of conditionals via the chain rule, and making a Markov assumption. Thus, for example, a first order Markov grammar conditions the probability of the category of the i-th child of the left-hand side on the category of the left-hand side and the category of the (i-1)-th child of the left-hand side. The benefits of Markov grammars for a top-down parser of the sort we are using is detailed in Roark (2003). Further, as in Roark (2001a; 2003), the production"
N03-1027,J98-2005,0,0.0105297,"ion. A context-free grammar (CFG) G = (V, T, P, S † ), consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , and (4) where τA is the left-hand side dependent prior weighting parameter. This choice of prior parameters defines the MAP estimate of the probability of expansion γi from the lefthand side A as ˆ i |A) P(γ (νi − 1) + ci ω ˆ i = PK PK k=1 (νk − 1) + k=1 ck 1 ≤ i ≤ K. e c(A) α β + c(A) (7) 1 An additional condition for well-formedness is that the PCFG is consistent or tight, i.e. there is no probability mass lost to infinitely large trees. Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus. 2.2 Model Interpolation If the left-hand side dependent prior weighting parameter is chosen as  λ c(A) 1−λ , 0 < λ < 1 if c(A) > 0 τA = (8) 1 otherwise the MAP adaptation reduces to model interpolation using interpolation parameter λ: b i |A) P(γ 2.3 = λ e c(A) 1−λ P(γi |A) + c(A → γi ) = λ e 1−λ P(γi λ c(A) 1−λ + c(A) |A) + P(γi |A) λ 1−λ +1 e i |A) + (1 − λ)P(γi |A) = λP(γ (9) Other Tying Candidates While we will not be presenting empirical results for other paramete"
N03-1027,P97-1003,0,0.0634743,"tes, so that they are worked on earlier. The figure-ofmerit consists of the probability of the parse to that point times a look-ahead statistic, which is an estimate of how much probability mass it will take to connect the parse with the next word. It is a generative parser that does not require any pre-processing, such as POS tagging or chunking. It has been demonstrated in the above papers to perform competitively on standard statistical parsing tasks with full coverage. Baseline results below will provide a comparison with other well known statistical parsers. The PCFG is a Markov grammar (Collins, 1997; Charniak, 2000), i.e. the production probabilities are estimated by decomposing the joint probability of the categories on the right-hand side into a product of conditionals via the chain rule, and making a Markov assumption. Thus, for example, a first order Markov grammar conditions the probability of the category of the i-th child of the left-hand side on the category of the left-hand side and the category of the (i-1)-th child of the left-hand side. The benefits of Markov grammars for a top-down parser of the sort we are using is detailed in Roark (2003). Further, as in Roark (2001a; 2003"
N03-1027,W01-0521,0,0.612524,"arge amounts of annotated training data. Johnson and Riezler (2000) looked at adding features to a maximum entropy model for stochastic unification-based grammars (SUBG), from corpora that are not annotated with the SUBG, but rather with simpler treebank annotations for which there are much larger treebanks. Hwa (2001) demonstrated how active learning techniques can reduce the amount of annotated data required to converge on the best performance, by selecting from among the candidate strings to be annotated in ways which promote more informative examples for earlier annotation. Hwa (1999) and Gildea (2001) looked at adapting parsing models trained on large amounts of annotated data from outside of the domain of interest (out-of-domain), through the use of a relatively small amount of in-domain annotated data. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain treebank, and found an advantage to adaptation over direct grammar induction. Gildea (2001) simply added the out-of-domain treebank to his in-domain training data, and derived a very small benefit for his high accuracy, lexicalized parser, concludin"
N03-1027,P99-1010,0,0.0172198,"he absence of large amounts of annotated training data. Johnson and Riezler (2000) looked at adding features to a maximum entropy model for stochastic unification-based grammars (SUBG), from corpora that are not annotated with the SUBG, but rather with simpler treebank annotations for which there are much larger treebanks. Hwa (2001) demonstrated how active learning techniques can reduce the amount of annotated data required to converge on the best performance, by selecting from among the candidate strings to be annotated in ways which promote more informative examples for earlier annotation. Hwa (1999) and Gildea (2001) looked at adapting parsing models trained on large amounts of annotated data from outside of the domain of interest (out-of-domain), through the use of a relatively small amount of in-domain annotated data. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain treebank, and found an advantage to adaptation over direct grammar induction. Gildea (2001) simply added the out-of-domain treebank to his in-domain training data, and derived a very small benefit for his high accuracy, lexicalized"
N03-1027,W01-0710,0,0.0149222,"ction A fundamental concern for nearly all data-driven approaches to language processing is the sparsity of labeled training data. The sparsity of syntactically annotated corpora is widely remarked upon, and some recent papers present approaches to improving performance in the absence of large amounts of annotated training data. Johnson and Riezler (2000) looked at adding features to a maximum entropy model for stochastic unification-based grammars (SUBG), from corpora that are not annotated with the SUBG, but rather with simpler treebank annotations for which there are much larger treebanks. Hwa (2001) demonstrated how active learning techniques can reduce the amount of annotated data required to converge on the best performance, by selecting from among the candidate strings to be annotated in ways which promote more informative examples for earlier annotation. Hwa (1999) and Gildea (2001) looked at adapting parsing models trained on large amounts of annotated data from outside of the domain of interest (out-of-domain), through the use of a relatively small amount of in-domain annotated data. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) t"
N03-1027,A00-2021,0,0.0678216,"Missing"
N03-1027,C00-1052,1,0.825295,"revious children are treated exactly as other conditioning features from the left context. Table 1 gives the conditioning features that were used for all empirical trials in this paper. There are different conditioning features for parts-of-speech (POS) and non-POS non-terminals. Deleted interpolation leaves out one feature at a time, in the reverse order as they are presented in the table 1. The grammar that is used for these trials is a PCFG that is induced using relative frequency estimation from a transformed treebank. The trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A → Aγ. The transformed trees look as in figure 1. The transform has the benefit for a topdown incremental parser of this sort of delaying many of the parsing decisions until later in the string, without unduly disrupting the immediate dominance relationships that provide conditioning features for the probabilistic model. (a) (b) NP NP P  PPP XXXX X  PP NP   HHH NP NN bb  NNP Jim (c) POS NNP ,l , l IN NP Jim dog with . . . ’s POS ’s NP"
N03-1027,J98-4004,0,0.085918,"Missing"
N03-1027,P92-1017,0,0.0347493,"larger treebanks. Hwa (2001) demonstrated how active learning techniques can reduce the amount of annotated data required to converge on the best performance, by selecting from among the candidate strings to be annotated in ways which promote more informative examples for earlier annotation. Hwa (1999) and Gildea (2001) looked at adapting parsing models trained on large amounts of annotated data from outside of the domain of interest (out-of-domain), through the use of a relatively small amount of in-domain annotated data. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain treebank, and found an advantage to adaptation over direct grammar induction. Gildea (2001) simply added the out-of-domain treebank to his in-domain training data, and derived a very small benefit for his high accuracy, lexicalized parser, concluding that even a large amount of out-of-domain data is of little use for lexicalized parsing. Statistical model adaptation based on sparse in-domain data, however, is neither a new problem nor unique to parsing. It has been studied extensively by researchers working on acoustic modeling for automatic speech"
N03-1027,J01-2004,1,0.815008,"−λ α otherwise β (10) for some threshold θ. Such a schema may do a better job of managing how quickly the model moves away from the prior, particularly if there is a large difference in the respective sizes of the in-domain and out-of domain corpora. We leave the investigation of such approaches to future research. Before providing empirical results on the count merging and model interpolation approaches, we will introduce the parser and parsing models that were used. 3 Grammar and parser For the empirical trials, we used a top-down, left-to-right (incremental) statistical beam-search parser (Roark, 2001a; Roark, 2003). We refer readers to the cited papers for details on this parsing algorithm. Briefly, the parser maintains a set of candidate analyses, each of which is extended to attempt to incorporate the next word into a fully connected partial parse. As soon as “enough” candidate parses have been extended to the next word, all parses that have not yet attached the word are discarded, and the parser moves on to the next word. This beam search is parameterized with a base beam parameter γ, which controls how many or how few parses constitute “enough”. Candidate parses are ranked by a figure"
N03-1027,J03-4003,0,\N,Missing
N04-4006,W02-1001,0,0.0453598,"ckoff weight and h0 the backoff history for history h. The principal difficulty in MAP adaptation of this sort is determining the mixing parameters τh in Eq. 2. Following (Bacchiani and Roark, 2003), we chose a single mixing parameter for each model that we built, i.e. τh = τ for all states h in the model. 2.2 argmin (w[π] + λΦ(π) · α ¯t) π∈L where |wk |denotes the size of the sample wk . Then: p˜(w |h) = = Perceptron algorithm Our discriminative n-gram model training approach uses the perceptron algorithm, as presented in (Roark et al., 2004), which follows the general approach presented in (Collins, 2002). For brevity, we present the algorithm, not in full generality, but for the specific case of n-gram model training. The training set consists of N weighted word lattices produced by the baseline recognizer, and a gold-standard α ¯ t+1 [k] = α ¯ t [k] + Φ(ˆ πt )[k] − Φ(GL )[k] (5) Note that if π ˆt = GL , then the features are left unchanged. As shown in (Roark et al., 2004), the perceptron feature weight vector can be encoded in a deterministic weighted finite state automaton (FSA), so that much of the feature weight update involves basic FSA operations, making the training relatively efficie"
N06-1040,P99-1059,0,0.0141759,"icantly slowing down the parsing algoBrian Roark Center for Spoken Language Understanding OGI at Oregon Health & Science University 20000 NW Walker Road Beaverton, Oregon 97006 roark@cslu.ogi.edu rithm. In the case of bilexical grammars, where categories in binary grammars are annotated with their lexical heads, the grammar factor contributes an additional O(n2 |VD |3 ) complexity, leading to an overall O(n5 |VD |3 ) parsing complexity, where |VD |is the number of delexicalized non-terminals (Eisner, 1997). Even with special modifications to the basic CYK algorithm, such as those presented by Eisner and Satta (1999), improvements to the stochastic model are obtained at the expense of efficiency. In addition to the significant cost in efficiency, increasing the non-terminal set impacts parameter estimation for the stochastic model. With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created. Klein and Manning (2003b) showed that, by making certain linguistically-motivated node label annotations, but avoiding certain other kinds of sta"
N06-1040,P99-1066,0,0.0240649,"4, and 99, and rule production sets of 11659, 6354, and 3803, respectively. These reductions in the size of the non-terminal set from the original factored grammar result in an order of magnitude reduction in complexity of the CYK algorithm. One common strategy in statistical parsing is what can be termed an approximate coarse-to-fine approach: a simple PCFG is used to prune the search space to which richer and more complex models are applied subsequently (Charniak, 2000; Charniak and Johnson, 2005). Producing a “coarse” chart as efficiently as possible is thus crucial (Charniak et al., 1998; Blaheta and Charniak, 1999), making these factorizations particularly useful. 2.2 CYK parser and baselines To illustrate the importance of this reduction in nonterminals for efficient parsing, we will present baseline parsing results for a development set. For these baseline trials, we trained a PCFG on sections 2-21 of the Penn WSJ Treebank (40k sentences, 936k words), and evaluated on section 24 (1346 sentences, 32k words). The parser takes as input the weighted k-best POS-tag sequences of a final NNS depends on the preceding NN, despite the Markov order-0 factorization. Because of our focus on efficient CYK, we accep"
N06-1040,P05-1022,0,0.526135,"o 10105, with 23220 productions. With a Markov factorization of orders 2, 1 and 0 we get non-terminal sets of size 2492, 564, and 99, and rule production sets of 11659, 6354, and 3803, respectively. These reductions in the size of the non-terminal set from the original factored grammar result in an order of magnitude reduction in complexity of the CYK algorithm. One common strategy in statistical parsing is what can be termed an approximate coarse-to-fine approach: a simple PCFG is used to prune the search space to which richer and more complex models are applied subsequently (Charniak, 2000; Charniak and Johnson, 2005). Producing a “coarse” chart as efficiently as possible is thus crucial (Charniak et al., 1998; Blaheta and Charniak, 1999), making these factorizations particularly useful. 2.2 CYK parser and baselines To illustrate the importance of this reduction in nonterminals for efficient parsing, we will present baseline parsing results for a development set. For these baseline trials, we trained a PCFG on sections 2-21 of the Penn WSJ Treebank (40k sentences, 936k words), and evaluated on section 24 (1346 sentences, 32k words). The parser takes as input the weighted k-best POS-tag sequences of a final"
N06-1040,W98-1115,0,0.0204376,"l sets of size 2492, 564, and 99, and rule production sets of 11659, 6354, and 3803, respectively. These reductions in the size of the non-terminal set from the original factored grammar result in an order of magnitude reduction in complexity of the CYK algorithm. One common strategy in statistical parsing is what can be termed an approximate coarse-to-fine approach: a simple PCFG is used to prune the search space to which richer and more complex models are applied subsequently (Charniak, 2000; Charniak and Johnson, 2005). Producing a “coarse” chart as efficiently as possible is thus crucial (Charniak et al., 1998; Blaheta and Charniak, 1999), making these factorizations particularly useful. 2.2 CYK parser and baselines To illustrate the importance of this reduction in nonterminals for efficient parsing, we will present baseline parsing results for a development set. For these baseline trials, we trained a PCFG on sections 2-21 of the Penn WSJ Treebank (40k sentences, 936k words), and evaluated on section 24 (1346 sentences, 32k words). The parser takes as input the weighted k-best POS-tag sequences of a final NNS depends on the preceding NN, despite the Markov order-0 factorization. Because of our foc"
N06-1040,A00-2018,0,0.792666,"a start symbol S † ∈ V , and a set of production P of the form: A → α, where A ∈ V and α ∈ (V ∪ T )∗ . A PCFG is a CFG with a probability assigned to each production. Thus, the probabilities of the productions expanding a given non-terminal sum to one. 2.1 Smoothing and factorization PCFGs induced from the Penn Treebank have many productions with long sequences of non-terminals on the RHS. Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away 313 (Collins, 1997; Charniak, 2000): P(X → Y1 ...Yn )= P(Y1 |X) ≈ P(Y1 |X) n Y i=2 n Y P(Yi |X, Y1 · · · Yi−1 ) P(Yi |X, Yi−k · · · Yi−1 ). i=2 Making such a Markov assumption is closely related to grammar transformations required for certain efficient parsing algorithms. For example, the CYK parsing algorithm takes as input a Chomsky Normal Form PCFG, i.e., a grammar where all productions are of the form X → Y Z or X → a, where X, Y , and Z are non-terminals and a a terminal symbol.1 . Binarized PCFGs are induced from a treebank whose trees have been factored so that n-ary productions with n> 2 become sequences of n−1 binary p"
N06-1040,P97-1003,0,0.207917,"Missing"
N06-1040,1997.iwpt-1.10,0,0.0525901,"lly increase the number of grammar productions as well as the ambiguity of the grammar, thereby significantly slowing down the parsing algoBrian Roark Center for Spoken Language Understanding OGI at Oregon Health & Science University 20000 NW Walker Road Beaverton, Oregon 97006 roark@cslu.ogi.edu rithm. In the case of bilexical grammars, where categories in binary grammars are annotated with their lexical heads, the grammar factor contributes an additional O(n2 |VD |3 ) complexity, leading to an overall O(n5 |VD |3 ) parsing complexity, where |VD |is the number of delexicalized non-terminals (Eisner, 1997). Even with special modifications to the basic CYK algorithm, such as those presented by Eisner and Satta (1999), improvements to the stochastic model are obtained at the expense of efficiency. In addition to the significant cost in efficiency, increasing the non-terminal set impacts parameter estimation for the stochastic model. With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created. Klein and Manning (2003b) showed"
N06-1040,H05-1099,1,0.820322,"l present baseline parsing results for a development set. For these baseline trials, we trained a PCFG on sections 2-21 of the Penn WSJ Treebank (40k sentences, 936k words), and evaluated on section 24 (1346 sentences, 32k words). The parser takes as input the weighted k-best POS-tag sequences of a final NNS depends on the preceding NN, despite the Markov order-0 factorization. Because of our focus on efficient CYK, we accept these higher order dependencies rather than producing unary productions. Only n-ary rules n>2 are factored. 314 perceptron-trained tagger, using the tagger documented in Hollingshead et al. (2005). The number of tagger candidates k for all trials reported in this paper was 0.2n, where n is the length of the string. From the weighted k-best list, we derive a conditional probability of each tag at position i by taking the sum of the exponential of the weights of all candidates with that tag at position i (softmax). The parser is an exhaustive CYK parser that takes advantage of the fact that, with the grammar factorization method described, factored non-terminals can only occur as the second child of a binary production. Since the bulk of the non-terminals result from factorization, this"
N06-1040,W05-1506,0,0.0374458,"the best performance is reached with both relatively few factored nonterminal splits, and a relatively small efficiency impact. The non-factored splits provide substantial accuracy improvements at relatively small efficiency cost. Table 3 shows the 1-best and reranked 50-best results for the baseline Markov order-2 model, and the best-performing model using factored and nonfactored non-terminal splits. We present the efficiency of the model in terms of words-per-second over the entire dev set, including the longer strings (maximum length 116 words)5 . We used the k-best decoding algorithm of Huang and Chiang (2005) with our CYK parser, using on-demand k-best backpointer calculation. We then trained a MaxEnt reranker on sections 2-21, using the approach outlined in Charniak and Johnson (2005), via the publicly available reranking code from that paper.6 We used the default features that come with that package. The processing time in the table includes the time to parse and rerank. As can be seen from the trials, there is some overhead to these processes, but the time is still dominated by the base parsing. We present the k-best results to demonstrate the benefits of using a better model, such as the one w"
N06-1040,J98-4004,0,0.149294,"severe speed vs. accuracy tradeoff in stochastic context-free parsing, which can be explained by the grammar factor in the running-time complexity of standard parsing algorithms such as the CYK algorithm (Kasami, 1965; Younger, 1967). That algorithm has complexity O(n3 |P |), where n is the length in words of the sentence parsed, and |P |is the number of grammar productions. Grammar nonterminals can be split to encode richer dependencies in a stochastic model and improve parsing accuracy. For example, the parent of the left-hand side (LHS) can be annotated onto the label of the LHS category (Johnson, 1998), hence differentiating, for instance, between expansions of a VP with parent S and parent VP. Such annotations, however, tend to substantially increase the number of grammar productions as well as the ambiguity of the grammar, thereby significantly slowing down the parsing algoBrian Roark Center for Spoken Language Understanding OGI at Oregon Health & Science University 20000 NW Walker Road Beaverton, Oregon 97006 roark@cslu.ogi.edu rithm. In the case of bilexical grammars, where categories in binary grammars are annotated with their lexical heads, the grammar factor contributes an additional"
N06-1040,N03-1016,0,0.462596,"ized non-terminals (Eisner, 1997). Even with special modifications to the basic CYK algorithm, such as those presented by Eisner and Satta (1999), improvements to the stochastic model are obtained at the expense of efficiency. In addition to the significant cost in efficiency, increasing the non-terminal set impacts parameter estimation for the stochastic model. With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created. Klein and Manning (2003b) showed that, by making certain linguistically-motivated node label annotations, but avoiding certain other kinds of state splits (mainly lexical annotations) models of relatively high accuracy can be built without resorting to smoothing. The resulting grammars were small enough to allow for exhaustive CYK parsing; even so, parsing speed was significantly impacted by the state splits: the test-set parsing time reported was about 3s for average length sentences, with a memory usage of 1GB. This paper presents an automatic method for deciding which state to split in order to create concise and"
N06-1040,P03-1054,0,0.28229,"ized non-terminals (Eisner, 1997). Even with special modifications to the basic CYK algorithm, such as those presented by Eisner and Satta (1999), improvements to the stochastic model are obtained at the expense of efficiency. In addition to the significant cost in efficiency, increasing the non-terminal set impacts parameter estimation for the stochastic model. With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created. Klein and Manning (2003b) showed that, by making certain linguistically-motivated node label annotations, but avoiding certain other kinds of state splits (mainly lexical annotations) models of relatively high accuracy can be built without resorting to smoothing. The resulting grammars were small enough to allow for exhaustive CYK parsing; even so, parsing speed was significantly impacted by the state splits: the test-set parsing time reported was about 3s for average length sentences, with a memory usage of 1GB. This paper presents an automatic method for deciding which state to split in order to create concise and"
N06-1040,J93-2004,0,0.0267434,"combination with an existing reranker to provide competitive WSJ parsing results. The remainder of the paper is structured as follows. Section 2 gives a brief description of PCFG induction from treebanks, including non-terminal label-splitting, factorization, and relative frequency estimation. Section 3 discusses the statistical criteria that we explored to determine structural zeros and thus select non-terminals for the factored PCFG. Finally, Section 4 reports the results of parsing experiments using our exhaustive k-best CYK parser with the concise PCFGs induced from the Penn WSJ treebank (Marcus et al., 1993). 2 Grammar induction A context-free grammar G = (V, T, S † , P ), or CFG in short, consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , and a set of production P of the form: A → α, where A ∈ V and α ∈ (V ∪ T )∗ . A PCFG is a CFG with a probability assigned to each production. Thus, the probabilities of the productions expanding a given non-terminal sum to one. 2.1 Smoothing and factorization PCFGs induced from the Penn Treebank have many productions with long sequences of non-terminals on the RHS. Probability estimates of the RHS given the LHS"
N09-1073,P99-1066,0,0.0360181,", using the default search parameterization of the Charniak parser, the Roark and Hollingshead (2008) results demonstrated no parser speedup using the techniques, rather an accuracy improvement, which we attributed to a better use of the amount of search permitted by that default parameterization. We only demonstrated efficiency improvements by reducing the amount of search via the Charniak search parameterization. There we showed a nice speedup of the parser versus the default, while maintaining accuracy levels. However, internal heuristics of the Charniak search, such as attention shifting (Blaheta and Charniak, 1999; Hall and Johnson, 2004), can make this accuracy/efficiency tradeoff somewhat difficult to interpret. Furthermore, one might ask whether O(N 2 ) complexity is as good as can be achieved through the paradigm of using finite-state constraints to close chart cells. What methods of constraint would be required to achieve O(N log N ) or linear complexHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 647–655, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ity? Would such constraints degrade performance, or can the"
N09-1073,A00-2018,0,0.602033,"guage Understanding Division of Biomedical Computer Science Oregon Health & Science University {roark,hollingk}@cslu.ogi.edu Abstract that these techniques can improve pipeline efficiency for well-known parsing tasks. In contrast, in Roark and Hollingshead (2008), we derived and applied the finite-state constraints so as to guarantee a reduction in the worst-case complexity of the context-free parsing pipeline from O(N 3 ) in the length of the string N to O(N 2 ) by closing chart cells to entries. We demonstrated the application of such constraints to the well-known Charniak parsing pipeline (Charniak, 2000), which resulted in no accuracy loss when the constraints were applied. In this paper, we extend methods from Roark and Hollingshead (2008) for reducing the worst-case complexity of a context-free parsing pipeline via hard constraints derived from finite-state tagging pre-processing. Methods from our previous paper achieved quadratic worst-case complexity. We prove here that alternate methods for choosing constraints can achieve either linear or O(N log2 N ) complexity. These worst-case bounds on processing are demonstrated to be achieved without reducing the parsing accuracy, in fact in some"
N09-1073,W07-2206,0,0.134293,"formance by combining complexity bounding methods with additional high precision constraints. 1 Introduction Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. For example, the well-known Ratnaparkhi parser (Ratnaparkhi, 1999) used a finite-state POS-tagger and NPchunker to reduce the search space for his Maximum Entropy parsing model, and achieved linear observed-time performance. Other recent examples of the utility of finite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic et al. (2007), Hollingshead and Roark (2007), and Roark and Hollingshead (2008). Note that by making use of constraints derived from preprocessing, they are no longer performing full exact inference—these are approximate inference methods, as are the methods presented in this paper. Most of these parsing pipeline papers show empirically 647 While it is important to demonstrate that these sorts of complexity-reducing chart constraints do not interfere with the operation of high-accuracy, stateof-the-art parsing approaches, existing pruning techniques used within such parsers can obscure the impact of these"
N09-1073,P06-2038,0,0.062271,"y, we demonstrate improved performance by combining complexity bounding methods with additional high precision constraints. 1 Introduction Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. For example, the well-known Ratnaparkhi parser (Ratnaparkhi, 1999) used a finite-state POS-tagger and NPchunker to reduce the search space for his Maximum Entropy parsing model, and achieved linear observed-time performance. Other recent examples of the utility of finite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic et al. (2007), Hollingshead and Roark (2007), and Roark and Hollingshead (2008). Note that by making use of constraints derived from preprocessing, they are no longer performing full exact inference—these are approximate inference methods, as are the methods presented in this paper. Most of these parsing pipeline papers show empirically 647 While it is important to demonstrate that these sorts of complexity-reducing chart constraints do not interfere with the operation of high-accuracy, stateof-the-art parsing approaches, existing pruning techniques used within such parsers can ob"
N09-1073,P04-1006,0,0.0137102,"arameterization of the Charniak parser, the Roark and Hollingshead (2008) results demonstrated no parser speedup using the techniques, rather an accuracy improvement, which we attributed to a better use of the amount of search permitted by that default parameterization. We only demonstrated efficiency improvements by reducing the amount of search via the Charniak search parameterization. There we showed a nice speedup of the parser versus the default, while maintaining accuracy levels. However, internal heuristics of the Charniak search, such as attention shifting (Blaheta and Charniak, 1999; Hall and Johnson, 2004), can make this accuracy/efficiency tradeoff somewhat difficult to interpret. Furthermore, one might ask whether O(N 2 ) complexity is as good as can be achieved through the paradigm of using finite-state constraints to close chart cells. What methods of constraint would be required to achieve O(N log N ) or linear complexHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 647–655, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ity? Would such constraints degrade performance, or can the finite-state models be a"
N09-1073,P07-1120,1,0.858641,"lexity bounding methods with additional high precision constraints. 1 Introduction Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. For example, the well-known Ratnaparkhi parser (Ratnaparkhi, 1999) used a finite-state POS-tagger and NPchunker to reduce the search space for his Maximum Entropy parsing model, and achieved linear observed-time performance. Other recent examples of the utility of finite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic et al. (2007), Hollingshead and Roark (2007), and Roark and Hollingshead (2008). Note that by making use of constraints derived from preprocessing, they are no longer performing full exact inference—these are approximate inference methods, as are the methods presented in this paper. Most of these parsing pipeline papers show empirically 647 While it is important to demonstrate that these sorts of complexity-reducing chart constraints do not interfere with the operation of high-accuracy, stateof-the-art parsing approaches, existing pruning techniques used within such parsers can obscure the impact of these constraints on search. For exam"
N09-1073,H05-1099,1,0.89795,"se they involve closed children cells. 4 4.1 Constraint Selection High Precision vs Complexity Bounding The chart constraints that are extracted from the finite-state tagger come in the form of set exclusions, e.g., d 6∈ E. Rather than selecting constraints from the single, best-scoring tag sequence output by the tagger, we instead rely on the whole distribution over possible tag strings to select constraints. We have two separate tagging tasks, each with two possible tags of each word wi in each string: (1) B or ¬B; and (2) E or ¬E, where ¬X signifies that wi 6∈ X for X ∈ {B, E}. The tagger (Hollingshead et al., 2005) uses log linear models trained with the perceptron algorithm, and derives, via the forward-backward algorithm, the posterior probability of each of the two tags at each word, so that Pr(B) + Pr(¬B) = 1. Then, for every word wi in the string, the tags B and E are associated with a posterior probability that gives us a score for wi ∈ B and wi ∈ E. All possible set memberships wi ∈ X in the string can be ranked by this score. From this ranking, a decision boundary can be set, such that all word/set pairs wi ∈ B or wj ∈ E with abovethreshold probability are accepted, and all pairs below threshold"
N09-1073,J93-2004,0,0.0318696,"urve. However, as mentioned earlier, the existing complicated system of search heuristics in the Charniak parser makes interpretation of the results more difficult. What can be said from the previous results is that constraining parsers in this way can improve performance of even the highest accuracy parsers. Yet those results do not provide much of an indication of how performance is impacted for general context-free inference. For this paper, we use an exact inference (exhaustive search) CYK parser, using a simple probabilistic context-free grammar (PCFG) induced from the Penn WSJ Treebank (Marcus et al., 1993). The PCFG is transformed to Chomsky Normal Form through right-factorization, and is smoothed with a Markov (order-2) transform. Thus a production such as Z → Y X W V becomes three rules: (1) Z → Y Z:X+W ; (2) Z:X+W → X Z:W +V ; and (3) Z:W +V → W V . Note that only two child categories are encoded within the new factored categories, instead of all of the remaining children as in our previous factorization example. This so-called ‘Markov’ grammar provides some smoothing of the PCFG; the resulting grammar is also smoothed using lower order Markov grammars. We trained on sections 2-21 of the tre"
N09-1073,C08-1094,1,0.484397,"Abstract that these techniques can improve pipeline efficiency for well-known parsing tasks. In contrast, in Roark and Hollingshead (2008), we derived and applied the finite-state constraints so as to guarantee a reduction in the worst-case complexity of the context-free parsing pipeline from O(N 3 ) in the length of the string N to O(N 2 ) by closing chart cells to entries. We demonstrated the application of such constraints to the well-known Charniak parsing pipeline (Charniak, 2000), which resulted in no accuracy loss when the constraints were applied. In this paper, we extend methods from Roark and Hollingshead (2008) for reducing the worst-case complexity of a context-free parsing pipeline via hard constraints derived from finite-state tagging pre-processing. Methods from our previous paper achieved quadratic worst-case complexity. We prove here that alternate methods for choosing constraints can achieve either linear or O(N log2 N ) complexity. These worst-case bounds on processing are demonstrated to be achieved without reducing the parsing accuracy, in fact in some cases improving the accuracy. The new methods achieve observed performance comparable to the previously published quadratic complexity meth"
N10-1085,W02-1022,0,0.0194585,"tational resources, work on automatically ordering prenominal modifiers In this paper, we introduce a novel approach to prenominal modifier ordering adapted from multiple sequence alignment (MSA) techniques used in computational biology. MSA is generally applied to DNA, RNA, and protein sequences, aligning three or more biological sequences in order to determine, for example, common ancestry (Durbin et al., 1999; Gusfield, 1997; Carrillo and Lipman, 1988). MSA techniques have not been widely applied in NLP, but have produced some promising results for building a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006). We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers. Our technique utilizes simple features within the raw text, and does not require any semantic information. We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) and higher recall and F-measure than that report"
N10-1085,N03-1003,0,0.0282148,"ly ordering prenominal modifiers In this paper, we introduce a novel approach to prenominal modifier ordering adapted from multiple sequence alignment (MSA) techniques used in computational biology. MSA is generally applied to DNA, RNA, and protein sequences, aligning three or more biological sequences in order to determine, for example, common ancestry (Durbin et al., 1999; Gusfield, 1997; Carrillo and Lipman, 1988). MSA techniques have not been widely applied in NLP, but have produced some promising results for building a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006). We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers. Our technique utilizes simple features within the raw text, and does not require any semantic information. We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) and higher recall and F-measure than that reported in Mitchell (2009) when tested on th"
N10-1085,W05-1009,0,0.0128009,"and achieved performance competitive with — and in many cases, superior to — the best results previously reported. In our current work, we have focused on relatively simple features, which should be adaptable to other languages without expensive resources or much linguistic insight. We are interested in exploring richer sources of features for ordering information. We found simple morphological features provided discriminative clues for otherwise ambiguous instances, and believe that richer morphological features might be helpful even in a language as morphologically impoverished as English. Boleda et al. (2005) achieved promising preliminary results using morphology for classifying adjectives in Catalan. Further, we might be able to capture some of the semantic relationships noted by psychological analyses (Ziff, 1960; Martin, 1969) by labeling words which belong to known semantic classes (e.g., colors, size denominators, etc.). We intend to explore deriving such labels from resources such as WordNet or OntoNotes. We also plan to continue exploration of MSA training methods. We see considerable room for refinement in generative MSA models; our maximum likelihood training provides a strong starting p"
N10-1085,W02-1001,0,0.0322134,"Missing"
N10-1085,P00-1012,0,0.78311,"results for building a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006). We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers. Our technique utilizes simple features within the raw text, and does not require any semantic information. We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) and higher recall and F-measure than that reported in Mitchell (2009) when tested on the same corpus. 600 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600–608, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Related work In one of the first attempts at automatically ordering prenominal modifiers, Shaw and Hatzivassiloglou (1999) present three empirical methods to order a variety of prenominal modifier types. Their approach provides ordering decisions for adjectives, gerunds (such"
N10-1085,W09-0608,1,0.657872,"uilding a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006). We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers. Our technique utilizes simple features within the raw text, and does not require any semantic information. We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) and higher recall and F-measure than that reported in Mitchell (2009) when tested on the same corpus. 600 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600–608, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Related work In one of the first attempts at automatically ordering prenominal modifiers, Shaw and Hatzivassiloglou (1999) present three empirical methods to order a variety of prenominal modifier types. Their approach provides ordering decisions for adjectives, gerunds (such as “running” in"
N10-1085,P99-1018,0,0.481204,"but have produced some promising results for building a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006). We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers. Our technique utilizes simple features within the raw text, and does not require any semantic information. We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) and higher recall and F-measure than that reported in Mitchell (2009) when tested on the same corpus. 600 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600–608, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Related work In one of the first attempts at automatically ordering prenominal modifiers, Shaw and Hatzivassiloglou (1999) present three empirical methods to order a variety of prenominal modifier types. Their approach provides ordering decisions for adjectives,"
N13-1021,N06-1014,0,0.0726746,"ow sympathetic and made a collection for her so that she can feed the children. Figure 2: An example retelling with 12 recalled story elements. 3 Unsupervised generative automated scoring with word alignment In previous work (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), we developed a pipeline for automatically scoring narrative retellings for the WLM task. The utterances corresponding to a retelling were recognized using an ASR system. The story elements were identified from the 1-best ASR transcript using word alignments produced by the Berkeley aligner (Liang et al., 2006), an EM-based word alignment package developed to align parallel texts for machine translation. The word alignment model was estimated in an unsupervised manner from a parallel corpus consisting of source narrative and manual transcripts of retellings from a small set of training subjects, and from a pairwise parallel corpus of manual retelling transcripts. During inference or test, the ASR transcripts of the retellings were aligned using the estimated alignment model to the source narrative text. If a word in the retelling was mapped by the alignment model to a content word in the source narr"
N13-1021,W12-2401,1,0.57847,"Missing"
N13-1021,W09-1119,0,0.017445,"xEnt models benefit from incorporating this extra information. The MaxEnt models improve their performance substantially for all three training scenarios, while the gains for the CRF models are more modest, especially for the unsupervised approach where the performance degrades or does not change much, since some context information is already captured by the Markov order 1 features. BIO tagset As detailed in Section 4.1, story elements sometimes span multiple words, so for the CRF models we investigated two different schemes for tagging, following typical practice in named entity extraction (Ratinov and Roth, 2009) and syntactic chunking (Sha and Pereira, 2003). The BIO tagging scheme makes the distinction between the tokens from the story elements that are in the beginning from the ones that are not. The O tag is assigned to the tokens that do not belong to any of the story elements. The IO tagging uses a single tag for the tokens that fall in the same story element, which is the approach we have followed so far. In addition to presenting results using context dependent features, Table 5 presents results with the BIO tagset. For the supervised and hybrid approaches, the BIO tagging provides insignifica"
N13-1021,N03-1028,0,0.0382238,"a information. The MaxEnt models improve their performance substantially for all three training scenarios, while the gains for the CRF models are more modest, especially for the unsupervised approach where the performance degrades or does not change much, since some context information is already captured by the Markov order 1 features. BIO tagset As detailed in Section 4.1, story elements sometimes span multiple words, so for the CRF models we investigated two different schemes for tagging, following typical practice in named entity extraction (Ratinov and Roth, 2009) and syntactic chunking (Sha and Pereira, 2003). The BIO tagging scheme makes the distinction between the tokens from the story elements that are in the beginning from the ones that are not. The O tag is assigned to the tokens that do not belong to any of the story elements. The IO tagging uses a single tag for the tokens that fall in the same story element, which is the approach we have followed so far. In addition to presenting results using context dependent features, Table 5 presents results with the BIO tagset. For the supervised and hybrid approaches, the BIO tagging provides insignificant but consistent gains for most of the scenari"
N13-1021,N12-1011,0,0.0217648,"r using either approach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to relate these elements using diverse words or phrases, and an automated method of identifying these elements needs to model the permissible variants and paraphrasings. In previous work (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), we developed models based on automatic word-alignment methods, as"
N13-1084,J93-1003,0,0.365078,"Missing"
N18-2085,K17-1003,0,0.0498703,"n languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural language models can effectively learn complex dependencies, even in openvocabulary settings (Hwang and Sung, 2017; Kawakami et al., 2017). Whether the models are able to learn particular syntactic interactions is an intriguing question, and some methodologies have been presented to tease apart under what circumstances variously-trained models encode attested interactions (Linzen et al., 2016; Enguehard et al., 2017). While the sort of detailed, constructionspecific analyses in these papers is surely informative, our evaluation is language-wide. MT researchers have investigated whether an English sentence contains enough information to predict the fine-grained inflections used in its foreignlanguage translations (see Kirov et al., 2017). 7 Conclusion We have presented a clean method for the crosslinguistic comparison of language modeling: We assess whether a language modeling technique can compress a sentence and its translations equally well. We show an interesting correlation between the morphological r"
N18-2085,P82-1020,0,0.743223,"Missing"
N18-2085,N12-1043,0,0.119,"Missing"
N18-2085,P17-1137,0,0.266302,"se they appear in words that were (arbitrarily) designated as OOV in that language. Such models are known as “open-vocabulary” LMs. Notation. Let ∪˙ denote disjoint union, i.e., A ∪˙ B = C iff A ∪ B = C and A ∩ B = ∅. Let Σ be a discrete alphabet of characters, including a distinguished unknown-character symbol ?.2 A charQ|c|+1 acter LM then defines p(c) = i=1 p(ci |c&lt;i ), where we take c|c|+1 to be a distinguished end-ofstring symbol EOS. In this work, we consider two open-vocabulary LMs, as follows. LSTM LM. While neural language models can also take a hybrid approach (Hwang and Sung, 2017; Kawakami et al., 2017), recent advances indicate that full character-level modeling is now competitive with word-level modeling. A large part of this is due to the use of recurrent neural networks (Mikolov et al., 2010), which can generalize about Baseline n-gram LM. We train “flat” hybrid word/character open-vocabulary n-gram models (Bisani and Ney, 2005), defined over strings Σ+ 2 The set of graphemes in these languages can be assumed to be closed, but external graphemes may on rare occasion appear in random text samples. These are rare enough to not materially affect the metrics. 3 The model can be extended to h"
N18-2085,L18-1293,1,0.78621,"Missing"
N18-2085,sproat-etal-2014-database,0,0.0190663,"95 nl 0.90 de et hu lt sk 0.85 ro en sv da 0.80 pt pl cs el lv es sl bg it fi fr 0.9 1.0 BPEC (LSTM over forms) 1.1 Figure 2: Each dot is a language, and its coordinates are the BPEC values for the LSTM LMs over words and lemmata. The top and right margins show kernel density estimates of these two sets of BPEC values. All dots follow the blue regression, but stay below the green line (y = x), and the darker dots—which represent languages with higher counting complexity—tend to fall toward the right but not toward the top, since counting complexity is correlated only with the BPEC over words. Sproat et al. (2014) present a corpus of close translations of sentences in typologically diverse languages along with detailed morphosyntactic and morphosemantic annotations, as the means for assessing linguistic complexity for comparable messages, though they expressly do not take an information-theoretic approach to measuring complexity. In the linguistics literature, McWhorter (2001) argues that certain languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural languag"
N18-2085,L16-1680,0,0.050938,"Missing"
N18-2085,E17-2018,1,0.880447,"Missing"
N18-2085,2005.mtsummit-papers.11,0,0.0633624,"anking under BPEC shows that the LSTM has the easiest time modeling English itself. Scandinavian languages Danish and Swedish have BPEC closest to English; these languages are typologically and genetically similar to English. Experiments and Results n-gram versus LSTM. As expected, the LSTM outperforms the baseline n-gram models across the board. In addition, however, n-gram modeling yields relatively poor performance on some languages, such as Dutch, with only modestly more complex inflectional morphology than English. Our experiments are conducted on the 21 languages of the Europarl corpus (Koehn, 2005). The corpus consists of utterances made in the European parliament and are aligned cross-linguistically by a unique utterance id. With the exceptions (noted in Table 1) of Finnish, Hungarian and Estonian, which are Uralic, the languages are Indo-European. 7 4 Characters appearing &lt; 100 times in train are ?. Other phenomena—e.g., perhaps, compounding— may also be poorly modeled by n-grams. The Impact of Inflectional Morphology. Another major take-away is that rich inflectional morphology is a difficulty for both n-gram and LSTM LMs. In this section we give numbers for the LSTMs. Studying Fig."
P02-1037,J01-2004,1,\N,Missing
P02-1037,P01-1017,0,\N,Missing
P03-1006,J94-3001,0,\N,Missing
P03-1006,J92-4003,0,\N,Missing
P04-1007,P03-1006,1,0.526898,"atures Φi for i = 1 . . . d implemented by D. The second concerns the choice of parameters αi for i = 0 . . . d which assign weights to the n-gram features as well as the baseline feature Φ0 . Before describing methods for training a discriminative language model using perceptron and CRF algorithms, we give a little more detail about the structure of D, focusing on how n-gram language models can be implemented with finite-state techniques. 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions (Allauzen et al., 2003). Every string accepted by such an automaton has a single path through the automaton, and the weight of the string is the sum of the weights of the transitions in that path. In such a representation, every state in the automaton represents an n-gram history h, e.g. wi−2 wi−1 , and there are transitions leaving the state for every word wi such that the feature hwi has a weight. There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state. This failure transit"
P04-1007,W02-1001,1,0.422692,"eter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach. In the next section of the paper we describe how global linear models can be applied to speech recognition. In particular, we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques. 2.1 Global linear models We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y. We assume the following components: (1) Training examples (xi , yi ) for i = 1 . . . N . (2) A function GEN which enumerates a set of candidates GEN(x) for an input x. (3) A representation Φ mapping each (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . (4) A parameter vector α ¯ ∈ Rd . The components GEN, Φ and α ¯ define a mapping from an input x to an output F (x) through F (x) = argmax Φ(x, y) · α ¯ (1) y∈GEN(x) P where Φ(x, y) · α ¯ is the inner product s αs Φs (x, y). The learning task is to set the parameter val"
P04-1007,P99-1069,1,0.599285,"training algorithm in decoding heldout and test examples in our experiments. Say α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We no"
P04-1007,W02-2018,0,0.0613897,"ctive function: LLR (α) ¯ = N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ − i=1 ||α|| ¯ 2 2σ 2 (3) The value σ dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are α ¯ ∗ = argmaxα¯ LLR (¯ α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR . There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α ¯ as input, and in turn returns LLR (¯ α) as well as the gradient of LLR at α ¯ . The derivative of the objective function with respect to a parameter αs at parameter values α ¯ is   N X X ∂LLR αs Φs (xi , yi ) − = pα ¯ (y|xi )Φs (xi , y)− 2 ∂αs σ i=1 (4) y∈GEN(xi ) Note that LLR (¯ α) is a convex function, so that there is a globally optimal solution and the optimization method 2 will find it. The use of the Gaussian prior term ||¯ α ||/2σ 2 in the objective fun"
P04-1007,W03-0430,0,0.0804031,"r vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of training examples 2 Note"
P04-1007,N03-1028,0,0.0263195,"α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of tr"
P04-1015,W02-1001,1,0.645823,"al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). In this approach the training and decoding problems are very closely related – the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative mod"
P04-1015,P02-1036,0,0.018976,"Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1 Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm"
P04-1015,C00-1052,1,0.543517,"gh a tree transform; next, we limit the left-child chains consisting of more than two non-terminal categories to those actually observed in the training data more than once. Left-child chains of length less than or equal to two are all those observed in training data. As a practical matter, the set of leftchild chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech (POS) tags T for x. Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A → Aγ. The transformed trees look as in figure 3. The transform has the benefit of dramatically reducing the number of left-child chains, without unduly disrupting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2"
P04-1015,P99-1069,0,0.0137957,"pproach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate “arbitrary” global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N"
P04-1015,J98-4004,0,0.00832404,"Missing"
P04-1015,P02-1035,0,0.00810474,"tly proposed methods allow a model to incorporate “arbitrary” global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1 Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existi"
P04-1015,J01-2004,1,0.690269,"this approach the training and decoding problems are very closely related – the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. We also describe several refinements to the training algorithm, and demonstrate their impact on convergence properties of the method. Finally, we describe training the perceptron model with the negative log probability given by the generative model as another feature. This provides the perceptron algorithm with a better starting point, leading to large improve"
P04-1015,J97-4005,0,\N,Missing
P04-1015,J05-1003,1,\N,Missing
P04-1015,P02-1034,1,\N,Missing
P05-1063,P01-1017,0,0.142246,"channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably m"
P05-1063,P98-1035,0,0.216145,"4; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tag"
P05-1063,W02-1001,1,0.587262,"d-out data. In (2) Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector α ¯ using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jeline"
P05-1063,J91-3004,0,0.0204762,"Missing"
P05-1063,P99-1069,0,0.0134391,"ion of this input. We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is X X L(¯ α) = log P (si |ai , α) ¯ −C αj2 (4) i where P (si |ai , α ¯) = Input: A parameter specifying the number of iterations over the training set, T . A value for the first parameter, α. A feature-vector representation Φ(a, w) ∈ Rd . Training examples (ai , wi ) for i = 1 . . . m. An n-best list GEN(ai ) for each training utterance. We take si to be the member of GEN(ai ) which has the lowest WER when compared to wi . Initialization: Set α1 = α, and αj = 0 for j = 2 . . . d. Algorithm: For t = 1 . . . T"
P05-1063,P04-1007,1,0.593855,", Φ(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w). The optimal string under our new model is defined as w∗ = arg max (β log Pl (w) + hα, ¯ Φ(a, w)i+ w log Pa (a|w)) where the arg max is taken over all strings in the 1000-best list, and where α ¯ ∈ Rd is a parameter vector specifying the “weight” for each feature in Φ (note that we define hx, yi to be the inner, or dot 1 where β &gt; 0 is some value that reflects the relative importance of the language model; β is typically chosen by optimization on held-out data. In (2) Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector α ¯ using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a ve"
P05-1063,J01-2004,1,0.898369,"hods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the mo"
P05-1063,N03-1028,0,0.0152592,"αi ¯ . e w∈GEN(a ) i Here, each si is the member of GEN(ai ) which has lowest WER with respect to the target transcription wi . The first term in L(¯ α) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parameter values. C is a constant that dictates the relative weighting given to the two terms. The optimal parameters are defined as α ¯ ∗ = arg max L(¯ α) α ¯ We refer to these models as global conditional loglinear models (GCLMs). Each of these algorithms has advantages. A number of results—e.g., in Sha and Pereira (2003) and Roark et al. (2004b)—suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method. However the perceptron converges very quickly, often in just a few passes over the training set—in comparison GCLM’s can take tens or hundreds of gradient calculations before convergence. In addition, the perceptron can be used as an effective feature selection technique, in that 510 at each training example it only increments features seen on si or yi , effectively ignoring all other features seen on members of GEN(ai ). For example, in the experiments in Roark et al"
P05-1063,P94-1011,0,0.0172554,"ion 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is con"
P05-1063,J95-2002,0,0.0243637,"Missing"
P05-1063,W02-1031,0,0.409897,"ry similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modifi"
P05-1063,P02-1025,0,0.169167,"g method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit"
P05-1063,W03-1021,0,0.0551928,"tice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic depende"
P05-1063,J03-4003,1,\N,Missing
P05-1063,C98-1035,0,\N,Missing
P05-1063,N04-1021,0,\N,Missing
P06-1021,N01-1016,0,0.406874,"rally lack appropriate rules for analyzing these constructions. One possible response to this mismatch between grammatical resources and the brute facts of disfluent speech is to make one look more like the other, for the purpose of parsing. In this separate-processing approach, reparanda are located through a variety of acoustic, lexical or string-based techniques, then excised before submission to a parser (Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Spilker et al., 2000; Johnson and Charniak, 2004). The resulting parse tree then has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in reco"
P06-1021,A00-2018,0,0.0169867,"ation × √ both × √ EDIT F none Parseval F Break index POST (Ratnaparkhi, 1996) which was itself trained on Switchboard. Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI ‘p’ symbol higher than the threshold value of 0.75. Annotation speech repairs. The first two use the CYK algorithm to find the most likely parse tree on a grammar read-off from example trees annotated as in Figures 2 and 4. The third experiment measures the benefit from syntactic indicators alone in Charniak’s lexicalized parser (Charniak, 2000). The tables in subsections 4.1, 4.2, and 4.3 summarize the accuracy of output parse trees on two measures. One is the standard Parseval F-measure, which tracks the precision and recall for all labeled constituents as compared to a gold-standard parse. The other measure, EDIT-finding F, restricts consideration to just constituents that are reparanda. It measures the per-word performance identifying a word as dominated by EDITED or not. As in previous studies, reference transcripts were used in all √ cases. A check ( ) indicates an experiment where prosodic breaks where automatically inferred b"
P06-1021,P99-1053,0,0.591037,"way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs continue a syntactic category that was left unfi"
P06-1021,N04-1011,0,0.0312862,"Missing"
P06-1021,H05-1030,1,0.8458,"ecision and recall trade-off on its detection can be adjusted using a threshold on the posterior probability of predicting “p”, as shown in Figure 3. In essence, the large number of acoustic and prosodic features related to disfluency are encoded via the ToBI label ‘p’, and provided as additional observations to the PCFG. This is unlike previous work on incorporating prosodic information (Gre0.6 0.5 Probability of Miss 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 Probability of False Alarm 0.4 0.5 0.6 Figure 3: DET curve for detecting disfluent breaks from acoustics. gory et al., 2004; Lease et al., 2005; Kahn et al., 2005) as described further in Section 6. 3 Syntactic parallelism The other striking property of speech repairs is their parallel character: subsequent repair regions ‘line up’ with preceding reparandum regions. This property can be harnessed to better estimate the length of the reparandum by considering parallelism from the perspective of syntax. For instance, in Figure 4(a) the unfinished reparandum noun phrase is repaired by another noun phrase – the syntactic categories are parallel. 3.1 Levelt’s WFR and Conjunction The idea that the reparandum is syntactically parallel to the repair can be trac"
P06-1021,J93-2004,0,0.0291288,"ion of Levelt’s WFR can be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements"
P06-1021,C00-2169,0,0.0526183,"Missing"
P06-1021,H94-1020,0,0.0752404,"an be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements on a grammar • distin"
P06-1021,J83-3003,0,0.415711,"has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs conti"
P06-1021,W05-1519,0,0.0261845,"Missing"
P06-1021,H91-1073,0,0.273826,"onstituents labeled EDITED. Such NP NP NP Prosodic disjuncture Everyday experience as well as acoustic analysis suggests that the syntactic interruption in speech repairs is typically accompanied by a change in prosody (Nakatani and Hirschberg, 1994; Shriberg, 1994). For instance, the spectrogram corresponding to example (2), shown in Figure 1, (2) DT NNP the jehovah NNP POS witness EDITED CC NP CC−BRK or NNPS or~+ mormons ’s Figure 2: Propagating BRK, the evidence of disfluent juncture, from acoustics to syntax. disjuncture symbols are identified in the ToBI labeling scheme as break indices (Price et al., 1991; Silverman et al., 1992). The availability of a corpus annotated with ToBI labels makes it possible to design a break index classifier via supervised training. The corpus is a subset of the Switchboard corpus, consisting of sixty-four telephone conversations manually annotated by an experienced linguist according to a simplified ToBI labeling scheme (Ostendorf et al., 2001). In ToBI, degree of disjuncture is indicated by integer values from 0 to 4, where a value of 0 corresponds to clitic and 4 to a major phrase break. In addition, a suffix p denotes perceptually disfluent events reflecting,"
P06-1021,W96-0213,0,\N,Missing
P06-1021,P83-1019,0,\N,Missing
P06-1021,J99-4003,0,\N,Missing
P06-1021,P04-1005,0,\N,Missing
P07-1062,H05-1099,1,0.871451,"Missing"
P07-1062,W98-1124,0,0.00996526,"c sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “"
P07-1062,J93-2004,0,0.0288444,"ucture annotation, this represents a large step forward in our ability to 489 automatically parse the discourse structure of text, whatever annotation approach we choose. 2 Methods 2.1 Data For our experiments we use the Rhetorical Structure Theory Discourse Treebank (Carlson et al., 2002), which we will denote RST-DT, a corpus annotated with discourse segmentation and relations according to Rhetorical Structure Theory (Mann and Thompson, 1988). The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The segmentation of sentences in the RST-DT is into clause-like units, known as elementary discourse units, or edus. We will use the two terms ‘edu’ and ‘segment’ interchangeably throughout the rest of the paper. Human agreement for this segmentation task is quite high, with agreement between two annotators at an F-score of 98.3 for unlabeled segmentation (Soricut and Marcu, 2003). The RST-DT corpus annotates edu breaks, which typically include sentence boundaries, but sentence boundaries are not explicitly annotated in the corpus. To perform sentence-level processing and evaluation, we alig"
P07-1062,H05-1033,0,0.117947,"ng approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. 1 Introduction Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005), sentence compression (Sporleder and Lapata, 2005), natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006). These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the"
P07-1062,H91-1060,0,0.0391514,"in section 2.3, for comparison with reported results in Sporleder and Lapata (2005), our F1-score is defined accordingly, i.e., seg490 Segmentation system F1 Sporleder and Lapata best (reported) 88.40 SPADE Sporleder and Lapata configuration (reported): 87.06 current configuration: 91.04 Table 1: Segmentation results on the Sporleder and Lapata (2005) data set, with accuracy defined to include sentence initial segmentation boundaries. mentation boundaries j such that 0 ≤ j &lt; k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. 2.3 Baseline SPADE setup The publicly available SPADE package, which encodes the approach in Soricut and Marcu (2003), is taken as the baseline for this paper. We made several modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to mak"
P07-1062,N03-1030,0,0.504935,"ches differ across these corpora, the requirement of sentence segmentation into 488    H H  H H H  Nucleus H  HH   Nucleus  P  PP P H P  PP Satellite HH Satellite   PP according to CEO Smith PPP P Prices have dropped but remain quite high Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recent years. Using the RST Treebank as training and evaluation data, Soricut and Marcu (2003) demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm – subsequently packaged together into the publicly available SPADE discourse parser1 – make use of the output of the"
P07-1062,C04-1007,0,0.0886707,"Missing"
P07-1062,W00-0726,0,0.017332,"Missing"
P07-1062,P05-1022,0,0.0612634,"l modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself. This allowed us to make two modifications that improved performance: turning off tokenization in the Charniak parser, and reranking. The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has already been tokenized in the Penn Treebank style. Secondly, Charniak and Johnson (2005) showed how reranking of the 50best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the Sporleder and Lapata (2005) 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabe"
P07-1062,A00-2018,0,0.0500084,"demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm – subsequently packaged together into the publicly available SPADE discourse parser1 – make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. Sporleder and Lapata (2005) also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003), was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis1 http://www.isi.edu/publications/licensed-sw/spade/ Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488–4"
P07-1062,W06-1317,0,0.0752958,"Missing"
P07-1062,W02-1001,0,0.00970181,"urse parsing that it performs, in order to evaluate the improvements to discourse parsing yielded by any improvements to segmentation. 2.4 Segmentation classifier For this paper, we trained a binary classifier, which was applied independently at each word wi in the string w1 . . . wk , to decide whether that word is the last in a segment. Note that wk is the last word in the string, and is hence ignored. We used a loglinear model with no Markov dependency between adjacent tags,3 and trained the parameters of the model with the perceptron algorithm, with averaging to control for over-training (Collins, 2002). Let C={E, I} be the set of classes: segmentation boundary (E) or non-boundary (I). Let f (c, i, w1 . . . wk ) be a function that takes as input a class value c, a word index i and the word string w1 . . . wk and returns a d-dimensional vector of feature values for that word index in that string with that class. For example, one feature might be (c = E, wi = the), which returns the value 1 when c = E and the current word is ‘the’, and returns 0 otherwise. Given a d-dimensional parameter vector φ, the output of the classifier is that class which maximizes the dot product between the feature an"
P07-1062,J05-2005,0,0.104251,"se units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string “Prices have dropped but remain quite high, according to CEO Smith,” which has three discourse segments, each labeled with either “Nucleus” or “Satellite” depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known RST Treebank (Carlson et al., 2002); the Discourse GraphBank (Wolf and Gibson, 2005); and the Penn Discourse Treebank (Miltsakaki et al., 2004). While the annotation approaches differ across these corpora, the requirement of sentence segmentation into 488    H H  H H H  Nucleus H  HH   Nucleus  P  PP P H P  PP Satellite HH Satellite   PP according to CEO Smith PPP P Prices have dropped but remain quite high Figure 1: Example Nucleus/Satellite labeled sentence-level discourse tree. sub-sentential discourse units is shared across all approaches. These resources have facilitated research into stochastic models and algorithms for automatic discourse struct"
P07-1062,C00-2137,0,0.028685,"rived solely from shallow finite-state tagging. Our primary concern is with intra-sentential discourse seg493 mentation, but we are also interested in how much the improved segmentation helps discourse parsing. The syntactic parser we use for all context-free syntactic parses used in either SPADE or our classifier is the Charniak parser with reranking, as described in Charniak and Johnson (2005). The Charniak parser and reranker were trained on the sections of the Penn Treebank not included in the RST-DT test set. All statistical significance testing is done via the stratified shuffling test (Yeh, 2000). 3.1 Segmentation Table 2 presents segmentation results for SPADE and four versions of our classifier. The “Basic finitestate” system uses only finite-state sequence features as defined in Section 2.5.1, while the “Full finite-state” also includes the finite-state approximation features from Section 2.5.3. The “Context-free” system uses the SPADE-inspired features detailed in Section 2.5.2, but none of the features from Sections 2.5.1 or 2.5.3. Finally, the “All features” section includes features from all three sections.5 Note that the full finite-state system is considerably better than the"
P07-1062,miltsakaki-etal-2004-penn,0,\N,Missing
P07-1062,C04-1020,0,\N,Missing
P07-1120,P99-1066,0,0.180753,"straints from three sources: the reranking stage of the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is a two-stage parsing pipeline, in which the first stage us"
P07-1120,J98-2004,0,0.0878423,"e derive these base-phrase constraints from three sources: the reranking stage of the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is a two-stage parsing pipeline,"
P07-1120,P05-1022,0,0.21927,"rch in an area believed to contain high-quality candidates. Another scenario is to use a different model altogether to constrain the pipeline. In this scenario, A “pipeline” system consists of a sequence of processing stages such that the output from one stage provides the input to the next. Each stage in such a pipeline identifies a subset of the possible solutions, and later stages are constrained to find solutions within that subset. For example, a part-of-speech tagger could constrain a “base phrase” chunker (Ratnaparkhi, 1999), or the n-best output of a parser could constrain a reranker (Charniak and Johnson, 2005). A pipeline is typically used to reduce search complexity for rich models used in later stages, usually at the risk that the best solutions may be pruned in early stages. Pipeline systems are ubiquitous in natural language processing, used not only in parsing (Ratnaparkhi, 1999; Charniak, 2000), but also machine translation (Och and Ney, 2003) and speech recognition (Fiscus, 1997; Goel et al., 2000), among others. (i) (ii) Despite the widespread use of pipelines, they have been understudied, with very little work on general techniques for designing and improving pipeline systems (although cf."
P07-1120,W98-1115,0,0.0277564,"h a base-phrase tree. We derive these base-phrase constraints from three sources: the reranking stage of the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is"
P07-1120,N06-1022,0,0.0119549,"the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is a two-stage parsing pipeline, in which the first stage uses a vanilla PCFG to populate a chart of parse c"
P07-1120,A00-2018,0,0.901188,"uch a pipeline identifies a subset of the possible solutions, and later stages are constrained to find solutions within that subset. For example, a part-of-speech tagger could constrain a “base phrase” chunker (Ratnaparkhi, 1999), or the n-best output of a parser could constrain a reranker (Charniak and Johnson, 2005). A pipeline is typically used to reduce search complexity for rich models used in later stages, usually at the risk that the best solutions may be pruned in early stages. Pipeline systems are ubiquitous in natural language processing, used not only in parsing (Ratnaparkhi, 1999; Charniak, 2000), but also machine translation (Och and Ney, 2003) and speech recognition (Fiscus, 1997; Goel et al., 2000), among others. (i) (ii) Despite the widespread use of pipelines, they have been understudied, with very little work on general techniques for designing and improving pipeline systems (although cf. Finkel et al. (2006)). This paFigure 1: Two Venn diagrams, representing (i) constraints per presents one such general technique, here ap- derived from later stages of an iterated pipelined system; and plied to stochastic parsing, whereby output from (ii) constraints derived from a different mod"
P07-1120,W06-1673,0,0.0837903,"A pipeline is typically used to reduce search complexity for rich models used in later stages, usually at the risk that the best solutions may be pruned in early stages. Pipeline systems are ubiquitous in natural language processing, used not only in parsing (Ratnaparkhi, 1999; Charniak, 2000), but also machine translation (Och and Ney, 2003) and speech recognition (Fiscus, 1997; Goel et al., 2000), among others. (i) (ii) Despite the widespread use of pipelines, they have been understudied, with very little work on general techniques for designing and improving pipeline systems (although cf. Finkel et al. (2006)). This paFigure 1: Two Venn diagrams, representing (i) constraints per presents one such general technique, here ap- derived from later stages of an iterated pipelined system; and plied to stochastic parsing, whereby output from (ii) constraints derived from a different model. 952 S A A S B B Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 952–959, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics represented in Fig. 1(ii), the other model constrains the early stage to be consistent with some subset of solutions ("
P07-1120,P96-1024,0,0.0546052,"y want to favor precision over recall to avoid erroneous constraints within the pipeline as much as possible. Here we discuss how a technique presented in Goodman’s thesis (1998) can be applied to do this. We will first present this within a general chart parsing approach, then move to how we use it for nbest lists. Let T be the set of trees for a particular input, and let a parse T ∈ T be considered as a set of labeled spans. Then, for all labeled spans X ∈ T , we can calculate the posterior probability γ(X) as follows: X P(T )JX ∈ T K P (1) γ(X) = 0 0 ∈T P(T ) T T ∈T 1 if X ∈ T 0 otherwise. Goodman (1996; 1998) presents a method for using the posterior probability of constituents to maximize the expected labeled recall of binary branching trees, as follows:  where JX ∈ T K = Tb = argmax T ∈T X γ(X) (2) X∈T Essentially, find the tree with the maximum sum of the posterior probabilities of its constituents. This is done by computing the posterior probabilities of constituents in a chart, typically via the InsideOutside algorithm (Baker, 1979; Lari and Young, 1990), followed by a final CYK-like pass to find the tree maximizing the sum. For non-binary branching trees, where precision and recall m"
P07-1120,P04-1006,0,0.132963,"the reranking stage of the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is a two-stage parsing pipeline, in which the first stage uses a vanilla PCFG to pop"
P07-1120,H05-1099,1,0.937981,"a different set (B) results, which may include better results than A. Whereas when iterating we are guaranteed that the new subset S will overlap at least partially with the original subset A, that is not the case when making use of constraints from a separately trained model. In this paper, we investigate pipeline iteration within the context of the Charniak and Johnson (2005) parsing pipeline, by constraining parses to be consistent with a base-phrase tree. We derive these base-phrase constraints from three sources: the reranking stage of the parsing pipeline; a finite-state shallow parser (Hollingshead et al., 2005); and a combination of the output from these two sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accura"
P07-1120,J93-2004,0,0.0360492,"or the CoNLL-2000 Shared Task (Tjong Kim Sang and Buchholz, 2000), base phrases correspond directly to constituents that appear in full parses, and hence can provide a straightforward constraint on edges within a chart parser. In contrast, shallow phrases collapse certain non-constituents—such as auxiliary chains—into a single phrase, and hence are not directly applicable as constraints on a chart parser. We have two methods for deriving base-phrase annotations for a string. First, we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The treebank trees are pre-processed identically to the procedure for training the Charniak parser, e.g., empty nodes and function tags are removed. The shallow parser is trained using the perceptron algorithm, with a feature set nearly identical to that from Sha and Pereira (2003), and achieves comparable performance to that paper. See Hollingshead et al. (2005) for more details. Second, base phrases can be extracted from the full-parse output of the Charniak and Johnson (2005) reranker, via a simple script to extract nodes with only preterminal children. Table 1 shows these systems’ bracke"
P07-1120,P06-1043,0,0.0294488,"sources. We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output. The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline (Charniak et al., 1998; Caraballo and Charniak, 1998; Blaheta and Charniak, 1999; Hall and Johnson, 2004; Charniak et al., 2006) as well as many focused on optimizing final parse accuracy (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006). This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks. 2 Approach Our approach uses the Charniak state-of-the-art parsing pipeline. The well-known Charniak (2000) coarse-to-fine parser is a two-stage parsing pipeline, in which the first stage uses a vanilla PCFG to populate a chart of parse constituents. The second stage, constrained to only those items in the firststage chart, uses a refined grammar to generate an n"
P07-1120,J03-1002,0,0.0028316,"ble solutions, and later stages are constrained to find solutions within that subset. For example, a part-of-speech tagger could constrain a “base phrase” chunker (Ratnaparkhi, 1999), or the n-best output of a parser could constrain a reranker (Charniak and Johnson, 2005). A pipeline is typically used to reduce search complexity for rich models used in later stages, usually at the risk that the best solutions may be pruned in early stages. Pipeline systems are ubiquitous in natural language processing, used not only in parsing (Ratnaparkhi, 1999; Charniak, 2000), but also machine translation (Och and Ney, 2003) and speech recognition (Fiscus, 1997; Goel et al., 2000), among others. (i) (ii) Despite the widespread use of pipelines, they have been understudied, with very little work on general techniques for designing and improving pipeline systems (although cf. Finkel et al. (2006)). This paFigure 1: Two Venn diagrams, representing (i) constraints per presents one such general technique, here ap- derived from later stages of an iterated pipelined system; and plied to stochastic parsing, whereby output from (ii) constraints derived from a different model. 952 S A A S B B Proceedings of the 45th Annual"
P07-1120,N03-1028,0,0.129325,"ituents—such as auxiliary chains—into a single phrase, and hence are not directly applicable as constraints on a chart parser. We have two methods for deriving base-phrase annotations for a string. First, we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The treebank trees are pre-processed identically to the procedure for training the Charniak parser, e.g., empty nodes and function tags are removed. The shallow parser is trained using the perceptron algorithm, with a feature set nearly identical to that from Sha and Pereira (2003), and achieves comparable performance to that paper. See Hollingshead et al. (2005) for more details. Second, base phrases can be extracted from the full-parse output of the Charniak and Johnson (2005) reranker, via a simple script to extract nodes with only preterminal children. Table 1 shows these systems’ bracketing accuracy on both the base-phrase and shallow parsing tasks for WSJ section 24; each system was trained on WSJ sections 02-21. From this table we can see that base phrases are substantially more difficult than shallow phrases to annotate. Output from the finite-state shallow pars"
P07-1120,W00-0726,0,0.0488809,"ate shallow parser Base Phrases 91.9 92.8 91.7 Shallow Phrases 94.4 94.8 94.3 Table 1: F-scores on WSJ section 24 of output from two parsers on the similar tasks of base-phrase parsing and shallowphrase parsing. For evaluation, base and shallow phrases are extracted from the Charniak/Johnson full-parse output. allow us to optionally provide base-phrase trees to constrain the first stage of parsing. 2.1 Base Phrases Following Ratnaparkhi (1999), we define a base phrase as any parse node with only preterminal children. Unlike the shallow phrases defined for the CoNLL-2000 Shared Task (Tjong Kim Sang and Buchholz, 2000), base phrases correspond directly to constituents that appear in full parses, and hence can provide a straightforward constraint on edges within a chart parser. In contrast, shallow phrases collapse certain non-constituents—such as auxiliary chains—into a single phrase, and hence are not directly applicable as constraints on a chart parser. We have two methods for deriving base-phrase annotations for a string. First, we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St. Journal (WSJ) Treebank (Marcus et al., 1993). The treebank trees are pre-processed identi"
P07-1120,C00-2137,0,0.0418281,"verall processing time by a factor of two, we also compare against output obtained by doubling the coarse-parser’s beam threshold. The last row in Table 4 shows that the increased threshold yields an insignificant improvement over the baseline, despite a very large processing burden. We applied our best-performing model (Unconstrained ∪ Combo, λ=0.5) to the test set, WSJ section 23, for comparison against the baseline system. Table 5 shows a 0.4 percent F-score improvement over the baseline for that section, which is statistically significant at p < 0.001, using the stratified shuffling test (Yeh, 2000). 5 Conclusion & Future Work In summary, we have demonstrated that pipeline iteration can be useful in improving system performance, by constraining early stages of the pipeline with output derived from later stages. While the current work made use of a particular kind of constraint—base phrases—many others could be extracted as well. Preliminary results extending the work presented in this paper show parser accuracy improvements from pipeline iteration when using constraints based on an unlabeled partial bracketing of the string. Higher-level phrase segmentations or fully specified trees over"
P11-1045,P11-2119,1,0.888988,"Missing"
P11-1045,J98-2004,0,0.379668,"teration, the highest-scoring edge is popped off of the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate α ˆ (Ni,j ) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competing edges can be compared directly, avoiding the difficulty inherent in agenda-based approaches of comparing edges of radically different span lengths and characteristics; and 2) Since the agendas are very small, the overhead of agenda maintenance — a large component of agenda-based parse time — is minimal. 442 2.3 Beam-search parsing CYK parsing with a beam-search is a local pru"
P11-1045,P05-1022,0,0.0532447,"highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a non-trivial problem, and most often approache"
P11-1045,A00-2018,0,0.934872,"in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods Proceedings of the 49th Annual Meeting of the Association for Com"
P11-1045,P10-1146,0,0.0241465,"he most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow — too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accura"
P11-1045,W02-1001,0,0.129157,"ts, we first map cells in the training corpus to tuples: Φ(S, i, j) = (x, y) (1) where x is a feature-vector representation of the chart cell and y is the target class 1 if the cell contains an edge from the maximum likelihood parse 443 tree, 0 otherwise. The feature vector x is encoded with the chart cell’s absolute and relative span width, as well as unigram and bigram lexical and part-ofspeech tag items from wi−1 . . . wj+2 . Given feature/target tuples (x, y) for every chart cell in every sentence of a training corpus τ , we train a weight vector θ using the averaged perceptron algorithm (Collins, 2002) to learn an open/closed binary decision boundary: θˆ = argmin θ X Lλ (H(θ · x), y) (2) (x,y)∈Φ(τ ) where H(·) is the unit step function: 1 if the inner product θ · x &gt; 0, and 0 otherwise; and Lλ (·, ·) is an asymmetric loss function, defined below. When predicting cell closure, all misclassifications are not equal. If we leave open a cell which contains no edges in the maximum likelihood (ML) parse, we incur the cost of additional processing, but are still able to recover the ML tree. However, if we close a chart cell which contains an ML edge, search errors occur. To deal with this imbalance"
P11-1045,W97-0302,0,0.0841457,"in probability relative to the highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a"
P11-1045,J98-4004,0,0.0440124,"onstituent parsing is slow — too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntacti"
P11-1045,P03-1054,0,0.2073,"ing is slow — too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure ve"
P11-1045,P05-1010,0,0.00851235,"ntial consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often un"
P11-1045,J08-4003,0,0.0283745,"t variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inf"
P11-1045,P10-2037,0,0.0684996,"agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate α ˆ (Ni,j ) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competing edges can be compared directly, avoiding the difficulty inherent in agenda-based approaches of comparing edges of radically different span lengths and characteristics; and 2) Since the agendas are very small, the overhead of agenda maintenance — a large component of agenda-based parse time — is minimal. 442 2.3 Beam-search parsing CYK parsing with a beam-search is a local pruning strategy, comparing edges within the same"
P11-1045,N07-1051,0,0.697417,"rammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,"
P11-1045,P06-1055,0,0.0310125,"ons. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constitue"
P11-1045,D10-1069,0,0.0111188,"o several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods Pro"
P11-1045,J08-2005,0,0.0274391,"onstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow — too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of t"
P11-1045,C08-1094,1,0.572226,"unsupervised parsing. 441 Figure 1: Inside (grey) and outside (white) representations of an example chart edge Ni,j . proach is broadly applicable to a wide range of scenarios, including tuning the search to new domains where domain mismatch may yield very different efficiency/accuracy operating points. In the next section, we present prior work on approximate inference in parsing, and discuss how our method to learn optimal beam-search parameters unite many of their strengths into a single framework. We then explore using our approach to open or close cells in the chart as an alternative to Roark and Hollingshead (2008; 2009). Finally, we present results which combine cell closure and adaptive beam-width prediction to achieve the most efficient parser. 2 2.1 Background Preliminaries and notation Let S = w1 . . . w|S |represent an input string of |S |words. Let wi,j denote the substring from word wi+1 to wj ; i.e., S = w0,|S |. We use the term chart edge to refer to a non-terminal spanning a specific substring of the input sentence. Let Ni,j denote the edge labeled with non-terminal N spanning wi,j , for example NP3,7 . We define an edge’s figure-of-merit (FOM) as an estimate of the product of its inside (β)"
P11-1045,N09-1073,1,0.717726,"Missing"
P11-1045,W05-0638,0,0.0195898,"to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow — too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3 |G|) where n is the length of the sentence and 440 |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an"
P11-1045,C10-2168,0,0.142089,"Missing"
P11-1045,H91-1042,0,\N,Missing
P11-1045,P97-1003,0,\N,Missing
P11-1045,J03-4003,0,\N,Missing
P11-1045,N03-1016,0,\N,Missing
P11-2001,P03-1006,1,0.843689,"as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. 1 Introduction and Motivation Representing smoothed n-gram language models as weighted finite-state transducers (WFST) is most naturally done with a failure transition, which reflects the semantics of the “otherwise” formulation of smoothing (Allauzen et al., 2003). For example, the typical backoff formulation of the probability of a word w given a history h is as follows  P(w |h) = P(w |h) if c(hw) &gt; 0 αh P(w |h0 ) otherwise (1) where P is an empirical estimate of the probability that reserves small finite probability for unseen n-grams; αh is a backoff weight that ensures normalization; and h0 is a backoff history typically achieved by excising the earliest word in the history h. The principle benefit of encoding the WFST in this way is that it only requires explicitly storing n-gram transitions for observed n-grams, i.e., count greater than zero, as"
P11-2041,N07-1051,0,\N,Missing
P11-2041,J93-2004,0,\N,Missing
P11-2041,N10-1085,1,\N,Missing
P11-2041,P00-1012,0,\N,Missing
P11-2041,P99-1018,0,\N,Missing
P11-2041,W09-0608,1,\N,Missing
P11-2119,P11-1045,1,0.889471,"Missing"
P11-2119,A00-2018,0,0.223641,"se-to-fine, agenda, and beam-search pruning. 1 Introduction While there have been great advances in the statistical modeling of hierarchical syntactic structure in the past 15 years, exact inference with such models remains very costly and most rich syntactic modeling approaches resort to heavy pruning, pipelining, or both. Graph-based pruning methods such as best-first and beam-search have both be used within context-free parsers to increase their efficiency. Pipeline systems make use of simpler models to reduce the search space of the full model. For example, the well-known Charniak parser (Charniak, 2000) uses a simple grammar to prune the search space for a richer model in a second pass. 676 Roark and Hollingshead (2008; 2009) have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy. In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al. (2010) and Bodenstab et al. (2011) both extend this approach by classifying chart cells with a fin"
P11-2119,P97-1003,0,0.550425,"Missing"
P11-2119,W02-1001,0,0.222763,"ins. In the next section, we discuss how to learn such contexts via a finite-state tagger. 4 Tagging Unary Constraints 5 To automatically predict if word wi from sentence w can be spanned by an SWC production, we train a binary classifier from supervised data using sections 2-21 of the Penn WSJ Treebank for training, section 00 as heldout, and section 24 as development. The class labels of all words in the training data are extracted from the treebank, where wi ∈ U if wi is observed with a SWC production and wi ∈ U otherwise. We train a log linear model with the averaged perceptron algorithm (Collins, 2002) using unigram word and POS-tag2 features from a five word window. We also trained models with bi-gram and trigram features, but tagging accuracy did not improve. Because the classifier output is imposing hard constraints on the search space of the parser, we may want to choose a tagger operating point that favors precision over recall to avoid over-constraining the downstream parser. To compare the tradeoff between possible precision/recall values, we apply the softmax activation function to the perceptron output to obtain the posterior probability of wi ∈ U : P (U |wi , θ) = (1 + exp(−f (wi"
P11-2119,P01-1044,0,0.0723065,"Missing"
P11-2119,C08-1094,1,0.816799,"ly pruned parsers using graph-based and pipelining techniques still see substantial speedups 3 The Berkeley parser does maintain meta-information about where non-terminals have been placed in the chart, giving it some of the advantages of cross-product grammar access. 680 F-score 72.2 72.6 74.3 74.6 88.4 88.5 88.7 88.7 89.7 89.7 89.7 89.6 90.2 90.1 90.2 90.2 Seconds 1,358 1,125 380 249 586 486 349 283 1,116 900 716 679 564 495 320 289 Speedup 1.2x 3.6x 5.5x 1.2x 1.7x 2.1x 1.2x 1.6x 1.6x 1.1x 1.8x 2.0x Table 3: Test set results applying unary constraints (UC) and cell-closing (CC) constraints (Roark and Hollingshead, 2008) to various parsers. with the additional application of unary constraints. Furthermore, unary constraints consistently provide an additive efficiency gain when combined with cellclosing constraints. 6 Conclusion We have presented a new method to constrain context-free chart parsing and have shown it to be orthogonal to many forms of graph-based and pipeline pruning methods. In addition, our method parallels the cell closing paradigm and is an elegant complement to recent work, providing a finite-state tagging framework to potentially constrain all areas of the search space – both multi-word an"
P11-2119,N09-1073,1,0.864115,"Missing"
P11-2119,C10-2168,0,0.266625,"Missing"
P11-2119,N07-1051,0,\N,Missing
P11-4007,W10-1304,1,0.886757,"Missing"
P12-3011,J94-3001,0,0.391573,"iring by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string. 4 NGram Library The OpenGrm NGram library contains tools for building, manipulating and using n-gram language models represented as weighted finite-state transducers. The same finite-state topology"
P12-3011,P96-1031,1,0.748517,"adds the weight 3 (in the tropical semiring by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string. 4 NGram Library The OpenGrm NGram library contains tools for building, manipulating and using n-gram language models represented as weighted finite-state transduc"
P13-1005,P08-1058,0,0.0193206,"ly used in scenarios requiring relatively small footprint models. For example, applications running on mobile devices or in low latency streaming scenarios may be required to limit the complexity of models and algorithms to achieve the desired operating profile. As a result, statistical language models – an important component of many such applications – are often trained on very large corpora, then modified to fit within some pre-specified size bound. One method to achieve significant space reduction is through randomized data structures, such as Bloom (Talbot and Osborne, 2007) or Bloomier (Talbot and Brants, 2008) filters. These data structures permit efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the corpus. Beyond count thresholding, th"
P13-1005,D07-1049,0,0.0331062,"guage applications, however, are commonly used in scenarios requiring relatively small footprint models. For example, applications running on mobile devices or in low latency streaming scenarios may be required to limit the complexity of models and algorithms to achieve the desired operating profile. As a result, statistical language models – an important component of many such applications – are often trained on very large corpora, then modified to fit within some pre-specified size bound. One method to achieve significant space reduction is through randomized data structures, such as Bloom (Talbot and Osborne, 2007) or Bloomier (Talbot and Brants, 2008) filters. These data structures permit efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the"
P13-1005,C00-2137,0,0.0879815,"point precision issues when allowing the backoff recalculation to run indefinitely. 6 Table 4: WER reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from Chelba et al. (2010), and a mixture model. KneserNey results are shown for: a) original pruning; and b) with -prune-history-lm switch. The perplexity reductions that were achieved for these models do translate to real word error rate reductions at both stages of between 0.5 and 0.9 percent absolute. All of these gains are statistically significant at p &lt; 0.0001 using the stratified shuffling test (Yeh, 2000). For pruned Kneser-Ney models, fixing the state marginals with the -prune-history-lm switch reduces the WER versus the original pruned model, but no reductions were achieved vs. baseline methods. Table 5 presents perplexity and WER results for less heavily pruned models, where the pruning thresholds were set to yield approximately 1.5 million n-grams (4 times more than the previous models); and another set at around 5 million n-grams, as well as the full, unpruned models. While the robust gains we’ve observed up to now persist with the 1.5M n-gram models (WER reductions significant, Witten-Be"
P13-1005,P12-3011,1,0.923479,"model. Let hki = wi−k . . . wi−1 be the previous k words in the sequence. Then the smoothed model is defined recursively as follows: P(wi |hki ) =  P(wi |hki ) =  β(hki wi ) if hki wi ∈ G (1) k−1 α(hki , hk−1 ) P(w |h ) otherwise i i i where β(hki wi ) is the parameter associated with the n-gram hki wi and α(hki , hk−1 ) is the backoff i cost associated with going from state hki to state hk−1 . We assume that, if hw ∈ G then all prefixes i and suffixes of hw are also in G. Figure 1 presents a schema of an automaton representation of an n-gram model, of the sort used in the OpenGrm library (Roark et al., 2012). States represent histories h, and the words w, whose probabilities are conditioned on h, label the arcs, leading to the history state for the subsequent word. State labels are provided in Figure 1 as a convenience, to show the (implicit) history encoded by the state, e.g., ‘xyz’ indicates that the state represents a history with the previous three symbols being x, y and z. Failure arcs, labeled with a φ in Figure 1, encode an “otherwise” semantics and have as destination the origin state’s backoff history. Many higher order states will back off to the same lower order state, specifically tho"
P14-2060,P12-3006,0,0.0892958,"ut the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is proba"
P14-2060,P12-1109,0,0.0365213,"be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying"
P14-2060,P10-1079,0,0.111581,"on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand,"
P14-2060,P12-1055,0,0.0417532,"be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying"
P14-2060,I11-1109,0,0.0499091,"with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a produ"
P14-2060,P12-3011,1,0.734833,"for terms that could be abbreviations of full words that occur in the same context. Thus: the heating dry svc/service clng/cooling clng/cleaning center system system contributes evidence that svc is an abbreviation of service. Similarly instances of clng in contexts that can contain cooling or cleaning are evidence that clng could be an abbreviation of either of these words. (The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar (Roark et al., 2012) is used that, among other things, specifies that: the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted.2 We count a pair of words as ‘co-occurring’ if they are observed in the same context. For a given context C, e.g., the center, let WC be the set of words found in that context. Then, for any pair of words u, v, we can assign a pair count based on the count of contexts where both occur: Methods Since our target app"
P14-2060,J93-1003,0,0.117038,"LM from the data. Because of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations Table 1: Examples of automatically mined abbreviation/expansion pairs. P Let c(u) be defi"
P14-2060,P13-1155,0,0.0435058,"relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviation"
P14-2060,P06-1125,0,0.460358,"Missing"
P14-2060,C08-1056,0,0.150982,"size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations Table 1: Examples of automatically mined abbreviation/expansion pairs. P Let c(u) be defined as v c(u, v). From th"
P14-2060,D13-1007,0,0.23954,"ich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral wi"
P14-2060,P11-2013,0,0.110172,"pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for se"
P14-2129,I13-1141,1,0.873102,"Missing"
P14-2129,J99-2004,0,0.448262,"out fully connected trees is to focus on providing full hierarchical annotations for structures within a local window, ignoring global constituents outside that window. We follow the XML community in naming structures of this type hedges (not to be confused with the rhetorical device of the same name), due to the fact that they are like smaller versions of trees which occur in sequences. Such structures may be of utility to various structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subsequent inference, much as finite-state models such as supertagging (Bangalore and Joshi, 1999) or chart cell constraints (Roark and Hollingshead, 2008; Roark et al., 2012) are used. In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing (Eisner and Smith, 2005; Dreyer et al., 2006), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models (Sarawagi and Cohen, 2004), which allow finite-state models to reason about segments rather than just tags by imposing segment"
P14-2129,P11-1045,1,0.847602,"t F1. Table 3 presents results of our best configurations on the eval set, section 23. The results show the same patterns as on the development set. Finally, Figure 3 shows the speed of inference, la¯ concerned/B ¯ that/B ¯ much/B “Analysts/B are/B ¯ the/B ¯ high-yield/B ¯ market/B ¯ will/B of/B ¯ treacherous/B ¯ for/B ¯ investors/B ¯ ./B” remain/B 3 Experimental Results We ran all experiments on the WSJ Penn Treebank corpus (Marcus et al., 1999) using section 2-21 for training, section 24 for development, and section 23 for testing. We performed exhaustive CYK parsing using the BUBS parser2 (Bodenstab et al., 2011) with Berkeley SM6 latent-variable grammars (Petrov and Klein, 2007) learned by the Berkeley grammar trainer with default settings. We compute accuracy from the 1-best Viterbi tree extracted from the chart using the standard EVALB script. Accuracy results are reported as precision, recall and F1-score, the harmonic mean between the two. In all trials, we evaluate accuracy with respect to the hedge transformed reference 1 2 Table 2: Hedge segmentation and parsing results on section 24 for L = 7. Segmentation None Oracle Unlabeled Labeled Rare words occur less than 5 times in the training data."
P14-2129,J11-1005,0,0.0163956,"the need for improved segmentation. 4 vide a significant speedup in parsing, but that cascading errors remain a problem. There are many directions of future work to pursue here. First, the current results are all for exhaustive CYK parsing, and we plan to perform a detailed investigation of the performance of hedgebank parsing with prioritization and pruning methods of the sort available in BUBS (Bodenstab et al., 2011). Further, this sort of annotation seems well suited to incremental parsing with beam search, which has been shown to achieve high accuracies even for fully connected parsing (Zhang and Clark, 2011). Improvements to the transform (e.g., grouping items not in hedges under non-terminals) and to the segmentation model (e.g., increasing precision at the expense of recall) could improve accuracy without greatly reducing efficiency. Finally, we intend to perform an extrinsic evaluation of this parsing in an on-line task such as simultaneous translation. Conclusion and Future Work We proposed a novel partial parsing approach for applications that require a fast syntactic analysis of the input beyond shallow bracketing. The spanlimit parameter allows tuning the annotation of internal structure a"
P14-2129,W02-1001,0,0.197056,"s, we try a labeled segmentawith B and B ¯C tags where C is hedge contion with BC and B stituent type. We restrict the types to the most important types – following the 11 chunk types annotated in the CoNLL-2000 chunking task (Sang and Buchholz, 2000) – by replacing all other types with a new type OUT. Thus, “Analysts” is labeled BG ; “much”, BNP ; “will”, BVP and so on. To automatically predict the class of each word position, we train a multi-class classifier from labeled training data using a discriminative linear model, learning the model parameters with the averaged perceptron algorithm (Collins, 2002). We follow Roark et al. (2012) in the features they used to label words as beginning or ending constituents. The segmenter extracts features from word and POS-tag input sequences and hedge-boundary tag output sequences. The feature set includes trigrams of surrounding words, trigrams of surrounding POS tags, and hedge-boundary tags of the previous words. An additional orthographical feature set is used to tag rare1 and unknown words. This feature set includes prefixes and suffixes of the words (up to 4 characters), and presence of a hyphen, digit, or an upper-case character. Reported results"
P14-2129,W06-2929,0,0.0281642,"t that they are like smaller versions of trees which occur in sequences. Such structures may be of utility to various structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subsequent inference, much as finite-state models such as supertagging (Bangalore and Joshi, 1999) or chart cell constraints (Roark and Hollingshead, 2008; Roark et al., 2012) are used. In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing (Eisner and Smith, 2005; Dreyer et al., 2006), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models (Sarawagi and Cohen, 2004), which allow finite-state models to reason about segments rather than just tags by imposing segment length limits. In the XML community, trees and hedges are used for models of XML document instances and for the contents of elements (Br¨uggemann-Klein and Wood, 2004). As far as we know, this paper is the first to consider this sort of partial parsing approach for natural language. We pursue this topic via tr"
P14-2129,W05-1504,0,0.036927,"me name), due to the fact that they are like smaller versions of trees which occur in sequences. Such structures may be of utility to various structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subsequent inference, much as finite-state models such as supertagging (Bangalore and Joshi, 1999) or chart cell constraints (Roark and Hollingshead, 2008; Roark et al., 2012) are used. In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing (Eisner and Smith, 2005; Dreyer et al., 2006), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models (Sarawagi and Cohen, 2004), which allow finite-state models to reason about segments rather than just tags by imposing segment length limits. In the XML community, trees and hedges are used for models of XML document instances and for the contents of elements (Br¨uggemann-Klein and Wood, 2004). As far as we know, this paper is the first to consider this sort of partial parsing approach for natural language. We pu"
P14-2129,C08-1094,1,0.816828,"l hierarchical annotations for structures within a local window, ignoring global constituents outside that window. We follow the XML community in naming structures of this type hedges (not to be confused with the rhetorical device of the same name), due to the fact that they are like smaller versions of trees which occur in sequences. Such structures may be of utility to various structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subsequent inference, much as finite-state models such as supertagging (Bangalore and Joshi, 1999) or chart cell constraints (Roark and Hollingshead, 2008; Roark et al., 2012) are used. In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing (Eisner and Smith, 2005; Dreyer et al., 2006), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models (Sarawagi and Cohen, 2004), which allow finite-state models to reason about segments rather than just tags by imposing segment length limits. In the XML community, trees and hedges a"
P14-2129,J12-4002,1,0.804304,"structures within a local window, ignoring global constituents outside that window. We follow the XML community in naming structures of this type hedges (not to be confused with the rhetorical device of the same name), due to the fact that they are like smaller versions of trees which occur in sequences. Such structures may be of utility to various structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subsequent inference, much as finite-state models such as supertagging (Bangalore and Joshi, 1999) or chart cell constraints (Roark and Hollingshead, 2008; Roark et al., 2012) are used. In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing (Eisner and Smith, 2005; Dreyer et al., 2006), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models (Sarawagi and Cohen, 2004), which allow finite-state models to reason about segments rather than just tags by imposing segment length limits. In the XML community, trees and hedges are used for models of"
P14-2129,W00-0726,0,0.154578,"unary or POS799 tag hedges together under a new non-terminal labeled G. Unlabeled segmentation tags for the words in the example sentence in Figure 1(b) are: Hedge Parsing Acc/Eff Parser P R F1 w/s Full w/full CYK 88.8 89.2 89.0 2.4 Hedgebank 87.6 84.4 86.0 25.7 Table 1: Hedge parsing results on section 24 for L = 7. In addition to the simple unlabeled segmentation ¯ tags, we try a labeled segmentawith B and B ¯C tags where C is hedge contion with BC and B stituent type. We restrict the types to the most important types – following the 11 chunk types annotated in the CoNLL-2000 chunking task (Sang and Buchholz, 2000) – by replacing all other types with a new type OUT. Thus, “Analysts” is labeled BG ; “much”, BNP ; “will”, BVP and so on. To automatically predict the class of each word position, we train a multi-class classifier from labeled training data using a discriminative linear model, learning the model parameters with the averaged perceptron algorithm (Collins, 2002). We follow Roark et al. (2012) in the features they used to label words as beginning or ending constituents. The segmenter extracts features from word and POS-tag input sequences and hedge-boundary tag output sequences. The feature set"
P19-1171,J92-1002,0,0.720201,"rds’ prefixes w&lt;k and suffixes w>k .1 3.2 A variational upper bound Entropy, the workhorse of information theory, captures the uncertainty of a probability distribution. In our language modeling case, the quantity is H(W ) ≡ ∑ Pr(w) log w∈Σ∗ 1 . Pr(w) (1) Entropy is the average number of bits required to represent a string in the distribution, under an optimal coding scheme. When computing it, we are faced with two problems: We do not know the distribution over word-forms Pr(W ) and, even if we did, computing Equation 1 requires summing over the infinite set of possible strings Σ∗ . We follow Brown et al. (1992) in tackling these problems together. Approximating Pr(W ) with any known distribution Q(W ), we get a variational upper bound on H(W ) from their cross-entropy, i.e. H(W ) ≤ HQ (W ) = ∑ w∈Σ∗ 1 Pr(w) log (2a) 1 . Q(w) (2b) In line with, e.g., Cucerzan and Yarowsky (2003), we treat affixes as word-initial or word-final sequences, regardless of their status as attested morphological entities. 1753 Equation 2b still requires knowledge of Pr(W ) and involves an infinite sum, though. Nonetheless, we ˜ of samples from Pr(W ) to can use a finite set W get an empirical estimate of this value. HQ (W )"
P19-1171,N03-1006,0,0.110592,"y is the average number of bits required to represent a string in the distribution, under an optimal coding scheme. When computing it, we are faced with two problems: We do not know the distribution over word-forms Pr(W ) and, even if we did, computing Equation 1 requires summing over the infinite set of possible strings Σ∗ . We follow Brown et al. (1992) in tackling these problems together. Approximating Pr(W ) with any known distribution Q(W ), we get a variational upper bound on H(W ) from their cross-entropy, i.e. H(W ) ≤ HQ (W ) = ∑ w∈Σ∗ 1 Pr(w) log (2a) 1 . Q(w) (2b) In line with, e.g., Cucerzan and Yarowsky (2003), we treat affixes as word-initial or word-final sequences, regardless of their status as attested morphological entities. 1753 Equation 2b still requires knowledge of Pr(W ) and involves an infinite sum, though. Nonetheless, we ˜ of samples from Pr(W ) to can use a finite set W get an empirical estimate of this value. HQ (W ) ≈ 1 N 1 , log ∑ N i=1 ˜ (i) Q w ˜ ∼ Pr(W ) ˜ (i) ∈ W w (3) with equality if we let N → We now use Equation 3 as an estimate for the entropy of a lexicon. tighter. There is nothing principled that we can say about the result, except that it is consistent. The procedure f"
P19-1171,P16-1225,0,0.313803,"meaning, and the signifier, which has no meaning but manifests the form of the sign. Saussure himself, however, also documented instances of sound symbolism in language (Saussure, 1912). In this paper, we present computational evidence of relevance to both aspects of Saussure’s work. While dominant among linguists, arbitrariness has been subject to both long theoretical debate (Wilkins, 1668; Eco, 1995; Johnson, 2004; Pullum and Scholz, 2007) and numerous empirical and experimental studies (Hutchins, 1998; Bergen, 2004; Monaghan et al., 2011; Abramova and Fern´andez, 2016; Blasi et al., 2016; Gutierrez et al., 2016; Dautriche et al., 2017). Taken as a whole, these studies suggest non-trivial interactions in the form– meaning interface between the signified and the signifier (Dingemanse et al., 2015). Although the new wave of studies on form– meaning associations range across multiple languages, methods and working hypotheses, they all converge on two important dimensions: 1. The description of meaning is parameterized with pre-defined labels—e.g., by using existing ontologies like List et al. (2016). 2. The description of forms is restricted to the presence, absence or sheer number of occurrence of part"
P19-1171,P18-1027,0,0.0299014,"s shown in Equation 5b. We use upper bounds on both the entropy and conditional entropy measures, so our calculated mutual information is an estimate. This estimate is as good as our bounds are tight, being perfect when Pr(W ) = Q(W ) and Pr(W |V ) = Q(W |V ). Still, as we subtract two upper bounds, we cannot guarantee that our MI estimate approaches the real MI from above or below because we do not know which of the entropies’ bounds are 2 ∏ Pr (wi |w&lt;i ) . (7) i=1 Recurrent neural networks are great representation extractors, being able to model long dependencies—up to a few hundred tokens (Khandelwal et al., 2018)—and complex distributions Pr(wi |w&lt;i ) (Mikolov et al., 2010; Sundermeyer et al., 2012). We choose LSTM language models in particular, the state-of-the-art for character-level language modeling (Merity et al., 2018).3 Our architecture embeds a word—a sequence of tokens wi ∈ Σ—using an embedding lookup table, resulting in vectors zi ∈ Rd . These are fed into an LSTM, which produces high-dimensional representations of the sequence (hidden states): Systematicity will thus be framed as (statistically significant) nonzero mutual information I(V ;W ). 3.4 Recurrent neural LM Pr(w) = Systematicity a"
P19-1171,L16-1379,0,0.0168626,"tchins, 1998; Bergen, 2004; Monaghan et al., 2011; Abramova and Fern´andez, 2016; Blasi et al., 2016; Gutierrez et al., 2016; Dautriche et al., 2017). Taken as a whole, these studies suggest non-trivial interactions in the form– meaning interface between the signified and the signifier (Dingemanse et al., 2015). Although the new wave of studies on form– meaning associations range across multiple languages, methods and working hypotheses, they all converge on two important dimensions: 1. The description of meaning is parameterized with pre-defined labels—e.g., by using existing ontologies like List et al. (2016). 2. The description of forms is restricted to the presence, absence or sheer number of occurrence of particular units (such as phones, syllables or handshapes). We take an information-theoretic approach to quan1751 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1751–1764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tifying the relationship between form and meaning using flexible representations in both domains, rephrasing the question of systematicity: How much does certainty of one reduce uncertain"
P19-1171,W18-1206,0,0.166979,". In addition to this lexicon-level characterization of systematicity, we also show that this paradigm can be leveraged for studying more narrowlydefined form-meaning associations such as phonesthemes—submorphemic, meaning-bearing units— in the style of Gutierrez et al. (2016). These short sound sequences typically suggest some aspect of meaning in the words that contain them, like -ump for rounded things in English. Previous computational studies, whether focusing on characterizing the degree of systematicity (Monaghan et al., 2014b,a, 2011; Shillcock et al., 2001), discovering phonesthemes (Liu et al., 2018), or both (Gutierrez et al., 2016), have invariably framed systematicity in terms of distances and/or similarities–the relation between word-form distance/similarity on the one hand (e.g., based on string edit distance) and semantic distance/similarity on the other (e.g., as defined within a semantic vector space). Our methods have the virtue of not relying on some predefined notion of similarity or distance in either domain for our measurement of systematicity. Empirically, we focus on two experimental regimes. First, we focus on a large corpus (CELEX) of phone transcriptions in Dutch, Englis"
P19-1491,P18-1007,0,0.0157771,"We restrict the set of characters to those that we see at least 25 times in the training set, replacing all others with a new symbol ^, as is common and easily defensible in openvocabulary language modeling (Mielke and Eisner, 2018). We make an exception for Chinese, where we only require each character to appear at least twice. These thresholds result in negligible “out-of-alphabet” rates for all languages. 7 In practice, in both training and testing, we only evaluate the probability of the canonical segmentation of the held-out string, rather than the total probability of all segmentations (Kudo, 2018; Mielke and Eisner, 2018, Appendix D.2). 8 Figure 2 shows the 21 languages of the Europarl dataset. Optimal values: 0.2 (et); 0.3 (fi, lt); 0.4 (de, es, hu, lv, sk, sl); 0.5 (da, fr, pl, sv); 0.6 (bg, ru); 0.7 (el); 0.8 (en); 0.9 (it, pt). 3 6 5.8 5.6 5.4 ·106 hu nl pl el ro cs pt es it en et fr sk bg sl fi lv lt sv da 3.2 de hu pl el cs nl pt ro it es et en sk fr sl lt bg fi lv da sv hu de hu pl cs el pt nl et it ro es fr en sl sk bg lt fi lv sv da 0.2 hu de pl cs ro nl el et pt es it fi lt fr sk en sl bg lv sv da hu de hu de de pl cs ro nl et el it pt es lt fi sk sl fr lv en bg pl cs et nl"
P19-1491,E12-1026,0,0.0141882,") the raw character sequence length—are statistically significant indicators of modeling difficulty within our large set of languages. In contrast, we fail to reproduce our earlier results from Cotterell et al. (2018),1 which suggested morphological complexity as an indicator of modeling complexity. In fact, we find no tenable correlation to a wide variety of typological features, taken from the WALS dataset and other sources. Additionally, exploiting our model’s ability to handle missing data, we directly test the hypothesis that translationese leads to easier language-modeling (Baker, 1993; Lembersky et al., 2012). We ultimatelycast doubt on this claim, showing that, under the strictest controls, translationese is different, but not any easier to model according to our notion of difficulty. We conclude with a recommendation: The world being small, typology is in practice a small-data problem. there is a real danger that cross-linguistic studies will under-sample and thus over-extrapolate. We outline directions for future, more robust, investigations, and further caution that future work of this sort should focus on datasets with far more languages, something our new methods now allow. 2 The Surprisal o"
P19-1491,P16-1162,0,0.00976653,".5 If we were to assume that our language models were perfect in the sense that they captured the true probability distribution of a language, we could make the former claim; but we suspect that much of the difference can be explained by our imperfect LMs rather than inherent differences in the expressed information (see the discussion in footnote 3). 2.3 BPE-RNNLM BPE-based open-vocabulary language models make use of sub-word units instead of either words or characters and are a strong baseline on multiple languages (Mielke and Eisner, 2018). Before training the RNN, byte pair encoding (BPE; Sennrich et al., 2016) is applied globally to the training corpus, splitting each word (i.e., each space-separated substring) into one or more units. The RNN is then trained over the sequence of units, which looks like this: “The |ex|os|kel|eton |is |gener|ally |blue”. The set of subword units is finite and determined from training data only, but it is a superset of the alphabet, making it possible to explain any novel word in held-out data via some segmentation.7 One important thing to note is that the size of this set can be tuned by specifying the number of BPE merges, allowing us to smoothly vary between a word"
P19-1491,L16-1680,0,0.0569772,"Missing"
P19-1491,K17-3009,0,0.0601694,"Missing"
P19-1491,H01-1035,0,0.0770762,"led by the data that it did not make much sense to spend time on them. Specifically, for full inference, we implemented all models in STAN (Carpenter et al., 2017), a 4.2 14 One could also use a Cauchy distribution instead of the Laplace distribution to get even heavier tails, but we saw little difference between the two in practice. 15 Further enhancements are possible: we discuss our “Model 3” in Appendix B, but it did not seem to fit better. The Bible: 62 Languages The Bible is a religious text that has been used for decades as a dataset for massively multilingual NLP (Resnik et al., 1999; Yarowsky et al., 2001; Agi´c et al., 2016). Concretely, we use the 5 de pl hu fr lt da el it bg fi cs ro nl pt es et sl en sk sv lv hardly affected when tuning the number of BPE merges per-language instead of globally, validating our approach of using the BPE model for our experiments. A bigger difference seems to be the choice of char-RNNLM vs. BPE-RNNLM, which changes the ranking of languages both on Europarl data and on Bibles. We still see German as the hardest language, but almost all other languages switch places. Specifically, we can see that the variance of the char-RNNLM is much higher. chars BPE (0.4|V |"
P19-1491,N18-1202,0,0.0467576,"al properties that make certain languages harder to language-model than others. One of the oldest tasks in NLP (Shannon, 1951) is language modeling, which attempts to estimate a distribution ?(x) over strings x of a language. Recent years have seen impressive improvements with recurrent neural language models (e.g., Merity et al., 2018). Language modeling is an important component of tasks such as speech recognition, machine translation, and text normalization. It has also enabled the construction of contextual word embeddings that provide impressive performance gains in many other NLP tasks (Peters et al., 2018)—though those downstream evaluations, too, have focused on a small number of (mostly English) datasets. In prior work (Cotterell et al., 2018), we compared languages in terms of the difficulty of language modeling, controlling for differences in content by using a multi-lingual, fully parallel text corpus. Few such corpora exist: in that paper, we made Introduction Do current NLP tools serve all languages? Technically, yes, as there are rarely hard constraints that prohibit application to specific languages, as long as there is data annotated for the task. However, in practice, the answer is m"
P19-1491,W09-0106,0,\N,Missing
P19-1491,2005.mtsummit-papers.11,0,\N,Missing
P19-1491,mayer-cysouw-2014-creating,0,\N,Missing
P19-1491,N18-2085,1,\N,Missing
P19-1491,L18-1293,1,\N,Missing
P19-1491,D18-1312,0,\N,Missing
P19-1491,Q15-1030,0,\N,Missing
P98-2182,J93-1003,0,0.0114492,"structure; even relatively common nouns m~v not occur in the corpus more than a handful of times in such a context. The two figures of merit t h a t we employ, one to select and one to produce a final rank, use the following two counts for each noun: 1. a noun's co-occurrences with seed words 2. a noun's co-occurrences with any word To select new seed words, we take the ratio of count 1 to count 2 for the noun in question. This is similar to the figure of merit used in R&:S, and also tends to promote low frequency nouns. For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning for details). This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random. For instance, suppose t h a t two words occur forty times each, and they co-occur twenty times in a millionword corpus. This would be more surprising for two completely random distributions than if they had each occurred twice and had always co-occurred. A simple probability does not capture this fact. The rationale for using two different statistics for this task is t h"
P98-2182,P95-1007,0,0.011228,"f the resulting compound has already been output, the entry is skipped. Each noun is evaluated as follows: First, the head of t h a t noun is determined. To get a sense of what is meant here, consider the following compound: nuclear-powered aircraft carrier. In evaluating the word nuclearpowered, it is unclear if this word is attached to aircraft or to carrier. While we know t h a t the head of the entire compound is carrier, in order to properly evaluate the word in question, we must determine which of the words following it is its head. This is done, in the spirit of the Dependency Model of Lauer (1995), by selecting the noun to its right in the compound with the highest probability of occuring with the word in question when occurring in a noun compound. (In the case t h a t two nouns have the same probability, the rightmost noun is chosen.) Once the head of the word is determined, the ratio of count 1 (with the head noun chosen) to count 2 is compared to an empirically set cutoff. If it falls below t h a t cutoff, it is omitted. If it does not fall below the cutoff, then it is kept (provided its head noun is not later omitted). 6 Outline of the algorithm The input to the algorithm is a pars"
P98-2182,J93-2004,0,0.0327198,"onents: one to deal with conjunctions, lists, and appositives; and the other to deal with noun compounds. All compound nouns in the former constructions are represented by the head of the compound. We made the simplifying assumptions t h a t a compound noun is a string of consecutive nouns (or, in certain cases, adjectives - see discussion below), and t h a t the head of the compound is the rightmost noun. To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993). We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. The relation is stipulated to be transitive, so t h a t all head nouns in a list co-occur with each other (e.g. in the phrase planes, trains, and automobiles all three nouns are counted as co-occuring with each other). Two head nouns co-occur in this algorithm if they meet the following four conditions: 1. they are both dominated by a common NP node 2. no dominating S or VP nodes are dominated by t h a t same NP node 3. all head nouns t h a t precede one, precede the other 4. there is a"
P98-2182,W97-0313,0,0.511932,"ple, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and o u t p u t a ranked list Our algorithm uses roughly this same generic st"
P98-2182,P95-1026,0,0.00787085,"inadequate. For example, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and o u t p u t a ranked list Our algorithm us"
P99-1054,W98-1115,1,0.831692,"lly, an LC grammar performs best with our parser when right binarized, for the same reasons outlined above. We use transform composition to apply first one transform, then another to the o u t p u t of the first. We denote this A o B where (A o B) (t) = B (A (t)). After applying the left-corner transform, we then binarize the resulting grammar 5, i.e. LC o RB. Another probabilistic LC parser investigated (Manning and Carpenter, 1997), which utilized an LC parsing architecture (not a transformed grammar), also got a performance boost 4The very efficient bottom-up statistical parser detailed in Charniak et al. (1998) measured efficiency in terms of total edges popped. An edge (or, in our case, a parser state) is considered when a probability is calculated for it, and we felt that this was a better efficiency measure than simply those popped. As a baseline, their parser considered an average of 2216 edges per sentence in section 22 of the WSJ corpus (p.c.). 5Given that the LC transform involves nullary productions, the use of RB0 is not needed, i.e. nullary productions need only be introduced from one source. Thus binarization with left corner is always to unary (RB1). Transform Rules in Grammar Left Corne"
P99-1054,J93-2004,0,0.0254098,"4 Percent of Sentences Parsed* 34.16 33.99 91.27 97.37 Avg. States Considered 19270 96813 10140 13868 *Length ~ 40 (2245 sentences in F23 Avg. Labelled Precision and Recall t .65521 .65539 .71616 .73207 Avg. MLP Labelled Prec/Rec t .76427 .76095 .72712 .72327 Avg. length -- 21.68) Ratio of Avg. Prob to Avg. MLP Prob t .001721 .001440 .340858 .443705 t o f those sentences parsed Table 1: The effect of different approaches to binarization ing the effect of different binarization approaches on parser performance. The grammars were induced from sections 2-21 of the Penn Wall St. Journal Treebank (Marcus et al., 1993), and tested on section 23. For each transform tested, every tree in the training corpus was transformed before grammar induction, resulting in a transformed PCFG and lookahead probabilities estimated in the standard way. Each parse returned by the parser was detransformed for evaluation 3. The parser used in each trial was identical, with a base b e a m factor c~ = 10 -4. The performance is evaluated using these measures: (i) the percentage of candidate sentences for which a parse was found (coverage); (ii) the average number of states (i.e. rule expansions) considered per candidate sentence"
P99-1054,P80-1024,0,0.77665,"Missing"
P99-1054,J98-4004,1,\N,Missing
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
W00-1604,J98-2004,1,\N,Missing
W00-1604,A00-2018,1,\N,Missing
W00-1604,J98-4004,0,\N,Missing
W00-1604,J93-2004,0,\N,Missing
W00-1604,W97-0301,0,\N,Missing
W00-1604,P97-1003,0,\N,Missing
W00-1604,J01-2004,1,\N,Missing
W00-1604,W00-1603,0,\N,Missing
W00-1604,P99-1066,1,\N,Missing
W04-0303,W97-0301,0,\N,Missing
W04-0303,P99-1069,0,\N,Missing
W04-0303,P04-1015,1,\N,Missing
W04-0303,J01-2004,1,\N,Missing
W04-0303,P02-1034,0,\N,Missing
W07-1001,P95-1037,0,0.113842,"the deleted NP), and its path continues up the tree. Empty NPs are annotated in our manual parse trees but not in the automatic parses, which may result in a small disagreement in the Frazier scores for manual and automatic trees. 3 Every non-terminal node beginning with an S, including SQ and SINV, were counted as sentence nodes. Sequences of sentence nodes, i.e. an SBAR appearing directly under an S node, were only counted as a single sentence node and thus only contributed to the score once. she was a cook in a school cafeteria Figure 3: Dependency graph for the example string. tributed to Magerman (1995), to percolate lexical heads up the tree. Figure 3 shows the dependency graph that results from this head percolation approach, where each link in the graph represents a dependency relation from the modifier to the head. For example, conventional head percolation rules specify the VP as the head of the S, so ‘was’, as the head of the VP, is thus the lexical head of the entire sentence. The lexical heads of the other children of the S node are called modifiers of the head of the S node; thus, since ‘she’ is the head of the subject NP, there is a dependency relation between ‘she’ and ‘was’. Lin"
W07-1001,J93-2004,0,0.0278425,"tactic annotation is critical – different conventions of structural annotation will yield different scores. We will thus spend the next section briefly detailing the syntactic annotation conventions that were followed for this work. This is followed by a section describing a range of complexity measures to be derived from these annotations. Finally, we present empirical results on the samples of spoken narrative retellings. 2 Syntactic annotation For manual syntactic annotation of collected data (see Section 4), we followed the syntactic annotation conventions of the well-known Penn Treebank (Marcus et al., 1993). This provides several key benefits. First, there is an extensive annotation guide that has been developed, not just for written but also for spoken language, so that consistent annotation was facilitated. Second, the large out-of-domain corpora, in particular the 1 million words of syntactically annotated Switchboard telephone conversations, provide a good starting point for training domain adapted parsing models. Finally, we can use multiple domains for evaluating the correlations between various syntactic complexity measures. There are characteristics of Penn Treebank annotation that can i"
W07-1001,C92-1032,0,0.211165,"P 1 NP NP 1 0 NP NP 1 PP 0 1 NN 1 0 DT DT PP 1 IN NN IN NP 2 NP 1 0 DT 1 NN NN DT NN NN she Frazier score: 2.5 she Yngve score: 1 was 1 a 2 cook 1 in 1 a school cafeteria 2 1 0 a 2 cook 0 in 1 a 1 school cafeteria 0 0 Figure 2: Parse tree fragments with scores for Frazier scoring. Figure 1: Parse tree with branch scores for Yngve scoring. is reached, just NP is on the stack. Thus, the Yngve score for these two words is 1. When the next word ‘a’ is reached, however, there are two categories on the stack: PP and NN, so this word receives an Yngve score of 2. Stack size has been related by some (Resnik, 1992) to working memory demands, although it most directly measures deviation from right-branching trees. 1 To calculate the size of the stack at each word, we can use the following simple algorithm. At each node in the tree, label the branches from that node to each of its children, beginning with zero at the rightmost child and continuing to the leftmost child, incrementing the score by one for each child. Hence, each rightmost branch in the tree of Figure 1 is labeled with 0, the leftmost branch in all binary nodes is labeled with 1, and the leftmost branch in the ternary node is labeled with 2."
W07-1001,N01-1016,0,0.0104251,"ce, touched by the woman’s story, took up a collection for her. Subjects are asked to re-tell this story immediately after it is told to them (LM I), as well as after approximately 30 minutes of unrelated activities (LM II). We transcribed each retelling, and manually annotated syntactic parse trees according to the Penn Treebank annotation guidelines. Algorithms for automatically extracting syntactic complexity markers from parse trees were written to accept either man5 4.3 Parsing For automatic parsing, we made use of the wellknown Charniak parser (Charniak, 2000). Following best practices (Charniak and Johnson, 2001), we removed sequences covered by EDITED nodes in the tree from the strings prior to parsing. For this paper, EDITED nodes were identified from the manual parse, not automatically. Table 2 shows parsing accuracy of our annotated retellings under three parsing model training conditions: 1) trained on approximately 1 million words of Wall St. Journal (WSJ) text; 2) trained on approximately 1 million words of Switchboard (SWBD) corpus telephone conversations; and 3) using domain adaptation techniques starting from the SWBD Treebank. The SWBD outof-domain system reaches quite respectable accuracie"
W07-1001,P80-1024,0,0.802642,"Missing"
W07-1001,A00-2018,0,0.251361,"d they had not eaten for two days. The police, touched by the woman’s story, took up a collection for her. Subjects are asked to re-tell this story immediately after it is told to them (LM I), as well as after approximately 30 minutes of unrelated activities (LM II). We transcribed each retelling, and manually annotated syntactic parse trees according to the Penn Treebank annotation guidelines. Algorithms for automatically extracting syntactic complexity markers from parse trees were written to accept either man5 4.3 Parsing For automatic parsing, we made use of the wellknown Charniak parser (Charniak, 2000). Following best practices (Charniak and Johnson, 2001), we removed sequences covered by EDITED nodes in the tree from the strings prior to parsing. For this paper, EDITED nodes were identified from the manual parse, not automatically. Table 2 shows parsing accuracy of our annotated retellings under three parsing model training conditions: 1) trained on approximately 1 million words of Wall St. Journal (WSJ) text; 2) trained on approximately 1 million words of Switchboard (SWBD) corpus telephone conversations; and 3) using domain adaptation techniques starting from the SWBD Treebank. The SWBD"
W07-1001,P05-1025,0,0.54272,"statistically significant differences between clinical subject groups for a number of syntactic complexity measures, and these differences are preserved with automatic parsing. Different measures show different patterns for our data set, indicating that using multiple, complementary measures is important for such an application. 1 Introduction Natural language processing (NLP) techniques are often applied to electronic health records and other clinical datasets. Another potential clinical use of NLP is for processing patient language samples, which can be used to assess language development (Sagae et al., 2005) or the impact of neurodegenerative impairments on speech and language (Roark et al., 2007). In this paper, we present methods for automatically measuring syntactic complexity of spoken language samples elicited during neuropsychological exams of elderly subjects, and examine the utility of these measures for discriminating between clinically defined groups. Mild Cognitive Impairment (MCI), and in particular amnestic MCI, the earliest clinically defined stage of Alzheimer’s-related dementia, often goes undiagnosed due to the inadequacy of common screening tests such as the Mini-Mental State Ex"
W10-1304,W05-1107,0,0.686621,"both generative and discriminative. See Roark (2009) for extensive discussion of these issues. Here we will consider ngram language models of various orders, estimated via smoothed relative frequency estimation (see § 3.1). The principal novelty in the current approach is the principled incorporation of error probabilities into the binary coding approaches, and the experimental demonstration of how linear coding for grids or RSVP interfaces compare to Huffman coding and row/column scanning for grids. 3 Methods 3.1 Character-based language models For this paper, we use character n-gram models. Carpenter (2005) has an extensive comparison of large scale character-based language models, and we adopt smoothing methods from that paper. It presents a version of Witten-Bell smoothing (Witten and Bell, 1991) with an optimized hyperparameter K, which is shown to be as effective as KneserNey smoothing (Kneser and Ney, 1995) for higher order n-grams. We refer readers to that paper for details on this standard n-gram language modeling approach. For the experimental results presented here, we trained unigram and 8-gram models from the NY Times portion of the English Gigaword corpus. We performed extensive norm"
W10-1304,W03-2508,0,0.0283197,"r can move dynamically, because such dynamic layouts facilitate integration of richer language models. For example, if we re-calculate character probabilities after each typed character, then we could re-arrange the characters in the grid so that the most likely are placed in the upper left-hand corner for row/column scanning. Conventional wisdom, however, is that the cognitive overhead of processing a different grid arrangement after every character would slow down typing more than the speedup due to the improved binary coding (Baletsa et al., 1976; Lesher et al., 1998). The GazeTalk system (Hansen et al., 2003), which presents the user with a 3×4 grid and captures which cell the user’s gaze fixates upon, is an instance of a dynamically changing grid. The cell layouts are configurable, but typically one cell contains a set of likely word completions; others are allocated to space and backspace; and around half of the cells are 29 allocated to the most likely single character continuation of the input string, based on language model predictions. Hansen et al. (2003) report that users produced more words per minute with a static keyboard than with the predictive grid interface, illustrating the impact"
W10-1304,N07-2044,0,0.0282769,"redictions. Hansen et al. (2003) report that users produced more words per minute with a static keyboard than with the predictive grid interface, illustrating the impact of the cognitive overhead that goes along with this sort of scanning. The likely word completions in the GazeTalk system illustrates another common way in which language modeling is integrated into AAC typing systems. Much of the language modeling research within the context of AAC has been for word completion/prediction for keystroke reduction (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007). The typical scenario for this is allocating a region of the interface to contain a set of suggested words that complete what the user has begun typing. The expectation is to derive a keystroke savings when the user selects one of the alternatives rather than typing the rest of the letters. The cognitive load of monitoring a list of possible completions has made the claim that this speeds typing controversial (Anson et al., 2004); yet some results have shown this to speed typing under certain conditions (Trnka et al., 2007). One innovative language-model-driven"
W10-1304,D07-1053,0,0.012896,"t al. (2003) report that users produced more words per minute with a static keyboard than with the predictive grid interface, illustrating the impact of the cognitive overhead that goes along with this sort of scanning. The likely word completions in the GazeTalk system illustrates another common way in which language modeling is integrated into AAC typing systems. Much of the language modeling research within the context of AAC has been for word completion/prediction for keystroke reduction (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007). The typical scenario for this is allocating a region of the interface to contain a set of suggested words that complete what the user has begun typing. The expectation is to derive a keystroke savings when the user selects one of the alternatives rather than typing the rest of the letters. The cognitive load of monitoring a list of possible completions has made the claim that this speeds typing controversial (Anson et al., 2004); yet some results have shown this to speed typing under certain conditions (Trnka et al., 2007). One innovative language-model-driven AAC typing interface is Dasher"
W11-0610,P08-2002,0,0.0123659,"tchboard corpus to be the most effective large corpus that we had at our disposal for this study. As the size of our small corpus grows, we intend to make use of the text to assist with model building, but for this study, we used all out-of-domain data for n-gram language models and parsing models. Using Switchboard also allowed us to use the same corpus to train both n-gram and parsing models. Surprisal-based features. Surprisal, or the unexpectedness of a word or syntactic category in a given context, is often used as a psycholinguistic measure of sentence-processing difficulty (Hale, 2001; Boston et al., 2008). Although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in Table 2, which often contain common words used in surprising ways, and the nondevelopmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate. To derive surprisal-based features, each sentence is parsed using the Roark (2001) incremental top-down parser relying on a model built again on the Switchboard corpus. The inc"
W11-0610,A00-2018,0,0.101662,"ir ill-formed syntactic structure. Following Roark et al. (in press), in which the authors explored the relationship between linguistic structural complexity and cognitive decline, and Sagae (2005), in which the authors used automatic syntactic annotation to assess syntactic development, we also investigated the following measures of linguistic complexity: words per clause, tree nodes per word, dependency length per word, and Ygnve and Frazier scores per word. Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switchboard treebank. Briefly, words per clause is the total number of words divided by the total number of clauses; and tree nodes per word is the total number of nodes in the parse tree divided by the number of words. The dependency length for a word is the distance (in word tokens) between that word and its governor, as determined through standard head-percolation methods from the output of the Charniak parser. We calculate the mean of this length over all words in the utterance. The Yngve score of a word is the size of the stack of a shift-reduce parser after that wor"
W11-0610,N09-1006,0,0.163289,"te 1) the three types of errors described above, and 2) the three diagnostic groups discussed above. In the next three sections, we will discuss the various linguistic features we extract; methods for using these features to classify each sentence according to its error type for the purpose of automatic error-detection; and methods for using these features, calculated for each subject, for diagnostic classification. 4 Features N-gram cross entropy. Following previous work in both error detection (Gamon et al., 2008; Leacock and Chodorow, 2003) and neurodevelopmental diagnostic classification (Gabani et al., 2009), we begin with simple bigram language model features. A bigram language model provides information about the likelihood of a given item (e.g., a word or part of speech) in a sentence given the previous item in that sentence. We suspect that some of the types of unusual language investigated here, in particular those seen in the syntactic errors shown in Table 2, are characterized by unlikely words (drinked) and word or part-of-speech sequences (a games, all of 91 out) and hence might be distinguished by language model-based scores. We build a word-level bigram language model and a part-of-spe"
W11-0610,I08-1059,0,0.0199629,"omplexity, and surprisal that have the potential to objectively capture qualities that differentiate 1) the three types of errors described above, and 2) the three diagnostic groups discussed above. In the next three sections, we will discuss the various linguistic features we extract; methods for using these features to classify each sentence according to its error type for the purpose of automatic error-detection; and methods for using these features, calculated for each subject, for diagnostic classification. 4 Features N-gram cross entropy. Following previous work in both error detection (Gamon et al., 2008; Leacock and Chodorow, 2003) and neurodevelopmental diagnostic classification (Gabani et al., 2009), we begin with simple bigram language model features. A bigram language model provides information about the likelihood of a given item (e.g., a word or part of speech) in a sentence given the previous item in that sentence. We suspect that some of the types of unusual language investigated here, in particular those seen in the syntactic errors shown in Table 2, are characterized by unlikely words (drinked) and word or part-of-speech sequences (a games, all of 91 out) and hence might be disting"
W11-0610,N01-1021,0,0.00781885,"e of the Switchboard corpus to be the most effective large corpus that we had at our disposal for this study. As the size of our small corpus grows, we intend to make use of the text to assist with model building, but for this study, we used all out-of-domain data for n-gram language models and parsing models. Using Switchboard also allowed us to use the same corpus to train both n-gram and parsing models. Surprisal-based features. Surprisal, or the unexpectedness of a word or syntactic category in a given context, is often used as a psycholinguistic measure of sentence-processing difficulty (Hale, 2001; Boston et al., 2008). Although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in Table 2, which often contain common words used in surprising ways, and the nondevelopmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate. To derive surprisal-based features, each sentence is parsed using the Roark (2001) incremental top-down parser relying on a model built again on the Switc"
W11-0610,D09-1034,1,0.783997,"ed that it might also capture some of the language characteristics of the semantic errors like those in Table 2, which often contain common words used in surprising ways, and the nondevelopmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate. To derive surprisal-based features, each sentence is parsed using the Roark (2001) incremental top-down parser relying on a model built again on the Switchboard corpus. The incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in Roark et al. (2009). For each sentence, we collected the mean surprisal (equivalent to the cross entropy given the model); the mean syntactic surprisal; and the mean lexical surprisal. The lexical and syntactic surprisal are a decomposition of the total surprisal into that portion due to probability mass associated with building non-terminal structure (syntactic surprisal) and that portion due to probability mass associated with building terminal lexical items in the tree (lexical surprisal). We refer the reader to that paper for further details. Other linguistic complexity measures The nondevelopmental syntax e"
W11-0610,J01-2004,1,0.454394,"ten used as a psycholinguistic measure of sentence-processing difficulty (Hale, 2001; Boston et al., 2008). Although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in Table 2, which often contain common words used in surprising ways, and the nondevelopmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate. To derive surprisal-based features, each sentence is parsed using the Roark (2001) incremental top-down parser relying on a model built again on the Switchboard corpus. The incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in Roark et al. (2009). For each sentence, we collected the mean surprisal (equivalent to the cross entropy given the model); the mean syntactic surprisal; and the mean lexical surprisal. The lexical and syntactic surprisal are a decomposition of the total surprisal into that portion due to probability mass associated with building non-terminal structure (syntactic surprisal) and that portion due to"
W11-0610,P05-1025,0,0.278924,"Missing"
W11-2303,J93-2004,0,0.039489,"ions. 27 The idea in using this data was to provide some number of utterances of dialog context (from the 10 previous dialog turns), and then ask subjects to provide word completions for some number of subsequent utterances. While the Switchboard corpus does represent the kind of conversational dialog we are interested in, it is a spoken language corpus, yet we are modeling written (typed) language. The difference between written and spoken language does present something of an issue for our task. To mitigate this mismatch somewhat, we made use of the Switchboard section of the Penn Treebank (Marcus et al., 1993), which contains syntactic annotations of the Switchboard transcripts, including explicit marking of disfluencies (“EDITED” non-terminals in the treebank), interjections or parentheticals such as “I mean” or “you know”. Using these syntactic annotations, we produced edited transcripts that omit much of the spoken language specific phenomena, thus providing a closer approximation to the kind of written dialogs we would like to simulate. In addition, we decased the corpus and removed all characters except the following: the 26 letters of the English alphabet, the apostrophe, the space, and the d"
W11-2303,W09-0601,0,0.0161603,"ginbotham (2008; 2009) proposed a novel method that uses automatic speech recognition (ASR) on the speech of the communication partner, extracts noun phrases from the speech, and presents those noun phrases on the AAC device, with frame sentences that the AAC user can select. Thus if the communication partner says “Paris”, the AAC user will be able to select from phrases like “Tell me more about Paris” or “I want to talk about Paris”. This can speed up the conversation by providing topically-relevant responses. Perhaps the most elaborate system of this kind is the How Was School Today system (Reiter et al., 2009). This system, which is geared towards children with severe communication disabilities, uses data from sensors, the Web, and other sources as input for a natural language generation system. The system acquires information about the child’s day in school: which classes he or she attended, what activities there were, information about visitors, food choices at the cafeteria, and so forth. The data are then used to generate natural language sentences, which are converted to speech via a speech synthesizer. At the end of the day, the child uses a menu to select sentences that he or she wants the s"
W11-2303,N07-2044,0,0.100839,"ugh alternatives (Lesher et al., 1998). Typing speed is a challenge, yet is critically important for usability, and as a result there is a significant line of research into 22 {gibbons,mfo}@ohsu.edu the utility of statistical language models for improving typing speed (McCoy et al., 2007; Koester and Levine, 1996; Koester and Levine, 1997; Koester and Levine, 1998). Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007; Todman et al., 2008). To the extent that the predictions are accurate, the number of keystrokes required to type a message can be dramatically reduced, greatly speeding typing. AAC devices for spontaneous and novel text generation are intended to empower the user of the system, to place them in control of their own communication, and reduce their reliance on others for message formulation. As a result, all such devices (much like standard personal computers) are built for a single user, with a single keyboard and/or alternative input interface, which is driven b"
W11-2303,D07-1053,0,0.523895,"sher et al., 1998). Typing speed is a challenge, yet is critically important for usability, and as a result there is a significant line of research into 22 {gibbons,mfo}@ohsu.edu the utility of statistical language models for improving typing speed (McCoy et al., 2007; Koester and Levine, 1996; Koester and Levine, 1997; Koester and Levine, 1998). Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007; Todman et al., 2008). To the extent that the predictions are accurate, the number of keystrokes required to type a message can be dramatically reduced, greatly speeding typing. AAC devices for spontaneous and novel text generation are intended to empower the user of the system, to place them in control of their own communication, and reduce their reliance on others for message formulation. As a result, all such devices (much like standard personal computers) are built for a single user, with a single keyboard and/or alternative input interface, which is driven by the user of the system. The"
W11-2305,W05-1107,0,0.0287031,"d. Thus for ease of visual scanning, we chose in this study to use alphabetic ordering. 3.1 Language models and binary codes We follow Roark et al. (2010) and build characterbased smoothed 8-gram language models from a normalized 42M character subset of the English gigaword corpus and the CMU pronunciation dictionary. This latter lexicon is used to increase coverage of words that are unobserved in the corpus, and is included in training as one observation per word in the lexicon. Smoothing is performed with a generalized version of Witten-Bell smoothing (Witten and Bell, 1991) as presented in Carpenter (2005). Text normalization and smoothing parameterizations were as presented in Roark et al. (2010). Probability of the delete symbol ← was taken to be 0.05 in all trials (the same as the probability of an error, see Section 3.2), and all other probabilities derived from the trained n-gram language model. 3.2 Huffman scanning Our first scanning condition replicates the Huffman scanning from Roark et al. (2010), with two differences. First, as stated above, we use an alphabetic ordering of the grid as shown in Figure 2, in place of their frequency ordered grid. Second, rather than calibrating the sca"
W11-2305,W10-1304,1,0.885835,"Missing"
W11-2920,P11-1045,1,0.819788,"the hT, T i semiring is quite distinct from SpMV in the real semiring, we anticipate that some of those algorithms will apply, and would be of particular inter4 Evaluation We compare exhaustive and pruned parsing efficiency with several other competitive parsing implementations. We performed all matrix-encoded parsing and parallelization experiments using the open-source BUBS parser (Bodenstab and Dunlop, 2011). The parser framework is grammar agnostic, permitting experiments on grammars of various sizes, and it implements both exhaustive inference and ‘Adaptive Beam Pruning’, as described in Bodenstab et al. (2011). For exhaustive parsing, we use BUBS implementation of Algorithms 2 and 3 and Mark Johnson’s highly optimized C implementation, lncky (Johnson, 2006) as baselines; for pruned inference, we compare with the Charniak parser (Charniak, 2000), the 5 6 Since W2 represents a midpoint, we could alter the definition to specify that W2 ∈ N, but the standard tropical semiring is adequate and slightly simpler. On most modern CPUs, linear memory access patterns allow aggressive and effective data pre-fetching into cache, avoiding costly CPU stalls. 168 Categories Binarized Rules F-score BUBS Grammar loop"
W11-2920,J98-2004,0,0.22633,"al., 1999). The Markov-order-0 and Markov-order-2 grammars were markovized as described in Manning and Schuetze (1999). The parent-annotated grammar further splits the states of the Markov-order-2 grammar by annotating each non-terminal with its parent category, as described in Johnson (1998). This expands the vocabulary greatly, but the rule4.2 Pruned Serial Search Most state-of-the-art context-free parsers resort to approximate inference techniques to decode efficiently. These methods include Coarse-to-Fine (Petrov et al., 2006), A* (Klein and Manning, 2003; Pauls et al., 2010), best-first (Caraballo and Charniak, 1998; Charniak, 2000), and beam 7 By default, the Berkeley parser marginalizes over the latent-variables in the grammar and retrieves the Max-Rule parse tree; for fair comparison with our approach, we report timings in its simpler Viterbi-search mode. 169 Charniak (2000) Berkeley (CTF Viterbi) Adaptive Beam w/Alg. 3 Adaptive Beam w/Alg. 4 F-score 90.3 89.3 89.0 89.1 Sent/sec 1.7 4.7 10.2 21.9 eterization, the search space explored by our approach is identical to that explored by Bodenstab et al., (modulo minor differences in unary processing), so the efficiencies achieved are directly comparable."
W11-2920,A00-2018,0,0.807306,"etitive parsing implementations. We performed all matrix-encoded parsing and parallelization experiments using the open-source BUBS parser (Bodenstab and Dunlop, 2011). The parser framework is grammar agnostic, permitting experiments on grammars of various sizes, and it implements both exhaustive inference and ‘Adaptive Beam Pruning’, as described in Bodenstab et al. (2011). For exhaustive parsing, we use BUBS implementation of Algorithms 2 and 3 and Mark Johnson’s highly optimized C implementation, lncky (Johnson, 2006) as baselines; for pruned inference, we compare with the Charniak parser (Charniak, 2000), the 5 6 Since W2 represents a midpoint, we could alter the definition to specify that W2 ∈ N, but the standard tropical semiring is adequate and slightly simpler. On most modern CPUs, linear memory access patterns allow aggressive and effective data pre-fetching into cache, avoiding costly CPU stalls. 168 Categories Binarized Rules F-score BUBS Grammar loop (Algorithm 2) BUBS Left-child loop (Algorithm 3) Johnson (2006) SpMV (this paper) Markov-0 100 3859 60.7 0.16 0.13 0.10 0.04 Markov-2 3092 13649 71.9 1.92 0.50 0.22 0.26 Parent 6971 25229 77.5 23.4 0.8 0.3 1.2 Berkeley SM6 1134 1,725,570"
W11-2920,P97-1002,0,0.0681925,"observed child pair. c is represented as independent segments safe for lock-free mutation by independent threads (see Section 3.4). To perform the matrix-vector operation in parallel, we retain the same segments of c, and segment G similarly. Each thread t multiplies its segment Gt · ct , producing a vector αt . We then merge the αt vectors into the final α. Since |V |<< |c|, this final merge is quite efficient. ber of expensive grammar operations from O(n3 ) to O(n2 ). We note the similarity to the formalisms of Valiant (1975), which transforms parsing into boolean matrix multiplication, and Lee (1997), which inverts that transformation. However, the similarity is only superficial; Valient’s algorithm populates an upper-triangular matrix, the elements of which are equivalent to CYK chart cells. Each matrix element is a subset of V , the observed population of the analogous chart cell. The matrix is populated by a transitive closure operation, which takes the place of the CYK algorithm. Our matrix operation, on the other hand, is concerned with the population of individual chart cells, the operation accomplished by Valient’s ∗ operator. Decoupling the midpoint iteration from grammar intersec"
W11-2920,J98-4004,0,0.0609929,"core can execute 2 simultaneous threads, for a total of 24 concurrent threads. For the parsers implemented in Java, we used the Oracle 1.6.0 26 Virtual Machine. 4.1 Exhaustive Serial Search In Table 1, we present exhaustive search results with four grammars, each induced from the Penn Treebank Sections 2-21 (Marcus et al., 1999). The Markov-order-0 and Markov-order-2 grammars were markovized as described in Manning and Schuetze (1999). The parent-annotated grammar further splits the states of the Markov-order-2 grammar by annotating each non-terminal with its parent category, as described in Johnson (1998). This expands the vocabulary greatly, but the rule4.2 Pruned Serial Search Most state-of-the-art context-free parsers resort to approximate inference techniques to decode efficiently. These methods include Coarse-to-Fine (Petrov et al., 2006), A* (Klein and Manning, 2003; Pauls et al., 2010), best-first (Caraballo and Charniak, 1998; Charniak, 2000), and beam 7 By default, the Berkeley parser marginalizes over the latent-variables in the grammar and retrieves the Max-Rule parse tree; for fair comparison with our approach, we report timings in its simpler Viterbi-search mode. 169 Charniak (200"
W11-2920,P01-1044,0,0.232749,"uch applications (Bjrne et al., 2010; Banko, 1999). Most constituent parsers leverage the dynamic programming “chart” structure of the CYK algorithm, even when performing approximate inference. The inner loop of the CYK algorithm computes an argmax for each constituent span by intersecting the set of observed child categories spanning adjacent substrings with the set of rule productions in the grammar. This ‘grammar intersection’ operation is the most computationally intensive component of the algorithm. Prior work has shown that the grammar encoding can greatly affect parsing efficiency (c.f Klein and Manning (2001), Moore (2004), Penn and Munteanu (2003)); in this paper we present a matrix encoding that can encode very large grammars to maximize inference efficiency. This matrix grammar encoding allows a refactoring of the CYK algorithm with two beneficial properties: 1) the number of expensive grammar intersection operations is reduced from O(n3 ) to 163 Proceedings of the 12th International Conference on Parsing Technologies, pages 163–174, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. tion 3 we provide a detailed presentation of our grammar encoding, data"
W11-2920,P10-2037,0,0.032315,"Missing"
W11-2920,P03-1026,0,0.0322365,"nko, 1999). Most constituent parsers leverage the dynamic programming “chart” structure of the CYK algorithm, even when performing approximate inference. The inner loop of the CYK algorithm computes an argmax for each constituent span by intersecting the set of observed child categories spanning adjacent substrings with the set of rule productions in the grammar. This ‘grammar intersection’ operation is the most computationally intensive component of the algorithm. Prior work has shown that the grammar encoding can greatly affect parsing efficiency (c.f Klein and Manning (2001), Moore (2004), Penn and Munteanu (2003)); in this paper we present a matrix encoding that can encode very large grammars to maximize inference efficiency. This matrix grammar encoding allows a refactoring of the CYK algorithm with two beneficial properties: 1) the number of expensive grammar intersection operations is reduced from O(n3 ) to 163 Proceedings of the 12th International Conference on Parsing Technologies, pages 163–174, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. tion 3 we provide a detailed presentation of our grammar encoding, data structures, and intersection method. In"
W11-2920,P06-1055,0,0.693838,"p; in practice, we are likely to see some tradeoff. We will demonstrate interesting patterns of the tradeoff between throughput and latency with various parallelization methods, allowing consumers to tailor parsing strategies to particular application requirements. Our grammar intersection method is amenable to graphics processors (GPUs) and similar massively-parallel architectures. In this work, we perform our analysis on a multicore CPU system. We demonstrate the utility of this approach using a number of different grammars, including the latent variable grammar used by the Berkeley parser (Petrov et al., 2006). We show large speedups compared to a traditional CYK implementation for serial inference, parsing over 20 sentences per second with the Berkeley grammar. Parallelizing this algorithm reduces average latency to .026 seconds. The remainder of this paper is organized as follows: we begin in Section 2 with background on the CYK algorithm and various general and CYKspecific parallelization considerations. In SecAbstract We present a matrix encoding of contextfree grammars, motivated by hardware-level efficiency considerations. We find efficiency gains of 2.5–9× for exhaustive inference and approx"
W11-2920,W11-2921,0,0.0143864,"ination between threads — when we split a single grammar intersection operation across many threads, each task is quite small. At this fine granularity, locking of shared data structures is impractical, so we must divide tasks such that they share immutable data (the grammar and current cell population) but do not simultaneously mutate the same target data structures (e.g., individual threads may populate separate ranges of non-terminals in the target cell, but must not attempt to populate the same range). Even with careful task division, the task management may overwhelm the potential gains. Youngmin et al. (2011) presented one approach to this problem; we present another approach in Section 3.2. 3 3 Of course, we can utilize sentence-level and cell-level parallelism as well. 4 All grammar encodings discussed, including our own, only alter efficiency; accuracy remains unchanged. 3.1 Methods Matrix Grammar Encoding Retrieval of valid grammar rules and their probabilities is an essential component of context-free parsing. High-accuracy grammars can exceed millions of productions, so efficient model access is critical to parsing performance. Prior work on encoding the grammar as a finite state automaton ("
W11-2920,N09-1073,1,0.905918,"Missing"
W11-2920,P11-2001,1,0.818079,"ulation of individual chart cells, the operation accomplished by Valient’s ∗ operator. Decoupling the midpoint iteration from grammar intersection is not contingent on our matrixvector encoding. The optimization in Graham et al. (1980) also refactors the CYK algorithm to result in O(n2 ) grammar intersection operations by changing the dynamic programming to iterate through right (or left) child cells and build new (parent) categories in multiple chart cells at once. 3.3 Lexicographic Semiring We now present Algorithm 4 more formally as an application of a lexicographic semiring (Golan, 1999). Roark et al. (2011) recently applied lexicographic semirings to language-model encoding. We will follow their notational conventions, and refer the interested reader to their detailed discussion. A semiring is a ring, possibly lacking negation, defining two operations ⊕ and ⊗ and their respective identity elements ¯0 and ¯1 (Kuich and Salomaa, 1985). One common example in speech and language applications is the tropical semiring (R∪{∞}, min, +, ∞, 0). min is the ⊕ operation, 167 with identity ∞, and + is the ⊗, with identity 0. This definition is often used for Viterbi search, using negative log probabilities as"
W11-2920,D08-1018,0,0.476307,"r encoding from Section 3.1, which decouples midpoint iteration from grammar intersection, and can reduce the cell population cost considerably. We begin by pointing to Algorithm 1, the standard CYK algorithm. The argmax on line 7 intersects the set of observed child categories spanning adjacent substrings (stored in chart cells) with the set of rule productions found in the grammar. Algorithms 2 and 3 show two possible grammar intersection methods, one which loops over productions in the grammar (Alg. 2) and one which loops over leftchildren prior to looking for grammar productions (Alg. 3). Song et al. (2008) explored a number of such grammar intersection methods, and found Algorithm 3 to be superior for right-factored grammars. We now present a novel intersection method based on the grammar encoding from Section 3.1. The description in this section is informal, with midpoints omitted for clarity. In Section 3.3, we will formalize the method as an application of a lexicographic semiring. We represent the population of each chart cell α as a vector in R|V |. Each dimension of this vector represents the (log) probability of a non-terminal 166 c Child Pair (DT,NP) (DT,NN) (NN,NN) ··· (NP,VP) (DT,S) ("
W11-2920,J03-4003,0,\N,Missing
W11-2920,N03-1016,0,\N,Missing
W12-2107,P11-1045,1,\N,Missing
W12-2401,P01-1008,0,0.011304,"e WLM for the purpose of identifying dementia, using latent semantic analysis to measure the semantic distance between a retelling Dx MCI Non-MCI n 72 163 Age 88.7 87.3 Education 14.9 yr 15.1 yr Table 1: Subject demographic data. and the source narrative. Although scoring automation is not typically used in a clinical setting, the objectivity offered by automated measures is particularly important for tests like the WLM, which are often administered by practitioners working in a community setting and serving a diverse population. Researchers working on NLP tasks such as paraphrase extraction (Barzilay and McKeown, 2001), word-sense disambiguation (Diab and Resnik, 2002), and bilingual lexicon induction (Sahlgren and Karlgren, 2005), often rely on aligned parallel or comparable corpora. Recasting the automated scoring of a neuropsychological test as another NLP task involving the analysis of parallel texts, however, is a relatively new idea. We hope that the methods presented here will both highlight the flexibility of techniques originally developed for standard NLP tasks and attract attention to the wide variety of biomedical data sources and potential clinical applications for these techniques. 3 3.1 Data"
W12-2401,J93-2003,0,0.0178176,"ues that we use are entirely unsupervised. Therefore, as in the case with most experiments involving word alignment, we build a model for the data we wish to evaluate using that same data. We do, however, use the retellings from the 26 individuals who were not experimental subjects as a development set for tuning the various parameters of our system, which is described below. 4 4.1 Word Alignment Baseline alignment We begin by building two word alignment models using the Berkeley aligner (Liang et al., 2006), a state-of-the-art word alignment package that relies on IBM mixture models 1 and 2 (Brown et al., 1993) and an HMM. We chose to use the Berkeley aligner, rather than the more widely used Giza++ alignment package, for this task because its joint training and posterior decoding algorithms yield lower alignment error rates on most data sets and because it offers functionality for testing an existing model on new data and for outputting posterior probabilities. The smaller of our two Berkeley-generated models is trained on Corpus 1 (the source-to-retelling parallel corpus described above) and ten copies of Corpus 3 (the word identity corpus). The larger model is trained on Corpus 1, Corpus 2 (the p"
W12-2401,P02-1033,0,0.0288821,"ent semantic analysis to measure the semantic distance between a retelling Dx MCI Non-MCI n 72 163 Age 88.7 87.3 Education 14.9 yr 15.1 yr Table 1: Subject demographic data. and the source narrative. Although scoring automation is not typically used in a clinical setting, the objectivity offered by automated measures is particularly important for tests like the WLM, which are often administered by practitioners working in a community setting and serving a diverse population. Researchers working on NLP tasks such as paraphrase extraction (Barzilay and McKeown, 2001), word-sense disambiguation (Diab and Resnik, 2002), and bilingual lexicon induction (Sahlgren and Karlgren, 2005), often rely on aligned parallel or comparable corpora. Recasting the automated scoring of a neuropsychological test as another NLP task involving the analysis of parallel texts, however, is a relatively new idea. We hope that the methods presented here will both highlight the flexibility of techniques originally developed for standard NLP tasks and attract attention to the wide variety of biomedical data sources and potential clinical applications for these techniques. 3 3.1 Data Subjects The data examined in this study was collec"
W12-2401,N09-1006,0,0.316545,"Missing"
W12-2401,N06-1014,0,0.422018,"erature, but NLP is also very well suited for analyzing patient language data, in terms of both content and linguistic features, for neurological evaluation. NLP-driven analysis of clinical language data has been used to assess language development (Sagae et al., 2005), language impairment (Gabani Previous approaches to alignment-based narrative analysis (Prud’hommeaux and Roark, 2011a; Prud’hommeaux and Roark, 2011b) have relied exclusively on modified versions of standard word alignment algorithms typically applied to large bilingual parallel corpora for building machine translation models (Liang et al., 2006; Och et al., 2000). Scores extracted from the alignments produced using these algorithms achieved fairly high classifi1 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 1–10, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics cation accuracy, but the somewhat weak alignment quality limited performance. In this paper, we compare these word alignment approaches to a new approach that uses traditionally-derived word alignments between retellings as the input for graphbased exploration of the alignment space in order to"
W12-2401,J03-1002,0,0.0106033,"and ten copies of Corpus 3 (the word identity corpus). The larger model is trained on Corpus 1, Corpus 2 (the pairwise retelling corpus), and 100 copies of Corpus 3. Both models are then tested on the 470 retellings from our 235 experimental subjects. In addition, we use both models to align every retelling to every other retelling so that we will have all pairwise alignments available for use in the graph-based model. Figure 3: Depiction of word graph. Figure 4: Changes in AER as λ increases. The first two rows of Table 2 show the precision, recall, F-measure, and alignment error rate (AER) (Och and Ney, 2003) for these two Berkeley aligner models. We note that although AER for the larger model is lower, the time required to train the model is significantly larger. The alignments generated by the Berkeley aligner serve not only as a baseline for comparison but also as a springboard for the novel graph-based method of alignment we will now discuss. the normalized posterior-weighted alignments that the Berkeley aligner proposes between each word and (1) words in the source narrative, and (2) words in the other retellings, as depicted in Figure 3. Starting at a particular node (i.e., a word in one of"
W12-2401,C00-2163,0,0.120164,"Missing"
W12-2401,W07-1001,1,0.90496,"Missing"
W12-2401,P05-1025,0,0.0192818,"in this work attest to the potential utility of this graph-based method for enhancing multilingual word alignment and alignment of comparable corpora for more standard NLP tasks. 1 Introduction Much of the work in biomedical natural language processing has focused on mining information from electronic health records, clinical notes, and medical literature, but NLP is also very well suited for analyzing patient language data, in terms of both content and linguistic features, for neurological evaluation. NLP-driven analysis of clinical language data has been used to assess language development (Sagae et al., 2005), language impairment (Gabani Previous approaches to alignment-based narrative analysis (Prud’hommeaux and Roark, 2011a; Prud’hommeaux and Roark, 2011b) have relied exclusively on modified versions of standard word alignment algorithms typically applied to large bilingual parallel corpora for building machine translation models (Liang et al., 2006; Och et al., 2000). Scores extracted from the alignments produced using these algorithms achieved fairly high classifi1 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 1–10, c Montr´eal, Canada, June 8,"
W12-2401,W99-0604,0,\N,Missing
W12-2401,P00-1056,0,\N,Missing
W13-1701,W10-2107,0,0.202254,"The TD and ALN groups, as well as the ALI and SLI groups, are matched on language and overall cognitive abilities, while the ALN and ALI groups are matched on autism symptomatology but not on language and overall cognitive abilities; all groups are matched on chronological age. Regarding our algorithmic approach, we note that automatic detection of relatively subtle errors may be exceedingly difficult, but perhaps such subtle errors are less critical for diagnosis than more obvious ones. Most prior work in grammaticality detection in spoken language has focused on specialized detectors (e.g., Caines and Buttery 2010; Hassanali and Liu 2011), such as mis-use of particular verb constructions rather than coarser detectors for the presence of diverse classes of errors. We demonstrate that these specialized error detectors can break down when confronted with real world dialogue, and that in general, the features in these detectors restricts their utility in detecting other sorts of errors. 2 We implement a detector to automatically extract coarse SALT codes from an uncoded transcript. This detector only depends upon part of speech tags, as opposed to the parse features that are often used in grammaticality de"
W13-1701,P05-1022,0,0.0181936,"Missing"
W13-1701,A00-2018,0,0.109353,"nsist of sub-detectors for the errors shown in Table 5. The rule-based and statistical detectors perform well, with the statistical detector outperforming the rule-based one (F1=0.967 vs. 0.929). The statistical detector, however, requires each error identified by any of the sub-detectors to be manually identified in the training data. We reimplement both the rule based and statistical detectors proposed by Hassanali and Liu, and apply it to our data, with three modifications. The first two are minor: 1) we substitute the CharniakJohnson reranking parser (2005) for Charniak’s original parser (Charniak, 2000), and 2) we use the scikit multinomial naive bayes classifier (Pedregosa et al., 2011) instead of the one in WEKA (Hall et al., 2009). The third difference is that we use these detectors to identify SALT error codes rather than the errors these classifiers were originally built to detect. The mapping of the original errors to SALT error codes is given in Table 5. To clarify, if we are training the ‘Missing Verb’ detector, then any utterance with an [OW] code is taken to be a positive example. This issue does not present itself with the rulebased detector because it is not trained. Note that th"
W13-1701,N09-1006,0,0.0353546,"Missing"
W13-1701,W11-1411,0,0.818468,"well as the ALI and SLI groups, are matched on language and overall cognitive abilities, while the ALN and ALI groups are matched on autism symptomatology but not on language and overall cognitive abilities; all groups are matched on chronological age. Regarding our algorithmic approach, we note that automatic detection of relatively subtle errors may be exceedingly difficult, but perhaps such subtle errors are less critical for diagnosis than more obvious ones. Most prior work in grammaticality detection in spoken language has focused on specialized detectors (e.g., Caines and Buttery 2010; Hassanali and Liu 2011), such as mis-use of particular verb constructions rather than coarser detectors for the presence of diverse classes of errors. We demonstrate that these specialized error detectors can break down when confronted with real world dialogue, and that in general, the features in these detectors restricts their utility in detecting other sorts of errors. 2 We implement a detector to automatically extract coarse SALT codes from an uncoded transcript. This detector only depends upon part of speech tags, as opposed to the parse features that are often used in grammaticality detectors. In most cases, t"
W14-3209,P04-1005,0,0.0311409,"perspective, meaning that edited detection is performed with features from the speech signal in addition to a transcript. These approaches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a text-first approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their 3 Overview of SALT Corpora We explore nine corpora included with the SALT software. Table 1 has"
W14-3209,P93-1007,0,0.449664,"Missing"
W14-3209,N01-1016,0,0.0746863,"igations have approached edited word detection from what Nakatani et al. (1993) have termed ‘speech-first’ perspective, meaning that edited detection is performed with features from the speech signal in addition to a transcript. These approaches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a text-first approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external d"
W14-3209,N13-1102,0,0.670914,"ion, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their 3 Overview of SALT Corpora We explore nine corpora included with the SALT software. Table 1 has a high level overview of these corpora, showing where each was collected, the age ranges of the speakers, and the size of each corpus both in terms of transcripts and utterances. Note that only utterances spoken by the child are counted, as we throw out all others. Table 1 shows sev"
W14-3209,C00-2137,0,0.108026,"in mazes, and therefore can be detected trivially with a simple rule. Furthermore, because partial words are excluded from evaluation, the performance metrics are comparable across corpora, even if they vary widely in the frequency of partial words. For both space and clarity, we do not present the complete results of every experiment in this paper, although they are available online3 . Instead, we present the complete baseline results, and then report F1 scores that are significantly better than the baseline. We establish statistical significance by using a randomized paired-sample test (see Yeh (2000) or Noreen (1989)) to compare the baseline system (system A) and the proposed system (system B). First, we compute the difference d in F1 score between systems A and B. Then, we repeatedly construct a random set of predictions for each input item by choosing between the outputs of system A and B with equal probability. We compute the F1 score of these random predictions, and if it exceeds the F1 score of the baseline system by at least d, we count the iteration as a success. The significance level is at most the number of successes divided by one more than the number of trials (Noreen, 1989)."
W14-3209,P11-1071,0,0.0132704,"ches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a text-first approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their 3 Overview of SALT Corpora We explore nine corpora included with the SALT software. Table 1 has a high level overview of these corpora, showing where each was collected, the age ranges of the speakers, and the size of each corpu"
W16-2404,P12-3011,1,0.942258,"lly aims to minimize the KL divergence from the unpruned model (Stolcke, 1998). Storing such a large n-gram model in a single WFST prior to model pruning is not feasible in many situations. For example, speech recognition first pass models may be trained as a mixture of models from many domains, each of which are trained on billions or tens of billions of sentences (Sak et al., 2013). Even with modest count thresholding, the size of such models before entropybased pruning would be on the order of tens of billions of n-grams. Storing this model in the WFST n-gram format of the OpenGrm library (Roark et al., 2012) allocates an arc for every n-gram (other than end-ofstring n-grams) and a state for every n-gram prefix. Even using very efficient specialized n-gram representations (Sorensen and Allauzen, 2011), a single FST representing this model would require on the order of 400GB of storage, making it difficult to access and process on a single processor. In this paper, we present methods for the distributed representation and processing of large WFST-based n-gram language models by partitioning them into multiple blocks or shards. Our sharding approach meets two key desiderata: 1) each sub-model shard"
W16-2404,D07-1090,0,0.140004,"Missing"
W16-2404,W11-2123,0,0.0166047,"present some numbers on shard characteristics when large models are trained from a very large data set. Functionality to support distributed n-gram modeling has been added to the open-source OpenGrm library. 1 Introduction Training n-gram language models on ever increasing amounts of text continues to yield large model improvements for tasks as diverse as machine translation (MT), automatic speech recognition (ASR) and mobile text entry. One approach to scaling n-gram model estimation to peta-byte scale data sources and beyond, is to distribute the storage, processing and serving of n-grams (Heafield, 2011). In some scenarios – most notably ASR – a very common approach is to heavily prune models trained on large resources, and then pre-compose the resulting model off-line with other models (e.g., a pronunciation lexicon) in order to optimize the model for use at time of firstpass decoding (Mohri et al., 2002). Among other 32 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 32–41, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics x &lt;S&gt; the the ϵ &lt;S&gt; end the state that encodes h. There is exactly one unigram state (labeled with  in F"
W17-4002,W11-3501,0,0.251157,"from the Hindi example in the last section, the number of possible symbols in Brahmic scripts is large, due to the combinations of consonants and vowels via diacritics, and the multi-symbol ligatures. While keyboard layouts do exist for these scripts, transliteration from romanized input to the target Brahmic script is a common form of keyboard input, especially for mobile devices. As mentioned earlier, there are many romanization systems for these languages (in contrast to broadly conventionalized Pinyin systems), making this sort of transliteration keyboard challenging for Indic languages (Ahmed et al., 2011). Romanization and transliteration Transliteration – converting from one writing system to another – is a widespread sequence-tosequence mapping problem that arises in multiple contexts. For example, proper names must be represented in various writing systems, so transliterating names and places can be very important for translation or querying knowledge bases. While it is true that परणब मुखज is the president of India and was born in प म बंगाल, it is more useful for those who do not read the Devanagari script to be presented with the information that Indian president Pranab Mukherjee was bor"
W17-4002,P03-1006,1,0.777661,"F ⊆ Q are ﬁnal states; a weight semiring K; and a set of transitions (q, σ, δ, w, q ′ ) ∈ E , where q, q ′ ∈ Q are, respectively, the source and destination states of the transition, σ ∈ Σ, δ ∈ ∆ and w ∈ K. A weighted ﬁnite-state automaton is a special case where Σ = ∆ and, for every transition (q, σ, δ, w, q ′ ) ∈ E , σ = δ . For the work in this paper, we make use of the OpenFst library (Allauzen et al., 2007) to encode and manipulate WFSTs, and, unless otherwise stated, use the tropical semiring for weights. Encoding n-gram language models as WFSTs involves the use of failure-transitions (Allauzen et al., 2003) with a particular ‘canonical’ structure that corresponds to backoff smoothing. We use such an encoding for building word-based language models in the target language of the keyboard application, and also as an intermediate representation in the training of our transliteration 12 transducer, as described in Section 3.1. For this language model training and encoding, we make use of the OpenGrm n-gram library (Roark et al., 2012), which provides counting, smoothing and pruning functions resulting in an OpenFst encoded model. In speech recognition, a pronunciation lexicon, consisting of words fou"
W17-4002,P98-1036,0,0.71443,"ence mapping problem that arises in multiple contexts. For example, proper names must be represented in various writing systems, so transliterating names and places can be very important for translation or querying knowledge bases. While it is true that परणब मुखज is the president of India and was born in प म बंगाल, it is more useful for those who do not read the Devanagari script to be presented with the information that Indian president Pranab Mukherjee was born in West Bengal. Hence, much work in transliteration is focused on translation and information retrieval (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Haizhou et al., 2004; Gupta et al., 2014). Knight and Graehl (1998) took pronunciation as a mediating variable in mapping between the two writing systems, and explicitly modeled grapheme-to-phoneme and cross-lingual pronunciation mapping in their model. For example, the probability of mapping to “Sanskrit” from संसकृत would include probabilities for English pronunciation given the written form (e.g., S AE N S K R IH T), the Hindi pronunciation (S AH N S K R AX T) given the English pronunciation, and the Devanagari string given the Hindi pronunciation. Haizhou et a"
W17-4002,P12-3011,1,0.752747,"ipulate WFSTs, and, unless otherwise stated, use the tropical semiring for weights. Encoding n-gram language models as WFSTs involves the use of failure-transitions (Allauzen et al., 2003) with a particular ‘canonical’ structure that corresponds to backoff smoothing. We use such an encoding for building word-based language models in the target language of the keyboard application, and also as an intermediate representation in the training of our transliteration 12 transducer, as described in Section 3.1. For this language model training and encoding, we make use of the OpenGrm n-gram library (Roark et al., 2012), which provides counting, smoothing and pruning functions resulting in an OpenFst encoded model. In speech recognition, a pronunciation lexicon, consisting of words found in the vocabulary of the n-gram model along with their pronunciations, can be compiled into a WFST with input vocabulary Σ of phones and output vocabulary ∆ of words. When this lexicon L is composed with the n-gram model G, various optimizations can be carried out on the resulting transducer, to share structure and accrue costs as early as possible (Mohri et al., 2002). Similar optimizations are possible for keyboard models,"
W17-4002,W03-1508,0,0.258706,"m that arises in multiple contexts. For example, proper names must be represented in various writing systems, so transliterating names and places can be very important for translation or querying knowledge bases. While it is true that परणब मुखज is the president of India and was born in प म बंगाल, it is more useful for those who do not read the Devanagari script to be presented with the information that Indian president Pranab Mukherjee was born in West Bengal. Hence, much work in transliteration is focused on translation and information retrieval (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Haizhou et al., 2004; Gupta et al., 2014). Knight and Graehl (1998) took pronunciation as a mediating variable in mapping between the two writing systems, and explicitly modeled grapheme-to-phoneme and cross-lingual pronunciation mapping in their model. For example, the probability of mapping to “Sanskrit” from संसकृत would include probabilities for English pronunciation given the written form (e.g., S AE N S K R IH T), the Hindi pronunciation (S AH N S K R AX T) given the English pronunciation, and the Devanagari string given the Hindi pronunciation. Haizhou et al. (2004) took a more direc"
W17-4002,P04-1021,0,0.229714,"ontexts. For example, proper names must be represented in various writing systems, so transliterating names and places can be very important for translation or querying knowledge bases. While it is true that परणब मुखज is the president of India and was born in प म बंगाल, it is more useful for those who do not read the Devanagari script to be presented with the information that Indian president Pranab Mukherjee was born in West Bengal. Hence, much work in transliteration is focused on translation and information retrieval (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Haizhou et al., 2004; Gupta et al., 2014). Knight and Graehl (1998) took pronunciation as a mediating variable in mapping between the two writing systems, and explicitly modeled grapheme-to-phoneme and cross-lingual pronunciation mapping in their model. For example, the probability of mapping to “Sanskrit” from संसकृत would include probabilities for English pronunciation given the written form (e.g., S AE N S K R IH T), the Hindi pronunciation (S AH N S K R AX T) given the English pronunciation, and the Devanagari string given the Hindi pronunciation. Haizhou et al. (2004) took a more direct modeling approach, s"
W19-3112,P03-1006,1,0.767756,", 2018). Alternatively, an LSTM model can be trained by federated learning (Koneˇcn`y et al., 2016; Hard et al., 2018), then converted to a WFA at the server for fast ondevice inference. This not only may improve performance, but also provide additional privacy. We allow failure transitions (Aho and Corasick, 1975; Mohri, 1997) in the target WFA, which are taken only when no immediate match is possible at a given state, for compactness. For example, in the WFA representation of a backoff k-gram model, failure transitions can compactly implement the backoff (Katz, 1987; Chen and Goodman, 1998; Allauzen et al., 2003; Novak et al., 2013; Hellsten et al., 2017). The inclusion of failure transitions will complicate our analysis and algorithms but is highly desirable in applications such as keyboard decoding. Further, to avoid redundancy that leads to inefficiency, we assume the target model is deterministic, which requires at each state there is at most one transition labeled with a given symbol. The approximation problem can be divided into two steps: (1) select an unweighted automaton A that will serve as the topology of the target automaton and (2) weight the automaton A to form our ˆ The main goal of th"
W19-3112,J05-2002,0,0.116567,"Missing"
W19-3112,W17-4002,1,0.782957,"e trained by federated learning (Koneˇcn`y et al., 2016; Hard et al., 2018), then converted to a WFA at the server for fast ondevice inference. This not only may improve performance, but also provide additional privacy. We allow failure transitions (Aho and Corasick, 1975; Mohri, 1997) in the target WFA, which are taken only when no immediate match is possible at a given state, for compactness. For example, in the WFA representation of a backoff k-gram model, failure transitions can compactly implement the backoff (Katz, 1987; Chen and Goodman, 1998; Allauzen et al., 2003; Novak et al., 2013; Hellsten et al., 2017). The inclusion of failure transitions will complicate our analysis and algorithms but is highly desirable in applications such as keyboard decoding. Further, to avoid redundancy that leads to inefficiency, we assume the target model is deterministic, which requires at each state there is at most one transition labeled with a given symbol. The approximation problem can be divided into two steps: (1) select an unweighted automaton A that will serve as the topology of the target automaton and (2) weight the automaton A to form our ˆ The main goal of this weighted approximation A. paper is the la"
W19-3112,D11-1127,1,0.85586,"Missing"
W19-3112,P12-3011,1,0.811942,"words consisting of all words that appeared more than 50 times in the training corpus. Using this vocabulary, we create a trigram Katz model and prune it to contain 2M n-grams using entropy pruning (Stolcke, 2000), which we use as a baseline in all our experiments. We use Katz smoothing since it is amenable to pruning (Chelba et al., 2010). The perplexity of this model on the test set is 144.4.7 All algorithms were implemented using the open-source OpenFst and OpenGrm n-gram and stochastic automata (SFst) libraries8 with the last library including these implementations (Allauzen et al., 2007; Roark et al., 2012; Allauzen and Riley, 2018). ● Katz: baseline Katz: WFA−Approx Katz: WFA−SampleApprox(N) LSTM: WFA−SampleKatz(N) LSTM: WFA−SampleApprox(N) ● ● ● 5e+05 1e+06 ● ● 2e+06 5e+06 1e+07 # of samples (N) ● ● 2e+07 Figure 5: Test set perplexity for Katz baseline and various approximations of that baseline and of an LSTM model trained on the same data. Note that the Katz baseline and Katz WFA-Approx plots are identical. ps onto the same topology using WFA-A PPROX and WFA-S AMPLE A PPROX(·) and then compute perplexity on the test corpus. The results are presented in Figure 5. The test perplexity of the W"
W19-3112,P00-1073,0,0.309802,"n 4.3. For all the experiments we use the 1996 CSR Hub4 Language Model data, LDC98T31 from the Broadcast News (BN) task. We use the processed form of the corpus and further process it to downcase all the words and remove punctuation. The resulting dataset has 132M words in the training set, 20M words in the test set, and has 240K unique words. From this, we create a vocabulary of approximately 32K words consisting of all words that appeared more than 50 times in the training corpus. Using this vocabulary, we create a trigram Katz model and prune it to contain 2M n-grams using entropy pruning (Stolcke, 2000), which we use as a baseline in all our experiments. We use Katz smoothing since it is amenable to pruning (Chelba et al., 2010). The perplexity of this model on the test set is 144.4.7 All algorithms were implemented using the open-source OpenFst and OpenGrm n-gram and stochastic automata (SFst) libraries8 with the last library including these implementations (Allauzen et al., 2007; Roark et al., 2012; Allauzen and Riley, 2018). ● Katz: baseline Katz: WFA−Approx Katz: WFA−SampleApprox(N) LSTM: WFA−SampleKatz(N) LSTM: WFA−SampleApprox(N) ● ● ● 5e+05 1e+06 ● ● 2e+06 5e+06 1e+07 # of samples (N)"
W19-3114,W11-3501,0,0.474579,"erry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languages (Ahmed et al., 2011). The lack of standard orthography and resulting spelling variation found in romanization is also found in other natural language scenarios, such as OCR of historical documents (Garrette and Alpert-Abrams, 2016) and writing of dialectal 7 2.3 Mobile keyboard decoding Virtual keyboards of the sort typically used on mobile devices convert a temporal sequence of interactions with the touchscreen (taps or continuous gestures) into text. Like speech recognition or optical character recognition, the mapping of noisy, continuous input signals to discrete text strings involves stochastic inference; fu"
W19-3114,Q18-1022,0,0.0256046,"ion retrieval due to loanwords and proper names (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languages (Ahmed et al., 2011). The lack of standard orthogr"
W19-3114,P04-1021,0,0.264574,"“bgrashtachsr” while intending “bhrashtachar”, the keyboard should produce “bhrashtachar” not another romanization such as “bhrastachar”. Given annotator-romanized Wikipedia text, we evaluate our ability to correctly recognize the actual romanizations used. Transliteration and romanized text The need to transliterate between writing systems comes up in many application scenarios, but early work on the topic was largely focused on the needs of machine translation and information retrieval due to loanwords and proper names (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also"
W19-3114,P98-1036,0,0.0642299,"consistent spelling. If the user noisily types “bgrashtachsr” while intending “bhrashtachar”, the keyboard should produce “bhrashtachar” not another romanization such as “bhrastachar”. Given annotator-romanized Wikipedia text, we evaluate our ability to correctly recognize the actual romanizations used. Transliteration and romanized text The need to transliterate between writing systems comes up in many application scenarios, but early work on the topic was largely focused on the needs of machine translation and information retrieval due to loanwords and proper names (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad e"
W19-3114,D09-1111,0,0.0352126,"translation and information retrieval due to loanwords and proper names (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languages (Ahmed et al., 2011)."
W19-3114,P12-3011,1,0.866186,"have methods that must be aware of the input representation. The spatial model cost and the language model cost are combined by the decoder to score competing output strings. Typically these scored string alternatives will be compared to the literal string and only selected if the difference in score is above some threshold, to avoid spurious changes to what the user typed (Ouyang et al., 2017). To accept any string (including any possible literal string), a loop transition for every character with some fixed cost can be included at the unigram state (the base of the smoothing recursion, see Roark et al., 2012), so that every string in Σ∗ has non-zero probability. In addition to decoding for auto-correction, the language model may also be used for word prediction and completion, i.e., showing suggestions in a small dynamic portion of the keyboard. In this paper, we do not have much to say about this part of the process, other than to point out when its demands make certain approaches more complicated than others. Such an architecture has also been used for transliteration from Latin script input to native script output (Hellsten et al., 2017), by interposing a finite-state transducer (FST) between t"
W19-3114,N16-1055,0,0.285323,"Missing"
W19-3114,J17-2003,0,0.0247238,"l., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languages (Ahmed et al., 2011). The lack of standard orthography and resulting spelling variation found in romanization is also found in"
W19-3114,P07-1119,0,0.0468591,"ed on the needs of machine translation and information retrieval due to loanwords and proper names (Knight and Graehl, 1998; Chen et al., 1998; Virga and Khudanpur, 2003; Li et al., 2004). These approaches either explicitly modeled pronunciation in the languages (Knight and Graehl, 1998) or more directly modeled correspondences in the writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languag"
W19-3114,W19-3112,1,0.837463,"of any higher order n-grams in the language model. We achieve this in two steps. First, we build two automata that accept all and only this set of words: a weighted automaton W , which weights the path for each word with the appropriate cost for that word within the language model; and an unweighted automaton A which encodes the same set of words as W and has been determinized and minimized. Next we create a weighted automaton Wm that has the same topology as A, but which is weighted to minimize the KL-divergence (Kullback and Leibler, 1951) between W and Wm , using methods from Suresh et al. (2019). This is an approximation of the distribution represented in W over a much more compact topology. The methods to perform this approximation are part of the open-source OpenGrm stochastic automata (SFst) library (available at http://www.opengrm.org). We then integrate Wm into the larger language model automaton, with the unigram state of the language model serving as both the start and final state for the paths corresponding to those in Wm . This can be straightforwardly accomplished by using the Replace operation in the OpenFst library (http://www.openfst.org). 3.3 3.4 Transducer to canonical"
W19-3114,habash-etal-2012-conventional,0,0.143429,"r PersoArabic scripts, the issue is with historical lack of font and encoding support in certain scenarios. 5 109 prising that the Hindi word for this shows up in many comments. It is, however, variously romanized. By our count: 16 times it is romanized as “bhrastachar”; 7 times as “bhrashtachar”; and once each as “barashtachaar”, “bharastachar”, “bharstachar”, “bhastachar” and “bhrstachar”. Google Translate provides both a translation and a romanization of the word (“bhrashtaachaar”7 ), a form which interestingly is not found in our (admittedly small) example blog comment sample. 2.2 Arabic (Habash et al., 2012). For this study, we make use of Wikipedia data originally written in the native script that has been romanized, and our task is to permit accurate text entry on mobile keyboards, rather than transliteration to the native script or normalization for use in other downstream tasks. In this case “accurate text entry” means fidelity to the intended text, even if that intended text is written without consistent spelling. If the user noisily types “bgrashtachsr” while intending “bhrashtachar”, the keyboard should produce “bhrashtachar” not another romanization such as “bhrastachar”. Given annotator-"
W19-3114,W17-4002,1,0.366425,"writing systems (Li et al., 2004). Models for machine transliteration have continued to improve, through the use of improved modeling methods including many-to-many substring alignment-based modeling, discriminative decoding, and multilingual multitask learning (Sherif and Kondrak, 2007; Cherry and Suzuki, 2009; Kunchukuttan et al., 2018), or by mining likely transliterations in large corpora (Sajjad et al., 2017). Transliteration models are also being deployed in increasingly challenging use scenarios, such as mixed-script information retrieval (Gupta et al., 2014) or for mobile text entry (Hellsten et al., 2017). The volume of romanized text in languages that use other writing systems is an acknowledged issue, one which has grown in importance with the advent of SMS messaging and social media, due to the prevalence of romanized input method editors (IMEs) for these languages (Ahmed et al., 2011). The lack of standard orthography and resulting spelling variation found in romanization is also found in other natural language scenarios, such as OCR of historical documents (Garrette and Alpert-Abrams, 2016) and writing of dialectal 7 2.3 Mobile keyboard decoding Virtual keyboards of the sort typically use"
W19-3114,W03-1508,0,0.597962,"Missing"
