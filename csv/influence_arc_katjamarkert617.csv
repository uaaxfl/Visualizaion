2020.acl-main.452,D17-1098,0,0.0438956,"ummarization. We accomplish this by word-level extraction from the source sentence. Constrained Sentence Generation. Neural sentence generation is usually accomplished in an autoregressive way, for example, by recurrent neu2 https://catalog.ldc.upenn.edu/ LDC2003T05 5033 ral networks generating words left-to-right. This is often enhanced by beam search (Sutskever et al., 2014), which keeps a beam of candidates in a partially greedy fashion. A few studies allow hard constraints on this decoding procedure. Hokamp and Liu (2017) use grid-beam search to impose lexical constraints during decoding. Anderson et al. (2017) propose constrained beam search to predict fixed image tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design our specific search space and search actions for word extraction. In previous work on text s"
2020.acl-main.452,D15-1075,0,0.0756501,"Missing"
2020.acl-main.452,N16-1012,0,0.199713,"search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer method (Dorr et al., 2003) also uses hand-crafted linguistic rules to remove constituents from a parse tree until a certain length is reached. Rush et al. (2015) propose a supervised abstractive sentence summarization system with an attention mechanism (Bahdanau et al., 2015), and they also introduce a dataset for headline generation derived from Gigaword.2 Subsequent models for this dataset were also supervised and mostly based on Seq2seq architectures (Nallapati et al., 2016; Chopra et al., 2016; Wang et al., 2019). Recently, unsupervised approaches for sentence summarization have attracted increasing attention. Fevry and Phang (2018) learn a denoising autoencoder and control the summary length by a length embedding. Wang and Lee (2018) and Baziotis et al. (2019) use cycle-consistency (He et al., 2016) to learn the reconstruction of the source sentence and return the intermediate discrete representation as a summary. Zhou and Rush (2019) use beam search to optimize a scoring function, which considers language fluency and contextual matching. Our work can be categorized under unsuperv"
2020.acl-main.452,P06-2019,0,0.0291048,"s from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer method (Dorr et al., 2003) also uses hand-crafted linguistic rules to remove constituents from a parse tree until a certain length is reached. Rush et al. (2015) propose a supervised abstractive sentence summarization system with an attention mechanism (Bahdanau et al., 2015), and they also introduce a dataset for headline generation derived from Gigaword.2 Subsequent m"
2020.acl-main.452,N19-1423,0,0.0053991,"ummary keeps the key information of the input sentence. We adopt the cosine similarity between sentence embeddings as fSIM (y; x) = cos(e(x), e(y)), (3) where e is a sentence embedding method. In our work, we use unigram word embeddings learned by the sent2vec model (Pagliardini et al., 2018). Then, e(x) is computed as the average of these unigram embeddings, weighted by the inverse-document frequency (idf ) of the words. We use sent2vec because it is trained in an unsupervised way on individual sentences. By contrast, other unsupervised methods like SiameseCBOW (Kenter et al., 2016) or BERT (Devlin et al., 2019) use adjacent sentences as part of the training signal. Length Constraint. Our discrete searching approach is able to impose the output length as a hard constraint, allowing the model to generate summaries of any given length. Suppose the desired output length is s, then our length scorer is 3 γ → (y) · fSIM (y; x) · fLEN (y; s), f (y; x, s) = f← LM |y| Y 1 1 . → (y |y ) ←− (yi |y>i ) p− p LM i <i LM i We use the terminology unsupervised summarization, following Zhou and Rush (2019). While we train the language models on the desired target language, we do not need parallel source-target pairs,"
2020.acl-main.452,P19-1331,0,0.152094,"Missing"
2020.acl-main.452,W03-0501,0,0.0855958,"e copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer method (Dorr et al., 2003) also uses hand-crafted linguistic rules to remove constituents from a parse tree until a certain length is reached. Rush et al. (2015) propose a supervised abstractive sentence summarization system with an attention mechanism (Bahdanau et al., 2015), and they also introduce a dataset for headline generation derived from Gigaword.2 Subsequent models for this dataset were also supervised and mostly based on Seq2seq architectures (Nallapati et al., 2016; Chopra et al., 2016; Wang et al., 2019). Recently, unsupervised approaches for sentence summarization have attracted increasing attention. Fevr"
2020.acl-main.452,D18-1443,0,0.0170696,"ummarization systems. 2 Related Work Text Summarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts"
2020.acl-main.452,P16-1154,0,0.0235287,"by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer method (Dorr et al., 2003) also uses hand-"
2020.acl-main.452,N09-1041,0,0.0683446,"are: • We propose a novel method for unsupervised sentence summarization by hill climbing with word-level extraction. • We outperform current unsupervised sentence summarization systems, including more complex sentence reconstruction models. • We show that ROUGE F1 is sensitive to summary length and thus emphasize the importance of explicitly controlling summary length for a fair comparison among different summarization systems. 2 Related Work Text Summarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et a"
2020.acl-main.452,P82-1020,0,0.725086,"Missing"
2020.acl-main.452,P17-1141,0,0.0133267,"fluency and contextual matching. Our work can be categorized under unsupervised sentence summarization. We accomplish this by word-level extraction from the source sentence. Constrained Sentence Generation. Neural sentence generation is usually accomplished in an autoregressive way, for example, by recurrent neu2 https://catalog.ldc.upenn.edu/ LDC2003T05 5033 ral networks generating words left-to-right. This is often enhanced by beam search (Sutskever et al., 2014), which keeps a beam of candidates in a partially greedy fashion. A few studies allow hard constraints on this decoding procedure. Hokamp and Liu (2017) use grid-beam search to impose lexical constraints during decoding. Anderson et al. (2017) propose constrained beam search to predict fixed image tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design o"
2020.acl-main.452,P16-1089,0,0.0714394,"Missing"
2020.acl-main.452,D16-1140,0,0.0193827,"to predict fixed image tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design our specific search space and search actions for word extraction. In previous work on text summarization, length embeddings (Kikuchi et al., 2016; Fan et al., 2018) have been used to indicate the desired summary length. However, these are not hard constraints, because the model may learn to ignore such information. 3 Proposed Model Given a source sentence x = (x1 , x2 , . . . , xn ) as input, our goal is to generate a shorter sentence y = (y1 , y2 , . . . , ym ) as a summary of x. We perform word-level extraction, in addition keeping the original word order intact. Thus, y is a subsequence of x. Our word-level extraction optimizes a manually defined objective function f (y; x, s), where the summary length s is predefined (s < n) and no"
2020.acl-main.452,2020.acl-main.707,1,0.785682,"hich keeps a beam of candidates in a partially greedy fashion. A few studies allow hard constraints on this decoding procedure. Hokamp and Liu (2017) use grid-beam search to impose lexical constraints during decoding. Anderson et al. (2017) propose constrained beam search to predict fixed image tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design our specific search space and search actions for word extraction. In previous work on text summarization, length embeddings (Kikuchi et al., 2016; Fan et al., 2018) have been used to indicate the desired summary length. However, these are not hard constraints, because the model may learn to ignore such information. 3 Proposed Model Given a source sentence x = (x1 , x2 , . . . , xn ) as input, our goal is to generate a shorter sentence y = (y1 , y2 , . . . , ym ) as a"
2020.acl-main.452,2020.acl-main.703,0,0.0288413,"Missing"
2020.acl-main.452,W04-1013,0,0.128519,"line, which extracts the first 75 characters as the summary. We call it Lead-C-75. For the Gigaword dataset, the reference has 8 words on average, and it is common to compare with a Lead variant that chooses the first 8 words. We call this baseline Lead-N-n when we choose n words. For fair comparison with previous work (Baziotis et al., 2019; Fevry and Phang, 2018) in Section 5.2, we further introduce a new variant that returns the first p percent of source words as the summary. We denote this baseline by Lead-P-p. 4.3 ROUGE Scores Summarization systems are commonly evaluated by ROUGE scores (Lin, 2004). The ROUGE-1 (or ROUGE-2) score computes the unigram (or bigram) overlap of a generated summary and the reference. ROUGE-L calculates the longest common subsequence. Depending on the dataset, either ROUGE Recall or ROUGE F1 variant is adopted. Since the ROUGE Recall metric is not normalized with regard to length, DUC2004 standard evaluation truncates the summary at 75 characters. This procedure was also adopted by Rush et al. (2015) for the headline generation task, but later Chopra et al. (2016) proposed to report the “more balanced” ROUGE F1 metric for the Gigaword headline generation datas"
2020.acl-main.452,P19-1102,0,0.0120621,"ummarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer meth"
2020.acl-main.452,2020.acl-main.28,1,0.925914,"is is often enhanced by beam search (Sutskever et al., 2014), which keeps a beam of candidates in a partially greedy fashion. A few studies allow hard constraints on this decoding procedure. Hokamp and Liu (2017) use grid-beam search to impose lexical constraints during decoding. Anderson et al. (2017) propose constrained beam search to predict fixed image tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design our specific search space and search actions for word extraction. In previous work on text summarization, length embeddings (Kikuchi et al., 2016; Fan et al., 2018) have been used to indicate the desired summary length. However, these are not hard constraints, because the model may learn to ignore such information. 3 Proposed Model Given a source sentence x = (x1 , x2 , . . . , xn ) as input, our goal i"
2020.acl-main.452,W18-2706,0,0.0185452,"tags in an image transcription task. Miao et al. (2019) propose a Metropolis–Hastings sampler for sentence generation, where hard constraints can be incorporated into the target distribution. This is further extended to simulated annealing (Liu et al., 2020), or applied to the text simplification task (Kumar et al., 2020). Different from the above concurrent work, this paper applies the stochastic search framework to text summarization, and design our specific search space and search actions for word extraction. In previous work on text summarization, length embeddings (Kikuchi et al., 2016; Fan et al., 2018) have been used to indicate the desired summary length. However, these are not hard constraints, because the model may learn to ignore such information. 3 Proposed Model Given a source sentence x = (x1 , x2 , . . . , xn ) as input, our goal is to generate a shorter sentence y = (y1 , y2 , . . . , ym ) as a summary of x. We perform word-level extraction, in addition keeping the original word order intact. Thus, y is a subsequence of x. Our word-level extraction optimizes a manually defined objective function f (y; x, s), where the summary length s is predefined (s < n) and not subject to optimi"
2020.acl-main.452,K18-1040,0,0.424278,"Computational Linguistics a selected word and a non-selected word. We accept the new candidate if its score is higher than the current one. In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words. Due to the nature of the search action, our approach is able to explicitly control the length of a summary as a hard constraint. In all previous work, the summary length is weakly controlled by length embeddings or a soft length penalty (Zhou and Rush, 2019; Wang and Lee, 2018; Fevry and Phang, 2018; Baziotis et al., 2019). Thus, the generated summaries by different systems vary considerably in average length, for example, ranging from 9 to 15 on a headline corpus (Section 4.1). Previous work uses ROUGE F1 to compare summaries that might differ in length. We show that ROUGE F1 is unfortunately sensitive to summary output length, in general favoring models that produce longer summaries. Therefore, we argue that controlling the output length should be an integral part of the summarization task and that a fair system comparison can only be conducted between summaries in the same length brac"
2020.acl-main.452,W04-3252,0,0.0127563,"zation by hill climbing with word-level extraction. • We outperform current unsupervised sentence summarization systems, including more complex sentence reconstruction models. • We show that ROUGE F1 is sensitive to summary length and thus emphasize the importance of explicitly controlling summary length for a fair comparison among different summarization systems. 2 Related Work Text Summarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive"
2020.acl-main.452,P19-1207,0,0.0449978,"Missing"
2020.acl-main.452,D18-1451,0,0.175531,"summary, while preserving key information (Rush et al., 2015). Sentence summarization has wide applications, for example, news headline generation and text simplification. State-of-the-art sentence summarization systems are based on sequence-to-sequence neural networks (Rush et al., 2015; Nallapati et al., 2016; Wang et al., 2019), which require massive parallel data for training. Therefore, unsupervised sentence summarization has recently attracted increasing interest. Cycle-consistency approaches treat the summary as a discrete latent variable and use it to reconstruct the source sentence (Wang and Lee, 2018; Baziotis et al., 2019). Such latent-space generation fails to explicitly model the resemblance between the source sentence and the target summary. 1 for Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method"
2020.acl-main.452,N18-1049,0,0.0364121,"arget corpus.3 In this case, the fluency scorer also measures whether the summary style is consistent with the target language. This could be important in certain applications, e.g., headline generation, where the summary language differs from the input in style. Semantic Similarity. A semantic similarity scorer ensures that the summary keeps the key information of the input sentence. We adopt the cosine similarity between sentence embeddings as fSIM (y; x) = cos(e(x), e(y)), (3) where e is a sentence embedding method. In our work, we use unigram word embeddings learned by the sent2vec model (Pagliardini et al., 2018). Then, e(x) is computed as the average of these unigram embeddings, weighted by the inverse-document frequency (idf ) of the words. We use sent2vec because it is trained in an unsupervised way on individual sentences. By contrast, other unsupervised methods like SiameseCBOW (Kenter et al., 2016) or BERT (Devlin et al., 2019) use adjacent sentences as part of the training signal. Length Constraint. Our discrete searching approach is able to impose the output length as a hard constraint, allowing the model to generate summaries of any given length. Suppose the desired output length is s, then o"
2020.acl-main.452,D19-1389,0,0.0756793,"model even if we do not have supervision of parallel sourcetarget data. Results for DUC2004. Table 2 shows the results on the DUC2004 data. As this dataset is for test only, we directly transfer the models HC article and HC title from the headline generation corpus with the same hyperparameters (except for length). As shown in the table, we outperform all previous methods and the Lead-C-75 baseline. The results are consistent with Table 1, showing the generalizability of our approach. Human Evaluation. We conduct human evaluation via pairwise comparison of system outputs, in the same vein as (West et al., 2019). The annotator sees the source sentence along with the headline generated by our system and a competing method, presented in random order. The annotator is asked to compare the fidelity and fluency of the two systems, choosing among the three options (i) the first headline is better (ii) the second headline is better, and (iii) both headlines are equally good/bad. This task is repeated for 100 instances with 5 annotators each. The final label is selected by majority voting. The inter-annotator agreement (Krippendorff’s alpha) is 0.25 when our model is compared with Wang and Lee (2018) and 0.1"
2020.acl-main.452,N18-1202,0,0.0251118,"Missing"
2020.acl-main.452,W00-0403,0,0.307755,"utions of this paper are: • We propose a novel method for unsupervised sentence summarization by hill climbing with word-level extraction. • We outperform current unsupervised sentence summarization systems, including more complex sentence reconstruction models. • We show that ROUGE F1 is sensitive to summary length and thus emphasize the importance of explicitly controlling summary length for a fair comparison among different summarization systems. 2 Related Work Text Summarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarizati"
2020.acl-main.452,D16-1138,0,0.0244808,"Missing"
2020.acl-main.452,P19-1628,0,0.0449722,"ion. • We outperform current unsupervised sentence summarization systems, including more complex sentence reconstruction models. • We show that ROUGE F1 is sensitive to summary length and thus emphasize the importance of explicitly controlling summary length for a fair comparison among different summarization systems. 2 Related Work Text Summarization. The task can be categorized by source text types, such as multi-document summarization (Erkan and Radev, 2004; Radev et al., 2000; Haghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence su"
2020.acl-main.452,P19-1503,0,0.110723,"plicitly group summarization systems by output length brackets.1 e bid ove wa s th e wo r &apos;s ld big g mi est n bh er p bil li an ton n tu oun es ce it day d Abstract dr op its pin g co n ho trov sti ers ia ta le l Institute of Computational Linguistics, Heidelberg University, Germany {rschuman, markert}@cl.uni-heidelberg.de 2 University of Alberta, Canada; Alberta Machine Intelligence Institute (Amii) doublepower.mou@gmail.com 3 University of Waterloo, Canada {yao.lu, ovechtom}@uwaterloo.ca Our code and system outputs are available at: https://github.com/raphael-sch/HC_ Sentence_Summarization Zhou and Rush (2019) propose a left-to-right beam search approach based on a heuristically defined scoring function. However, beam search is biased towards the first few words of the source. In this paper, we propose a hill-climbing approach to unsupervised sentence summarization, directly extracting words from the source sentence. This is motivated by the observation that humanwritten reference summaries exhibit high word overlap with the source sentence, even preserving word order to a large extent. To perform word extraction for summarization, we define a scoring function — similar to Miao et al. (2019) and Zh"
2020.acl-main.452,W04-1010,0,0.168848,"Missing"
2020.acl-main.452,D15-1044,0,0.293614,"bid Figure 1: Summarizing a sentence x by hill climbing. Each row is a Boolean vector at at a search step t . A black cell indicates a word is selected, and vice versa. Randomly swapping two values in the Boolean vector yields a new summary that is scored by an objective function that measures language fluency and semantic similarity. If the new summary increases the objective, this summary is accepted as the current best solution. Rejected solutions are not depicted. Introduction Sentence summarization transforms a long source sentence into a short summary, while preserving key information (Rush et al., 2015). Sentence summarization has wide applications, for example, news headline generation and text simplification. State-of-the-art sentence summarization systems are based on sequence-to-sequence neural networks (Rush et al., 2015; Nallapati et al., 2016; Wang et al., 2019), which require massive parallel data for training. Therefore, unsupervised sentence summarization has recently attracted increasing interest. Cycle-consistency approaches treat the summary as a discrete latent variable and use it to reconstruct the source sentence (Wang and Lee, 2018; Baziotis et al., 2019). Such latent-space"
2020.acl-main.452,P17-1099,0,0.0720643,"ghighi and Vanderwende, 2009) and single-document summarization (Mihalcea and Tarau, 2004; Zhou and Hovy, 2004; Zheng and Lapata, 2019). Traditional approaches are mostly extractive, i.e., they extract entire sentences from a document. Recently, sequence-to-sequence (Seq2Seq) models have been used for abstractive summaries, where the system is able to synthesize new sentences (Nallapati et al., 2016, 2017; Gehrmann et al., 2018; Lewis et al., 2019; Fabbri et al., 2019). The copy mechanism (Gu et al., 2016) in a Seq2Seq model can be viewed as word-level extraction in abstractive summarization (See et al., 2017; Paulus et al., 2018). Both state-of-the-art extractive and abstractive approaches are usually supervised. Sentence summarization yields a short summary for a long sentence. Hori and Furui (2004) and Clarke and Lapata (2006) extract single words from the source sentence based on language model fluency and linguistic constraints. They search via dynamic programming with a trigram language model, which restricts the model capacity. The Hedge Trimmer method (Dorr et al., 2003) also uses hand-crafted linguistic rules to remove constituents from a parse tree until a certain length is reached. Rush"
2020.acl-main.452,W19-2303,0,0.0375487,"Missing"
2020.acl-main.452,N19-1071,0,\N,Missing
2020.coling-main.332,E06-1042,0,0.0856454,"ell as classifier performance for the 3704 metaphors with a conventionality score in the VUA-SEQ test set. On the x-axis, we find how often a word type was seen in training. On the y-axis, we have buckets of conventionality scores. In the fields we see the number of test tokens in the bucket as well as accuracy/recall of the BERT model on this bucket. 3730 5 Related Work Datasets. In some datasets, each word is labeled for metaphoricity (VUA Metaphor corpus, (Steen, 2010)) whereas in others only one target word in a bigram or a sentence is labeled (Mohammad et al., 2016; Shutova et al., 2016; Birke and Sarkar, 2006; Tsvetkov et al., 2014; Turney et al., 2011, among others). We concentrate on datasets where each word is labeled as (i) these are highly appropriate for the sequence labeling tasks that language models excel at and (ii) the 2018 and 2020 Metaphor Shared Tasks (Leong et al., 2018; Leong et al., 2020) use such corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, mak"
2020.coling-main.332,E17-2084,0,0.195158,"008) or different domain mappings. Some exceptions exist, such as the conventionality annotation in (Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Su et al., 2020; Gong et al., 2020, among others) use a variety of information sources such as abstractness/concreteness features, semantic class information, part-of-speech tags, property norms and outside lexical databases as well as multimodal and multilingual information. The recent state of the art models we discuss (Gao et al., 2018; Wu et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 201"
2020.coling-main.332,D19-1227,0,0.170425,"system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its context. Dankers (Dankers et al., 2019) enhance a fine-tuned BERT model (Dankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Stowe (Stowe et al., 2019) use the ELMO model of Gao et al. (2018) but show that additional, linguistically motivated training data enhances performance. As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. BERT is a fine-tuned BERT model we implemented. Parameter de"
2020.coling-main.332,N19-1423,0,0.0307323,"a metaphorical pattern such as Argument is War, there are degrees of conventionality with Example 1 being more conventional than Examples 2 and 3. To a human, unconventional metaphors tend to be more noticeable. Metaphor detection has been studied extensively in NLP in recent years (see (Veale et al., 2016; Shutova et al., 2017) for overviews). State-of-the-art approaches in metaphor detection build strongly on language models and word embeddings, with more than half of the participants in the 2020 Shared Task on Metaphor Detection (Leong et al., 2020) using a variant of BERT language models (Devlin et al., 2019). Evaluations on the standard metaphor recognition test sets report scores that creep up steadily, using such methods. We investigate whether these models really are able to learn general properties of metaphor. To do so and to go beyond word sense disambiguation, they should be able to (i) recognise conventional and unconventional metaphors (ii) be able to perform well on rarer word types that often This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 In our examples, metaphoric words are marked"
2020.coling-main.332,D18-1171,0,0.208127,"pect everybody to ’goose-step’ in the same direction [...] Conventional metaphors are frequent word usages that often have their own dictionary entries whereas novel readings are rare and cannot be found in standard lexical resources. Other aspects also contribute to a metaphor’s conventionality, such as whether they do follow a metaphoric pattern. Recognising novel metaphors is important: Shutova (2015) argues ”that NLP applications do not necessarily need to address highly conventional and lexicalized metaphors that can be interpreted using standard word sense disambiguation techniques”. Do Dinh et al. (2018) have extended the VUA corpus with reliable novelty scores for content word metaphors. Their annotation guidelines define conventionality and novelty based on frequency of use (often used in everyday language vs. not usually used in everyday language). The scores range from −1 indicating conventional metaphors to 1 for the most novel metaphors. For example, the metaphor in Example 5 has the score 0.765. Whereas Do Dinh et al. (2018) and Simpson et al. (2019) tackle novelty scoring given gold standard metaphoric/literal information, we investigate how the novelty of a metaphor affects automatic"
2020.coling-main.332,P14-2121,0,0.142308,"ch corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, making it hard to assess system capabilities for the recognition of various metaphor types, such as conventional vs. novel metaphors, deliberately used vs. unintentional metaphors (Steen, 2008) or different domain mappings. Some exceptions exist, such as the conventionality annotation in (Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al."
2020.coling-main.332,D18-1060,0,0.103878,"interpreted with standard word sense disambiguation techniques. • We show that metaphor recognition systems are strongly dependent on the frequency of word types in the training data. • As a positive result, we show that the systems have increasing generalisation capabilities in that they perform better on unknown word types if synonyms or morphological variations have been seen in the training data. 2 2.1 Models and Datasets Models We report on the following models, all except the baseline being based on a sequence of progressively stronger language models. Lex-BL is a baseline suggested by Gao et al. (2018) that assigns metaphoric if the word has been annotated as metaphoric more often than literal in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE"
2020.coling-main.332,2020.figlang-1.21,0,0.830238,"ankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Stowe (Stowe et al., 2019) use the ELMO model of Gao et al. (2018) but show that additional, linguistically motivated training data enhances performance. As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. BERT is a fine-tuned BERT model we implemented. Parameter details are in the Supplement. ILLI (Gong et al., 2020) is one of the 3 best-performing systems on the 2020 Metaphor Detection Shared Task (Leong et al., 2020). Its most basic form is a simple fine-tuned RoBERTa (Liu et al., 2019) language model (ILLI-ROB). Its most sophisticated version (ILLI-F-ENS) adds a wide variety of linguistic features and an ensemble based on 3 different runs on different train/dev splits. The system code is available but at too short notice for us to conduct a fine-grained analysis of this system yet. 3723 DM is the 2020 Shared Task winner (Su et al., 2020). It uses RoBERTa enriched with POS features and two transformers,"
2020.coling-main.332,P16-1018,0,0.073459,"(Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Su et al., 2020; Gong et al., 2020, among others) use a variety of information sources such as abstractness/concreteness features, semantic class information, part-of-speech tags, property norms and outside lexical databases as well as multimodal and multilingual information. The recent state of the art models we discuss (Gao et al., 2018; Wu et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Gong et al., 2020; Su et al., 2020) use sequence labeling and build on embeddings and/or language mod"
2020.coling-main.332,W13-0902,0,0.0137945,"y which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, making it hard to assess system capabilities for the recognition of various metaphor types, such as conventional vs. novel metaphors, deliberately used vs. unintentional metaphors (Steen, 2008) or different domain mappings. Some exceptions exist, such as the conventionality annotation in (Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019;"
2020.coling-main.332,W17-1903,0,0.0250385,"Missing"
2020.coling-main.332,W18-0907,0,0.791344,"Datasets Models We report on the following models, all except the baseline being based on a sequence of progressively stronger language models. Lex-BL is a baseline suggested by Gao et al. (2018) that assigns metaphoric if the word has been annotated as metaphoric more often than literal in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its conte"
2020.coling-main.332,2020.figlang-1.3,0,0.799605,"Missing"
2020.coling-main.332,P18-1113,0,0.100856,"unn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Su et al., 2020; Gong et al., 2020, among others) use a variety of information sources such as abstractness/concreteness features, semantic class information, part-of-speech tags, property norms and outside lexical databases as well as multimodal and multilingual information. The recent state of the art models we discuss (Gao et al., 2018; Wu et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Gong et al., 2020; Su et al., 2020) use sequence labeling and build on embeddings and/or language models. Leong et al."
2020.coling-main.332,P19-1378,0,0.143361,"iteral in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its context. Dankers (Dankers et al., 2019) enhance a fine-tuned BERT model (Dankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained"
2020.coling-main.332,S16-2003,0,0.273902,"on between frequency and conventionality as well as classifier performance for the 3704 metaphors with a conventionality score in the VUA-SEQ test set. On the x-axis, we find how often a word type was seen in training. On the y-axis, we have buckets of conventionality scores. In the fields we see the number of test tokens in the bucket as well as accuracy/recall of the BERT model on this bucket. 3730 5 Related Work Datasets. In some datasets, each word is labeled for metaphoricity (VUA Metaphor corpus, (Steen, 2010)) whereas in others only one target word in a bigram or a sentence is labeled (Mohammad et al., 2016; Shutova et al., 2016; Birke and Sarkar, 2006; Tsvetkov et al., 2014; Turney et al., 2011, among others). We concentrate on datasets where each word is labeled as (i) these are highly appropriate for the sequence labeling tasks that language models excel at and (ii) the 2018 and 2020 Metaphor Shared Tasks (Leong et al., 2018; Leong et al., 2020) use such corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a bi"
2020.coling-main.332,D14-1162,0,0.0906892,"that assigns metaphoric if the word has been annotated as metaphoric more often than literal in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its context. Dankers (Dankers et al., 2019) enhance a fine-tuned BERT model (Dankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not availa"
2020.coling-main.332,N18-1202,0,0.0633458,"nnotated as metaphoric more often than literal in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its context. Dankers (Dankers et al., 2019) enhance a fine-tuned BERT model (Dankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not available, we report only the results in their p"
2020.coling-main.332,D17-1162,0,0.443975,"main mappings. Some exceptions exist, such as the conventionality annotation in (Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Su et al., 2020; Gong et al., 2020, among others) use a variety of information sources such as abstractness/concreteness features, semantic class information, part-of-speech tags, property norms and outside lexical databases as well as multimodal and multilingual information. The recent state of the art models we discuss (Gao et al., 2018; Wu et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Gong et al., 20"
2020.coling-main.332,shutova-teufel-2010-metaphor,0,0.153105,"ou use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, making it hard to assess system capabilities for the recognition of various metaphor types, such as conventional vs. novel metaphors, deliberately used vs. unintentional metaphors (Steen, 2008) or different domain mappings. Some exceptions exist, such as the conventionality annotation in (Do Dinh et al., 2018; Dunn, 2014), an annotation akin to deliberateness in (Klebanov and Flor, 2013) and annotated domain mappings in (Shutova and Teufel, 2010). However, most of these were small scale and/or are not publically available, the exception being the conventionality ratings by Do Dinh et al. (2018), which we use in this paper. Metaphor recognition. Data-driven approaches to metaphor recognition (Turney et al., 2011; Tsvetkov et al., 2014; Shutova et al., 2016; Shutova et al., 2017; Bulat et al., 2017; Rei et al., 2017; K¨oper and im Walde, 2017; Wu et al., 2018; Gao et al., 2018; Gutierrez et al., 2016; Mao et al., 2018; Mao et al., 2019; Dankers et al., 2019; Stowe et al., 2019; Su et al., 2020; Gong et al., 2020, among others) use a var"
2020.coling-main.332,N16-1020,0,0.606675,"d conventionality as well as classifier performance for the 3704 metaphors with a conventionality score in the VUA-SEQ test set. On the x-axis, we find how often a word type was seen in training. On the y-axis, we have buckets of conventionality scores. In the fields we see the number of test tokens in the bucket as well as accuracy/recall of the BERT model on this bucket. 3730 5 Related Work Datasets. In some datasets, each word is labeled for metaphoricity (VUA Metaphor corpus, (Steen, 2010)) whereas in others only one target word in a bigram or a sentence is labeled (Mohammad et al., 2016; Shutova et al., 2016; Birke and Sarkar, 2006; Tsvetkov et al., 2014; Turney et al., 2011, among others). We concentrate on datasets where each word is labeled as (i) these are highly appropriate for the sequence labeling tasks that language models excel at and (ii) the 2018 and 2020 Metaphor Shared Tasks (Leong et al., 2018; Leong et al., 2020) use such corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal"
2020.coling-main.332,J17-1003,0,0.0432539,"Missing"
2020.coling-main.332,J15-4002,0,0.0508372,"Missing"
2020.coling-main.332,P19-1572,0,0.0115613,"2019; Gong et al., 2020; Su et al., 2020) use sequence labeling and build on embeddings and/or language models. Leong et al. (2020) state that more than half of participants in the 2020 Shared Task use a variation of BERT. We investigate their properties and performance levels in more detail than previously done, including analysis for conventionality, frequency and generalisation via morphology and semantic similarity. Novel vs. conventionalized metaphors. We investigated how conventionality impacts metaphor recognition. Recent work (Dunn, 2014; Do Dinh et al., 2018; Parde and Nielsen, 2018; Simpson et al., 2019) has assigned novelty scores to (given) metaphors. However, they have either not investigated the influence of novelty on metaphor detection per se or not worked in a sequence labeling, full-text paradigm. We have shown that assigning metaphor novelty scores assuming that metaphors have already been reliably detected is currently somewhat unrealistic as a metaphor’s novelty has a strong influence on being detected in the first place by current models. 6 Conclusion and Future Work We compared several recent models for metaphor recognition, which all build on language models and/or embeddings. W"
2020.coling-main.332,K19-1034,0,0.77371,"l. (2018) but explicitly model two linguistically-motivated factors that might indicate metaphoricity: firstly, the potential clash between contextual and literal meaning of the word to be labeled, and secondly, the possible conflict between the literal meaning of the word to be labeled and its context. Dankers (Dankers et al., 2019) enhance a fine-tuned BERT model (Dankers-BERT) with a multitask setup that learns metaphor and emotion labels jointly (Dankers). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Stowe (Stowe et al., 2019) use the ELMO model of Gao et al. (2018) but show that additional, linguistically motivated training data enhances performance. As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. BERT is a fine-tuned BERT model we implemented. Parameter details are in the Supplement. ILLI (Gong et al., 2020) is one of the 3 best-performing systems on the 2020 Metaphor Detection Shared Task (Leong et al., 2020). Its most basic form is a simple fine-tuned RoBERTa (Liu et al., 2019) language model (ILLI-ROB). Its most sophisticated versi"
2020.coling-main.332,2020.figlang-1.4,0,0.887217,"we implemented. Parameter details are in the Supplement. ILLI (Gong et al., 2020) is one of the 3 best-performing systems on the 2020 Metaphor Detection Shared Task (Leong et al., 2020). Its most basic form is a simple fine-tuned RoBERTa (Liu et al., 2019) language model (ILLI-ROB). Its most sophisticated version (ILLI-F-ENS) adds a wide variety of linguistic features and an ensemble based on 3 different runs on different train/dev splits. The system code is available but at too short notice for us to conduct a fine-grained analysis of this system yet. 3723 DM is the 2020 Shared Task winner (Su et al., 2020). It uses RoBERTa enriched with POS features and two transformers, one focusing on the whole sentence context and one on a more local context. DM-ENS builds an ensemble across nine different runs of DM. Their system output is available.3 Our analysis is based on the DM outputs in their submit folder, more specifically answer9 for DM as well as ensemble3 for DM-ENS (both for the VUA-ALL-POS task). The results vary only marginally from the reported best results in their paper. 2.2 Datasets The VUA Metaphor Corpus4 (Steen, 2010) consists of 115 texts of four different genres: academic, conversati"
2020.coling-main.332,P14-1024,0,0.591035,"mance for the 3704 metaphors with a conventionality score in the VUA-SEQ test set. On the x-axis, we find how often a word type was seen in training. On the y-axis, we have buckets of conventionality scores. In the fields we see the number of test tokens in the bucket as well as accuracy/recall of the BERT model on this bucket. 3730 5 Related Work Datasets. In some datasets, each word is labeled for metaphoricity (VUA Metaphor corpus, (Steen, 2010)) whereas in others only one target word in a bigram or a sentence is labeled (Mohammad et al., 2016; Shutova et al., 2016; Birke and Sarkar, 2006; Tsvetkov et al., 2014; Turney et al., 2011, among others). We concentrate on datasets where each word is labeled as (i) these are highly appropriate for the sequence labeling tasks that language models excel at and (ii) the 2018 and 2020 Metaphor Shared Tasks (Leong et al., 2018; Leong et al., 2020) use such corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, making it hard to assess s"
2020.coling-main.332,D11-1063,0,0.752079,"phors with a conventionality score in the VUA-SEQ test set. On the x-axis, we find how often a word type was seen in training. On the y-axis, we have buckets of conventionality scores. In the fields we see the number of test tokens in the bucket as well as accuracy/recall of the BERT model on this bucket. 3730 5 Related Work Datasets. In some datasets, each word is labeled for metaphoricity (VUA Metaphor corpus, (Steen, 2010)) whereas in others only one target word in a bigram or a sentence is labeled (Mohammad et al., 2016; Shutova et al., 2016; Birke and Sarkar, 2006; Tsvetkov et al., 2014; Turney et al., 2011, among others). We concentrate on datasets where each word is labeled as (i) these are highly appropriate for the sequence labeling tasks that language models excel at and (ii) the 2018 and 2020 Metaphor Shared Tasks (Leong et al., 2018; Leong et al., 2020) use such corpora. We have shown that it matters substantially which dataset partition and setup within the VUA corpus you use and encourage future work to not compare systems working on the two different setups anymore. Most datasets include only a binary metaphor/literal annotation per word, making it hard to assess system capabilities fo"
2020.coling-main.332,W18-0913,0,0.254505,"lt, we show that the systems have increasing generalisation capabilities in that they perform better on unknown word types if synonyms or morphological variations have been seen in the training data. 2 2.1 Models and Datasets Models We report on the following models, all except the baseline being based on a sequence of progressively stronger language models. Lex-BL is a baseline suggested by Gao et al. (2018) that assigns metaphoric if the word has been annotated as metaphoric more often than literal in the training set, and literal otherwise (including for word types unseen in training). Wu (Wu et al., 2018) is a system based on skip-gram word2vec (Mikolov et al., 2013), POS tags and word clusters with a CNN and BiLSTM plus ensemble learning, and is the winner of the 2018 Metaphor Detection Shared Task (Leong et al., 2018). As code or system output is not available, we report only the results in their paper and leave it out of fine-grained analysis. Gao (Gao et al., 2018) uses concatenated GLOVE (Pennington et al., 2014) and ELMO embeddings (Peters et al., 2018) and a BiLSTM. Mao (Mao et al., 2019) build on Gao et al. (2018) but explicitly model two linguistically-motivated factors that might ind"
2020.coling-main.556,N15-1171,0,0.0158371,"English-speaking news frames events through casts of characters, and van den Berg et al. (2020), who studied the effect of naming and titling on the perception of entities in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work"
2020.coling-main.556,P15-2072,0,0.0155642,"ties in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective languag"
2020.coling-main.556,D16-1148,0,0.0125291,"construction of the English BASIL corpus of lexical and informational bias (Fan et al., 2019), which is the corpus our models are tested on. Fan et al. (2019) emphasize that, unlike more commonly studied kinds of bias, informational bias label assignments depend very heavily on context. The sentence-level and span-level BASIL annotations were thus provided by human annotators who saw sentences in their article context. However, the computational models in Fan et al. (2019), which are based on BERT (Devlin et al., 2018), only treat sentences in isolation. Framing of entities is also studied by Card et al. (2016), who examined how English-speaking news frames events through casts of characters, and van den Berg et al. (2020), who studied the effect of naming and titling on the perception of entities in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Fi"
2020.coling-main.556,D18-2029,0,0.0183538,"erm Memory (BiLSTM; Hochreiter and Schmidhuber 1997) to encode news documents. In the case of ArtCIM, a single BiLSTM encodes the article. In the case of EvCIM, three BiLSTMs encode each document in the triple of Fox News, New York Times and Huffington Post articles on the same event. Sentence representations for the target sentence as well as for the input to the BiLSTMs are obtained by taking the average of the last four layers of fine-tuned base RoBERTa (Figure 1, Stage 1). We found this to be more effective than other kinds of pooling, and also more effective than sentence USE embeddings (Cer et al., 2018) or Sentence-Bert embeddings (Reimers and Gurevych, 2019). BiLSTMs then encode a context representation of the article the target sentence appears in (ArtCIM), or of each of the three articles on the same event (EvCIM). At the final stage, the encodings of the target sentence and the context documents are concatenated and passed to a linear classifier (Figure 1, Stage 2). Classification is thus based both on the content of the target sentence, which the baseline captures very well, and the article or event context, which the baseline has no access to. Domain context. To integrate domain contex"
2020.coling-main.556,W18-6509,0,0.0186369,"un to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective language (Recasens et al., 2013; Pang et al., 2008; Wiebe et al., 2004; Hube and Fetahu, 2019), and work on the classification of documents or news outlets into leanings or ideologies (Iyyer et al., 2014). The difference between these various kinds of subjective language and informational bias lies in its"
2020.coling-main.556,D19-1383,0,0.387101,"d RoBERTa (Liu et al., 2019b). By further pre-training RoBERTa on domain-specific and task-specific datasets, Gururangan et al. (2020) made it possible to perform sentence-level classification using models that have been exposed to domain context. In another line of work, several methods have been developed to allow PLM models to take larger sequences than sentences as their input (Pappagari et al., 2019; Adhikari et al., 2019). Of these, only one specifically performs sequential sentence classification (i.e. the task of providing labels for each of the sentences in the multi-sentence input) (Cohan et al., 2019). There exist nonPLM approaches to sequential sentence classification as well. These consist of hierarchical sequence encoders with a final CRF layer (Dernoncourt and Lee, 2017; Jin and Szolovits, 2018) and a BiLSTMbased approach that contextualises Universal Sentence Encodings and also integrates information that is specific to the domain of movie plot synopses (Papalampidi et al., 2019). None of these techniques have previously been applied to informational bias detection. 3 Method We experiment with different kinds of context to assess which one or which ones are helpful for informational b"
2020.coling-main.556,I17-2052,0,0.0158286,"e-level classification using models that have been exposed to domain context. In another line of work, several methods have been developed to allow PLM models to take larger sequences than sentences as their input (Pappagari et al., 2019; Adhikari et al., 2019). Of these, only one specifically performs sequential sentence classification (i.e. the task of providing labels for each of the sentences in the multi-sentence input) (Cohan et al., 2019). There exist nonPLM approaches to sequential sentence classification as well. These consist of hierarchical sequence encoders with a final CRF layer (Dernoncourt and Lee, 2017; Jin and Szolovits, 2018) and a BiLSTMbased approach that contextualises Universal Sentence Encodings and also integrates information that is specific to the domain of movie plot synopses (Papalampidi et al., 2019). None of these techniques have previously been applied to informational bias detection. 3 Method We experiment with different kinds of context to assess which one or which ones are helpful for informational bias detection. We define four types of context: direct textual context, article context, event context and domain context. Direct textual context. Direct textual context consis"
2020.coling-main.556,D19-1664,0,0.0317966,"Missing"
2020.coling-main.556,D18-1393,0,0.0111852,"6), who examined how English-speaking news frames events through casts of characters, and van den Berg et al. (2020), who studied the effect of naming and titling on the perception of entities in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related w"
2020.coling-main.556,L16-1591,0,0.0140493,"ed by Card et al. (2016), who examined how English-speaking news frames events through casts of characters, and van den Berg et al. (2020), who studied the effect of naming and titling on the perception of entities in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Sub"
2020.coling-main.556,N09-1057,0,0.0305512,"rames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective language (Recasens et al., 2013; Pang et al., 2008; Wiebe et al., 2004; Hube and Fetahu, 2019), and work on the classification of documents or news outlets into leanings or ideologies (Iyyer et al., 2014). The difference between these various kinds of subjective language and informational bias lies in its exclusion of neutral and objective language. In framing research, any text that could lead an impartial third party to recognise a non-neutral viewpoint towards a topic or entity can be said to contain framing,"
2020.coling-main.556,2020.acl-main.740,0,0.03327,"Missing"
2020.coling-main.556,P14-1105,0,0.0222127,"ing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective language (Recasens et al., 2013; Pang et al., 2008; Wiebe et al., 2004; Hube and Fetahu, 2019), and work on the classification of documents or news outlets into leanings or ideologies (Iyyer et al., 2014). The difference between these various kinds of subjective language and informational bias lies in its exclusion of neutral and objective language. In framing research, any text that could lead an impartial third party to recognise a non-neutral viewpoint towards a topic or entity can be said to contain framing, even if it is objective and neutral in tone. Approaches. Sentence-level classification tasks have seen great increases in performance through the use of pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019b). By further pre-training RoBERT"
2020.coling-main.556,D18-1349,0,0.0555041,"Missing"
2020.coling-main.556,K19-1047,0,0.026656,"Missing"
2020.coling-main.556,D19-1180,0,0.0814485,"Pappagari et al., 2019; Adhikari et al., 2019). Of these, only one specifically performs sequential sentence classification (i.e. the task of providing labels for each of the sentences in the multi-sentence input) (Cohan et al., 2019). There exist nonPLM approaches to sequential sentence classification as well. These consist of hierarchical sequence encoders with a final CRF layer (Dernoncourt and Lee, 2017; Jin and Szolovits, 2018) and a BiLSTMbased approach that contextualises Universal Sentence Encodings and also integrates information that is specific to the domain of movie plot synopses (Papalampidi et al., 2019). None of these techniques have previously been applied to informational bias detection. 3 Method We experiment with different kinds of context to assess which one or which ones are helpful for informational bias detection. We define four types of context: direct textual context, article context, event context and domain context. Direct textual context. Direct textual context consists of the directly neighboring sentences around the target sentence. These may be helpful for disambiguating sentences with multiple possible interpretations, for noticing patterns in the type of content preceding a"
2020.coling-main.556,P13-1162,0,0.0295925,"d the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective language (Recasens et al., 2013; Pang et al., 2008; Wiebe et al., 2004; Hube and Fetahu, 2019), and work on the classification of documents or news outlets into leanings or ideologies (Iyyer et al., 2014). The difference between these various kinds of subjective language and informational bias lies in its exclusion of neutral and objective language. In framing research, any text that could lead an impartial third party to recognise a non-neutral viewpoint towards a topic or entity can be said to contain framing, even if it is objective and neutral in tone. Approaches. Sentence-level classification tasks have seen great incr"
2020.coling-main.556,D19-1410,0,0.0158858,"1997) to encode news documents. In the case of ArtCIM, a single BiLSTM encodes the article. In the case of EvCIM, three BiLSTMs encode each document in the triple of Fox News, New York Times and Huffington Post articles on the same event. Sentence representations for the target sentence as well as for the input to the BiLSTMs are obtained by taking the average of the last four layers of fine-tuned base RoBERTa (Figure 1, Stage 1). We found this to be more effective than other kinds of pooling, and also more effective than sentence USE embeddings (Cer et al., 2018) or Sentence-Bert embeddings (Reimers and Gurevych, 2019). BiLSTMs then encode a context representation of the article the target sentence appears in (ArtCIM), or of each of the three articles on the same event (EvCIM). At the final stage, the encodings of the target sentence and the context documents are concatenated and passed to a linear classifier (Figure 1, Stage 2). Classification is thus based both on the content of the target sentence, which the baseline captures very well, and the article or event context, which the baseline has no access to. Domain context. To integrate domain context, we apply domain-adapted RoBERTa from Gururangan et al."
2020.coling-main.556,P15-1157,0,0.0244494,"ities is also studied by Card et al. (2016), who examined how English-speaking news frames events through casts of characters, and van den Berg et al. (2020), who studied the effect of naming and titling on the perception of entities in English and German. Most framing research focuses not on entities but on the framing of topics and events. The study of topic framing in news has a long history in social science (Entman, 1993; Berinsky and Kinder, 2006; 6316 Baumgartner et al., 2008; Gentzkow and Shapiro, 2010) and has begun to attract attention from the natural language processing community (Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018; Baumer et al., 2015). For topic framing research there exists the Media Frames Corpus of news annotated for the framing of same-sex marriage, smoking, and immigration (Card et al., 2015) and the Gun Violence Frame Corpus (Liu et al., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨u"
2020.coling-main.556,2020.lrec-1.606,1,0.81348,"Missing"
2020.coling-main.556,J04-3002,0,0.0977581,"., 2019a) annotated for framing in news on gun violence. Computational analysis and classification experiments have been done on framing in Russian news (Field et al., 2018), on detecting frames in English headlines (Liu et al., 2019a; Chen et al., 2018), and on detecting frames in a multilabel, multi-lingual setting (Aky¨urek et al., 2020). Subjectivity. Related work also includes work on implicit sentiment through syntactic structures (Greene and Resnik, 2009) and partisan phrases (Yano et al., 2010), work on explicit stance and subjective language (Recasens et al., 2013; Pang et al., 2008; Wiebe et al., 2004; Hube and Fetahu, 2019), and work on the classification of documents or news outlets into leanings or ideologies (Iyyer et al., 2014). The difference between these various kinds of subjective language and informational bias lies in its exclusion of neutral and objective language. In framing research, any text that could lead an impartial third party to recognise a non-neutral viewpoint towards a topic or entity can be said to contain framing, even if it is objective and neutral in tone. Approaches. Sentence-level classification tasks have seen great increases in performance through the use of"
2020.coling-main.556,H05-1044,0,0.116753,"Missing"
2020.lrec-1.218,N18-1065,0,0.0234187,"ted input corpora from static background corpora. This methodology, however, has been discarded by the majority of TLS research in favor of using pre-filtered datasets. We also investigate the impact IR methods have on TLS performance. In particular, given an event of interest E, we want to create systematically different event-specific input corpora CE from a large, static background corpus CB in a reproducible fashion. This allows us to evaluate the performance of different TLS systems, given different input CE s. As our background corpus to draw articles from, we used the newsroom dataset (Grusky et al., 2018). Not only has it already been used to evaluate single-document summarization systems (see e.g., Shi et al. (2019) or Mendes et al. (2019)), but it also provides a large amount of data to experiment with different IR methods – this makes results much more reproducible since the background corpus is always fixed. The corpus contains approx. 1.3M articles, covering the years 1998 to 2016. One of the key reasons why we opted to use a different dataset than other TLS work lies in the way previous benchmark datasets have been generated. As described in Section 2., the most commonly used datasets, c"
2020.lrec-1.218,P15-1155,0,0.0194688,"ever, using Google obviates the reproducibility of the dataset generation since Google not only personalizes search results, but also exhibits bias dependent on time and location of search. Using it to collect articles is therefore highly dependent on factors outside the scope of researchers. In addition, there are potential copyright problems with regard to the collected texts. Lastly, the method is not applicable to a given static background corpus (for example a corpus that a news provider might have). Online summarization. A research field related to TLS is online or update summarization (Kedzie et al., 2015; Kedzie et al., 2016). Here summaries are generated whilst an event is ongoing, often from highly redundant social media streams. A standard dataset comes from the TREC Temporal Summarization track (Aslam et al., 2013; Aslam et al., 2014; Aslam et al., 2015). Relevance detection here plays the role of IR and is paid more heed than IR is in standard TLS. Apart from being in an offline setting, TLS normally generates timelines for much longer running events than update summarization and has a higher emphasis on dating events. 3. Background Corpus Our main goal is to facilitate a common practice"
2020.lrec-1.218,P12-1077,0,0.0247263,"taset extracted via Google with essentially the same method on four events, the crisis dataset. Both corpora have been made publicly available, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus CE to construct a summary. The information in the corresponding papers on the construction of CE , given an event E of interest, is scarce, with some papers (Yan et al., 2011) not explaining their method at all. We can overall distinguish two approaches: papers that built"
2020.lrec-1.218,P11-1052,0,0.048737,"s by features including length, number of named entities, unigram features, and averaged/summed tfidf scores. The system is trained using the reference timelines, by computing the F1 score of each sentence to the reference timeline. Constraints are added to keep temporal coherence. During prediction, the system greedily generates a timeline by predicting the F1 scores of sentences. tilse. Martschat and Markert (2018) present a system for TLS based on submodular functions. Submodular functions have previously been used in MDS. Based on the coverage and diversity functions for MDS introduced by Lin and Bilmes (2011), they introduce similar functions for TLS. In addition, a date selection function measures the importance of a date by the number of sentences referring to it. These three functions are then combined in an unweighted fashion into one objective function. Two constraints, the maximum number of dates in the timeline and the maximum number of sentences for a date, are set as well. In their experiments, tilse outperformed both regression and Chieu on the established datasets TL17 and crisis. 6.1. Sentence Filtering Since Chieu and Lee (2004), TLS algorithms (including tilse) usually employ additio"
2020.lrec-1.218,W04-1013,0,0.0796057,"ignificance wrt to simple and ‡ wrt to bm25 (all p = 0.05). the last day of the union of timelines. This serves as an approximation of the actual time range of the event. Within this range, we extract the top N documents for both bm25 and bm25boot , where N is the number of articles retrieved by simple.10 In order to make the resulting CE s compatible with the package tilse, we furthermore temporally tag each document with heideltime (Str¨otgen and Gertz, 2013). 7.2. Evaluation Automatic evaluation of TLS is mostly done with the same evaluation metrics as standard summarization, namely ROUGE (Lin, 2004). However, Martschat and Markert (2017) presented TLS-specific variants of ROUGE: concat, agreement and align+ m:1. These metrics perform evaluation by concatenating all daily summaries, evaluating only matching days, and evaluating aligned dates based on date and content similarity, respectively. We report ROUGE1 and ROUGE-2 F1 scores for the concat, agreement and align+ m:1 metrics. Since TLS has an emphasis on date selection – which is not present in standard summarization –, date selection is another important metric for TLS evaluation. We evaluate date selection using F1 score. Following"
2020.lrec-1.218,E17-2046,1,0.938729,"e, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus CE to construct a summary. The information in the corresponding papers on the construction of CE , given an event E of interest, is scarce, with some papers (Yan et al., 2011) not explaining their method at all. We can overall distinguish two approaches: papers that built their own textbased IR system using keywords and indexing, vs. papers that relied on commercial search engines or corpora built by other researcher"
2020.lrec-1.218,K18-1023,1,0.850966,"rk sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus CE to construct a summary. The information in the corresponding papers on the construction of CE , given an event E of interest, is scarce, with some papers (Yan et al., 2011) not explaining their method at all. We can overall distinguish two approaches: papers that built their own textbased IR system using keywords and indexing, vs. papers that relied on commercial search engines or corpora built by other researchers with search engines. 1 150k"
2020.lrec-1.218,N19-1397,0,0.0179262,"f using pre-filtered datasets. We also investigate the impact IR methods have on TLS performance. In particular, given an event of interest E, we want to create systematically different event-specific input corpora CE from a large, static background corpus CB in a reproducible fashion. This allows us to evaluate the performance of different TLS systems, given different input CE s. As our background corpus to draw articles from, we used the newsroom dataset (Grusky et al., 2018). Not only has it already been used to evaluate single-document summarization systems (see e.g., Shi et al. (2019) or Mendes et al. (2019)), but it also provides a large amount of data to experiment with different IR methods – this makes results much more reproducible since the background corpus is always fixed. The corpus contains approx. 1.3M articles, covering the years 1998 to 2016. One of the key reasons why we opted to use a different dataset than other TLS work lies in the way previous benchmark datasets have been generated. As described in Section 2., the most commonly used datasets, crisis and TL17, have been constructed via very sophisticated IR – namely, Google search. However, while this provides clear benefits from"
2020.lrec-1.218,C14-1114,0,0.472499,"Missing"
2020.lrec-1.218,N19-4012,0,0.0117593,"S research in favor of using pre-filtered datasets. We also investigate the impact IR methods have on TLS performance. In particular, given an event of interest E, we want to create systematically different event-specific input corpora CE from a large, static background corpus CB in a reproducible fashion. This allows us to evaluate the performance of different TLS systems, given different input CE s. As our background corpus to draw articles from, we used the newsroom dataset (Grusky et al., 2018). Not only has it already been used to evaluate single-document summarization systems (see e.g., Shi et al. (2019) or Mendes et al. (2019)), but it also provides a large amount of data to experiment with different IR methods – this makes results much more reproducible since the background corpus is always fixed. The corpus contains approx. 1.3M articles, covering the years 1998 to 2016. One of the key reasons why we opted to use a different dataset than other TLS work lies in the way previous benchmark datasets have been generated. As described in Section 2., the most commonly used datasets, crisis and TL17, have been constructed via very sophisticated IR – namely, Google search. However, while this provi"
2020.lrec-1.218,Y14-1054,0,0.427568,"2013b) introduced the TL17 dataset. For 17 human-generated timelines taken from news sites, they extracted 400 news articles highly ranked by Google per event, yielding 4,650 articles overall after duplication removal. Similarly, (Tran et al., 2015a; Tran et al., 2015b) provided a new dataset extracted via Google with essentially the same method on four events, the crisis dataset. Both corpora have been made publicly available, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-speci"
2020.lrec-1.218,P15-1154,1,0.932131,"). TLS systems use two input components for a given event E (such as the BP oil spill): an input corpus CE that contains news documents covering E and human-written timelines TE (such as the one in Table 1) for evaluation (and possibly training). To constrain CE s to news articles that only cover E, various IR methods have been employed. To this end, multiple corpora for evaluation of the task have been created over the past decade. Unfortunately, most of the resulting corpora are not publicly available. In addition, the only two publicly available corpora TL17 and crisis (Tran et al., 2013b; Tran et al., 2015a) have been collected via the use of a commercial search engine at a given point in time. This means that the method is not reproducible and there are potential copyright problems with regard to the collected texts (see also Kilgarriff (2007) for further criticism of using commercial search engines). There have been no empirical studies on the effects of different IR methods and the resulting CE on TLS. Instead, most prior work has focused on improving summarization systems, taking the corpora – and therefore the underlying IR methods – for granted. The systems run the risk of optimizing data"
2020.lrec-1.218,N15-1112,0,0.348099,"as most prior work has relied on IR as being given. We compare the impact of three simple, yet successively stronger, IR methods on three established TLS algorithms. We show that the IR impact is significant for all algorithms and that a weak TLS algorithm with strong prior data collection can rival a strong TLS algorithm with weaker IR. 1763 100k Keyword-based/Indexing IR. Chieu and Lee (2004) constructed timelines on activities of G8 leaders and built CE s by extracting articles from the English Gigaword corpus1 between January and June 2002 using leader names as simple keywords. Similarly, Wang et al. (2015) also used keyword search with entity names (such as Ukraine) to extract articles from the New York Times related to a specific event. Nguyen et al. (2014) used the Lucene2 search engine to extract articles from the AFP corpus. None of the resulting corpora are publicly available and the impact of keyword or search method choice on TLS has not been investigated. While name keyword search is attractive due to simplicity and potentially high recall, it might lack precision, especially on large background corpora CB . Commercial search engines. A different approach to finding relevant articles is"
2020.lrec-1.218,N16-1008,0,0.237875,"dataset. For 17 human-generated timelines taken from news sites, they extracted 400 news articles highly ranked by Google per event, yielding 4,650 articles overall after duplication removal. Similarly, (Tran et al., 2015a; Tran et al., 2015b) provided a new dataset extracted via Google with essentially the same method on four events, the crisis dataset. Both corpora have been made publicly available, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus CE to co"
2020.lrec-1.218,D11-1040,0,0.279483,"provided a new dataset extracted via Google with essentially the same method on four events, the crisis dataset. Both corpora have been made publicly available, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., 2016b; http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2003T05. 2 https://lucene.apache.org. 2016 2015 2014 2013 2012 2011 2010 2009 50k Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus CE to construct a summary. The information in the corresponding papers on the construction of CE , given an event E of interest, is scarce, with some papers (Yan et al., 2011) not explaining their method at all. We can overall distinguish two approac"
2020.lrec-1.606,E17-2092,0,0.0227357,"ould a supporter of X feel after reading this tweet?) The possible stance labels were: • positiv (positive) (1) • weder positiv noch negativ (neither positive nor negative) (0) • negativ (negative) (-1) • nicht lesbar / trifft nicht zu (cannot read / does not apply) (x) Annotators were given the tweet, its location, and the profile picture, name and description of the user who posted it. If a tweet was a response to another tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we remov"
2020.lrec-1.606,P15-2072,0,0.348457,"“to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less"
2020.lrec-1.606,D16-1148,0,0.368455,"Missing"
2020.lrec-1.606,D19-1664,0,0.179967,"588/data/ AOSUY6. In compliance with Twitter usage guidelines, we provide tweet ids rather than full tweets with their texts. 2 Published along with the main GTTC. 4924 ing politicians with a doctoral degree • evidence for the status-indicating function of naming in a language other than English and for an academic degree instead of a professional title • evidence that the status-indicating function of naming and titling is weaker in left-leaning than in rightleaning discourse 2. Related Work The framing of entities is a fairly new topic covered in only a handful of papers (Card et al., 2016; Fan et al., 2019). The only currently existing dataset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance"
2020.lrec-1.606,D18-1393,0,0.138455,"eality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the use of names and titl"
2020.lrec-1.606,L16-1591,0,0.289025,"pects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the"
2020.lrec-1.606,N13-1132,0,0.027577,"er tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we removed 3 annotators for being unreliable as judged by MACE. To measure agreement, we used Krippendorff’s alpha (Krippendorff, 2018), which is suitable for multi-coders ordinal annotation (Antoine et al., 2014). The 25 competent annotators had an agreement of 0.62. This is a bit higher than the 0.58 alpha value for stance annotation in van den Berg et al. (2019) (also 3 classes), and than the alpha value of 0.57 obtained"
2020.lrec-1.606,P11-1016,0,0.0327561,"n et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We h"
2020.lrec-1.606,S16-1003,0,0.0303821,"ch annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the rel"
2020.lrec-1.606,S17-2088,0,0.02895,"es prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the relation between naming and stance would depend on which naming function was more dominant: (i) marking of relative status (based on e.g. age or professional role) or (ii) marking of relative solidarity, also r"
2020.lrec-1.606,W10-0214,0,0.0113763,"taset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presi"
2020.lrec-1.606,W19-2101,1,0.288319,"Missing"
2021.eacl-main.160,2020.emnlp-main.751,0,0.354882,"Missing"
2021.eacl-main.160,2020.acl-main.124,0,0.0150595,"d designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We extend on their advice by conducting experi"
2021.eacl-main.160,K19-1038,0,0.0128727,"ng factor in nested designs. Figure 7 shows that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et"
2021.eacl-main.160,D18-1443,0,0.0207703,"of the interfaces for both approaches and full annotator instructions are given in Appendix A. a60 Figure 1: Schematic representation of our study design. Rows represent annotators, columns documents. Each blue square corresponds to a judgement of the summaries of all five systems for a document. Every rectangular group of blue squares forms one block. • The pointer generator summarizer (PG) (See et al., 2017), which is still often used as a baseline for abstractive summarization of summaries for Rank) within a block were presented in random order. • The abstractive sentence rewriter (ASR) of Gehrmann et al. (2018), which is a strong summarization system that does not rely on external pretraining for its generation step 4 • Seneca (Sharma et al., 2019), a system that combines explicit modelling of coreference information with an external coherence model • BART (Lewis et al., 2020), a transformer network that achieves SotA on CNN/DM. We randomly sample 100 documents from the popular CNN/DM corpus (Hermann et al., 2015) with corresponding summaries from all systems to form the item set for all our studies. Study design. We ensure a sufficient total number of annotators by using a block design. We separate"
2021.eacl-main.160,W10-0722,0,0.361075,"ivates us to compare both Likert and ranking annotations in Section 4.1. 2.2 Statistical Analysis If a significance test is conducted, most papers analyse their data either using ANOVA or a sequence of paired t-tests. Both tests are based on the assumption that judgements (or pairs of judgements, in case of paired t-test) are sampled independently from each other. However, in almost all studies, annotators give judgements on more than one summary from the same system. Thus the resulting judgements are only independent if we assume that all annotators behave identically. Given that prior work (Gillick and Liu, 2010; Amidei et al., 2018), as well as our own reliability analysis in Section 4.1, show that especially crowd-workers tend to disagree about judgements, this assumption does not seem warranted. As a consequence, traditional significance tests are at high risk of inflated type I error rates. This is well known in the broader field of linguistics (Barr et al., 2013), but is disregarded in summarization evaluation. We show in Section 5 that this is a substantial problem for current summarization evaluations and suggest alternative analysis methods. 2.3 Design Most papers only report the number of do"
2021.eacl-main.160,D15-1013,0,0.0255547,"regression, as document is no longer a grouping factor in nested designs. Figure 7 shows that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison st"
2021.eacl-main.160,D14-1130,0,0.0161571,"analysis is very similar to our experiments. In addition to their analysis, we show that ignoring grouping factors in statistical analysis of human annotations leads to inflated type I error rates. We also show that power can be increased by choosing nested over crossed designs with the same budget. The problem of underpowered studies has also been tackled outside of NLP by Brysbaert (2019). For psycholinguistics, Barr et al. (2013) demonstrate how generalizability of results is negatively impacted by ignoring grouping factors in the analysis. Mixed effect models have found use in NLP before (Green et al., 2014; Cagan et al., 2017; Karimova et al., 2018; Kreutzer et al., 2020), but to the best of our knowledge they have not been used in summary evaluation. 8 Conclusion We surveyed the current state of the art in manual summary quality evaluation and investigated methods, statistical analysis and design of these studies. We distill our findings into the following guidelines for manual summary quality evaluation: Method. Both ranking and Likert-type annotations are valid choices for quality judgements. However, we present preliminary evidence that the optimal choice of method is dependent on task char"
2021.eacl-main.160,P19-1330,0,0.0192994,"a without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We extend on their advice by conducting experimental studies specifically for summary evaluation. In addition, we show the importance of study design a"
2021.eacl-main.160,P18-1128,0,0.0237827,"to be a promising method for eliciting judgements in NLG tasks and offers a combination of ranking and rating approaches (Novikova et al., 2018; Santhanam and Shaikh, 2019). However, it has not yet found widespread use in the summarization community. While magnitude estimation has been shown to reduce annotator variance, our advice regarding experimental design and grouping factors in statistical analysis applies to this method as well, as annotators can still systematically differ in which systems they prefer. Statistical analysis. With regard to statistical analysis of experimental results, Dror et al. (2018) give advice for hypothesis testing in NLP. However, they do not touch on the problem of dependent samples. Rankel et al. (2011) analyse TAC data and show the importance of accounting for input documents in statistical analysis of summarizer performance and suggest the use of the Wilcoxon signed rank test for analysis. Sadeqi Azer et al. (2020) argue that p-values are often not well understood and advocate bayesian methods as an alternative. While the analysis in our paper is frequentist, the mixed effect model approach can also be integrated into a bayesian framework. Kulikov et al. (2019) mo"
2021.eacl-main.160,P17-2074,0,0.136814,"ability, with a smaller number of papers evaluating coherence and repetition. In the rest of this section we focus on the three aspects of evaluation we cover in this paper: How to elicit judgements, how these judgements are analysed statistically and how studies are designed. 2.1 Methods The majority of evaluations is conducted using Likert-type judgements, with the second most frequent method being rank-based annotations, including pairwise comparison. Best-worst scaling (BWS) is a specific type of ranking-oriented evaluation that requires annotators to specify only the first and last rank (Kiritchenko and Mohammad, 2017). QA (Narayan et al., 2018) is used for content evaluation only. This motivates us to compare both Likert and ranking annotations in Section 4.1. 2.2 Statistical Analysis If a significance test is conducted, most papers analyse their data either using ANOVA or a sequence of paired t-tests. Both tests are based on the assumption that judgements (or pairs of judgements, in case of paired t-test) are sampled independently from each other. However, in almost all studies, annotators give judgements on more than one summary from the same system. Thus the resulting judgements are only independent if"
2021.eacl-main.160,2020.eamt-1.15,0,0.0218541,"ir analysis, we show that ignoring grouping factors in statistical analysis of human annotations leads to inflated type I error rates. We also show that power can be increased by choosing nested over crossed designs with the same budget. The problem of underpowered studies has also been tackled outside of NLP by Brysbaert (2019). For psycholinguistics, Barr et al. (2013) demonstrate how generalizability of results is negatively impacted by ignoring grouping factors in the analysis. Mixed effect models have found use in NLP before (Green et al., 2014; Cagan et al., 2017; Karimova et al., 2018; Kreutzer et al., 2020), but to the best of our knowledge they have not been used in summary evaluation. 8 Conclusion We surveyed the current state of the art in manual summary quality evaluation and investigated methods, statistical analysis and design of these studies. We distill our findings into the following guidelines for manual summary quality evaluation: Method. Both ranking and Likert-type annotations are valid choices for quality judgements. However, we present preliminary evidence that the optimal choice of method is dependent on task characteristics: If many summaries are similar for a given aspect, Like"
2021.eacl-main.160,2020.acl-main.126,0,0.0246564,"wn to be effective in multiple NLP-tasks (Kiritchenko and Mohammad, 2017; Zopf, 2018), including NLG quality evaluation (Novikova et al., 2018). In this work we confirm this for coherence evaluation, although we find evidence that ranking is less efficient on repetition, where many documents do not exhibit any problems. We also add the dimension of annotator workload as a primary determinant of cost to the analysis of the comparison. 1868 Multiple methods have been suggested to reduce study cost by sample selection (Sakaguchi et al., 2014; Novikova et al., 2018; Sakaguchi and Van Durme, 2018; Liang et al., 2020) or integration with automatic metrics (Chaganty et al., 2018). These efforts complement ours, as care still needs to be taken in analysis and study design. Recently, rank-based magnitude estimation has been shown to be a promising method for eliciting judgements in NLG tasks and offers a combination of ranking and rating approaches (Novikova et al., 2018; Santhanam and Shaikh, 2019). However, it has not yet found widespread use in the summarization community. While magnitude estimation has been shown to reduce annotator variance, our advice regarding experimental design and grouping factors i"
2021.eacl-main.160,W04-1013,0,0.283054,"ti-document summarization and take a single or multiple full texts as input and also output text (SDS/MDS). This allows us to concentrate on recommendations for human evaluation of newly developed summarization systems.1 Out of the resulting 105 SDS/MDS system papers, we identify all papers that conduct at least one new comparative system evaluation with human annotators for further analysis, leading to 58 papers in the survey. The fact that this is only about half of all papers is troubling given that it has been recently demonstrated that current automatic evaluation measures such as ROUGE (Lin, 2004) are Number of Systems considered Number of Annotations per Summary Overall Number of Annotators Annotator Recruitment Statistical Evaluation 1 Excluded from the analysis are sentence summarization or headline generation papers, although most of the points we make hold for their evaluation campaigns as well. Summarization evaluation papers that do not present a new system but concentrate on sometimes large-scale system comparisons are discussed in the Related Work section instead. Lists of all included and excluded papers are given in Supplementary Material, which also contains exact evaluatio"
2021.eacl-main.160,P08-2051,0,0.0615465,"ts from the ordinal regression, as document is no longer a grouping factor in nested designs. Figure 7 shows that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system"
2021.eacl-main.160,D17-1306,0,0.0271178,"been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We extend on their advice by conducting experimental studies specifically for summary evaluation. In addition, we show the importance of study design and consideration of annotator-effects in analysis on real world data. The advice of Mathur et al. (2017) regarding annotation sequence effects should be taken into account in addition to our suggestions. Method Comparison. Ranking has been shown to be effective in multiple NLP-tasks (Kiritchenko and Mohammad, 2017; Zopf, 2018), including NLG quality evaluation (Novikova et al., 2018). In this work we confirm this for coherence evaluation, although we find evidence that ranking is less efficient on repetition, where many documents do not exhibit any problems. We also add the dimension of annotator workload as a primary determinant of cost to the analysis of the comparison. 1868 Multiple methods h"
2021.eacl-main.160,D19-1051,0,0.164229,"Missing"
2021.eacl-main.160,N18-1158,0,0.023116,"ers evaluating coherence and repetition. In the rest of this section we focus on the three aspects of evaluation we cover in this paper: How to elicit judgements, how these judgements are analysed statistically and how studies are designed. 2.1 Methods The majority of evaluations is conducted using Likert-type judgements, with the second most frequent method being rank-based annotations, including pairwise comparison. Best-worst scaling (BWS) is a specific type of ranking-oriented evaluation that requires annotators to specify only the first and last rank (Kiritchenko and Mohammad, 2017). QA (Narayan et al., 2018) is used for content evaluation only. This motivates us to compare both Likert and ranking annotations in Section 4.1. 2.2 Statistical Analysis If a significance test is conducted, most papers analyse their data either using ANOVA or a sequence of paired t-tests. Both tests are based on the assumption that judgements (or pairs of judgements, in case of paired t-test) are sampled independently from each other. However, in almost all studies, annotators give judgements on more than one summary from the same system. Thus the resulting judgements are only independent if we assume that all annotato"
2021.eacl-main.160,W19-8609,0,0.02078,"ults, Dror et al. (2018) give advice for hypothesis testing in NLP. However, they do not touch on the problem of dependent samples. Rankel et al. (2011) analyse TAC data and show the importance of accounting for input documents in statistical analysis of summarizer performance and suggest the use of the Wilcoxon signed rank test for analysis. Sadeqi Azer et al. (2020) argue that p-values are often not well understood and advocate bayesian methods as an alternative. While the analysis in our paper is frequentist, the mixed effect model approach can also be integrated into a bayesian framework. Kulikov et al. (2019) model annotator bias in such a framework but do not account for differences in annotator preferences. In work conducted in parallel to ours, Card et al. (2020) show that many human experiments in NLP underreport their experimental parameters and are underpowered, including Likert-type judgements. Their simulation approach to power analysis is very similar to our experiments. In addition to their analysis, we show that ignoring grouping factors in statistical analysis of human annotations leads to inflated type I error rates. We also show that power can be increased by choosing nested over cro"
2021.eacl-main.160,N04-1019,0,0.184368,"ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We extend on their advice by conducting experimental studies specifically for summary evaluation. In addition, we show the importan"
2021.eacl-main.160,W19-8643,0,0.0539233,"Missing"
2021.eacl-main.160,N18-2012,0,0.0548607,"Missing"
2021.eacl-main.160,2020.acl-main.703,0,0.0346412,"systems for a document. Every rectangular group of blue squares forms one block. • The pointer generator summarizer (PG) (See et al., 2017), which is still often used as a baseline for abstractive summarization of summaries for Rank) within a block were presented in random order. • The abstractive sentence rewriter (ASR) of Gehrmann et al. (2018), which is a strong summarization system that does not rely on external pretraining for its generation step 4 • Seneca (Sharma et al., 2019), a system that combines explicit modelling of coreference information with an external coherence model • BART (Lewis et al., 2020), a transformer network that achieves SotA on CNN/DM. We randomly sample 100 documents from the popular CNN/DM corpus (Hermann et al., 2015) with corresponding summaries from all systems to form the item set for all our studies. Study design. We ensure a sufficient total number of annotators by using a block design. We separated our corpus into 20 blocks of 5 documents and included all 5 summaries for each document in the same block, which results in 5 × 5 = 25 summaries per block. All items in a block were judged by the same set of three annotators. No annotator was allowed to judge more than"
2021.eacl-main.160,P19-1502,0,0.0216395,"45 29 10 14 6 2 8 32 10 9 6 9 4 2 6 22 3 14 11 4 1 13 17 16 6 12 16 45 2 20 12 3 23 19 3 5 32 25 35 9 9 4 7 32 St. 23 65 34 11 17 8 2 9 43 14 9 9 14 4 2 10 41 4 21 14 4 1 20 23 23 10 19 25 70 5 30 27 5 28 25 3 9 58 49 46 16 18 6 8 47 Table 1: Our survey for 58 system papers with 95 manual evaluation studies (2017-2019). We show numbers both for individual studies and per paper. As a paper may contain several studies with different parameters, counts in the paper column do not always add up. 1862 not good at predicting summary scores for modern systems (Schluter, 2017; Kryscinski et al., 2019; Peyrard, 2019). We assess both what studies ask annotators to judge, as well as how they elicit and analyse judgements. The survey was conducted by one of the authors: for most papers, the categories they fell into were obvious. For difficult cases (unclear specifications, papers that do not fit the normal mould) the two authors discussed the categorisations. Survey results are given in Table 1. Further details about the choices made in the survey, including category groupings/definitions and what is included under Other, can be found in Appendix B. As many papers conduct more than one human evaluation (for"
2021.eacl-main.160,P17-2005,0,0.0249478,"s document is no longer a grouping factor in nested designs. Figure 7 shows that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluatio"
2021.eacl-main.160,D11-1043,0,0.0509387,"Missing"
2021.eacl-main.160,2020.acl-main.506,0,0.0241938,"perimental design and grouping factors in statistical analysis applies to this method as well, as annotators can still systematically differ in which systems they prefer. Statistical analysis. With regard to statistical analysis of experimental results, Dror et al. (2018) give advice for hypothesis testing in NLP. However, they do not touch on the problem of dependent samples. Rankel et al. (2011) analyse TAC data and show the importance of accounting for input documents in statistical analysis of summarizer performance and suggest the use of the Wilcoxon signed rank test for analysis. Sadeqi Azer et al. (2020) argue that p-values are often not well understood and advocate bayesian methods as an alternative. While the analysis in our paper is frequentist, the mixed effect model approach can also be integrated into a bayesian framework. Kulikov et al. (2019) model annotator bias in such a framework but do not account for differences in annotator preferences. In work conducted in parallel to ours, Card et al. (2020) show that many human experiments in NLP underreport their experimental parameters and are underpowered, including Likert-type judgements. Their simulation approach to power analysis is ver"
2021.eacl-main.160,W14-3301,0,0.0749439,"Missing"
2021.eacl-main.160,P18-1020,0,0.0477711,"Missing"
2021.eacl-main.160,W19-8610,0,0.0238169,"mary determinant of cost to the analysis of the comparison. 1868 Multiple methods have been suggested to reduce study cost by sample selection (Sakaguchi et al., 2014; Novikova et al., 2018; Sakaguchi and Van Durme, 2018; Liang et al., 2020) or integration with automatic metrics (Chaganty et al., 2018). These efforts complement ours, as care still needs to be taken in analysis and study design. Recently, rank-based magnitude estimation has been shown to be a promising method for eliciting judgements in NLG tasks and offers a combination of ranking and rating approaches (Novikova et al., 2018; Santhanam and Shaikh, 2019). However, it has not yet found widespread use in the summarization community. While magnitude estimation has been shown to reduce annotator variance, our advice regarding experimental design and grouping factors in statistical analysis applies to this method as well, as annotators can still systematically differ in which systems they prefer. Statistical analysis. With regard to statistical analysis of experimental results, Dror et al. (2018) give advice for hypothesis testing in NLP. However, they do not touch on the problem of dependent samples. Rankel et al. (2011) analyse TAC data and show"
2021.eacl-main.160,E17-2007,0,0.017931,"t ANOVA CI Other/unspecified None Pa. 17 45 29 10 14 6 2 8 32 10 9 6 9 4 2 6 22 3 14 11 4 1 13 17 16 6 12 16 45 2 20 12 3 23 19 3 5 32 25 35 9 9 4 7 32 St. 23 65 34 11 17 8 2 9 43 14 9 9 14 4 2 10 41 4 21 14 4 1 20 23 23 10 19 25 70 5 30 27 5 28 25 3 9 58 49 46 16 18 6 8 47 Table 1: Our survey for 58 system papers with 95 manual evaluation studies (2017-2019). We show numbers both for individual studies and per paper. As a paper may contain several studies with different parameters, counts in the paper column do not always add up. 1862 not good at predicting summary scores for modern systems (Schluter, 2017; Kryscinski et al., 2019; Peyrard, 2019). We assess both what studies ask annotators to judge, as well as how they elicit and analyse judgements. The survey was conducted by one of the authors: for most papers, the categories they fell into were obvious. For difficult cases (unclear specifications, papers that do not fit the normal mould) the two authors discussed the categorisations. Survey results are given in Table 1. Further details about the choices made in the survey, including category groupings/definitions and what is included under Other, can be found in Appendix B. As many papers co"
2021.eacl-main.160,P17-1099,0,0.226989,"and powerful studies for system-level comparison when compared to studies with the same budget but only one judgement per summary. Evaluation Method Code and data for our experiments is available at https://github.com/julmaxi/summary_ lq_analysis. 2 Number of Documents in Evaluation Literature Survey We survey all summarization papers in ACL, EACL, NAACL, ConLL, EMNLP, TACL and the Computational Linguistics journal in the years 2017-2019. We choose this timeframe as we are interested in current practices in summarization evaluation: 2017 marks the publication of the pointer generator network (See et al., 2017), which has been highly influential for neural summarization. We focus our analysis on papers that present a novel system for single- or multi-document summarization and take a single or multiple full texts as input and also output text (SDS/MDS). This allows us to concentrate on recommendations for human evaluation of newly developed summarization systems.1 Out of the resulting 105 SDS/MDS system papers, we identify all papers that conduct at least one new comparative system evaluation with human annotators for further analysis, leading to 58 papers in the survey. The fact that this is only a"
2021.eacl-main.160,N19-1072,0,0.0189737,"wer when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We extend on their advice by conducting experimental studies specifically for summary evaluation. In addition, we show the importance of study design and consideration of ann"
2021.eacl-main.160,D19-1323,0,0.012205,"tudy design. Rows represent annotators, columns documents. Each blue square corresponds to a judgement of the summaries of all five systems for a document. Every rectangular group of blue squares forms one block. • The pointer generator summarizer (PG) (See et al., 2017), which is still often used as a baseline for abstractive summarization of summaries for Rank) within a block were presented in random order. • The abstractive sentence rewriter (ASR) of Gehrmann et al. (2018), which is a strong summarization system that does not rely on external pretraining for its generation step 4 • Seneca (Sharma et al., 2019), a system that combines explicit modelling of coreference information with an external coherence model • BART (Lewis et al., 2020), a transformer network that achieves SotA on CNN/DM. We randomly sample 100 documents from the popular CNN/DM corpus (Hermann et al., 2015) with corresponding summaries from all systems to form the item set for all our studies. Study design. We ensure a sufficient total number of annotators by using a block design. We separated our corpus into 20 blocks of 5 documents and included all 5 summaries for each document in the same block, which results in 5 × 5 = 25 sum"
2021.eacl-main.160,D19-1116,0,0.0162491,"d designs. Figure 7 shows that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish be"
2021.eacl-main.160,D19-1618,0,0.0166539,"ws that nested designs always have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluat"
2021.eacl-main.160,D19-1053,0,0.0151467,"lways have a power advantage over crossed designs, especially when few judgements are elicited. We also find that ART can be used to analyse data without loss of power when there are enough independent blocks. This might be attractive as ART is less computationally expensive than ordinal regression. Related Work Human evaluation has a long history in summarization research. This includes work on the correlation of automatic metrics with human judgements (Lin, 2004; Liu and Liu, 2008; Graham, 2015; Peyrard and Eckle-Kohler, 2017; Gao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019; Zhao et al., 2019; Fabbri et al., 2020; Gao et al., 2020) and improving the efficiency of the annotation process (Nenkova and Passonneau, 2004; Hardy et al., 2019; Shapira et al., 2019). The impact of annotator inconsistency on system ranking has been studied both by Owczarzak et al. (2012) and Gillick and Liu (2010). To the best of our knowledge, we are the first to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies. For general NLG evaluation, van der Lee et al. (2019) establish best practices for evaluation studies. We ext"
al-saif-markert-2010-leeds,W05-0312,0,\N,Missing
al-saif-markert-2010-leeds,W04-1602,0,\N,Missing
al-saif-markert-2010-leeds,P99-1006,0,\N,Missing
al-saif-markert-2010-leeds,J08-4004,0,\N,Missing
al-saif-markert-2010-leeds,C08-2022,0,\N,Missing
al-saif-markert-2010-leeds,prasad-etal-2008-penn,0,\N,Missing
al-saif-markert-2010-leeds,I08-7009,0,\N,Missing
al-saif-markert-2010-leeds,I08-7010,0,\N,Missing
al-saif-markert-2010-leeds,dukes-habash-2010-morphological,0,\N,Missing
asheghi-etal-2014-designing,W04-0811,0,\N,Missing
asheghi-etal-2014-designing,sharoff-etal-2010-web,1,\N,Missing
asheghi-etal-2014-designing,D08-1027,0,\N,Missing
asheghi-etal-2014-designing,J06-3004,0,\N,Missing
asheghi-etal-2014-designing,P08-1080,0,\N,Missing
asheghi-etal-2014-designing,J06-1005,0,\N,Missing
asheghi-etal-2014-designing,J08-4004,0,\N,Missing
asheghi-etal-2014-designing,D09-1030,0,\N,Missing
asheghi-etal-2014-designing,P97-1005,0,\N,Missing
asheghi-etal-2014-designing,P09-1076,0,\N,Missing
asheghi-etal-2014-designing,J00-4003,0,\N,Missing
asheghi-etal-2014-designing,rehm-etal-2008-towards,0,\N,Missing
asheghi-etal-2014-designing,P05-1045,0,\N,Missing
asheghi-etal-2014-designing,J09-4005,0,\N,Missing
C08-1104,E06-1027,0,0.615666,"er the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Subjectivity labels for senses add an additional layer of annotation to electronic lexica and allow to group many fine-grained senses into higher-level classes based on subjectivity/objectivity. This can increase the lexica’s usability. As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivityambiguous words (such as positive). In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. Moreover, the prevalence of different word senses in different domains also means that a subjective or an objective sense of a word might be dominant in different domains; thus, in a science text positive is likely not to have a subjective reading. The annotation of words as subjective and objective or positive and negative independent of sense or domain does not capture such distinctions. In this paper, we validate whet"
C08-1104,esuli-sebastiani-2006-sentiwordnet,0,0.63332,"itive) factor” (subjective) Introduction In recent years, subjectivity analysis and opinion mining have attracted considerable attention in the NLP community. Unlike traditional information extraction and document classification tasks which usually focus on extracting facts or categorizing documents into topics (e.g., “sports”, “politics”, “medicine”), subjectivity analysis focuses on determining whether a language unit (such as a word, sentence or document) expresses a private state, opinion or attitude and, if so, what polarity is expressed, i.e. a positive or negative attitude. Inspired by Esuli and Sebastiani (2006) and Wiebe and Mihalcea (2006), we explore the automatic detection of the subjectivity of word senses, in contrast to the more frequently explored task of determining the subjectivity of words (see Section 2). This is motivated by many words being c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Subjectivity labels for senses add an additional layer of annotation to electronic lexica and allow to group many fine-grained senses into higher-level classes based on subje"
C08-1104,I05-2011,0,0.0296266,"Missing"
C08-1104,W04-3239,0,0.065645,"Missing"
C08-1104,P04-1035,0,0.0509425,"sources that provide subjectivity information to our task. 4.2 Sentence Collections: Movie and MPQA It is reasonable to cast word sense subjectivity classification as a sentence classification task, with the glosses that WordNet provides for each sense as the sentences to be classified. Then we can in theory feed any collection of annotated subjective and objective sentences as training data into our classifier while the annotated Micro-WNOp corpus is used as test data. We experimented with two different available data sets to test this assumption. Movie-domain Subjectivity Data Set (Movie): Pang and Lee (2004) used a collection of labeled subjective and objective sentences in their work on review classification.5 The data set contains 5000 subjective sentences, extracted from movie reviews collected from the Rotten Tomatoes web formed best. 5 Available at http://www.cs.cornell.edu/ People/pabo/movie-review-data/ 828 site.6 The 5000 objective sentences were collected from movie plot summaries from the Internet Movie Database (IMDB). The assumption is that all the snippets from the Rotten Tomatoes pages are subjective (as they come from a review site), while all the sentences from IMDB are objective"
C08-1104,W02-1011,0,0.0116957,"Missing"
C08-1104,W03-0404,0,0.17389,"d also shows that subjectivity-ambiguous words are frequent. Section 4 describes our proposed classification algorithms in detail. Section 5 presents the experimental results and evaluation, followed by conclusions and future work in Section 6. 2 Related Work There has been extensive research in opinion mining at the document level, for example on product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005). Several other approaches focus on the subjectivity classification of sentences (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff and Wiebe, 2003). They often build on the presence of subjective words in the sentence to be classified. Closer to our work is the large body of work on the automatic, context-independent classification of words according to their polarity, i.e as positive or negative (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). They use either co-occurrence patterns in corpora or dictionarybased methods. Many papers assume that subjectivity recognition, i.e. separating subjective from objective words, has already been achieved prior to polarity recognition and tes"
C08-1104,W08-1207,1,0.883333,"Missing"
C08-1104,P05-1017,0,0.110023,"n product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005). Several other approaches focus on the subjectivity classification of sentences (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff and Wiebe, 2003). They often build on the presence of subjective words in the sentence to be classified. Closer to our work is the large body of work on the automatic, context-independent classification of words according to their polarity, i.e as positive or negative (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). They use either co-occurrence patterns in corpora or dictionarybased methods. Many papers assume that subjectivity recognition, i.e. separating subjective from objective words, has already been achieved prior to polarity recognition and test against word lists containing subjective words only (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005). However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automati"
C08-1104,P06-1134,0,0.659466,"Missing"
C08-1104,P07-1054,0,0.617416,"Missing"
C08-1104,P97-1023,0,0.21395,"has been extensive research in opinion mining at the document level, for example on product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005). Several other approaches focus on the subjectivity classification of sentences (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff and Wiebe, 2003). They often build on the presence of subjective words in the sentence to be classified. Closer to our work is the large body of work on the automatic, context-independent classification of words according to their polarity, i.e as positive or negative (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). They use either co-occurrence patterns in corpora or dictionarybased methods. Many papers assume that subjectivity recognition, i.e. separating subjective from objective words, has already been achieved prior to polarity recognition and test against word lists containing subjective words only (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005). However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder tas"
C08-1104,C04-1200,0,0.565017,"Missing"
C08-1104,H05-2017,0,\N,Missing
C08-1104,H05-1043,0,\N,Missing
C08-1104,W03-1014,0,\N,Missing
C18-1325,J90-1003,0,0.532214,"et al., 2016) in its default setting to train word embeddings on the SdeWaC-corpus (Faaß and Eckart, 2013), which contains about 880 million tokens and is a cleaned up version of the deWaC corpus (Baroni et al., 2009). The choice of fastText was motivated by the fact that fastText computes vectors for words by adding up the vectors for n-grams found in the words, which allows us to produce vectors for words not seen in the training data. Since many of our complex forms are (near-)hapaxes, this is a crucial benefit of fastText. Pointwise mutual information. We use Pointwise mutual information (Church and Hanks, 1990) to capture the level of association between the two components of the complex word. The expectation is that the components of regular compounds exhibit higher PMI-scores than the components of a complex word involving an affixoid. This is motivated by Tellenbach (1985)’s observation that for complex forms containing an affixoid use, paraphrases containing the morpheme in question as a free word are unlikely. By contrast, compositional compounds are often paraphrased using their components. As an example, the compositional compound Perserk¨onig is readily paraphrased as K¨onig der Perser ‘king"
C18-1325,W14-5805,0,0.0133934,"icular set of suffixoid candidates differences between the second component and the complex word’s supersenses may be particularly important, we experiment with an alternative set of supersense features (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persu"
C18-1325,faass-etal-2010-design,0,0.066989,"Missing"
C18-1325,N09-1002,0,0.0355169,"Missing"
C18-1325,W97-0802,0,0.154792,"Missing"
C18-1325,W14-2302,0,0.0139202,"entiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, p"
C18-1325,W09-4635,0,0.0733123,"res (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words"
C18-1325,L16-1413,0,0.0555122,"Missing"
C18-1325,P08-2028,0,0.0918526,"Missing"
C18-1325,remus-etal-2010-sentiws,0,0.0213281,"hether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to"
C18-1325,ruppenhofer-etal-2017-evaluating,1,0.803388,"ining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, politics, . . . ) whereas concrete words refer to things we can perceive (sound, scent, . . . ). The second dimension concerns imageability (img). A large subset of concrete wo"
C18-1325,schmid-etal-2004-smor,0,0.0612312,"Missing"
C18-1325,S13-1038,0,0.0555683,"Missing"
C18-1325,N09-1001,1,0.79684,"Missing"
C18-1325,S14-2033,0,0.0245143,"unishment). The final dimension, arousal, represents the intensity of emotion caused by a stimulus (alert vs. calm).8 We obtain affective ratings from the resource of K¨oper and Schulte im Walde (2016). It provides information on 350k words and is far more comprehensive than the affective norm data of Kanske and Kotz (2010) or Lahl et al. (2009). It is also much larger than commonly used polarity lexicons for German such as PolArt (Klenner et al., 2009) or GermanPolarityClues (Waltinger, 2010). Emotion. Since emotion information is commonly used in sentiment-related classification tasks (e.g. Tang et al. (2014), Sulis et al. (2016)), we wanted to see to what extent emotion information could benefit our task. For this purpose, we use the NRC Word-Emotion Association Lexicon (EmoLex) for English which was created by Mohammad and Turner (2013) using a crowdsourcing approach. EmoLex contains binary associations of words with the eight basic emotions (joy, sadness, anger, fear, disgust, surprise, trust, anticipation) of Plutchik (1962) . Although the German version of the lexicon was produced using machine translation, we use it here because we do not have a similarly large natively produced resource ava"
C18-1325,D11-1063,0,0.0294224,"for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 385"
C18-1325,waltinger-2010-germanpolarityclues,0,0.213529,"indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete ("
C18-1325,J04-3002,0,0.15836,"affixoids for the purposes of sentiment analysis. On the one hand, theoretical linguistic work that notes the expressive function of affixoids such as Meibauer This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3853 Proceedings of the 27th International Conference on Computational Linguistics, pages 3853–3865 Santa Fe, New Mexico, USA, August 20-26, 2018. (2013) suggests this. On the other hand, it is known from prior research on sentiment analysis that hapax words in general are often subjective (Wiebe et al., 2004). As we show in §3, affixoids tend to generate many (near-)hapax forms, which meshes with observations on their productivity in the morphological literature and the general expectation about the Zipfian distribution of word frequencies. Since hapaxes, by definition, cannot be readily analyzed based on their distribution in corpora, it would be very useful if we could make use of their intrinsic properties to classify such forms as affixoid uses (and therefore likely subjective) or not. The main task that we set ourselves in this paper is morpheme sense disambiguation: we want to classify compl"
C18-1325,N16-1094,1,0.855997,"e our complex forms are unlikely to be listed in lexical resources, we will usually lack information such as glosses, supersenses or example sentences for them. 1 https://github.com/josefkr/affixoids https://de.wiktionary.org 3 http://wortwarte.de/ 4 This assumption can be made plausible by the observations that longer words have more specific meanings than shorter ones, and that they tend to have fewer meanings than their head words, and fewer meanings than their components do on average (Altmann, 2002). 2 3854 In another strand of research involving German morphology and sentiment analysis, Wiegand et al. (2016) developed an approach to classify the first element of German compounds as expressing either the source or target of evaluation, or neither, relative to the second element, if in the first step of analysis the second element was determined to be subjective. As do we, those authors focused on noun-noun compounds and they did not address polarity classification. However, their approach targets higher frequency words as it relies on the availability of sufficient corpus data to enable the use of distributional similarity. For our dataset, we cannot directly model the distributional properties of"
C96-1084,C92-1023,0,0.181619,"Missing"
C96-1084,P86-1004,0,\N,Missing
C96-1084,H86-1011,0,\N,Missing
C96-1084,C94-2126,0,\N,Missing
C96-1084,E95-1033,1,\N,Missing
C96-1084,J95-2003,0,\N,Missing
C96-1084,P96-1036,1,\N,Missing
D09-1066,P89-1010,0,0.608825,"he windows of my house can be aided by the knowledge that windows are often (syntagmatically) associated with houses. 628 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 628–637, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP (Yarowsky and Florian, 2002; Lamjiri et al., 2004; Wang, 2005). While choosing an optimum window size for an application is often subject to trial and error, there are some generally recognized trade-offs between small versus large windows, such as the impact of data-sparseness, and the nature of the associations retrieved (Church and Hanks, 1989; Church and Hanks, 1991; Rapp, 2002) are often the basis of higher-order vector wordspace models used for predicting paradigmatic relationships: i.e. through the observation of words which share similar sets of syntagmatic associations. Therefore improvements made at the level we are concerned with may reasonably be expected to carry through to applications which hinge on the identification of paradigmatic relationships. After a discussion of previous work in Section 2, we formulate the exact association measures and parameter settings which we compare in Section 3, where we also introduce th"
D09-1066,C02-1007,0,0.611938,"equency-based and distance-based measures as predictors of human association scores as elicited in several different free word association tasks. In this work we focus on first-order association measures as predictors of syntagmatic associations. This is in contrast to second and higher-order measures which are better predictors of paradigmatic associations, or word similarity. The distinction between syntagmatic and paradigmatic relationship types is neither exact nor mutually exclusive, and many paradigmatic relationships can be observed syntagmatically in the text. Roughly in keeping with (Rapp, 2002), we hereby regard paradigmatic assocations as those based largely on word similarity (i.e. including those typically classed as synonyms, antonyms, hypernyms, hyponyms etc), whereas syntagmatic associations are all those words which strongly invoke one another yet which cannot readily be said to be similar. Typically these will have an identifiable semantic or grammatical relationship (meronym/holonym: stem – flower, verb/object: eat – food etc), or may have harder-to-classify topical or idiomatic relationships (family – Christmas, rock – roll). We will show in Section 3.2 that syntagmatic re"
D09-1066,J93-1003,0,0.171083,"ed. The 33,000 word count was satisfied after making these normalizations. In order to maximize the variety of the language in the samples, the subsets were built from approximately the first 2000 words only of each randomly selected document from the BNC (a similar strategy to that used in constructing the 1 million word Brown Corpus). Both a 10 million word and a 1 million word sample were constructed in this fashion, allowing us to also examine the effects of varying corpus size and content. 3.4 3.4.1 association. One popular and statistically appealing such measure is Log-Likelihood (LL) (Dunning, 1993). LL works on a similar principle to PMI but considers the ratio of the observed to expected co-occurrence frequencies for all contingencies (i.e. including those where the words do not co-occur). LL, as it most frequently appears in the literature, is not actually a measure of positive association: it also responds to significant negative association. Therefore LL is arguably not suited to the task in hand. Krenn & Evert (2001) experiment with one-tailed variants of LL and Chi-Squared measures, although they do not define these variants. Here, we construct a one-tailed variant of LL by simply"
D09-1066,C04-1147,0,0.526455,"of window-based measures which perform as well as or better in the human association norm task than established measures. 1 Introduction Automatic discovery of semantically associated words has attracted a large amount of attention in the last decades and a host of computational association measures have been proposed to deal with this task (see Section 2). These measures traditionally rely on the co-ocurrence frequency of two words in a corpus to estimate a relatedness score. There has been a recent emergence of distancebased language modelling techiques in NLP (Savicki and Hlavacova, 2002; Terra and Clarke, 2004) in which the number of tokens separating words is the essential quantity. While some of this work has considered distance-based alternatives to conventional association measures (Hardcastle, 2005; 1 where for example resolving my house – the windows to the windows of my house can be aided by the knowledge that windows are often (syntagmatically) associated with houses. 628 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 628–637, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP (Yarowsky and Florian, 2002; Lamjiri et al., 2004; Wang, 2005). While c"
D09-1066,J00-4003,0,0.0409127,"Missing"
D09-1066,E09-1098,1,0.908982,"evaluations similar to those described in (Baroni et al., 2008) and by Rapp (2002), we show that the best distance-based measures correlate better overall with human association scores than do the best window based configurations (see Section 4), and that they also serve as better predictors of the strongest human associations (see Section 5). 2 Measures based on distance between words in the text. The idea of using distance as an alternative to frequency for modelling language has been touched upon in recent literature (Savicki and Hlavacova, 2002; Terra and Clarke, 2004; Hardcastle, 2005). Washtell (2009) showed that it is possible to build distance-based analogues of existing syntagmatic association measures, by using the notions of mean and expected distance rather than of frequency. These measures have certain theoretical qualities - notably scale-independence and relative resilience to data-sparseness - which might be expected to provide gains in tasks such as the reproduction of human association norms from corpus data. The specific measure introduced by Washtell, called Co-Dispersion, is based upon an established biogeographic dispersion measure (Clark and Evans, 1954). We provide a thor"
D09-1066,W93-0310,0,0.116997,". . . distxyn ) 3.5 Co-occurrence Parameters For frequency-based co-occurrence statistics, the principle parameter is the window size. We will use five window sizes separated by a constant scaling factor, chosen so as to span those most commonly encountered in the literature, with some extension towards the upper end. We use w to represent this parameter, with w = 2 implying a window size of +/-2. The parameter values explored that of direction in the text. While the two may correlate, one can find ample counter-examples: jerky triggers beef more strongly than beef triggers jerky. 3 Note that Wettler & Rapp (1993) introduced a more general asymmetric measure for predicting human associations, by employing an exponent parameter to p(y). Our formulisation is equivalent to their measure with an exponent of 0.5, whereas they found an exponent of 0.66 to be most effective in their empirical study. Exponents of 0 and 1 result in CP and PMI respectively. 632 are w = 2, w = 10, w = 50, w = 250 and w = 1250. We examine such large window sizes so as to give a fairer comparison with the distance approach which is not bounded by a window, and in acknowledgement of the fact that the entire document as context has b"
D09-1066,W04-0833,0,0.0610928,"Missing"
D09-1066,P98-2127,0,0.0315941,"ew). All of these measures have in common that they require co-occurrence frequency to be specified, and therefore require some definition of a region within which to count co-occurrences. This region might be the entirety of a document at one extreme, or a bigram at the other. A versatile and hugely popular generalised approach is therefore to consider a ”‘window”’ of w words, where w can be varied to suit the application. Unsurprisingly, it has been found that this is a parameter which can have a significant impact upon performance Measures based on syntactic relations. Several researchers (Lin, 1998; Curran, 2003; Pado and Lapata, 2007) have used word space models based on grammatical relationships for detecting and quantifying (mostly paradigmatic) word associations. In this paper, we will not use syntactic relation measures for two main reasons. Firstly these depend on the availability of parsers, which is not a given for many languages. Secondly, this may not be the most pertinent approach for predicting human free associations, in which certain observed relationsips can be hard to express in terms of syntactic relationships. 3 Methodology Similar to (Rapp, 2002; Baroni et al., 2008,"
D09-1066,J07-2002,0,0.175645,"s have in common that they require co-occurrence frequency to be specified, and therefore require some definition of a region within which to count co-occurrences. This region might be the entirety of a document at one extreme, or a bigram at the other. A versatile and hugely popular generalised approach is therefore to consider a ”‘window”’ of w words, where w can be varied to suit the application. Unsurprisingly, it has been found that this is a parameter which can have a significant impact upon performance Measures based on syntactic relations. Several researchers (Lin, 1998; Curran, 2003; Pado and Lapata, 2007) have used word space models based on grammatical relationships for detecting and quantifying (mostly paradigmatic) word associations. In this paper, we will not use syntactic relation measures for two main reasons. Firstly these depend on the availability of parsers, which is not a given for many languages. Secondly, this may not be the most pertinent approach for predicting human free associations, in which certain observed relationsips can be hard to express in terms of syntactic relationships. 3 Methodology Similar to (Rapp, 2002; Baroni et al., 2008, among others), we use comparison to hu"
D09-1066,N04-3012,0,0.130204,"Missing"
D09-1066,J90-1003,0,\N,Missing
D09-1066,C98-2122,0,\N,Missing
D11-1068,al-saif-markert-2010-leeds,1,0.821024,", production rules and syntactic trees (Wang et al., 2010; Lin et al., 2009) as well as language modelling (Zhou et al., 2010). As we only deal with explicit connectives this work is not directly comparable to ours, although we do explore some of the suggested features for improving explicit connective disambiguation. 4 An Arabic Discourse Corpus We annotate news articles from the Arabic Penn Treebank (Part 1 v2.0) (Maamouri and Bies, 2004) for explicitly marked discourse relations. This is the first discourse-annotated corpus for Arabic, whose initial development stages we have described in (AlSaif and Markert, 2010). We summarize this previous work and extend it by including agreement studies for arguments in Sections 4.1 and 4.2. In Sections 4.3, 4.4 and 4.5. we then present a corpus study on the corpus which shows our major claim as to the importance and high levels of ambiguity of Arabic discourse connectives. 4.1 Annotation Principles We overall follow the annotation principles in the Penn Discourse Treebank for explicit connectives (for example, arguments can occur at any distance from the connectives). The relation set we use is a more coarse-grained version of the PDTB relations with two relations"
D11-1068,W05-0613,0,0.0260118,"ine” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need"
D11-1068,N07-1054,0,0.028454,"Missing"
D11-1068,P09-1075,0,0.0382628,"improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003;"
D11-1068,W03-1210,0,0.0584635,"uality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 (1) [John liked adventure,]Arg2 [ while]DC [Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1 [while]DC ,[their mother was cooking]Arg2 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for"
D11-1068,D09-1036,0,0.0249005,"ith regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are"
D11-1068,N10-1043,0,0.0133166,"a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder and Lascarides, 2008), production rules a"
D11-1068,W04-1602,0,0.0339762,"38 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder and Lascarides, 2008), production rules and syntactic trees (Wang et al., 2010; Lin et al., 2009) as well as language modelling (Zhou et al., 2010). As we only deal with explicit connectives this work is not directly comparable to ours, although we do explore some of the suggested features for improving explicit connective disambiguation. 4 An Arabic Discourse Corpus We annotate news articles from the Arabic Penn Treebank (Part 1 v2.0) (Maamouri and Bies, 2004) for explicitly marked discourse relations. This is the first discourse-annotated corpus for Arabic, whose initial development stages we have described in (AlSaif and Markert, 2010). We summarize this previous work and extend it by including agreement studies for arguments in Sections 4.1 and 4.2. In Sections 4.3, 4.4 and 4.5. we then present a corpus study on the corpus which shows our major claim as to the importance and high levels of ambiguity of Arabic discourse connectives. 4.1 Annotation Principles We overall follow the annotation principles in the Penn Discourse Treebank for explicit c"
D11-1068,P02-1047,0,0.0915894,"ent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang"
D11-1068,D08-1020,0,0.0419034,"tly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 (1) [John liked adventure,]Arg2 [ while]DC [Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1 [while]DC ,[their mother was cooking]Arg2 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse"
D11-1068,P09-2004,0,0.541334,"usage (such as the use of while as a noun). We show in Section 5 that we can distinguish discourse- and non-discourse usage for potential connectives in Arabic with very high reliability, even without parsed data, a fact that is important for languages with fewer high quality NLP tools available. We then present an algorithm for relation identification in Section 6 that shows small but significant gains over assigning the most frequent relation for each connective. We discuss future work and conclude in Section 7. 2 The Tasks The handling of explicit connectives can be split into three tasks (Pitler and Nenkova, 2009). The first task of discourse connective recognition distinguishes between the discourse usage and non-discourse usage of potential connectives. Whereas some poten tial connectives such as the Arabic connective áºË /lkn/but almost always have discourse usage, this is 737 not true for all potential connectives.1 Thus, the dis course usage of Arabic éJ.«P /r˙gbh/desire needs to be distinguished from its use as a noun. Conjunctions such as ð /w/and, ð@ /¯aw/or can have discourse usage or just conjoin two non-abstract entities as in èPA ð QÔ« /,mr w s¯arh/Omar and Sarah. The second task is disco"
D11-1068,C08-2022,0,0.309624,"unt Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) concentrate on disambiguating the three connectives since, while, when only, using a very small set of features indicating tense and temporal markers in arguments. They achieve good improvements over a “most frequent relation per connective” baseline. A more comprehensive study on all discourse connectives in the PDTB (Pitler et al., 2008; Pitler and Nenkova, 2009) reveals that most connectives are not ambiguous in English. Using syntactic features of the connective, they achieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse p"
D11-1068,P09-1077,0,0.02769,"re highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement o"
D11-1068,prasad-etal-2008-penn,0,0.288897,"iversity of Leeds Leeds, UK LS2 9JT markert@comp.leeds.ac.uk Abstract the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as C AUSAL or C ONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit C ONTRAST between the attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit T EMPORAL relation. In Ex. 3, an implicit C AUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. We present the first algorithms to automatically identify explicit discourse connectives and the relations they s"
D11-1068,I08-7010,0,0.565286,"iversity of Leeds Leeds, UK LS2 9JT markert@comp.leeds.ac.uk Abstract the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as C AUSAL or C ONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit C ONTRAST between the attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit T EMPORAL relation. In Ex. 3, an implicit C AUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. We present the first algorithms to automatically identify explicit discourse connectives and the relations they s"
D11-1068,C08-1101,0,0.0250936,"Missing"
D11-1068,N03-1030,0,0.0694744,"chieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framew"
D11-1068,P10-1073,0,0.058967,"relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporle"
D11-1068,P99-1006,0,0.2012,"o.uk Katja Markert University of Leeds Leeds, UK LS2 9JT markert@comp.leeds.ac.uk Abstract the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as C AUSAL or C ONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit C ONTRAST between the attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit T EMPORAL relation. In Ex. 3, an implicit C AUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. We present the first algorithms to automatically identify explicit discourse connectives and"
D11-1068,D07-1010,0,0.214736,"rabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse connective disambiguation) have been tackled for English.2 (Pitler 1 Arabic examples contain in order: the Arabic right-to-left script, the transliteration (standards ISO/R 233 and DIN 31635) and the English translation (if possible). 2 There is also substantial work on argument identification (Wellner and Pustejovski, 2007; Elwell and Baldridge, 2008) and Nenkova, 2009) use gold standard syntactic features as well as the connective surface string in a supervised model for discourse connective recognition. They achieve very high results with this approach. We will (i) show that similar features work well for Arabic (ii) take into account Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) conc"
D11-1068,W06-1317,0,0.0173442,"ven. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3 Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 738 2008) and often need world knowledge (Lin et al., 2009"
D11-1068,W05-0312,0,0.248206,"oduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified by the fact that this first study for a new language should"
D11-1068,I08-7009,0,0.096364,"[their mother was cooking]Arg2 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified by the fact that this first study for"
D11-1068,C10-2172,0,0.151922,"Missing"
D12-1017,S07-1109,0,0.0948328,"t of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1033,0,0.279243,"ntext, dependent on the semantic class studied and (ii) that an unsupervised approach — although lower than the supervised one — outperforms the supervised most frequent reading baseline and performs close to a standard supervised model with the basic set of lexico-syntactic features (Nissim and Markert, 2005). 2 Related Work The word sense disambiguation setting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions"
D12-1017,J91-1003,0,0.806461,"ation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words"
D12-1017,W98-0720,0,0.49228,"s limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a"
D12-1017,P03-1069,0,0.0257898,"tures as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction f"
D12-1017,S07-1031,0,0.309658,"tting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) c"
D12-1017,J98-2002,0,0.0128516,"ed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (more general) concepts that describe the set of collocates for a word w and grammatical relation r. Starting from the concept collocates gathered, we go upwards following WikiNet’s is a links, and for each node found that covers at least N concept collocates (N is a parameter, N=2 in the experiments presented here), the MDL score of the node is computed (Algorithm 2). We place a limit M on the number of upward steps in the hierarchy (M =3 in our experiments). The disjoint set of nodes that has the lowest overall MDL score is chosen (Γ), and"
D12-1017,D09-1095,1,0.884177,"t the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words related to the PMW from a"
D12-1017,nastase-etal-2010-wikinet,1,0.885388,"Missing"
D12-1017,S07-1101,0,0.312209,"ork in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1093,0,0.266348,"my resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred f"
D12-1017,J91-4003,0,0.727503,"anking showcases our second major innovation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the select"
D12-1017,D11-1091,0,0.0239153,"Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatic"
D12-1017,P09-3001,0,0.164701,"s grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occu"
D12-1017,N10-1091,0,0.0159107,"S // Remove hyponyms. for all {(c, c0 ) ∈ S 0 |(c0 , is a, c) ∈ W kN } do // update frequency f of c fc = fc + fc0 , f ∈ S delete c0 return S 0 187 discourse” assumption – a phrase that appears associated with a hyperlink once in the article body will be associated with the same hyperlink throughout the article (this applies to the article title as well, which is not hyperlinked in the article itself). This new version of the corpus was then split into sentences, and those without hyperlinks were removed. The remaining 18 million sentences were parsed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (mo"
D12-1017,C00-2128,0,0.0528555,"requent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatical patterns (a noun instead of a gerund or to-infinitive following the verb) and the problem is limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in meto"
D12-1017,bentivogli-etal-2010-building,0,\N,Missing
D13-1077,J08-1001,0,0.0257015,"ition Using a Rich Linguistic Feature Set Yufang Hou1 , Katja Markert2 , Michael Strube1 1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany (yufang.hou|michael.strube)@h-its.org 2 School of Computing, University of Leeds, UK scskm@leeds.ac.uk Abstract Example 1 cannot be established. This is a problem both for coherence theories such as Centering (Grosz et al., 1995) (where bridging is therefore incorporated as an indirect realization of previous entities) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering (Barzilay and Lapata, 2008). Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substantially improve bridging recognition without impairing performance on other IS classes. 1 Introduction In bridging or associative anaphora (Clar"
D13-1077,W12-1632,0,0.374131,"ve anaphora where anaphor and antecedent are in a similarity/exclusion relation, indicated by anaphor modifiers such as other or similar (Modjeska et al., 2003). 2 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are set in boldface; antecedents in italics. Full bridging resolution needs (i) recognition that a bridging anaphor is present and (ii) identification of the antecedent and contiguity relation. In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012). Each mention in a text gets assigned one IS class that describes its accessibility to the reader at a given point in a text, bridging being one possible class. We stay within this framework. Bridging recognition is a difficult task, so that we had to report very low results on this IS class in previous work (Markert et al., 2012). This is due to the phenomenon’s variety, leading to a lack of clear surface features for recognition. Instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from"
D13-1077,caselli-prodanof-2006-annotating,0,0.843433,"Missing"
D13-1077,J95-2003,0,0.178395,"phors are often licensed because of discourse structure Markert et al. (2012) local feature set f 1 FullPrevMention (b) f 2 FullPreMentionTime (n) f 3 PartialPreMention (b) f 4 ContentWordPreMention (b) f 5 Determiner (n) f 6 NPtype (n) f 7 NPlength (int) f 8 GrammaticalRole (n) f 9 NPNumber (n) f 10 PreModByCompMarker (b) f 11 SemanticClass (n) Markert et al. (2012) relational feature set f 12 HasChild (r) f 13 Precedes (r) Table 1: Markert et al.’s (2012) feature set, b indicates binary, n nominal, r relational features. and/or lexical or world knowledge. With regard to discourse structure, Grosz et al. (1995) observe that bridging is often needed to establish entity coherence between two adjacent sentences (Examples 1, 2, 4, 5, 6, 7 and 9). With regard to lexical and world knowledge, relational noun phrases (Examples 3, 4, 8 and 9), building parts (Example 1), set membership elements (Example 7), or, more rarely, temporal/spatial modification (Example 6) may favor a bridging reading. Motivated by these observations, we develop discourse structure and lexico-semantic features indicating bridging anaphora as well as features designed to separate genericity from bridging. 3.2 Features In Markert et a"
D13-1077,C96-1084,1,0.857455,"Missing"
D13-1077,N13-1111,1,0.837932,"Missing"
D13-1077,W03-2606,1,0.866159,"Missing"
D13-1077,P12-1084,1,0.81519,"concentrates on antecedent selection only (Poesio and Vieira, 1998; Poesio et al., 2004a; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013), assuming that bridging recognition has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, syntactically linked part-of and set-member relationships (such as the building’s lobby), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc. Both types are much easier to identif"
D13-1077,W03-1023,1,0.897984,"idging anaphors with the antecedent One building.2 (1) One building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby. Bridging is an important problem as it affects linguistic theory and applications alike. For example, without bridging resolution, entity coherence between the first and second coordinated clause in 1 We exclude comparative anaphora where anaphor and antecedent are in a similarity/exclusion relation, indicated by anaphor modifiers such as other or similar (Modjeska et al., 2003). 2 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are set in boldface; antecedents in italics. Full bridging resolution needs (i) recognition that a bridging anaphor is present and (ii) identification of the antecedent and contiguity relation. In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012). Each mention in a text gets assigned one IS class that describes its accessibility to the reader at a given poin"
D13-1077,nissim-etal-2004-annotation,0,0.297955,"ion has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, syntactically linked part-of and set-member relationships (such as the building’s lobby), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc. Both types are much easier to identify than anaphoric bridging cases.3 In addition, many non-anaphoric lexical cohesion cases have been annotated as bridging in Switchbard as well. We also separate bridging recognition and antecedent select"
D13-1077,W06-1612,0,0.474791,"Missing"
D13-1077,N13-1099,0,0.0899123,"Missing"
D13-1077,J98-2001,0,0.913747,"Missing"
D13-1077,P04-1019,0,0.875524,"lassify eight finegrained IS categories for NPs in written text: old, new and 6 mediated categories (syntactic, worldKnowledge, bridging, comparative, aggregate and function). This feature set (Table 1, f 1-f 13) works well to identify old, new and several mediated categories. However, it fails to recognize most bridging anaphora which we try to remedy in this work by including more diverse features. Discourse structure features (Table 2, f 1-f 3). Bridging occurs frequently in sentences where otherwise there would no entity coherence to previous sentences/clauses (see Grosz et al. (1995) and Poesio et al. (2004b) for discussions about bridging, entity coherence and centering transitions in the Centering framework). This is especially true for topic NPs (Halliday and Hasan, 1976) in such sentences. We follow these insights by identifying coherence gap sentences (see Examples 1, 4, 5, 6, 7, 9 and also 2): a sentence has a coherence gap (f 1) if it has none 816 new local features for bridging discourse f 1 IsCoherenceGap (b) structure f 2 IsSentFirstMention (b) f 3 IsDocFirstMention (b) semantics f 4 IsWordNetRelationalNoun (b) f 5 IsInquirerRoleNoun (b) f 6 IsBuildingPart (b) f 7 IsSetElement (b) f 8"
D13-1077,J04-3003,0,0.241221,"Missing"
D13-1077,P10-1005,0,0.258526,"fy set membership bridging cases (see Example 7), by checking whether the NP head is a number or indefinite pronoun (such as none, one, some) or modified by each, one. However, not all numbers are bridging cases (such as 1976) and we use f 9 to exclude such cases. Lassalle and Denis (2011) note that some bridging anaphors are indicated by spatial or temporal modifications (see Example 6). We use f 8 to detect this by compiling 20 such adjectives from Inquirer. Features to detect generic nouns (Table 2, f 11f 15). Generic NPs (Example 10) are easily confused with bridging anaphora. Inspired by Reiter and Frank (2010) who build on linguistic research, we develop features (f 11-f 15) to exclude generics. First, hypothetical entities are likely to refer to generic entities (Mitchell et al., 2002), We approximate this by determining whether the NP appears in an if-clause (f 11). Also the clause tense and mood may play a role to decide genericity (Reiter and Frank, 2010). This is often reflected by the main verb of a clause, so we extract its POS tag (f 12). Some NPs are commonly used generically, such as children, men, or the dollar. The ACE-2 corpus (distinct from our corpus) contains generic annotation . We"
D13-1077,J00-4003,0,0.403319,", 2013). 814 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 814–820, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Most bridging research concentrates on antecedent selection only (Poesio and Vieira, 1998; Poesio et al., 2004a; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013), assuming that bridging recognition has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, synt"
D14-1222,W03-2607,0,0.115884,"five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging r"
D14-1222,W12-1632,0,0.25603,"1; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridg"
D14-1222,W06-3915,0,0.334756,"re bridging anaphors are limited to definite NPs. They report preliminary results using the CoNLL scorer. However, we think the coreference resolution system and the evaluation metric for coreference resolution are 2082 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2082–2093, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics not suitable for bridging resolution since bridging is not a set problem. Another vein of research for bridging resolution focuses on formal semantics. Asher and Lascarides (1998) and Cimiano (2006) model bridging by integrating discourse structure and semantics from a formal semantics viewpoint. However, the implementation of such a theoretical framework is beyond the current capabilities of NLP since it depends heavily on commonsense entailment. In this paper, we propose a rule-based system for unrestricted bridging resolution. The system consists of eight rules which we carefully design based on linguistic intuitions, i.e., how the nature of bridging is reflected by various lexical, syntactic and semantic features. We evaluate our rulebased system on a corpus where bridging is reliabl"
D14-1222,T75-2034,0,0.828489,"shing entity coherence in a text. In Example 1, the links between the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert e"
D14-1222,N13-1111,1,0.526785,"r strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al.,"
D14-1222,P13-1116,0,0.0350868,"5; 1998) interprets bridging anaphora as a particular kind of functional concept, which in a given situation assign a necessarily unique correlate to a (implicit) possessor argument. He distinguishes between relational nouns (e.g. parts terms, kinship terms, role terms) and sortal nouns and points out that relational nouns are more frequently used as bridging anaphora than sortal nouns. Rule1 to Rule4 in our system aim to resolve such relational nouns. We design Rule5 and Rule6 to capture set bridging. Finally, Rule7 and Rule8 are motivated by previous work on implicit semantic role labeling (Laparra and Rigau, 2013) which focuses on few predicates. For all NPs in a document, each rule r is applied separately to predict a set of potential bridging links. Every rule has its own constraints on bridging anaphora and antecedents respectively. Bridging anaphors are diverse with regard to syntactic form and function: they can be modified by definite or indefinite determiners (Table 1), furthermore they can take the subject (e.g. Example 3 and Example 6) or other positions (e.g. Example 2 and Example 4) in sentences. The only frequent syntactic property shared is that bridging anaphors most often have a simple i"
D14-1222,W03-2606,1,0.92684,"coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic features to find bridging links in scientifi"
D14-1222,P12-1084,1,0.522065,"based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work (Markert et al., 2012; Hou et al., 2013a; Hou et al., 2013b) on bridging anaphora recognition and antecedent selection. Some of these features overlap with the atomic features used in the rule-based system. Table 4 shows all the features we use for recognizing bridging anaphora. “∗” indicates the resources are used in the rule-based system. We apply them to the first element a of a pairwise instance (a, c). Markert et al. (2012) and Hou et results of learning-based approaches on the same test set as the rule-based system. 2088 Markert et al. local feature set f 1 FullPrevMention (b) ∗ f 2 FullPreMentionTime (n) f 4 ContentWordPreMention (b) f 5 Determiner (n) ∗ f 7 NPlength (int) f 8 GrammaticalRole (n) ∗ f 10 PreModByCompMarker (b) ∗ Hou et al. local feature set features to identify bridging anaphora f 1 IsCoherenceGap (b) f 2 IsSentFirstMention (b) f 4 IsWordNetRelationalNoun (b) ∗ f 5 IsInquirerRoleNoun (b) f 7 IsSetElement (b) ∗ f 8 PreModSpatialTemporal (b) f 10 PreModifiedByCountry (b) ∗ f 11 AppearInIfClause ("
D14-1222,meyers-etal-2004-annotating,0,0.0537321,"c relations. Such syntactic patterns are also explored in Poesio et al. (2004) to resolve meronymy bridging. 4 We use Gigaword (Parker et al., 2011) with automatic POS tag and NP chunk information. Rule2: relative person NPs. This rule is used to capture the bridging relation between a relative (e.g. The husband) and its antecedent (e.g. She). A list of 110 such relative nouns is extracted from WordNet. However, some relative nouns are frequently used generically instead of being bridging, such as children. To exclude such cases, we compute the argument taking ratio α for an NP using NomBank (Meyers et al., 2004). For each NP, α is calculated via its head frequency in the NomBank annotation divided by the head’s total frequency in the WSJ corpus in which the NomBank annotation is conducted. The value of α reflects how likely an NP is to take arguments. For instance, the value of α is 0.90 for husband but 0.31 for children. To predict bridging anaphora more accurately, a conservative constraint is used. An NP from A is added to Ar2 if: (1) its head appears in the relative person list; (2) its argument taking ratio α is bigger than 0.5; and (3) it does not contain any nominal or adjective pre-modificati"
D14-1222,W03-1023,1,0.817243,"leo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed."
D14-1222,P02-1014,0,0.463912,"Missing"
D14-1222,J98-2001,0,0.691542,"n the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a"
D14-1222,W97-1301,0,0.895913,"share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic f"
D14-1222,P04-1019,0,0.921756,"noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic features to find bridg"
D14-1222,E12-1081,0,0.0897781,"obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only."
D14-1222,E14-3006,0,0.091871,"Missing"
D14-1222,J01-4004,0,0.578686,"6) or other positions (e.g. Example 2 and Example 4) in sentences. The only frequent syntactic property shared is that bridging anaphors most often have a simple internal structure concerning modification. Therefore we first create an initial list of potential bridging anaphora A which excludes NPs which have a complex syntactic structure. An NP is added to A if it does not contain any other NPs and do not have modifications strongly indicating comparative NPs (such as other symptoms)3 . Since head match is a strong indicator of coreference anaphora for definite NPs (Vieira and Poesio, 2000; Soon et al., 2001), we further exclude definite NPs from A if they have the same head as a previous NP. Then a set of potential bridging anaphors Ar is chosen from A based on r’s constraints on bridging anaphora. Finally, for each potential bridging anaphor ana ∈ 3 A small list of 10 markers such as such, another . . . and the presence of adjectives or adverbs in the comparative form are used to predict comparative NPs. 2084 Ar , a single best antecedent ante from a list of candidate NPs (Cana ) is chosen if the rule’s constraint on antecedents is applied successfully. Every rule has its own scope to form the a"
D14-1222,J00-4003,0,0.446504,"ding site. Fog shrouded the base before touchdown. Bridging resolution plays an important role in establishing (local) entity coherence. This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution, where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations. The system consists of eight rules which target different relations based on linguistic insights. Our rule-based system significantly outperforms a reimplementation of a previous rule-based system (Vieira and Poesio, 2000). Furthermore, it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system. Additionally, incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system. 1 Introduction Bridging resolution recovers the various nonidentity relations between anaphora and antecedents. It plays an important role in establishing entity coherence in a text. In Example 1, the links between the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlanti"
D14-1222,D13-1077,1,\N,Missing
D17-1212,N12-1009,0,0.0183059,"ments are extracted based on heuristics starting from the citing sentence and are expanded with sentences in a window of +/-2 sentences, depending on them containing cue words like ‘this’, ‘these’,. . . ‘above-mentioned’. We consider the approach in (O’Connor, 1982) as a baseline. Kaplan et al. (2016) proposed the task of determining the citation block based on a set of textual coherence features (e.g. grammatical or lexical coherence). The citation block starts from the citing sentence, with succeeding sentences classified (through SVMs or CRFs) according to whether they belong to the block. Abu-Jbara and Radev (2012) determine the citation block by first segmenting the sentences and then classifying individual words as being inside/outside the citation. Finally, the segment is classified depending on the word labels (majority of words being inside, at least one, or all of them). This approach is not applicable in our case due to the fact that words in Wikipedia text are not domain or genre-specific as one expects in scientific text, and as such their classification does not work. Citations in IR. The importance of determining the citation span has been acknowledged in the field of Information Retrieval (I"
D17-1212,D13-1101,0,0.0513356,"Missing"
D17-1212,P09-2004,0,0.0136004,"dence for δ. As an additional feature we compute fiJ which corresponds to the maximal jaccard similarity between δi and paragraphs p ∈ c. Finally, as we will show in our experimental evaluation in Section 5, there is a high correlation between the citation span length and the length of citation content in terms of sentences. Hence, we add as an additional feature f c the number of sentences in c. 4.3 Discourse Features Sentences and fragments within a sentence can be tied together by discourse relations. We annotate sentences with explicit discourse relations based on an approach proposed in (Pitler and Nenkova, 2009), using discourse connectives as cues. The explicit discourse relations belong to one of the following: temporal, contingency, expansion, comparison. After extracting a discourse connective in a sentence, we determine by its position to which fragment it belongs and mark the fragment accordingly. We denote with fidisc the discourse feature for the fragment δi .2 4.4 Temporal Features An important aspect that we consider here is the temporal difference between two consecutive fragments δi and δi−1 . If there exists a temporal date expression in δi and δi−1 and they point to different time-point"
D17-1212,C08-1087,0,0.0417767,"pedia text are not domain or genre-specific as one expects in scientific text, and as such their classification does not work. Citations in IR. The importance of determining the citation span has been acknowledged in the field of Information Retrieval (IR). The focus is on building citation indexes (Garfield, 1955) and improving the retrieval of scientific articles (Ritchie et al., 2008, 2006). Citing sentences on a fixed window size are used to index documents and aid the retrieval process. Summarization. Citations have been successfully employed to generate summaries of scientific articles (Qazvinian and Radev, 2008; Elkiss et al., 1991 2008). In all cases, citing statements are either extracted manually or via heuristics such as extracting only citing sentences. Similarly (Nanba and Okumura, 1999) expand the summaries in addition to the citing sentence based on cue words (e.g. ‘In this’, ‘However’ etc.). The work in (Qazvinian and Radev, 2010) goes one step beyond and considers sentences which do not explicitly cite another article. The task is to assign a binary label to a sentence, indicating whether it contains context for a cited paper. We use this approach as one of our competitors. Again, the prem"
D17-1212,P10-1057,0,0.0782109,"retrieval of scientific articles (Ritchie et al., 2008, 2006). Citing sentences on a fixed window size are used to index documents and aid the retrieval process. Summarization. Citations have been successfully employed to generate summaries of scientific articles (Qazvinian and Radev, 2008; Elkiss et al., 1991 2008). In all cases, citing statements are either extracted manually or via heuristics such as extracting only citing sentences. Similarly (Nanba and Okumura, 1999) expand the summaries in addition to the citing sentence based on cue words (e.g. ‘In this’, ‘However’ etc.). The work in (Qazvinian and Radev, 2010) goes one step beyond and considers sentences which do not explicitly cite another article. The task is to assign a binary label to a sentence, indicating whether it contains context for a cited paper. We use this approach as one of our competitors. Again, the premise is that citations are marked explicitly and additional citing sentences are found dependent on them. Comparison to our work. The language style and the composition of citations in Wikipedia and in scientific text differ significantly. Citations are explicit in scientific text (e.g. author names) and are usually the first word in"
D17-1212,W06-0804,0,0.110082,"Missing"
D19-5403,D18-1443,0,0.0222614,"2 103 117 89 #3 41 34 21 90 17 #2 23 10 9 46 6 #1 3 2 2 9 1 7.3 We have already described the differences between TLS and MDS and the limited direct applicability of MDS systems to TLS in Section 2.2. However, our methodology is inspired by the MDS system of Banerjee et al. (2015). We made major adaptations to this system for TLS by (i) using AP clustering to cluster sentences in a datesensitive way that dynamically adapts to the corpus size and (ii) augmenting sentence scoring and selection to the needs of TLS. Our system is also related to neural abstractive summarization (See et al., 2017; Gehrmann et al., 2018; Cohan et al., 2018; Paulus et al., 2018). However, these methods require large training corpora unavailable for TLS. Table 8: Results of the readability evaluation. We also report the number of times each category was chosen. (Nguyen et al., 2014; Chieu and Lee, 2004; Yan et al., 2011b,a; Wang et al., 2016; Tran et al., 2015, 2013b,a; Martschat and Markert, 2018). Several of these evaluate on corpora that are not publicly available (Chieu and Lee, 2004; Yan et al., 2011a,b) so that we cannot compare to their results. Since the advent of TL17 and Crisis, several evaluations have been performe"
D19-5403,W11-2123,0,0.0131928,"shock . following iterative formula: The people responsible for the attack have yet to be determined. wj ∈adj (wi ) Sentence Scoring and Selection Given the set of generated sentences, we wish to find sentences that are well-formed and informative about important dates and events. We encode these aspects into multiple scoring functions. 3.3.1 Linguistic Quality To encourage a readable output, we compute a linguistic quality score for each candidate sentence g by using the average probability of the tokens according to a 3-gram language model (Banerjee et al., 2015). We use the KenLM library (Heafield, 2011) with a pretrained model3 . We compute the LM-score fLM as follows: C∈C 3.3.4 3.3.2 1− f ∈F P where F is the set of scoring functions, i.e. F = {fpath , fLM , fT R , fdate , fcluster }. We select sentences greedily starting with the highest scoring ones as long as selecting them does not break any constraints. To reduce redundancy, we select at most one candidate from each cluster (Banerjee et al., 2015) and skip sentences with a cosine similarity of more than 0.5 to a previously selected sentence. Date Importance We determine the importance dimp(d) of a date d by counting how often it is ment"
D19-5403,N18-1150,0,0.0237084,"an example timeline about the Syrian civil war. (Source: Crisis dataset (Tran et al., 2015)) et al., 2016; Tran et al., 2015, 2013b,a; Martschat and Markert, 2018). However, TLS aggregates information from input corpora that are orders of magnitude larger than for traditional multidocument summarization (MDS) tasks. In addition, documents typically come from many different sources. In this setting, it might be advantageous to generate abstractive summaries that combine information from different sentences. While the state of the art in abstractive summarization is achieved by neural networks (Celikyilmaz et al., 2018), these systems require many document/gold summary pairs for training. TLS datasets, on the other hand, have many input documents, but only contain very few gold-standard timelines (between 19 and 22) (Tran et al., 2015, 2013b). Thus, very few input/gold timeline pairs are available for training. We therefore introduce an unsupervised abstractive TLS system that is inspired by the abstractive MDS system in Banerjee et al. (2015). We make the following contributions: Introduction Many newsworthy events are not isolated incidents but part of long-lasting developments. For example, the events of"
D19-5403,P17-2043,0,0.0205057,"(Manning et al., 2014) and use Heideltime (Str¨otgen and Gertz, 2013) for resolving time expressions. Unlike several other TLS systems (Martschat and Markert, 2018; Chieu and Lee, 2004), we do not filter sentences with topicspecific keywords (e.g. war or Syria) to be less dependent on additional human input.5 4.2 4.4 One advantage of abstractive summarization is its potential to increase the maximum attainable scores by forming more succinct sentences. We investigate this potential with an oracle to establish an upper bound on summary scores, following similar work for generic summarization (Hirao et al., 2017). As an oracle over all summaries is intractable, we approximate it by replacing the scoring function (Equation 6) with an oracle that predicts the ROUGE-1-agree F1-score of sentences. The rest of our pipeline remains unchanged. For the extractive oracle, we greedily select from all sentences in the input documents instead. The date of a sentence is the first exact time expression that appears in the sentence, or its DCT if there is none (Chieu and Lee, 2004). Experimental Setup and Constraints Like Martschat and Markert (2018), we generate one timeline per reference. We limit the number of da"
D19-5403,N18-2097,0,0.0413489,"Missing"
D19-5403,W04-1013,0,0.0759594,"rkert (2018), we generate one timeline per reference. We limit the number of dates to that in the reference, while the number of sentences per summary is set to the average number of sentences per summary in the reference. As abstractive systems generate new text, they could exploit sentence limits by generating very long sentences. We control for this by limiting the number of tokens instead in one algorithm variation. We estimate the maximum number of tokens in the same way as for the sentence constraint. 4.3 Oracle Summaries Evaluation Metrics Summarization is usually evaluated with ROUGE (Lin, 2004). This, however, ignores the temporal 4.5 4 The corresponding document ids can be found at www.cl.uni-heidelberg.de/˜steen/tls/ docids.txt. 5 However, we do let the competitor systems use filtering. Comparison Systems 4.5.1 Extractive Systems We compare our full system with three extractive comparison systems. The first two are from a col25 5 lection of TLS systems created by Martschat and Markert (2018).6 Chieu is a reimplementation of Chieu and Lee (2004), which uses the average cosine similarity of a sentence in a time-window around its date to determine importance and greedy selection. Thi"
D19-5403,P18-1128,0,0.013763,"d Markert, 2017), it has been previously used as an evaluation measure (Yan et al., 2011b,a; Wang et al., 2016). We report the F1 score for all ROUGE metrics. To assess how well the systems are at date selection, we compute the F1 score between the dates that have a summary in the gold timeline and in the system timeline. Finally, we report the copy rate as the proportion of sentences copied directly from the corpus into the summary. We use an approximate randomization test (Noreen, 1989) to check statistical significance and the Bonferroni correction to correct for comparing on two datasets (Dror et al., 2018). Corpus Cleaning and Preporcessing We found that some of the news articles to be summarised in both datasets contained full or partial gold timelines. This might cause TLS systems to inadvertently ”cheat” by using the leaked gold timelines. We have manually removed 19 such documents in TL17 and 28 in Crisis.4 We preprocess all corpora with Stanford CoreNLP (Manning et al., 2014) and use Heideltime (Str¨otgen and Gertz, 2013) for resolving time expressions. Unlike several other TLS systems (Martschat and Markert, 2018; Chieu and Lee, 2004), we do not filter sentences with topicspecific keyword"
D19-5403,P14-5010,0,0.00311426,"entences copied directly from the corpus into the summary. We use an approximate randomization test (Noreen, 1989) to check statistical significance and the Bonferroni correction to correct for comparing on two datasets (Dror et al., 2018). Corpus Cleaning and Preporcessing We found that some of the news articles to be summarised in both datasets contained full or partial gold timelines. This might cause TLS systems to inadvertently ”cheat” by using the leaked gold timelines. We have manually removed 19 such documents in TL17 and 28 in Crisis.4 We preprocess all corpora with Stanford CoreNLP (Manning et al., 2014) and use Heideltime (Str¨otgen and Gertz, 2013) for resolving time expressions. Unlike several other TLS systems (Martschat and Markert, 2018; Chieu and Lee, 2004), we do not filter sentences with topicspecific keywords (e.g. war or Syria) to be less dependent on additional human input.5 4.2 4.4 One advantage of abstractive summarization is its potential to increase the maximum attainable scores by forming more succinct sentences. We investigate this potential with an oracle to establish an upper bound on summary scores, following similar work for generic summarization (Hirao et al., 2017). As"
D19-5403,C10-1037,0,0.217822,"A, the so-called preference values, determines how suitable an item is to become an exemplar and thus regulates the number of exemplars. We construct A using TF-IDF vector cosine similarity between the input sentences, which has been shown to be a useful similarity metric for TLS (Martschat and Markert, 2018; Chieu and Lee, 2004). However, sentences in the same cluster should not only be similar but also describe the same dates. To determine which date a sentence refers to, we make the following assumptions: 3.2 Following Banerjee et al. (2015), we use the unsupervised, low-cost MSC-system by Filippova (2010) to generate summary candidates for each cluster. Given the sentence cluster C, the algorithm constructs a word-adjacency graph. The nodes are POS-tagged tokens and directed edges indicate adjacency of these tokens in one of the sentences. Occurrences of the same content word in different sentences are mapped to the same node. Given an edge eij , its weight w(eij ) is: • Every sentence can refer to the document creation time (DCT). freq(i) + freq(j) P freq(i) ∗ freq(j) ∗ s∈C diff (s, i, j) (2) where freq(i) is the number of tokens that have been mapped to node i and diff (s, i, j) indicates wh"
D19-5403,E17-2046,1,0.726382,"nstruct a keyword-based scoring function using TextRank (Mihalcea and Tarau, 2004) to efficiently score the importance of our candidates. TextRank scores keywords by constructing an undirected graph of content words where words are connected if they appear near each other. A score is computed for each node similarly to the PageRank algorithm (Page et al., 1999) using the 3 www.keithv.com/software/giga/lm_giga_ 20k_nvp_3gram.zip 24 Corp. TL17 Crisis #Tops 13 4 #TLs 19 22 #Sen/Top. 27,222 150,429 Comp. 0.0019 0.0002 Spr. 0.279 0.081 dimension of TLS. We thus use the two TLS measures proposed by Martschat and Markert (2017): agree Compute ROUGE only between daily summaries which have the same dates. Table 2: Dataset statistics, including number of topics, timelines and the average number of sentences to be summarized for each topic. We also report the median compression and spread of the timelines. align Align summaries in the output with those in the reference based on similarity and the distance between their dates, then compute the ROUGE score between aligned summaries. Distant alignments are punished. ratio of dates with summaries in the timeline to the number of dates in the timeline span. Low compression r"
D19-5403,K18-1023,1,0.182096,"active oracle experiments for TLS. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong compression. In these cases, our oracle experiments confirm that our approach also has a higher upper bound for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand. 1 Table 1: Beginning of an example timeline about the Syrian civil war. (Source: Crisis dataset (Tran et al., 2015)) et al., 2016; Tran et al., 2015, 2013b,a; Martschat and Markert, 2018). However, TLS aggregates information from input corpora that are orders of magnitude larger than for traditional multidocument summarization (MDS) tasks. In addition, documents typically come from many different sources. In this setting, it might be advantageous to generate abstractive summaries that combine information from different sentences. While the state of the art in abstractive summarization is achieved by neural networks (Celikyilmaz et al., 2018), these systems require many document/gold summary pairs for training. TLS datasets, on the other hand, have many input documents, but onl"
D19-5403,W04-3252,0,0.0302657,"aluate on the only two publicly available TLS datasets: Crisis (Tran et al., 2015) and Timeline 17 (TL17) (Tran et al., 2013b). Both contain human written timelines about topics such as civil wars or the BP oil disaster, collected from major news outlets. Each topic also has a set of related news articles scraped from the web (see Table 2). We also report the median compression rate and the median spread of the datasets. The compression rate is the ratio of sentences in a timeline to the number of input sentences. The spread is the We construct a keyword-based scoring function using TextRank (Mihalcea and Tarau, 2004) to efficiently score the importance of our candidates. TextRank scores keywords by constructing an undirected graph of content words where words are connected if they appear near each other. A score is computed for each node similarly to the PageRank algorithm (Page et al., 1999) using the 3 www.keithv.com/software/giga/lm_giga_ 20k_nvp_3gram.zip 24 Corp. TL17 Crisis #Tops 13 4 #TLs 19 22 #Sen/Top. 27,222 150,429 Comp. 0.0019 0.0002 Spr. 0.279 0.081 dimension of TLS. We thus use the two TLS measures proposed by Martschat and Markert (2017): agree Compute ROUGE only between daily summaries whi"
D19-5403,N15-1112,0,0.295057,"Missing"
D19-5403,N16-1008,0,0.594685,"Missing"
D19-5403,C14-1114,0,0.453761,"are intrinsically linked to events that happened during the beginning of that war in 2011. As the amount of reporting grows, it can be difficult to keep track of important events that may have happened a long time ago. Timeline summarization (TLS) alleviates this problem by providing users with automatically generated timelines that identify key dates in a larger development along with short summaries of the events on these dates. Table 1 shows an example of a timeline. Prior TLS systems are extractive, i.e. they identify important sentences in a corpus and copy them directly to the timeline (Nguyen et al., 2014; Chieu and Lee, 2004; Yan et al., 2011b,a; Wang 1. We introduce the first abstractive system for TLS and show that it outperforms current ex21 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 21–31 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Date: 2017-01-08 tractive TLS systems such as Martschat and Markert (2018) when the input corpora are large with low compression rate.1 Date: 2017-01-08 Date: 2017-01-08 0.2 0.6 0.3 ... Date: 2017-09-04 ... Date: 2017-09-04 Date: 2017-09-04 ... 0.1 2. We show that our system delivers signifi"
D19-5403,D11-1040,0,0.689205,"ppened during the beginning of that war in 2011. As the amount of reporting grows, it can be difficult to keep track of important events that may have happened a long time ago. Timeline summarization (TLS) alleviates this problem by providing users with automatically generated timelines that identify key dates in a larger development along with short summaries of the events on these dates. Table 1 shows an example of a timeline. Prior TLS systems are extractive, i.e. they identify important sentences in a corpus and copy them directly to the timeline (Nguyen et al., 2014; Chieu and Lee, 2004; Yan et al., 2011b,a; Wang 1. We introduce the first abstractive system for TLS and show that it outperforms current ex21 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 21–31 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Date: 2017-01-08 tractive TLS systems such as Martschat and Markert (2018) when the input corpora are large with low compression rate.1 Date: 2017-01-08 Date: 2017-01-08 0.2 0.6 0.3 ... Date: 2017-09-04 ... Date: 2017-09-04 Date: 2017-09-04 ... 0.1 2. We show that our system delivers significantly better performance than an abstr"
D19-5403,P17-1099,0,0.2169,"stractive components. While Chieu performs better than our system in ROUGE-1 concat on Crisis, it is much worse in all date-sensitive measures and on TL17. When comparing Submod and our abstractive system, we see behaviour similar to the oracles. On TL17, Submod achieves higher scores, though the differences are mostly not significant. On Crisis, however, we outperform Submod across all date-sensitive metrics and almost double the score in ROUGE-2 for agree and align. All improvements are significant except for ROUGE-1 align. As an abstractive comparison, we use the popular Pointer Generator (See et al., 2017) (Neural). It was trained on the CCN/Daily Mail single document summarization corpus (Hermann et al., 2015). We adapt it to TLS as follows: 1. We select the dates for the timeline by ranking them by their frequency dimp(d). 2. For each selected date d, we collect all sentences Sd from the corpus that refer to d. 3. For each collection Sd , we construct a pseudo document for the summarizer. Following Zhang et al. (2018) we use the LexRank score (Erkan and Radev, 2004) to rank the sentences in Sd . We add the top sentences to the document until we reach the maximum input size for the pointer gen"
E17-2046,D15-1013,0,0.0301244,"empty if the date is not included in any timeline. s(d) is the (possibly empty) summary of d in s. We define recall for a date d as P P cntr,s(d) (g) Most work on TLS adopts the ROUGE toolkit that is used for for standard summarization evaluation (Lin, 2004). ROUGE metrics evaluate a system summary s of one or more texts against a set R of reference summaries (without accounting for dating summaries). The most popular variants of ROUGE are the ROUGE-N metrics which measure the overlap of N-grams in system and reference summaries. Several ROUGE metrics are well correlated with human judgment (Graham, 2015). For a summary c, let us define the set of c’s Ngrams as ng(c). cntc (g) is the number of occurrences of an N-gram g in c. For two summaries c1 and c2 , cntc1 ,c2 (g) = min{cntc1 (g), cntc2 (g)} is the minimum number of occurrences of g in both c1 and c2 . ROUGE-N recall is then defined as2 P P r∈R g∈ng(r) cntr,s (g) P rec(R, s) = P , (1) r∈R g∈ng(r) cntr (g) rec(d, R, s) = r∈R(d) g∈ng(r) P P r∈R(d) g∈ng(r) cntr (g) . (3) rec(d, R, s) can be extended to the set of dates DR , typically by micro-averaging, that is P P P cntr,s(d) (g) rec(R, s) = d∈DR r∈R(d) g∈ng(r) P P P d∈DR r∈R(d) g∈ng(r) cnt"
E17-2046,P15-1155,0,0.0696969,"s a pre-specified constant. However, it is left open how to set this constant and different datings of the same event below the threshold difference would again not receive any penalty. 3.2 Other Metrics Some work evaluates TLS manually (Chieu and Lee, 2004; Tran et al., 2015). However, such evaluation is costly. A related task to TLS is the TREC update summarization task (Aslam et al., 2015). In contrast to TLS, this task requires online summarization by presenting the input as a stream of documents. The metric employed relies on manually matching sentences of reference and system timelines. Kedzie et al. (2015) modify TREC metrics for a fully 3 For convenience, we slightly overload notation. In the definition of standard ROUGE R and s were summaries, now they are timelines which contain summaries. 2 We rely on the representation of ROUGE-N presented in Lin and Bilmes (2011). 286 automatic setting, but still need a manually optimized threshold for establishing semantic matching. Moreover, the matching is binary: two summaries either match or do not match. The metric does not incorporate information about the degree of similarity between two summaries. Lastly, in the DUC 2007 and TAC 2008–2011 evaluat"
E17-2046,H05-1004,0,0.082526,"t-based ROUGE 4.1 1 |dr − ds |+ 1 cdr ,ds = 1 − (5) 1 . |dr − ds |+ 1 (9) We require that the alignment is injective.6 In Table 1, for example, the daily summaries for 2010-05-27 and 2010-05-28 would be aligned. that assigns each date dr ∈ DR in some reference timeline a date ds ∈ Ds in the system timeline. For evaluation, the summaries for the aligned dates are compared.5 apply the corresponding formulas for precision as discussed in Section 3. 6 If |DR |&gt; |Ds |, some dr ∈ DR will be unaligned. For these dates we set the n-gram counts to 0 in the numerator of Equation 7. 4 We are inspired by Luo (2005) who devises an alignment-based metric for coreference resolution. 5 We only discuss how recall is computed. For computing precision we instead consider alignments f : Ds → DR and 287 5.1 Date-content Alignment. The second instantiation, date-content alignment or align+, also includes semantic similarity in the costs. An approximation of semantic similarity is represented by the ROUGE-1 F1 score between two daily summaries. We set cdr ,ds  = 1− We derive tests that examine whether well-defined basic operations on reference timelines affect the metrics as expected. An example is the date remov"
E17-2046,D11-1040,0,0.413477,"nore the temporal aspect of the task or require strict date matching. We introduce variants of ROUGE that allow alignment of daily summaries via temporal distance or semantic similarity. We argue for the suitability of these variants in a theoretical analysis and demonstrate it in a battery of task-specific tests. 1 2 Introduction Task Description and Notation Given a query (such as BP oil spill) TLS needs to (i) extract the most important events for the query and their corresponding dates and (ii) obtain concise daily summaries for each selected date (Allan et al., 2001; Chieu and Lee, 2004; Yan et al., 2011b; Tran et al., 2015; Wang et al., 2016). Formally, a timeline is a sequence (d1 , s1 ), . . . , (dk , sk ) where the di are dates and the si are summaries for the dates di . Given are a query q and an associated corpus Cq that contains documents relevant to the query. The task of timeline summarization is to generate a timeline sq based on the documents in Cq . The number of dates in the generated timeline as well as the length of the daily summaries are typically controlled by the user. For evaluation we assume access to one or more reference timelines Rq = {r1q , . . . , rnq q }. In our not"
E17-2046,P16-1060,0,0.0427386,"Missing"
E17-2046,C14-1114,0,0.262543,"e, even though both timelines report on the existence and later failure of the “top kill” effort, although on different dates. This effect can be particularly problematic for longer-lasting events. while ROUGE-N precision is defined as P P r∈R g∈ng(s) cntr,s (g) P prec(R, s) = . (2) |R |g∈ng(s) cnts (g) ROUGE-N F1 is the harmonic mean of recall and precision. Concatenation-based ROUGE. The simplest and most popular way to apply ROUGE to TLS, which we refer to as concat, is to run ROUGE on documents obtained by concatenating the items of the timelines (Takamura et al., 2011; Yan et al., 2011a; Nguyen et al., 2014; Wang et al., 2016). Given a timeline t = (d1 , s1 ), . . . , (dk , sk ), we concatenate the si , which yields a document s0 . In s0 all date information is lost. We apply this transformation to the reference and the system timelines and use ROUGE on the resulting documents. This method discards any temporal information. As a result, different datings of the same event are not penalized. Most work does not address this issue at all. An exception is Takamura et al. (2011), who ignore word matches when the matched word only appears in a summary where the time difference exceeds a pre-specified"
E17-2046,N15-1112,0,0.10883,"ied constant. However, it is left open how to set this constant and different datings of the same event below the threshold difference would again not receive any penalty. 3.2 Other Metrics Some work evaluates TLS manually (Chieu and Lee, 2004; Tran et al., 2015). However, such evaluation is costly. A related task to TLS is the TREC update summarization task (Aslam et al., 2015). In contrast to TLS, this task requires online summarization by presenting the input as a stream of documents. The metric employed relies on manually matching sentences of reference and system timelines. Kedzie et al. (2015) modify TREC metrics for a fully 3 For convenience, we slightly overload notation. In the definition of standard ROUGE R and s were summaries, now they are timelines which contain summaries. 2 We rely on the representation of ROUGE-N presented in Lin and Bilmes (2011). 286 automatic setting, but still need a manually optimized threshold for establishing semantic matching. Moreover, the matching is binary: two summaries either match or do not match. The metric does not incorporate information about the degree of similarity between two summaries. Lastly, in the DUC 2007 and TAC 2008–2011 evaluat"
E17-2046,N16-1008,0,0.341029,"Missing"
E17-4007,P13-1174,0,0.0124829,"ywords in text to relevant Wikipedia pages; e.g. Mihalcea and Csomai (2007)) is used to identify entities in the text. Headlines are wikified using the TagMe API6 , a tool meant for short texts, making it suitable for headlines. 7 http://dumps.wikimedia.org/other/pagecounts-ez/ We found the previous day’s prominence to be closest to the actual on-the-day prominence. 5 8 http://www.cs.cmu.edu/ ark/TweetNLP/ 6 http://tagme.di.unipi.it/ 66 Wikipedia page views Wikipedia page views 1600 1400 1200 1000 800 600 400 in a headline with a positive or negative connotation (using a connotations lexicon (Feng et al., 2013)). Secondly, we measure the percentage of biased content words (using a bias lexicon (Recasens et al., 2013)). For example, the same political organisation can be described as far-right, nationalist, or fascist, each of these words indicating a bias towards a certain reading. NV3: Superlativeness. The size (JohnsonCartee, 2005, p.128), or magnitude (Harcup and O’Neill, 2001) of an event is considered to influence news selection. We focus on explicit linguistic indicators of event size: comparatives and superlatives (indicated by part-of-speech tags), and amplifiers (indicated with intensifiers"
E17-4007,baccianella-etal-2010-sentiwordnet,0,0.0104233,": Set cutoff = mean(M Ae ) + 2 × SD(M Ae ) 3: Bursts = di |M Ae (i) &gt; cutof f As a headline can have multiple entities, all prominence measures are aggregated via summation over all entities in H (see Table 2). NV2: Sentiment. This refers to sentimentcharged events (Johnson-Cartee, 2005) and using sentiment-charged language (Bednarek and Caple, 2012). Features relating to sentiment and emotionality have been shown to influence a news article’s virality (Berger and Milkman, 2012). However, this effect has not been studied for headlines. As direct measures of sentiment, we combine SentiWordNet (Baccianella et al., 2010) positivity and negativity scores of content words, and calculate sentiment and polarity scores following Kucuktunc et al. (2012). Sentiment can also be indirect. Firstly, a word may be in itself objective, but carry a negative connotation (e.g. scream). We therefore measure the percentage of content words 67 Table 2: Feature implementations and statistics on The Guardian. Notation is in Table 1. Measures: median and maximum values, prevalence (proportion of non-zero scores), and the Kruskall-Wallis test comparing the manual gold standard to automatic extraction (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.0"
E17-4007,P03-1054,0,0.00737085,"e was in a burst over a year (daysburste,d−365,d−1 ). SecExtraction of News Values We present feature engineering methods for six news values. These six were selected, because they occur frequently in news values taxonomies (cf. Section 2). The feature computation methods are summarised in Table 2. Although our goal is a generic framework, we are inspired by research in the news domain. Consequently, the features are informed by news values related to news content. Preprocessing. All headlines are part-ofspeech tagged (Stanford POS Tagger (Toutanova et al., 2003)) and parsed (Stanford Parser (Klein and Manning, 2003)). Wikification (a method of linking keywords in text to relevant Wikipedia pages; e.g. Mihalcea and Csomai (2007)) is used to identify entities in the text. Headlines are wikified using the TagMe API6 , a tool meant for short texts, making it suitable for headlines. 7 http://dumps.wikimedia.org/other/pagecounts-ez/ We found the previous day’s prominence to be closest to the actual on-the-day prominence. 5 8 http://www.cs.cmu.edu/ ark/TweetNLP/ 6 http://tagme.di.unipi.it/ 66 Wikipedia page views Wikipedia page views 1600 1400 1200 1000 800 600 400 in a headline with a positive or negative conn"
E17-4007,N03-1033,0,0.010694,"Firstly, burstiness indicates the number of days that e was in a burst over a year (daysburste,d−365,d−1 ). SecExtraction of News Values We present feature engineering methods for six news values. These six were selected, because they occur frequently in news values taxonomies (cf. Section 2). The feature computation methods are summarised in Table 2. Although our goal is a generic framework, we are inspired by research in the news domain. Consequently, the features are informed by news values related to news content. Preprocessing. All headlines are part-ofspeech tagged (Stanford POS Tagger (Toutanova et al., 2003)) and parsed (Stanford Parser (Klein and Manning, 2003)). Wikification (a method of linking keywords in text to relevant Wikipedia pages; e.g. Mihalcea and Csomai (2007)) is used to identify entities in the text. Headlines are wikified using the TagMe API6 , a tool meant for short texts, making it suitable for headlines. 7 http://dumps.wikimedia.org/other/pagecounts-ez/ We found the previous day’s prominence to be closest to the actual on-the-day prominence. 5 8 http://www.cs.cmu.edu/ ark/TweetNLP/ 6 http://tagme.di.unipi.it/ 66 Wikipedia page views Wikipedia page views 1600 1400 1200 1000 800"
E17-4007,P13-1162,0,0.021047,"s in the text. Headlines are wikified using the TagMe API6 , a tool meant for short texts, making it suitable for headlines. 7 http://dumps.wikimedia.org/other/pagecounts-ez/ We found the previous day’s prominence to be closest to the actual on-the-day prominence. 5 8 http://www.cs.cmu.edu/ ark/TweetNLP/ 6 http://tagme.di.unipi.it/ 66 Wikipedia page views Wikipedia page views 1600 1400 1200 1000 800 600 400 in a headline with a positive or negative connotation (using a connotations lexicon (Feng et al., 2013)). Secondly, we measure the percentage of biased content words (using a bias lexicon (Recasens et al., 2013)). For example, the same political organisation can be described as far-right, nationalist, or fascist, each of these words indicating a bias towards a certain reading. NV3: Superlativeness. The size (JohnsonCartee, 2005, p.128), or magnitude (Harcup and O’Neill, 2001) of an event is considered to influence news selection. We focus on explicit linguistic indicators of event size: comparatives and superlatives (indicated by part-of-speech tags), and amplifiers (indicated with intensifiers and downtoners). For the latter, we combine the lists in Quirk et al. (1985) and Biber (1991), obtaining wo"
E17-4007,D08-1027,0,0.0568534,"Missing"
E17-4007,P14-1017,0,0.0181529,"dlines pose an engineering challenge. This includes linguistic aspects like unusual use of tenses (Chovanec, 2014) and deliberate ambiguity (Brˆone and Coulson, 2010). There are also some domain-specific phenomena like click-baiting (Blom and Hansen, 2015). Headlines are typically short, which limits the amount of context that many NLP tools rely on. While feature engineering from headlines is less studied, there are research efforts that specifically address short texts. Tweets have attracted considerable attention, leading to the development of some Twitter-specific tools (e.g. TweetNLP5 ). Tan et al. (2014) is an example of feature engineering from tweets that looks specifically at wording and its effect on popularity. Another example of a text closely related to headlines are online content titles, e.g. image titles on Reddit (Lakkaraju et al., 2013). Many approaches include features like ratios for various parts of speech, sentiment, and similarity to a language model. However, they need to be adjusted to work with headlines. For example, since headlines offer limited context, sentiment analysis carried out on word-level is more appropriate (cf. Tan et al. (2014), Gatti et al. (2016), Szymansk"
H05-1079,C04-1180,1,0.6132,"lap is always a real number between 0 and 1 and also ensures independence of the length of the hypothesis. Apart from wnoverlap we take into account length (as measured by number of lemmas) of text and hypothesis, because in most of the observed cases for true entailments the hypothesis is shorter than the text as it contains less information. This is covered by three numerical features measuring the length of the text, of the hypothesis and the relative length of hypothesis with regard to the text. 3 Deep Semantic Analysis 3.1 Semantic Interpretation We use a robust wide-coverage CCG-parser (Bos et al., 2004) to generate fine-grained semantic representations for each T/H-pair. The semantic representation language is a first-order fragment of the DRSlanguage used in Discourse Representation Theory (Kamp and Reyle, 1993), conveying argument structure with a neo-Davidsonian analysis and including the recursive DRS structure to cover negation, disjunction, and implication. Consider for example: Example: 78 (FALSE) T: Clinton’s new book is not big seller here. H: Clinton’s book is a big seller. x1 x2 x3 book(x1) book(x2) ¬ x1=x2 clinton(x3) of(x1,x3) e4 x5 drs(T): ¬ big(x5) seller(x5) be(e4) agent(e4,x"
J05-3004,P99-1008,0,0.355508,"pendent knowledge; different ways of encoding information; and sense ambiguity. 2 In this article, we restrict the notion of definite NPs to NPs modified by the article ‘the.’ 3 These systems also use surface-level features (such as string matching), recency, and grammatical constraints. In this article, we concentrate on the lexical and semantic knowledge employed. 368 Markert and Nissim Knowledge Sources for Anaphora Resolution In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach. A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002). Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways. First, we suggest using the Web for anaphora resolution instead of the smallersize, but less noisy and more balanced, corpora used previously, making available a huge additional source of knowledge.4 Second, we do n"
J05-3004,P01-1009,0,0.0263244,"Missing"
J05-3004,T75-2034,0,0.779722,"Missing"
J05-3004,W03-0424,0,0.0238403,"Missing"
J05-3004,W03-2410,0,0.0992571,"Missing"
J05-3004,C96-1084,1,0.837701,"Missing"
J05-3004,N01-1008,0,0.736438,"Missing"
J05-3004,C92-2082,0,0.646129,"nd context-dependent knowledge; different ways of encoding information; and sense ambiguity. 2 In this article, we restrict the notion of definite NPs to NPs modified by the article ‘the.’ 3 These systems also use surface-level features (such as string matching), recency, and grammatical constraints. In this article, we concentrate on the lexical and semantic knowledge employed. 368 Markert and Nissim Knowledge Sources for Anaphora Resolution In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach. A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002). Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways. First, we suggest using the Web for anaphora resolution instead of the smallersize, but less noisy and more balanced, corpora used previously, making available a huge additional source of"
J05-3004,W97-1307,0,0.0561876,"er the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks. 5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical. 369 Computational Linguistics Volume 31, Number 3 2. The Knowledge Gap and Other Problems for Lexico-semantic Resources A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002; Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical and world knowledge for the resolution of full NP anaphora and the lack of such knowledge in existing ontologies (Section 2.1). In addition to this knowledge gap, we summarize other, methodological problems with the use of ontologies in anaphora resolution (Section 2.2). 2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads In the following, we discuss previous studies on the automatic resolution of cor"
J05-3004,J03-3005,0,0.0274785,"xical knowledge sources in isolation. It remains to be seen how the results carry forward when the knowledge sources interact with other features (for example, grammatical preferences). A similar issue concerns the integration of the methods into anaphoricity determination in addition to antecedent selection. Additionally, future work should explore the contribution of different knowledge sources for yet other anaphora types. 4 There is a growing body of research that uses the Web for NLP. As we concentrate on anaphora resolution in this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks. 5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical. 369 Computational Linguistics Volume 31, Number 3 2. The Knowledge Gap and Other Problems for Lexico-semantic Resources A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Ma"
J05-3004,C96-1021,0,0.101073,"Missing"
J05-3004,W03-2606,1,0.863702,"Missing"
J05-3004,W99-0108,0,0.061313,"Missing"
J05-3004,P98-2143,0,0.27725,"Missing"
J05-3004,W03-1023,1,0.797908,"Missing"
J05-3004,P04-1020,0,0.0119498,"hms. Second, the anaphoricity of the definite NPs in Case Study II has de facto been manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent. One of the reasons why pronoun resolution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a twostage process which we also follow in this article. Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors. Future work will explore the extent of such a decrease in performance. 44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algor"
J05-3004,C02-1139,0,0.058851,"NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent k"
J05-3004,P02-1014,0,0.313949,"NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent k"
J05-3004,poesio-etal-2002-acquiring,0,0.153059,"Missing"
J05-3004,P04-1019,0,0.502075,"ey invoke and that invoked by the antecedent and are also easily used to accommodate subjective viewpoints. They should therefore benefit especially from not relying wholly on standard taxonomic links. Different patterns can be developed for anaphora types that build on nonhyponymy relations. For example, bridging exploits meronymy and/or causal relations (among others). Therefore, patterns that express “part-of” links, for example, such as X of Y and genitives, would be appropriate. Indeed, these patterns have been recently used in Web search for antecedent selection for bridging anaphora by Poesio et al. (2004). They compare accuracy in antecedent selection for a method that integrates Web hits and focusing techniques with a method that uses WordNet and focusing, achieving comparable results for both methods. This strenghtens our hypothesis that antecedent selection for full NP anaphora without hand-modeled lexical knowledge has become feasible. 7. Conclusions We have explored two different ways of exploiting lexical knowledge for antecedent selection in other-anaphora and definite NP coreference. Specifically, we have compared a hand-crafted and -structured source of information such as WordNet and"
J05-3004,W97-1301,0,0.337964,"Missing"
J05-3004,preiss-etal-2004-anaphoric,0,0.0435016,"Missing"
J05-3004,N04-1010,0,0.0204219,"errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algorithms we have described can be considered blueprints for more complex versions. Specifically, the WordNet-based algorithms could be improved by exploiting information encoded in WordNet beyond explicitly encoded links (glosses could be mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]). The Webbased algorithms could similarly benefit from the exploration of different patterns and their combination, as well as from using non-pattern-based approaches for hyponymy detection (Shinzato and Torisawa 2004). In addition, we have evaluated the contribution of lexical resources in isolation rather than within a more sophisticated system that integrates additional non-lexical features. It is unclear whether integrating such knowledge sources in a full-resolution system might even out the differences between the Web-based and the WordNet-based algorithms or exacerbate them. Modjeska, Markert, and Nissim (2003) included a feature based on Web scores in a naive Bayes model for other-anaphora resolution that also used grammatical features and showed that the addition of the Web feature yielded an 11.4-"
J05-3004,W02-1040,0,0.0560673,"Missing"
J05-3004,P03-2012,0,0.041812,"nd, the anaphoricity of the definite NPs in Case Study II has de facto been manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent. One of the reasons why pronoun resolution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a twostage process which we also follow in this article. Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors. Future work will explore the extent of such a decrease in performance. 44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algorithms we have d"
J05-3004,J00-4005,0,0.0531786,"Missing"
J05-3004,J00-4003,0,0.396229,"ssions than (increasing) costs. Similarly, in Example (4), the NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and"
J05-3004,J03-4002,0,0.0157584,", far-reaching repercussions. (WSJ) (4) The ordinance, in Moon Township, prohibits locating a group home for the handicapped within a mile of another such facility. (WSJ) In Example (1), the definite noun phrase (NP) the periodical corefers with the magazine.2 In Example (2), the definite NP the hood can be felicitously used because a related entity has already been introduced by the NP the jacket, and a part-of relation between the two entities can be established. Examples (3)–(4) are instances of other-anaphora. Other-anaphora are a subclass of comparative anaphora (Halliday and Hasan 1976; Webber et al. 2003) in which the anaphoric NP is introduced by a lexical modifier (such as other, such, and comparative adjectives) that specifies the relationship (such as set-complement, similarity and comparison) between the entities invoked by anaphor and antecedent. For other-anaphora, the modifiers other or another provide a set-complement to an entity already evoked in the discourse model. In Example (3), the NP other, far-reaching repercussions refers to a set of repercussions excluding increasing costs and can be paraphrased as other (far-reaching) repercussions than (increasing) costs. Similarly, in Ex"
J05-3004,P03-1023,0,0.453898,"Missing"
J05-3004,S01-1035,0,\N,Missing
J05-3004,P99-1016,0,\N,Missing
J05-3004,M95-1017,0,\N,Missing
J05-3004,J01-4004,0,\N,Missing
J05-3004,C98-2138,0,\N,Missing
J18-2002,J08-4004,0,0.0411832,"idging recognition and antecedent selection (Section 6). This is the first full bridging resolution system that attempts the unrestricted phenomenon in a real setting. All our experiments are performed on ISNotes and therefore all our claims hold only for the news genre. Although we believe the benefit of joint optimization to hold across other genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of it"
J18-2002,P98-1013,0,0.260293,"Missing"
J18-2002,J08-1001,0,0.0199865,"to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed, as much as possible of the Polish center will be made from aluminum, steel and glass recycled from Warsaw’s abundant rubble. [2 sent.] The windows will open. The carpets won’t be glued down and walls will be coated with non-toxic finishes. Bridging plays an important role in establishing entity coherence in a text. Barzilay and Lapata (2008) model local coherence with the entity grid based on coreference only. However, Example (1) does not exhibit any coreferential entity coherence, and therefore entity coherence can only be established when bridging is resolved. Furthermore, text understanding applications such as textual entailment (Mirkin, Dagan, and Pado´ 2010), context question answering (Voorhees 2001), and opinion mining (Kobayashi, Inui, and Matsumoto 2007) have been shown to benefit from bridging resolution. The main contributions presented in this article lie in the following aspects: 1. We present an English corpus (IS"
J18-2002,bjorkelund-etal-2014-extended,0,0.371063,"Missing"
J18-2002,P11-1151,0,0.0783664,"Missing"
J18-2002,P09-1092,0,0.0900894,"Missing"
J18-2002,W12-1632,0,0.674482,"Missing"
J18-2002,J96-2004,0,0.556789,"lly, we evaluate bridging resolution as a pipeline consisting of bridging recognition and antecedent selection (Section 6). This is the first full bridging resolution system that attempts the unrestricted phenomenon in a real setting. All our experiments are performed on ISNotes and therefore all our claims hold only for the news genre. Although we believe the benefit of joint optimization to hold across other genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section"
J18-2002,caselli-prodanof-2006-annotating,0,0.739279,"Missing"
J18-2002,W06-3915,0,0.0297969,"us studies in Section 2.2. Section 2.3 reviews automatic algorithms for bridging resolution and Section 2.4 discusses bridging and implicit semantic role labeling. 2.1 Bridging: Theoretical Studies Theoretical studies on bridging include linguistic (Hawkins 1978; Prince 1981, 1992), psycholinguistic (Clark 1975; Clark and Haviland 1977; Garrod and Sanford 1982), pragmatic and cognitive (Erku¨ and Gundel 1987; Gundel, Hedberg, and Zacharski 2000; Matsui 2000; Schwarz 2000), as well as formal accounts (Hobbs et al. 1993; Bos, ¨ Buitelaar, and Mineur 1995; Asher and Lascarides 1998; Lobner 1998; Cimiano 2006; Irmer 2009). Our concept of bridging is closest to the notions of associative anaphora in Hawkins (1978) and (noncontained) inferrables in Prince (1981): noun phrases (NPs) that are not coreferent to a previous mention but the referent of which is identifiable via a lexicosemantic, frame, or encyclopedic relation to a previous mention, with this relation not being syntactically expressed. Relation types used are very diverse and antecedents can be noun phrases, verb phrases, or even whole sentences (Clark 1975; Asher and Lascarides 1998, inter alia). ¨ Several studies, such as Hawkins (1978)"
J18-2002,T75-2034,0,0.583996,"referring back to the antecedent “The business,” which itself refers back to “The Bakersfield Supermarket.” Both of these two anaphors refer to the same entity as their antecedents. Differently, the bridging anaphor “friends” does not refer to the same entity as its antecedent “its owner.” The phenomena illustrated in (1) and (2) have attracted a lot of interest under the heading of coreference resolution (Hobbs 1978; Hirschman and Chinchor 1997; Soon, Ng, and Lim 2001; Lee et al. 2013, 2017, inter alia). This article, however, focuses on the phenomenon illustrated in (3), known as bridging (Clark 1975) or associative anaphora (Hawkins 1978). Bridging anaphors are anaphoric noun phrases that are not coreferent but instead linked via associative relations to the antecedent. Bridging resolution has to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed, as much as possible of the Polish center will be made from aluminum, steel and glass recycled from Warsaw’s abundant rub"
J18-2002,J93-1003,0,0.136081,"Missing"
J18-2002,C12-1050,1,0.885472,"Missing"
J18-2002,J95-2003,0,0.789352,"Missing"
J18-2002,J86-3001,0,0.388309,"Missing"
J18-2002,C96-1084,1,0.561768,"Missing"
J18-2002,C16-1177,1,0.663164,"and Lobner (1998), limit bridging to definite NPs; we, however, believe that there is no clear difference in information status between the windows, on the one hand, and walls, on the other hand, in Example (1).6 3 Unfortunately, as we explain in Section 2.2, no other English corpus that is immediately usable for the full problem of bridging resolution is currently available for us to test our system on. 4 Quantitative results for bridging recognition are very similar to the previous framework, however. 5 We do not include the work that we conducted previously (Hou, Markert, and Strube 2014; Hou 2016), as these follow very different paradigms, using rule-based and neural network approaches, respectively. None of these approaches outperform our work in this article. 6 Prince (1992) also gives examples of indefinite bridging cases, so our observation is not new. 240 Hou, Markert, and Strube Unrestricted Bridging Resolution Our bridging notion differs from Clark (1975) in that we do not include coreferential cases: We believe coreference is different both from an IS viewpoint (always being discourse-old) as well as from a computational perspective in that coreference needs different methods t"
J18-2002,D13-1077,1,0.667552,"ther genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of iterative collective classification to unify the approaches to the two tasks.4 With regard to antecedent selection, we introduce several new features as well as the notion of using the discourse scope of an anaphor to adjust the set of potential antecedents it can refer back to (Section 5.3). We also now consider different evaluation"
J18-2002,N13-1111,1,0.870902,"ther genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of iterative collective classification to unify the approaches to the two tasks.4 With regard to antecedent selection, we introduce several new features as well as the notion of using the discourse scope of an anaphor to adjust the set of potential antecedents it can refer back to (Section 5.3). We also now consider different evaluation"
J18-2002,D07-1114,0,0.0301264,"Missing"
J18-2002,P13-1116,0,0.0447125,"Missing"
J18-2002,J13-4004,0,0.0308425,"(the antecedent). Figure 1 shows an excerpt of a news article with three anaphoric references: “its” is a pronominal anaphor referring back to the antecedent “The business,” which itself refers back to “The Bakersfield Supermarket.” Both of these two anaphors refer to the same entity as their antecedents. Differently, the bridging anaphor “friends” does not refer to the same entity as its antecedent “its owner.” The phenomena illustrated in (1) and (2) have attracted a lot of interest under the heading of coreference resolution (Hobbs 1978; Hirschman and Chinchor 1997; Soon, Ng, and Lim 2001; Lee et al. 2013, 2017, inter alia). This article, however, focuses on the phenomenon illustrated in (3), known as bridging (Clark 1975) or associative anaphora (Hawkins 1978). Bridging anaphors are anaphoric noun phrases that are not coreferent but instead linked via associative relations to the antecedent. Bridging resolution has to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed,"
J18-2002,D17-1018,0,0.10828,"Missing"
J18-2002,P12-1084,1,0.891789,"Missing"
J18-2002,W03-2606,1,0.847151,"Missing"
J18-2002,D14-1221,1,0.882604,"Missing"
J18-2002,meyers-etal-2004-annotating,0,0.0855715,"s” in (implicit) semantic role labeling, for example, One man in Example (2). Still, employees do occasionally try to smuggle out a gem or two. One man wrapped several diamonds in the knot of his tie. (2) In addition, implicit semantic role labeling for nominal predicates tries to link all possible implicit core roles for the nominal predicate in question. Yet not every nominal predicate under consideration is a bridging anaphor. Despite differences between implicit semantic role labeling and bridging resolution, these two tasks can benefit from each other. We explore statistics from NomBank (Meyers et al. 2004) to predict bridging anaphors (Section 4.3.2). Some of our features for bridging antecedent selection are inspired by Laparra and Rigau (2013) (Section 5.2.2). 3. ISNotes: A Corpus for Information Status ISNotes contains 50 texts from the Wall Street Journal portion of OntoNotes (Weischedel et al. 2011), in which all mentions (10,980 overall) are annotated for IS. The corpus can be downloaded from http://www.h-its.org/en/research/nlp/isnotes-corpus/. 3.1 ISNotes Annotation Scheme Information status in ISNotes. Information status describes the degree to which a discourse entity is available to"
J18-2002,E06-1015,0,0.0604157,"Missing"
J18-2002,W09-3017,0,0.475177,"Missing"
J18-2002,P02-1014,0,0.38831,"Missing"
J18-2002,W06-1612,0,0.531154,"Missing"
J18-2002,nissim-etal-2004-annotation,0,0.927923,"t only used for definite expressions. They achieved a kappa score of 0.78 for six top-level categories. However, the confusion matrix in Riester, Lorenz, and Seemann (2010) shows that the anaphoric bridging category is frequently confused with other categories: The two annotators agreed on fewer than a third of bridging anaphors. These previous corpus studies on bridging differ from ours in several ways. First, the definition of bridging is sometimes extended to include coreferential NPs 241 Computational Linguistics Volume 44, Number 2 with lexical variety (Vieira 1998) or non-anaphoric NPs (Nissim et al. 2004). Second, they put more restrictions on bridging than we do, limiting to definite NP anaphora (Poesio and Vieira 1998; Gardent and Manu´elian 2005; Caselli and Prodanof 2006; Riester, Lorenz, and Seemann 2010), to NP antecedents (all prior work), or to few relation types between anaphor and antecedent (Poesio 2004). Apart from these differences in definition of bridging, often reliability is not measured or low, especially for bridging recognition (Fraurud 1990; Poesio and Vieira 1998; Gardent and Manu´elian 2005; Nedoluzhko, M´ırovsky, ` and Pajas 2009; Riester, Lorenz, and Seemann 2010). 2.3"
J18-2002,N13-1099,0,0.0686676,"Missing"
J18-2002,N04-3012,0,0.0942611,"Missing"
J18-2002,W03-2605,0,0.16295,"Missing"
J18-2002,W04-2327,0,0.43661,"Missing"
J18-2002,poesio-etal-2002-acquiring,0,0.370081,"Missing"
J18-2002,P04-1019,0,0.924307,"Missing"
J18-2002,J04-3003,0,0.433952,"Missing"
J18-2002,J98-2001,0,0.846898,"he confusion matrix in Riester, Lorenz, and Seemann (2010) shows that the anaphoric bridging category is frequently confused with other categories: The two annotators agreed on fewer than a third of bridging anaphors. These previous corpus studies on bridging differ from ours in several ways. First, the definition of bridging is sometimes extended to include coreferential NPs 241 Computational Linguistics Volume 44, Number 2 with lexical variety (Vieira 1998) or non-anaphoric NPs (Nissim et al. 2004). Second, they put more restrictions on bridging than we do, limiting to definite NP anaphora (Poesio and Vieira 1998; Gardent and Manu´elian 2005; Caselli and Prodanof 2006; Riester, Lorenz, and Seemann 2010), to NP antecedents (all prior work), or to few relation types between anaphor and antecedent (Poesio 2004). Apart from these differences in definition of bridging, often reliability is not measured or low, especially for bridging recognition (Fraurud 1990; Poesio and Vieira 1998; Gardent and Manu´elian 2005; Nedoluzhko, M´ırovsky, ` and Pajas 2009; Riester, Lorenz, and Seemann 2010). 2.3 Bridging: Computational Approaches Most computational approaches for resolving bridging focus on antecedent selectio"
J18-2002,W97-1301,0,0.869618,"Missing"
J18-2002,prasad-etal-2008-penn,0,0.178129,"Missing"
J18-2002,D11-1099,0,0.0607682,"Missing"
J18-2002,E12-1081,0,0.687179,"Missing"
J18-2002,P10-1005,0,0.0780066,"Missing"
J18-2002,riester-etal-2010-recursive,0,0.25416,"Missing"
J18-2002,E14-3006,0,0.0396792,"Missing"
J18-2002,S10-1008,0,0.0267878,"le Labeling Semantic role labeling is the task of assigning semantic roles (such as Agent or Theme) to the semantic arguments associated with a predicate (e.g., a verb or a noun). In frame semantics (Baker, Fillmore, and Lowe 1998), core semantic roles (also called Core Frame Elements) are essential to the meaning of semantic situations while non-core semantic roles (e.g., time, manner) are less central. The majority of work on semantic role labeling only recognizes semantic arguments from the sentence where the predicate is present and thus ignores arguments from the wider discourse context. Ruppenhofer et al. (2010) organized a shared task to 243 Computational Linguistics Volume 44, Number 2 address the issue of non-local (implicit) argument identification for nominal and verbal predicates. There is partial overlap between bridging resolution and implicit semantic role labeling (i.e., in some bridging cases, antecedents are implicit semantic roles of bridging anaphors). However, bridging resolution considers all possible nominal bridging anaphors in running text. Some bridging anaphors are not considered “nominal predicates” in (implicit) semantic role labeling, for example, One man in Example (2). Still"
J18-2002,D09-1151,0,0.575118,"Missing"
J18-2002,D09-1018,0,0.0894786,"Missing"
J18-2002,J01-4004,0,0.672971,"Missing"
J18-2002,P14-2120,0,0.0662945,"Missing"
J18-2002,J00-4003,0,0.539882,"Missing"
J18-2002,P97-1072,0,0.83894,"Missing"
J18-2002,D14-1222,1,\N,Missing
K18-1023,E17-2046,1,0.609022,"t of scope for our paper, since computing the exact solution is intractable in TLS.7 Experiments 6.2 We evaluate the performance of modeling TLS as MDS and the effect of various temporalizations. 6.1 TLs timeline17 crisis Definition 1. Let V be some set and I ⊂ 2V be a collection of subsets of V . The tuple (V, I) is called an independence system if (i) ∅ ∈ I and (ii) B ∈ I and A ⊆ B implies A ∈ I. 6 Topics Evaluation Metrics Automatic evaluation of TLS is done by ROUGE (Lin, 2004). We report ROUGE-1 and ROUGE2 F1 scores for the concat, agreement and align+ m:1 metrics for TLS we presented in Martschat and Markert (2017). These metrics perform evaluation by concatenating all daily summaries, evaluating only matching days and evaluating aligned Data and Preprocessing We run experiments on timeline17 (Tran et al., 2013b) and crisis (Tran et al., 2015a). Both data sets consist of (i) journalist-generated timelines on events such as the Syrian War as well as (ii) corresponding corpora of news articles on the topic scraped via Google News. They are publically 8 http://www.l3s.de/˜gtran/timeline/ The datasets used in Chieu and Lee (2004) or Nguyen et al. (2014) are not available. 10 This procedure is in line with p"
K18-1023,E17-1037,0,0.0152673,"fic heuristic criteria (Chieu and Lee, 2004; Yan et al., 2011b; Wang et al., 2016, inter alia), often with manually determined parameters (Chieu and Lee, 2004; Yan et al., 2011b) or needing supervision (Wang et al., 2016). As features and architectures are rarely reused or indeed separated from each other, it is difficult to assess reported improvements. Moreover, none of these approaches give performance guarantees for the task, which are possible in MDS models based on function optimization (McDonald, 2007; Lin and Bilmes, 2011) that yield state-of-the art models for MDS (Hong et al., 2014; Hirao et al., 2017). In this paper we take a step back from the differences between MDS and TLS and consider the following question: Can MDS optimization models be expanded to yield scalable, well-performing TLS models that take into account the temporal properties of TLS, while keeping MDS advantages Introduction There is an abundance of reports on events, crises and disasters. Timelines (see Table 1) summarize and date these reports in an ordered overview. Automatic Timeline Summarization (TLS) constructs such timelines from corpora that contain articles about the corresponding event. In contrast to standard m"
K18-1023,C14-1114,0,0.560943,"and align+ m:1 metrics for TLS we presented in Martschat and Markert (2017). These metrics perform evaluation by concatenating all daily summaries, evaluating only matching days and evaluating aligned Data and Preprocessing We run experiments on timeline17 (Tran et al., 2013b) and crisis (Tran et al., 2015a). Both data sets consist of (i) journalist-generated timelines on events such as the Syrian War as well as (ii) corresponding corpora of news articles on the topic scraped via Google News. They are publically 8 http://www.l3s.de/˜gtran/timeline/ The datasets used in Chieu and Lee (2004) or Nguyen et al. (2014) are not available. 10 This procedure is in line with previous TLS work (Chieu and Lee, 2004). The focus of the current paper is not on further improving date assignment. 9 7 McDonald (2007) and Lin and Bilmes (2010) already report scalability issues for obtaining exact solutions for MDS, which is of smaller scale and has simpler constraints than our task. 234 dates based on date and content similarity, respectively. We evaluate date selection using F1 score. 6.3 as follows: iterating through the ranked sentence list, a sentence is added to the timeline depending on the extent of the sentences"
K18-1023,hong-etal-2014-repository,0,0.0232374,"timizing task-specific heuristic criteria (Chieu and Lee, 2004; Yan et al., 2011b; Wang et al., 2016, inter alia), often with manually determined parameters (Chieu and Lee, 2004; Yan et al., 2011b) or needing supervision (Wang et al., 2016). As features and architectures are rarely reused or indeed separated from each other, it is difficult to assess reported improvements. Moreover, none of these approaches give performance guarantees for the task, which are possible in MDS models based on function optimization (McDonald, 2007; Lin and Bilmes, 2011) that yield state-of-the art models for MDS (Hong et al., 2014; Hirao et al., 2017). In this paper we take a step back from the differences between MDS and TLS and consider the following question: Can MDS optimization models be expanded to yield scalable, well-performing TLS models that take into account the temporal properties of TLS, while keeping MDS advantages Introduction There is an abundance of reports on events, crises and disasters. Timelines (see Table 1) summarize and date these reports in an ordered overview. Automatic Timeline Summarization (TLS) constructs such timelines from corpora that contain articles about the corresponding event. In c"
K18-1023,P12-1077,0,0.632878,"Missing"
K18-1023,W04-1013,0,0.0977188,"ly on a global level. In TLS, the whole summary is naturally divided into per-day summaries. Criteria and constraints apply on a global level as well as on a per-day level. Even for the small number of DUC tasks that do focus on longer-running events, several differences to TLS still hold. First, the temporal dimension in the DUC gold standard summaries and system outputs is playing a minor role, with few explicit datings of events and a non-temporal structure of the output, leading again to the abovementioned differences in constraints and criteria. The ROUGE evaluation measures used in MDS (Lin, 2004) also do not take into account temporality and do not explicitly penalize wrong datings. Second, corpora in TLS typically contain thousands of documents per query (Tran et al., 2013b, 2015a). This is magnitudes larger than the corpora usually considered for MDS (Over and Yen, 2004). This leads to a low compression rate4 and requires approaches to be scalable. Timeline Summarization Given a query (such as Syrian war) TLS needs to (i) extract the most important events for the query and their corresponding dates and (ii) obtain concise daily summaries for each selected date (Allan et al., 2001; C"
K18-1023,N10-1134,0,0.0408596,"lfill Equations 5 and 6. Then G REEDY has a performance guarantee of 1/(k + 1). The lemma implies that for small k that is typical in TLS (e.g. k = 2), we obtain a good approximation with reasonable constraints. However, our performance guarantees are still weaker than for MDS (for example, 0.33 for k = 2 compared to 0.63 in MDS). The reason for this is that our constraints are more complex, going beyond the simple well-studied cardinality and knapsack constraints. We also observe that this is a worst-case bound: in practice the performance of the algorithm may approach the exact solution (as Lin and Bilmes (2010) show for MDS). However, such an analysis is out of scope for our paper, since computing the exact solution is intractable in TLS.7 Experiments 6.2 We evaluate the performance of modeling TLS as MDS and the effect of various temporalizations. 6.1 TLs timeline17 crisis Definition 1. Let V be some set and I ⊂ 2V be a collection of subsets of V . The tuple (V, I) is called an independence system if (i) ∅ ∈ I and (ii) B ∈ I and A ⊆ B implies A ∈ I. 6 Topics Evaluation Metrics Automatic evaluation of TLS is done by ROUGE (Lin, 2004). We report ROUGE-1 and ROUGE2 F1 scores for the concat, agreement"
K18-1023,P11-1052,0,0.0795079,"MDS community.2 Instead, approaches to TLS start from scratch, optimizing task-specific heuristic criteria (Chieu and Lee, 2004; Yan et al., 2011b; Wang et al., 2016, inter alia), often with manually determined parameters (Chieu and Lee, 2004; Yan et al., 2011b) or needing supervision (Wang et al., 2016). As features and architectures are rarely reused or indeed separated from each other, it is difficult to assess reported improvements. Moreover, none of these approaches give performance guarantees for the task, which are possible in MDS models based on function optimization (McDonald, 2007; Lin and Bilmes, 2011) that yield state-of-the art models for MDS (Hong et al., 2014; Hirao et al., 2017). In this paper we take a step back from the differences between MDS and TLS and consider the following question: Can MDS optimization models be expanded to yield scalable, well-performing TLS models that take into account the temporal properties of TLS, while keeping MDS advantages Introduction There is an abundance of reports on events, crises and disasters. Timelines (see Table 1) summarize and date these reports in an ordered overview. Automatic Timeline Summarization (TLS) constructs such timelines from cor"
K18-1023,P15-1154,1,0.814243,"is considerable variation in timeline parameters (Table 3), we evaluate against each reference timeline individually, providing systems with the parameters they need via extraction from the reference timeline, including range and needed length constraints. We set m to the number of sentences in the reference timeline, ` to the number of dates in the timeline, and k to the average length of the daily summaries. Most previous work uses different or unreported settings, which makes comparison difficult. For instance, Tran et al. (2013b) do not report how they obtain timeline length. Wang et al. (2015, 2016) create a constant-length summary for each day that has an article in the corpus, thereby comparing reference timelines with few days with predicted timelines that have summaries for each day. 6.4 Regression. Our second baseline is R EG, a supervised linear regression model (Tran et al., 2013b; Wang et al., 2015). We represent each sentence with features describing its length, number of named entities, unigram features, and averaged/summed tf-idf scores. During training, for each sentence, standard ROUGE-1 F1 w.r.t. the reference summary of the sentence’s date is computed. The model is trained"
K18-1023,N15-1112,0,0.386496,"is considerable variation in timeline parameters (Table 3), we evaluate against each reference timeline individually, providing systems with the parameters they need via extraction from the reference timeline, including range and needed length constraints. We set m to the number of sentences in the reference timeline, ` to the number of dates in the timeline, and k to the average length of the daily summaries. Most previous work uses different or unreported settings, which makes comparison difficult. For instance, Tran et al. (2013b) do not report how they obtain timeline length. Wang et al. (2015, 2016) create a constant-length summary for each day that has an article in the corpus, thereby comparing reference timelines with few days with predicted timelines that have summaries for each day. 6.4 Regression. Our second baseline is R EG, a supervised linear regression model (Tran et al., 2013b; Wang et al., 2015). We represent each sentence with features describing its length, number of named entities, unigram features, and averaged/summed tf-idf scores. During training, for each sentence, standard ROUGE-1 F1 w.r.t. the reference summary of the sentence’s date is computed. The model is trained"
K18-1023,N16-1008,0,0.463842,"Missing"
K18-1023,D11-1040,0,0.122799,"ed in this paper is available online.1 1 Katja Markert Institute of Computational Linguistics Heidelberg University 69120 Heidelberg, Germany markert@cl.uni-heidelberg.de Table 1: Excerpt from a Syrian War Reuters timeline. enhancing scalability and redundancy problems. These differences have significant consequences for constraints, objectives, compression rates and scalability (see Section 2.2). Due to these differences, most work on TLS has been separate from the MDS community.2 Instead, approaches to TLS start from scratch, optimizing task-specific heuristic criteria (Chieu and Lee, 2004; Yan et al., 2011b; Wang et al., 2016, inter alia), often with manually determined parameters (Chieu and Lee, 2004; Yan et al., 2011b) or needing supervision (Wang et al., 2016). As features and architectures are rarely reused or indeed separated from each other, it is difficult to assess reported improvements. Moreover, none of these approaches give performance guarantees for the task, which are possible in MDS models based on function optimization (McDonald, 2007; Lin and Bilmes, 2011) that yield state-of-the art models for MDS (Hong et al., 2014; Hirao et al., 2017). In this paper we take a step back from t"
markert-nissim-2002-towards,W98-0720,0,\N,Missing
markert-nissim-2002-towards,kilgarriff-rosenzweig-2000-english,0,\N,Missing
markert-nissim-2002-towards,N01-1009,0,\N,Missing
markert-nissim-2002-towards,C00-2128,0,\N,Missing
markert-nissim-2002-towards,P93-1012,0,\N,Missing
markert-nissim-2002-towards,J96-2004,0,\N,Missing
markert-nissim-2002-towards,P96-1006,0,\N,Missing
N09-1001,E06-1027,0,0.312032,"proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a"
N09-1001,esuli-sebastiani-2006-sentiwordnet,0,0.279036,"Missing"
N09-1001,P07-1054,0,0.589589,"Missing"
N09-1001,P97-1023,0,0.537557,"mainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences,"
N09-1001,D07-1115,0,0.259506,"m cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model f"
N09-1001,kamps-etal-2004-using,0,0.322992,"Missing"
N09-1001,C04-1200,0,0.056182,"sses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sens"
N09-1001,I05-2011,0,0.0166238,"hes to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as"
N09-1001,W04-3239,0,0.0227729,"ity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use"
N09-1001,P04-1035,0,0.0646507,"two disconnected subsets of all vertices, S and T with s ∈ S and t ∈ T . This corresponds to removing a set of edges from the graph. As similar items should be in the same part of the split, the best split is one which removes edges with low weights. In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices. X W (S, T ) = w(u, v) u∈S,v∈T Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002). 3.2 Why might Semi-supervised Minimum Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components. Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data. Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objec"
N09-1001,W02-1011,0,0.012597,"Missing"
N09-1001,W08-1207,1,0.934645,"ly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. 2 is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human"
N09-1001,P05-1017,0,0.453955,"Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word le"
N09-1001,W06-1639,0,0.0058557,"and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to w"
N09-1001,P02-1053,0,0.00307432,"Missing"
N09-1001,W03-1017,0,0.1196,"l across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document lev"
N09-1001,P06-1134,0,0.60917,"Missing"
N09-1001,H05-1044,0,0.403965,"Missing"
N09-1001,C08-1104,1,\N,Missing
N09-1001,H05-2017,0,\N,Missing
N09-1001,H05-1043,0,\N,Missing
N09-1001,W03-0404,0,\N,Missing
N10-1054,D09-1020,0,0.206923,"hinese translations are “ , ”. Similarly, for the word collaborate, the sense “work together on a common enterprise of project” (objective) corresponds to “ , ” in Chinese translation, and “cooperate as a traitor” (subjective) corresponds to “ , ”. Therefore, subjectivity information should be effective for improving lexical translation for what we previously (Su and Markert, 2008) termed subjectivity-ambiguous words, i.e. words with both subjective and objective senses such as positive and collaborate above. We therefore incorporate subjectivity word sense disambiguation (SWSD) as defined in Akkaya et al. (2009) into lexical substitution. SWSD is a binary classification task that decides in context whether a word occurs with one of its subjective or one of its objective senses. In contrast to standard  È4 Ð   r Ü ( H 357 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 357–360, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics multi-class Word Sense Disambiguation (WSD), it uses a coarse-grained sense inventory that allows to achieve higher accuracy than WSD and therefore introduces less noise when emb"
N10-1054,D07-1007,0,0.0919133,"Missing"
N10-1054,P07-1005,0,0.0853115,"Missing"
N10-1054,C08-1104,1,0.920769,"x.php/ Semeval_2010 In most cases, if the word positive is used in the sense “greater than zero” (objective) in an English context, the corresponding Chinese translation is “ ”; if “involving advantage or good”(subjective) is used, its Chinese translations are “ , ”. Similarly, for the word collaborate, the sense “work together on a common enterprise of project” (objective) corresponds to “ , ” in Chinese translation, and “cooperate as a traitor” (subjective) corresponds to “ , ”. Therefore, subjectivity information should be effective for improving lexical translation for what we previously (Su and Markert, 2008) termed subjectivity-ambiguous words, i.e. words with both subjective and objective senses such as positive and collaborate above. We therefore incorporate subjectivity word sense disambiguation (SWSD) as defined in Akkaya et al. (2009) into lexical substitution. SWSD is a binary classification task that decides in context whether a word occurs with one of its subjective or one of its objective senses. In contrast to standard  È4 Ð   r Ü ( H 357 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 357–360, c Los Angeles, California,"
N10-1054,P06-1134,0,0.0816341,"Missing"
N10-1054,S07-1009,0,\N,Missing
N13-1111,J08-1001,0,0.269857,"Missing"
N13-1111,W12-1632,0,0.285534,"Missing"
N13-1111,caselli-prodanof-2006-annotating,0,0.493834,"nts improves significantly over the local model. 2 Related Work Prior corpus-linguistic studies on bridging are beset by three main problems. First, reliability is not measured or low (Fraurud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridgi"
N13-1111,C12-1050,1,0.8319,"Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associated with a weight. The 5 polarity of the weights is"
N13-1111,J12-4003,0,0.0578218,"Missing"
N13-1111,W03-2606,1,0.905518,"Missing"
N13-1111,P12-1084,1,0.653732,"so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents according to their syntactic type or relations between them. However, we only discussed human agreement on bridging recognition in Markert et al. (2012), disregarding antecedent annotation. We also did not discuss the different types of bridging in the corpus. We will remedy this in Section 3. Automatic work on bridging distinguishes between recognition (Vieira and Poesio, 2000; Rahman and Ng, 2012; Cahill and Riester, 2012; Markert et al., 2012) and antecedent selection. Work on antecedent selection suffers from focusing on subproblems, e.g. only part-of bridging (Poesio et al., 2004; Markert et al., 2003) or definite NP anaphora (Lassalle and Denis, 2011). Most relevant for us is Lassalle and Denis (2011) who restrict anaphora to definite d"
N13-1111,N09-1018,0,0.0531989,"a template for constructing Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associated with a weight. The 5 po"
N13-1111,P10-1123,0,0.0778411,"rud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents acc"
N13-1111,W03-1023,1,0.936625,"Missing"
N13-1111,nissim-etal-2004-annotation,0,0.406101,"ates (e.g. Gerber and Chai (2012)). We consider all bridging anaphors in running text. The closest work to ours interpreting implicit role filling as anaphora resolution is Silberer and Frank (2012). 3 Corpus for Bridging: An Overview We use the dataset we created in Markert et al. (2012) with almost 11,000 NPs annotated for information status including 663 bridging NPs and their antecedents in 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al., 2011). Bridging anaphora can be any noun phrase. They are not limited to definite NPs as in previous work. In contrast to Nissim et al. (2004), antecedents are annotated and can be noun phrases, verb phrases or even clauses. Our bridging annotation is also not limited with regards to semantic relations between anaphor and antecedent. In Markert et al. (2012) we achieved high agreement for the overall information status annotation scheme between three annotators (κ between 75 and 80, dependent on annotator pairs) as well as for all subcategories, including bridging (κ over 60 for all annotator pairings, over 70 for two expert annotators). Here, we add the following new results: • Agreement for selecting bridging antecedents was aroun"
N13-1111,J98-2001,0,0.826665,"esolution may also be important for textual entailment (Mirkin et al., 2010). Bridging resolution can be divided into two tasks, recognizing that a bridging anaphor is present and finding the correct antecedent among a list of candidates. These two tasks have frequently been handled in a pipeline with most research concentrating on antecedent selection only. We also handle only the task of antecedent selection. Previous work on antecedent selection for bridging anaphora is restricted. It makes strong untested assumptions about bridging anaphora types or relations, limiting it to definite NPs (Poesio and Vieira, 1998; Poesio et al., 2004; Lassalle and Denis, 2011) or to part-of relations between anaphor and antecedent (Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011). We break new ground by considering all relations and anaphora/antecedent types and show that the variety of bridging anaphora is much higher than reported previously. Following work on coreference resolution, we apply a local pairwise model (Soon et al., 2001) for antecedent selection. We then develop novel semantic, syntactic and salience features for this task, showing strong improvements over one of the best known 907"
N13-1111,P04-1019,0,0.116393,"ortant for textual entailment (Mirkin et al., 2010). Bridging resolution can be divided into two tasks, recognizing that a bridging anaphor is present and finding the correct antecedent among a list of candidates. These two tasks have frequently been handled in a pipeline with most research concentrating on antecedent selection only. We also handle only the task of antecedent selection. Previous work on antecedent selection for bridging anaphora is restricted. It makes strong untested assumptions about bridging anaphora types or relations, limiting it to definite NPs (Poesio and Vieira, 1998; Poesio et al., 2004; Lassalle and Denis, 2011) or to part-of relations between anaphor and antecedent (Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011). We break new ground by considering all relations and anaphora/antecedent types and show that the variety of bridging anaphora is much higher than reported previously. Following work on coreference resolution, we apply a local pairwise model (Soon et al., 2001) for antecedent selection. We then develop novel semantic, syntactic and salience features for this task, showing strong improvements over one of the best known 907 Proceedings of NAACL-"
N13-1111,W03-2605,0,0.0691816,"allows us to: • model constraints that certain anaphora are likely to share the same antecedent; • model the global semantic connectivity of a salient potential antecedent to all anaphora in a text; • consider the union of potential antecedents for all anaphora instead of a static window-sized constraint. We show that this global model with the same local features but enhanced with global constraints improves significantly over the local model. 2 Related Work Prior corpus-linguistic studies on bridging are beset by three main problems. First, reliability is not measured or low (Fraurud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high ag"
N13-1111,D08-1068,0,0.018588,"ght. It can be viewed as a template for constructing Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associ"
N13-1111,riester-etal-2010-recursive,0,0.0828271,"rud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents acc"
N13-1111,S10-1008,0,0.0203059,"Missing"
N13-1111,S12-1001,0,0.0204899,"Missing"
N13-1111,J01-4004,0,0.0867864,"Missing"
N13-1111,J00-4003,0,0.592405,"Missing"
P03-1008,briscoe-carroll-2002-robust,0,0.0321221,"Missing"
P03-1008,J96-2004,0,0.132438,"Missing"
P03-1008,P92-1047,0,0.358394,"m generalises over two levels of contextual similarity. Resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine l"
P03-1008,J98-1002,0,0.0878372,"dividual word to treat regular sense distinctions.13 By exploiting additional similarity levels and integrating a thesaurus we further generalise the kind of inferences we can make and limit the size of annotated training data: as our sampling frame contains 553 different names, an annotated data set of 925 samples is quite small. These generalisations over context and collocates are also applicable to standard WSD and can supplement those achieved e.g., by subcategorisation frames (Martinez et al., 2002). Our approach to word similarity to overcome data sparseness is perhaps most similar to (Karov and Edelman, 1998). However, they mainly focus on the computation of similarity measures from the training data. We instead use an off-the-shelf resource without adding much computational complexity and achieve a considerable improvement in our results. 8 Conclusions We presented a supervised classification algorithm for metonymy recognition, which exploits the similarity between examples of conventional metonymy, operates on semantic classes and thereby enables complex inferences from training to test examples. We showed that syntactic head-modifier relations are a high precision feature for metonymy recogniti"
P03-1008,W02-1027,1,0.795724,"Missing"
P03-1008,markert-nissim-2002-towards,1,0.752782,"ctually quite regular (Lakoff and Johnson, 1980; Nunberg, 1995).2 In (2), “Pakistan”, the name of a location, refers to one of its national sports teams.3 (2) Pakistan had won the World Cup Similar examples can be regularly found for many other location names (see (3) and (4)). (3) England won the World Cup (4) Scotland lost in the semi-final In contrast to (1), the regularity of these examples can be exploited by a supervised machine learning algorithm, although this method is not pursued in standard approaches to regular polysemy and metonymy (with the exception of our own previous work in (Markert and Nissim, 2002a)). Such an algorithm needs to infer from examples like (2) (when labelled as a metonymy) that “England” and “Scotland” in (3) and (4) are also metonymic. In order to 2 Due to its regularity, conventional metonymy is also known as regular polysemy (Copestake and Briscoe, 1995). We use the term “metonymy” to encompass both conventional and unconventional readings. 3 All following examples are from the British National Corpus (BNC, http://info.ox.ac.uk/bnc). Pakistan had won the World Cup Scotland lost in the semi-final annotated corpus of occurrences of country names. context reduction Pakista"
P03-1008,W00-1326,0,0.011849,"rds of one semantic class and assign literal readings and metonymic patterns to new test instances of possibly different words of the same semantic class. This class-based approach enables one to, for example, infer the reading of (3) from that of (2). We use a decision list (DL) classifier. All features encountered in the training data are ranked in the DL (best evidence first) according to the following loglikelihood ratio (Yarowsky, 1995):  Log P r(readingi|f eaturek ) P r(readingj |f eaturek ) j6=i  P We estimated probabilities via maximum likelihood, adopting a simple smoothing method (Martinez and Agirre, 2000): 0.1 is added to both the denominator and numerator. The target readings to be distinguished are literal, place-for-people, place-forevent, place-for-product, othermet and mixed. All our algorithms are tested on our annotated corpus, employing 10-fold cross-validation. We evaluate accuracy and coverage: Acc = # correct decisions made # decisions made Cov = # decisions made # test data We also use a backing-off strategy to the most frequent reading (literal) for the cases where no decision can be made. We report the results as accuracy backoff (Accb ); coverage backoff is always 1. We are also"
P03-1008,C02-1112,0,0.0169953,"rare words that undergo regular sense alternations and do not have to annotate and train separately for every individual word to treat regular sense distinctions.13 By exploiting additional similarity levels and integrating a thesaurus we further generalise the kind of inferences we can make and limit the size of annotated training data: as our sampling frame contains 553 different names, an annotated data set of 925 samples is quite small. These generalisations over context and collocates are also applicable to standard WSD and can supplement those achieved e.g., by subcategorisation frames (Martinez et al., 2002). Our approach to word similarity to overcome data sparseness is perhaps most similar to (Karov and Edelman, 1998). However, they mainly focus on the computation of similarity measures from the training data. We instead use an off-the-shelf resource without adding much computational complexity and achieve a considerable improvement in our results. 8 Conclusions We presented a supervised classification algorithm for metonymy recognition, which exploits the similarity between examples of conventional metonymy, operates on semantic classes and thereby enables complex inferences from training to t"
P03-1008,P93-1012,0,0.655354,"similarity. Resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine learning algorithm based on previous e"
P03-1008,P02-1031,0,0.02341,"Missing"
P03-1008,W98-0720,0,0.693276,"he complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine learning algorithm based on previous examples does not seem feasible. However,"
P03-1008,P95-1026,0,0.128787,"cular word and assigns word senses to new test instances of the same word, (supervised) metonymy recognition can be trained on a set of labelled instances of different words of one semantic class and assign literal readings and metonymic patterns to new test instances of possibly different words of the same semantic class. This class-based approach enables one to, for example, infer the reading of (3) from that of (2). We use a decision list (DL) classifier. All features encountered in the training data are ranked in the DL (best evidence first) according to the following loglikelihood ratio (Yarowsky, 1995):  Log P r(readingi|f eaturek ) P r(readingj |f eaturek ) j6=i  P We estimated probabilities via maximum likelihood, adopting a simple smoothing method (Martinez and Agirre, 2000): 0.1 is added to both the denominator and numerator. The target readings to be distinguished are literal, place-for-people, place-forevent, place-for-product, othermet and mixed. All our algorithms are tested on our annotated corpus, employing 10-fold cross-validation. We evaluate accuracy and coverage: Acc = # correct decisions made # decisions made Cov = # decisions made # test data We also use a backing-off stra"
P03-1008,J99-4002,0,\N,Missing
P10-1077,P97-1005,0,0.4678,"Missing"
P10-1077,O97-1002,0,0.154657,"Missing"
P10-1077,C94-2174,0,0.327327,"Missing"
P10-1077,sharoff-etal-2010-web,1,0.854367,"Missing"
P10-1077,P09-1076,0,0.197465,"Missing"
P10-1077,P94-1019,0,\N,Missing
P12-1084,J08-4004,0,0.0219377,"e annotated by Annotator A. Finally, Annotator A carried out consistency checks over all texts. – The gold standard includes 10,980 true mentions (see Table 3). Table 1: Agreement Results κ Non-mention κ Old κ New κ Mediated/Knowledge κ Mediated/Synt κ Mediated/Aggregate κ Mediated/Func κ Mediated/Comp κ Mediated/Bridging A-B 81.5 80.5 76.6 82.1 88.4 87.0 6.0 81.8 70.8 A-C 78.9 83.2 74.0 78.4 87.8 85.4 83.2 78.3 60.6 Gold Standard B-C 86.0 79.3 74.3 74.1 87.6 86.0 6.9 81.2 62.3 Texts Mentions old Table 2: Agreement Results for individual categories percentage agreement, we measured Cohen’s κ (Artstein and Poesio, 2008) between all 3 possible annotator pairings. We also report single-category agreement for each category, where all categories but one are merged and then κ is computed as usual. Table 1 shows agreement results for the overall scheme at the coarse-grained (4 categories: non-mention, old, new, mediated) and the fine-grained level (9 categories: non-mention, old, new and the 6 mediated subtypes). The results show that the scheme is overall reliable, with not too many differences between the different annotator pairings.7 Table 2 shows the individual category agreement for all 9 categories. We achi"
P12-1084,J08-1001,0,0.0964698,"Notes corpus (Weischedel et al., 2011). We also report the first results on fine-grained IS classification by modelling further distinctions within the category of mediated mentions, such as comparative and bridging anaphora (see Examples 1 and 2, re795 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795–804, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics spectively).2 Fine-grained IS is a prerequisite to full bridging/comparative anaphora resolution, and therefore necessary to fill gaps in entity grids (Barzilay and Lapata, 2008) based on coreference only. Thus, Examples 1 and 2 do not exhibit any coreferential entity coherence but coherence can be established when the comparative anaphor others is resolved to others than freeway survivor Buck Helm, and the bridging anaphor the streets is resolved to the streets of Oranjemund, respectively. (1) the condition of freeway survivor Buck Helm . . . , improved, hospital officials said. Rescue crews, however, gave up hope that others would be found. (2) Oranjemund, the mine headquarters, is a lonely corporate oasis of 9,000 residents. Jackals roam the streets at night . . ."
P12-1084,P09-1092,0,0.293161,"nly on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information sta"
P12-1084,W11-1907,1,0.885694,"Missing"
P12-1084,J96-2004,0,0.15904,"ifferent annotator pairings.7 Table 2 shows the individual category agreement for all 9 categories. We achieve high reliability for most categories.8 Particularly interesting is the fact that hearer-old entities (mediated/knowledge) can be identified reliably although all annotators had substantially different backgrounds. The reliability of the category bridging is more annotatordependent, although still higher, sometimes considerably, than other previous attempts at bridg7 Often, annotation is considered highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta, 1996). However, the interpretation of κ is still under discussion (Artstein and Poesio, 2008). 8 The low reliability of the rare category func, when involving Annotator B, was explained by Annotator B forgetting about this category after having used it once. Pair A-C achieved high reliability (κ 83.2 for pair A-C). 798 coref generic deictic pr mediated world knowledge syntactic aggregate func comparative bridging new 50 10,980 3237 3,143 94 3,708 924 1,592 211 65 253 663 4,035 Table 3: Gold Standard Distribution 4 Features In this Section, we describe both the local as well as the relational featur"
P12-1084,N07-1030,0,0.0609095,"Missing"
P12-1084,P07-1041,1,0.847197,"Missing"
P12-1084,J95-2003,0,0.784558,"Missing"
P12-1084,J05-3004,1,0.860388,"cal classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic or deictic pronoun. We follo"
P12-1084,W03-1023,1,0.870658,"classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic o"
P12-1084,N07-1002,0,0.0225811,"ersational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1"
P12-1084,N09-1065,0,0.0165875,"n a subproblem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced,"
P12-1084,nissim-etal-2004-annotation,0,0.330285,"and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity is available to the hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to"
P12-1084,W06-1612,0,0.456196,"all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity is available to the hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to the reader improves automatic summarization of news. Therefore, we here model IS in written text, creating a new dataset which adds an IS layer to the already existing comprehensive annotation in the OntoNotes c"
P12-1084,P04-1035,0,0.00646741,"Missing"
P12-1084,P04-1019,0,0.672562,"2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential w"
P12-1084,W04-2327,0,0.0843417,"his to the higher syntactic complexity and semantic vagueness in the commentary corpus. Riester et al. (2010) annotated a 2 All examples in this paper are from the OntoNotes corpus. The mention in question is typed in boldface; antecedents, where applicable, are displayed in italics. 796 German news corpus marginally reliable (κ = 0.66) for their overall scheme but their confusion matrix shows even lower reliability for several subcategories, most importantly deixis and bridging. While standard coreference corpora do not contain IS annotation, some corpora annotated for bridging are emerging (Poesio, 2004; Korzen and Buch-Kromann, 2011) but they are (i) not annotated for comparative anaphora or other IS categories, (ii) often not tested for reliability or reach only low reliability, (iii) often very small (Poesio, 2004). To the best of our knowledge, we therefore present the first English corpus reliably annotated for a wide range of IS categories as well as full anaphoric information for three main anaphora types (coreference, bridging, comparative). Automatic recognition of IS. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. As their approach i"
P12-1084,P96-1039,0,0.0656944,"Missing"
P12-1084,D11-1099,0,0.485024,"lve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity i"
P12-1084,riester-etal-2010-recursive,0,0.579882,"the challenge of modeling IS via collective classification, using several novel linguistically motivated features. We reimplement Nissim’s (2006) and Rahman and Ng’s (2011) approaches as baselines and show that our approach outperforms these by a large margin for both coarse- and finegrained IS classification. 2 Related Work IS annotation schemes and corpora. We enhance the approach in Nissim et al. (2004) in two major ways (see also Section 3.1). First, comparative anaphora are not specifically handled in Nissim et al. (2004) (and follow-on work such as Ritz et al. (2008) and Riester et al. (2010)), although some of them might be included in their respective bridging subcategories. Second, we apply the annotation scheme reliably to a new genre, namely news. This is a non-trivial extension: Ritz et al. (2008) applied a variation of the Nissim et al. (2004) scheme to a small set of 220 NPs in a German news/commentary corpus but found that reliability then dropped significantly to the range of κ = 0.55 to 0.60. They attributed this to the higher syntactic complexity and semantic vagueness in the commentary corpus. Riester et al. (2010) annotated a 2 All examples in this paper are from the"
P12-1084,ritz-etal-2008-annotation,0,0.0923975,"Missing"
P12-1084,D09-1151,0,0.0852425,"sification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic or deictic pronoun. We follow the OntoNotes (Weischedel et al., 2011) definition of coreference to be able to integrate our annotations with it. This definition"
P12-1084,J11-4007,0,0.152808,"he hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to the reader improves automatic summarization of news. Therefore, we here model IS in written text, creating a new dataset which adds an IS layer to the already existing comprehensive annotation in the OntoNotes corpus (Weischedel et al., 2011). We also report the first results on fine-grained IS classification by modelling further distinctions within the category of mediated mentions, such as comparative and bridging anaphora (see Examples 1 and 2, re795 Proceedings of the 50th Annual Meeting of the Associa"
P12-1084,D09-1018,0,0.194881,"2 coarse-grained classes, including location, organisation, person and several classes for numbers (such as date, money or percent). 4.2 Relations for Collective Classification Both Nissim (2006) and Rahman and Ng (2011) classify each mention individually in a standard supervised ML setting, not considering potential dependencies between the IS categories of different 9 We changed the value of “full prev mention” from “numeric’ to {yes, no, NA}. 799 mentions. However, collective or joint classification has made substantial impact in other NLP tasks, such as opinion mining (Pang and Lee, 2004; Somasundaran et al., 2009), text categorization (Yang et al., 2002; Taskar et al., 2002) and the related task of coreference resolution (Denis and Baldridge, 2007). We investigate two types of relations between mentions that might impact on IS classification. Syntactic parent-child relations. Two mediated subcategories account for accessibility via syntactic links to another old or mediated mention: mediated/synt is used when at least one child of a mention is mediated or old, with child relations restricted to pre- or postnominal possessives as well as PP children in our scheme (see Section 3.1). mediated/aggregate is"
P12-1084,J00-4003,0,0.491739,"rd coreference corpora do not contain IS annotation, some corpora annotated for bridging are emerging (Poesio, 2004; Korzen and Buch-Kromann, 2011) but they are (i) not annotated for comparative anaphora or other IS categories, (ii) often not tested for reliability or reach only low reliability, (iii) often very small (Poesio, 2004). To the best of our knowledge, we therefore present the first English corpus reliably annotated for a wide range of IS categories as well as full anaphoric information for three main anaphora types (coreference, bridging, comparative). Automatic recognition of IS. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. As their approach is restricted to definites, they only analyse a subset of the mentions we consider carrying IS. Siddharthan et al. (2011) also concentrate on a subproblem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions,"
P12-1084,D09-1102,0,0.0171281,"blem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such"
P15-1154,C14-1160,0,0.0152181,"part of a chain of events that share major actors or demonstrate cause-effect. Table 1 shows at least two such chains: the (a1-4-5) chain of protests leading to Mubarak’s resignation and the necessity of new elections, as well as the similar (a8-10-26) chain on Mursi. These chains can also be observed in the corresponding news articles. For example, some background articles on Mubarak’s step-down will likely explain the reasons behind it. However, extracting such causal information can be difficult, as demonstrated by the still low results for discourse relation extraction (Lin et al., 2014; Braud and Denis, 2014). Instead, we use date reference graphs, which model which date refers to which other date. In our example, articles published on Mubarak’s resignation date might refer to the date when the protest started. Although weaker than direct causal links, these links are easy to extract and we will show that they are very useful. In addition, references from important dates (such as Mubarak’s resignation date) should be weighted higher than other references. This is akin to IR models such as PageRank, which weigh links from popular pages higher than links from less popular pages. The main contributio"
P15-1154,E14-1075,0,0.022894,"l-life, long-term news events. We also discuss variations in timeline construction over different events, as well as by different journalists. 2 Note that the date selection task uses dates as proxies for important events on that date. 3 Kessler et al. (2012) is also used in Nguyen et al. (2014)’s system. 2 Related Work Timeline summarization is a special case of multidocument summarization (MDS). As TLS organizes events by date, timelines can be generated by MDS systems (such as (Radev et al., 2004b; Radev et al., 2004a; McKeown et al., 2003; Erkan and Radev, 2004; Metzler and Kanungo, 2008; Hong and Nenkova, 2014) by applying their summarization techniques on news articles for every individual date to create corresponding daily summaries. However, manually written timelines normally only include a small number of dates; in addition, the temporal component imposes constraints on sentence selection for timeline summarization, such as the preference for little overlap between sentences selected for different dates (Yan et al., 2011b). Many studies specific to timeline summarization, such as (Swan and Allan, 2000; Allan et al., 2001; Chieu and Lee, 2004; Yan et al., 2011b; Tran et al., 2015), focus on the"
P15-1154,P12-1077,0,0.184187,"Missing"
P15-1154,N03-4008,0,0.0551055,"on approach outperforms previous approaches with evaluations on four real-life, long-term news events. We also discuss variations in timeline construction over different events, as well as by different journalists. 2 Note that the date selection task uses dates as proxies for important events on that date. 3 Kessler et al. (2012) is also used in Nguyen et al. (2014)’s system. 2 Related Work Timeline summarization is a special case of multidocument summarization (MDS). As TLS organizes events by date, timelines can be generated by MDS systems (such as (Radev et al., 2004b; Radev et al., 2004a; McKeown et al., 2003; Erkan and Radev, 2004; Metzler and Kanungo, 2008; Hong and Nenkova, 2014) by applying their summarization techniques on news articles for every individual date to create corresponding daily summaries. However, manually written timelines normally only include a small number of dates; in addition, the temporal component imposes constraints on sentence selection for timeline summarization, such as the preference for little overlap between sentences selected for different dates (Yan et al., 2011b). Many studies specific to timeline summarization, such as (Swan and Allan, 2000; Allan et al., 2001"
P15-1154,W04-3252,0,0.0307214,"ion date) and 2011-0125 (referred date). (2) Mr Ghonim is Google’s head of marketing for Middle East and North Africa and was in Egypt when the protests started on Jan 25 (DailyMail, 2011-02-02). We quantify the topical influence between dates as follows: Let Si→j = {sij } be the set of sentences that are published in di and refer to dj . We are interested in how relevant this connection is to the overall news event, looking at the content in Si→j . To do so, we represent the overall content of the news collection by a set of keywords Q = {q1 , q2 , ..., qn }, which are computed via TextRank (Mihalcea and Tarau, 2004).4 We compute a relevance score for each sentence sij in Si→j by the famous Okapi BM25 function (Robertson et al., 1994), which ranks a sentence more topical if it contains more as well as more of the most salient collection keywords Q.5 We compute topical influence (Itopical ) as either the maximum value or the sum value of the relevance scores of all sij . Imax topical (di , dj ) = If req∗topical (di , dj ) = 4 max BM 25(sij , Q) (3) X BM 25(sij , Q) (4) sij ∈Si→j sij ∈Si→j We set n=20 in practice. We use the standard BM25 parameter settings k1 = 1.2 and b = 0.75 1600 5 Intuitively, If req∗t"
P15-1154,C14-1114,0,0.203972,"es of the initial events? What happened to the main the Associated Press (AP). We leave out intermediate dates due to space constraints. The whole timeline includes 30 dates between 2011-01-25 and 2013-07-07. Though convenient for the reader, the manual creation of a timeline can take a long time even for experts. For example, the creator of the startup Timeline says that it initially took a multiperson team a full work day to create a single timeline.1 Therefore, automatic timeline summarization (TLS) has emerged as an NLP task in the past few years (Tran et al., 2013a; Kessler et al., 2012; Nguyen et al., 2014; Yan et al., 2011b; Yan et al., 2011a; Wang et al., 2012; Tran et al., 2013b; Tran et al., 2015). TLS has been divided into two subtasks: (i) ranking the dates between beginning 1 http://www.niemanlab.org/2015/02/ timeline-is-providing-historicalcontext-to-the-news-but-is-there-abusiness-model-to-support-it/. 1598 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1598–1607, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and end of the timel"
P15-1154,D11-1124,0,0.413787,"nts? What happened to the main the Associated Press (AP). We leave out intermediate dates due to space constraints. The whole timeline includes 30 dates between 2011-01-25 and 2013-07-07. Though convenient for the reader, the manual creation of a timeline can take a long time even for experts. For example, the creator of the startup Timeline says that it initially took a multiperson team a full work day to create a single timeline.1 Therefore, automatic timeline summarization (TLS) has emerged as an NLP task in the past few years (Tran et al., 2013a; Kessler et al., 2012; Nguyen et al., 2014; Yan et al., 2011b; Yan et al., 2011a; Wang et al., 2012; Tran et al., 2013b; Tran et al., 2015). TLS has been divided into two subtasks: (i) ranking the dates between beginning 1 http://www.niemanlab.org/2015/02/ timeline-is-providing-historicalcontext-to-the-news-but-is-there-abusiness-model-to-support-it/. 1598 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1598–1607, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and end of the timeline in order of im"
P15-1154,radev-etal-2004-mead,0,0.0630738,"el. We show that the proposed date selection approach outperforms previous approaches with evaluations on four real-life, long-term news events. We also discuss variations in timeline construction over different events, as well as by different journalists. 2 Note that the date selection task uses dates as proxies for important events on that date. 3 Kessler et al. (2012) is also used in Nguyen et al. (2014)’s system. 2 Related Work Timeline summarization is a special case of multidocument summarization (MDS). As TLS organizes events by date, timelines can be generated by MDS systems (such as (Radev et al., 2004b; Radev et al., 2004a; McKeown et al., 2003; Erkan and Radev, 2004; Metzler and Kanungo, 2008; Hong and Nenkova, 2014) by applying their summarization techniques on news articles for every individual date to create corresponding daily summaries. However, manually written timelines normally only include a small number of dates; in addition, the temporal component imposes constraints on sentence selection for timeline summarization, such as the preference for little overlap between sentences selected for different dates (Yan et al., 2011b). Many studies specific to timeline summarization, such"
P15-1154,S10-1071,0,0.0582647,"Missing"
R11-1037,J08-1001,0,0.139092,"Missing"
R11-1037,J93-2004,0,0.0366819,"in several EU countries. In the UK, this has accompanied a drop in interest rates. Footballers are vastly overpaid. Manchester United pay Wayne Rooney £200,000 per week. The interpretation of Entity Instantiations can often be difficult. Entity Instantiations occur in a variety of forms. Participating noun phrases (NPs) include pronouns and proper nouns, can When we refer to a subset, we mean a proper subset. We consider two equal sets to be coreferent, and not participating in an Entity Instantiation. 2 Examples 1, 2, 3, 8 and 9 are adapted from the Penn Treebank Wall Street Journal Corpus (Marcus et al., 1993). 268 Proceedings of Recent Advances in Natural Language Processing, pages 268–274, Hissar, Bulgaria, 12-14 September 2011. classify these semantic relationship might still be applicable to our problem. A variety of automatic RE algorithms have been developed, falling largely into two groups; those that learn from tree-kernels and those that use traditional, flat features. In one approach of the first type, (Zhou et al., 2007) use tree kernels to capture the structured information held in the parse trees of entities. They implement an algorithm which dynamically decides how much context to inc"
R11-1037,J05-3004,1,0.92076,"Missing"
R11-1037,C10-1018,0,0.138752,"Missing"
R11-1037,W03-2606,1,0.747626,"Missing"
R11-1037,J98-2001,0,0.149674,"Missing"
R11-1037,P05-1045,0,0.0151371,"Missing"
R11-1037,P04-1019,0,0.335866,"Missing"
R11-1037,prasad-etal-2008-penn,0,0.059665,"include several contextual features, hypothesising that NPs that occur in similar contexts may be more likely to be Entity Instantiations. We retrieve the Levin class (Levin, 1993) of each NP’s head verb, as well as the verb itself, noting examples such as Example 1 which has two similar verbs, ‘surge’ and ‘skyrocket’. We also calculate whether each NP is in a quotation, and include an approximation of the discourse relations present in the two sentences by identifying likely discourse connectives and mapping them to their most frequent explicit relation in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In cases such as Example 7, the presence of the discourse connective ‘however’ appears useful in establishing that no instantiation is present. Note that we do not use any PDTB annotations to discover the presence of implicit or explicit discourse relations in the two sentences. We use a supervised machine learning approach to detect which NP pairs comprise Entity Instantiations. Below we detail our feature set, experimental set-up and results. 4.1 Features Our features fall into five broad categories; surface, salience, syntactic, contextual and knowledge. These categories contain both feat"
R11-1037,W03-2410,0,0.0577153,"Missing"
R11-1037,recasens-etal-2010-typology,0,0.0623337,"Missing"
R11-1037,C92-2082,0,0.129448,"Missing"
R11-1037,Y09-1024,0,0.025137,"Missing"
R11-1037,P11-1053,0,0.103952,"Missing"
R11-1037,J00-4003,0,0.0778322,"Missing"
R11-1037,W11-1902,0,0.0585406,"Missing"
R11-1037,D07-1076,0,0.10451,"earning from the original, highly skewed data is much more difficult, and our highest F-scores are 0.1938 and 0.1414 for set members and subsets, respectively (see Table 4). Learning from data with this sort of distribution is difficult, regardless of the domain. In future we intend to use techniques such as SMOTE (Chawla et al., 2002) and One-Sided Selection (Kubat and Matwin, 1997) to address this heavy skew. 5 tant Entity Instantiations, as well as our current instantiations between adjacent sentences. Future machine learning approaches to consider are tree-kernel based approaches such as (Zhou et al., 2007). To tackle the high skew in our data, we will use techniques such as those detailed in (Kubat and Matwin, 1997) and (Chawla et al., 2002), and also look to methods such as active learning to acquire more positive instantiation examples. Acknowledgements Andrew McKinlay is funded by an EPSRC Doctoral Training Grant. This research draws on data provided by the University Research Program for Google Search, a service provided by Google to promote a greater understanding of the web. Conclusion and Future Work We propose a novel Information Extraction task: the detection of Entity Instantiations."
R11-1037,nissim-etal-2004-annotation,0,\N,Missing
R11-1037,C10-2172,0,\N,Missing
R11-1037,swampillai-stevenson-2010-inter,0,\N,Missing
R11-1037,D11-1135,0,\N,Missing
R11-1037,W04-2705,0,\N,Missing
R11-1037,D11-1068,1,\N,Missing
R11-1037,N07-1015,0,\N,Missing
R11-1037,hendrickx-etal-2008-coreference,0,\N,Missing
R11-1037,W98-1105,0,\N,Missing
R11-1037,A00-2002,0,\N,Missing
R11-1037,W97-0319,0,\N,Missing
R11-1037,J93-3003,0,\N,Missing
R11-1037,riester-etal-2010-recursive,0,\N,Missing
R11-1037,D10-1107,0,\N,Missing
R11-1037,E12-1081,0,\N,Missing
R11-1037,W97-0313,0,\N,Missing
R11-1037,N01-1008,0,\N,Missing
R11-1037,M91-1033,0,\N,Missing
R11-1037,M91-1028,0,\N,Missing
R11-1037,W97-1301,0,\N,Missing
R11-1037,J00-4005,0,\N,Missing
R11-1037,W06-1612,0,\N,Missing
R11-1037,C94-2183,0,\N,Missing
R11-1037,W09-1119,0,\N,Missing
R11-1037,A83-1020,0,\N,Missing
R11-1037,W09-3029,0,\N,Missing
R11-1037,D08-1020,0,\N,Missing
R11-1037,W03-0415,0,\N,Missing
R11-1037,M93-1023,0,\N,Missing
R11-1037,A83-1024,0,\N,Missing
R11-1037,H05-1033,0,\N,Missing
R11-1037,C94-1056,0,\N,Missing
R11-1037,H05-1013,0,\N,Missing
R11-1037,W09-3017,0,\N,Missing
R11-1037,P05-1053,0,\N,Missing
R11-1037,C02-1151,0,\N,Missing
R11-1037,W09-3210,0,\N,Missing
R11-1037,J03-4003,0,\N,Missing
R11-1037,W06-1651,0,\N,Missing
R11-1037,P99-1006,0,\N,Missing
R11-1037,P09-1113,0,\N,Missing
R11-1037,P10-1081,0,\N,Missing
R11-1037,P09-1077,0,\N,Missing
R11-1037,J03-4002,0,\N,Missing
R11-1037,P06-1102,0,\N,Missing
R11-1037,W04-1004,0,\N,Missing
R11-1037,P04-1049,0,\N,Missing
R11-1037,W10-4327,0,\N,Missing
R11-1037,P04-1054,0,\N,Missing
R11-1037,P04-1053,0,\N,Missing
R11-1037,J95-2003,0,\N,Missing
R11-1037,J94-4002,0,\N,Missing
R11-1037,P06-1095,0,\N,Missing
R11-1037,P07-1075,0,\N,Missing
R11-1037,P10-1142,0,\N,Missing
R11-1037,P02-1034,0,\N,Missing
R11-1037,D09-1036,0,\N,Missing
R11-1037,P03-1028,0,\N,Missing
R11-1037,I05-1034,0,\N,Missing
R11-1037,D09-1016,0,\N,Missing
R11-1037,P12-1007,0,\N,Missing
R11-1037,P04-1087,0,\N,Missing
R11-1037,J05-2005,0,\N,Missing
R11-1037,P06-1015,0,\N,Missing
R11-1037,M92-1024,0,\N,Missing
R11-1037,P87-1022,0,\N,Missing
R11-1037,P10-1073,0,\N,Missing
R11-1037,P10-1005,0,\N,Missing
R11-1037,W10-4310,0,\N,Missing
R11-1037,P08-1119,0,\N,Missing
R11-1037,N06-1025,0,\N,Missing
R11-1037,J09-3003,0,\N,Missing
R11-1037,P05-1052,0,\N,Missing
R11-1037,prasad-etal-2010-exploiting,0,\N,Missing
R11-1037,P02-1047,0,\N,Missing
R11-1037,W07-2416,0,\N,Missing
R11-1037,N03-1030,0,\N,Missing
R11-1037,P09-1076,0,\N,Missing
R11-1037,P01-1005,0,\N,Missing
R11-1037,J05-1004,0,\N,Missing
R11-1037,P99-1008,0,\N,Missing
R11-1037,P98-2182,0,\N,Missing
R11-1037,C98-2177,0,\N,Missing
R11-1037,J01-4004,0,\N,Missing
R11-1037,C08-2022,0,\N,Missing
R11-1037,P08-1030,0,\N,Missing
R11-1037,P05-1019,0,\N,Missing
R11-1037,P99-1047,0,\N,Missing
R11-1037,P07-1031,0,\N,Missing
R11-1037,P07-1067,0,\N,Missing
R11-1037,D11-1099,0,\N,Missing
R11-1037,S10-1006,0,\N,Missing
R11-1037,P98-1067,0,\N,Missing
R11-1037,C98-1064,0,\N,Missing
R11-1037,P84-1055,0,\N,Missing
R11-1037,I08-7009,0,\N,Missing
R11-1037,E06-1015,0,\N,Missing
R11-1037,W10-1817,0,\N,Missing
R11-1037,W12-1622,0,\N,Missing
R11-1037,R11-1002,0,\N,Missing
R11-1037,P12-1084,1,\N,Missing
R11-1037,R11-1022,0,\N,Missing
R11-1037,R11-1004,0,\N,Missing
R11-1037,P04-3022,0,\N,Missing
R11-1037,W03-2605,0,\N,Missing
R11-1037,M92-1028,0,\N,Missing
R11-1037,W12-1614,0,\N,Missing
R11-1037,N07-1016,0,\N,Missing
R11-1037,W99-0611,0,\N,Missing
R11-1037,N04-1020,0,\N,Missing
R11-1037,W04-3205,0,\N,Missing
R11-1037,hutchinson-2004-mining,0,\N,Missing
R11-1037,D07-1010,0,\N,Missing
R11-1037,D10-1048,0,\N,Missing
R11-1037,M98-1001,0,\N,Missing
R11-1037,M93-1001,0,\N,Missing
R11-1037,M91-1022,0,\N,Missing
R11-1037,M93-1009,0,\N,Missing
R11-1037,C96-1079,0,\N,Missing
R11-1037,W06-0901,0,\N,Missing
R11-1037,M91-1003,0,\N,Missing
R11-1037,H05-1091,0,\N,Missing
R11-1037,P06-1104,0,\N,Missing
R11-1037,O98-4002,0,\N,Missing
R11-1037,P03-1023,0,\N,Missing
R11-1037,M98-1012,0,\N,Missing
R11-1037,L10-1000,0,\N,Missing
R11-1037,I05-2045,0,\N,Missing
R11-1037,M92-1038,0,\N,Missing
S07-1007,E03-1067,0,0.0617854,"attlesnake tastes like chicken.2 Again, the meat read1 This example was taken from the Berkely Master Metaphor list (Lakoff and Johnson, 1980) . 2 From now on, all examples in this paper are taken from the British National Corpus (BNC) (Burnard, 1995), but Ex. 23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, co"
S07-1007,E06-1042,0,0.014413,"inguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, and Vietnam have haunted Bi"
S07-1007,J96-2004,0,0.0214098,", respectively. Before metonymy annotation, samples that were not understood by the annotators because of insufficient context were removed from the datsets. In addition, a sample was also removed if the name extracted was a homonym not in the desired semantic class (for example Mr. Greenland when annotating locations).4 For those names that do have the semantic class location or organisation, metonymy annotation was performed, using the categories described in Section 2. All training set annotation was carried out independently by both organisers. Annotation was highly reliable with a kappa (Carletta, 1996) of 3 Apart from class-specific metonymic readings, some patterns seem to apply across classes to all names. In the SemEval dataset, we annotated two of them. Chevrolet is feminine because of its sound (it’s a longer word than Ford, has an open vowel at the end, connotes Frenchness). https://www.cia.gov/cia/publications/ factbook/index.html 4 Given that the task is not about standard Named Entity Recognition, we assume that the general semantic class of the name is already known. larity: coarse, medium, or fine, with an increasing number and specification of target classification categories, a"
S07-1007,W98-0720,0,0.475797,"Missing"
S07-1007,P92-1047,0,0.881369,"Missing"
S07-1007,W07-0103,0,0.0175944,"tudies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, and Vietnam have haunted Bill Clinton’s campaign. In Ex. 2"
S07-1007,W02-1027,1,0.847667,"es 3 and 4 report accuracy for all systems. 6 Table 5 provides a summary of the results with lowest, highest, and average accuracy and f-scores for each subtask and granularity level.7 The task seemed extremely difficult, with 2 of the 5 systems (up13,FUH) participating in the location task not beating the baseline. These two systems relied mainly on shallow features with limited or no use of external resources, thus suggesting that these features might only be of limited use for identifying metonymic shifts. The organisers themselves have come to similar conclusions in their own experiments (Markert and Nissim, 2002). The systems using syntactic/grammatical features (GYDER, UTD-HLT-CG, XRCE-M) could improve over the baseline whether using manual annotation or parsing. These systems also made heavy use of feature generalisation. Classification granularity had only a small effect on system performance. Only few of the fine-grained categories could be distinguished with reasonable success (see the fscores in Table 5). These include literal readings, and place-for-people, org-for-members, and org-forproduct metonymies, which are the most frequent categories (see Tables 1 and 2). Rarer metonymic targets were e"
S07-1007,J04-1002,0,0.0186404,"23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1"
S07-1007,P03-1008,1,0.870149,"(Burnard, 1995), but Ex. 23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happe"
S07-1007,E06-3009,0,0.0761844,"issim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, an"
S07-1007,P93-1012,0,0.777408,"Missing"
sharoff-etal-2010-web,C94-2174,0,\N,Missing
sharoff-etal-2010-web,J08-3001,0,\N,Missing
sharoff-etal-2010-web,J08-4004,0,\N,Missing
sharoff-etal-2010-web,P09-1076,0,\N,Missing
W02-1027,W98-0720,0,\N,Missing
W02-1027,markert-nissim-2002-towards,1,\N,Missing
W02-1027,N01-1009,0,\N,Missing
W02-1027,N01-1011,0,\N,Missing
W02-1027,J99-4002,0,\N,Missing
W02-1027,P95-1026,0,\N,Missing
W02-1027,W00-1326,0,\N,Missing
W02-1027,P92-1047,0,\N,Missing
W02-1027,P93-1012,0,\N,Missing
W02-1027,J96-2004,0,\N,Missing
W02-1027,P96-1006,0,\N,Missing
W02-1027,A00-2009,0,\N,Missing
W02-1027,W99-0613,0,\N,Missing
W03-1023,P95-1017,0,0.141436,"Missing"
W03-1023,P99-1008,0,0.0363221,"Missing"
W03-1023,P01-1009,0,0.101889,"Missing"
W03-1023,W03-2607,0,0.0573518,"in our corpus). 6 Related Work and Discussion Modjeska (2002) presented two hand-crafted algorithms, SAL and LEX, which resolve the anaphoric references of other-NPs on the basis of grammatical salience and lexical information from WordNet, respectively. In our own previous work (Markert et 15 If several antecedents have the highest M Iant they all get value “webfirst”. al., 2003) we presented a preliminary symbolic approach that uses Web counts and a recency-based tie-breaker for resolution of other-anaphora and bridging descriptions. (For another Web-based symbolic approach to bridging see (Bunescu, 2003).) The approach described in this paper is the first machine learning approach to other-anaphora. It is not directly comparable to the symbolic approaches above for two reasons. First, the approaches differ in the data and the evaluation metrics they used. Second, our algorithm does not yet constitute a full resolution procedure. As the classifier operates on the whole set of antecedent-anaphor pairs, more than one potential antecedent for each anaphor can be classified as “antecedent=yes”. This can be amended by e.g. incremental processing. Also, the classifier does not know that each other-N"
W03-1023,W99-0501,0,0.0612947,"Missing"
W03-1023,C92-2082,0,0.138269,"ture set is encouraging. However, the semantic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that"
W03-1023,W02-1030,0,0.0176733,"mpetition learning approach (Connolly et al., 1997). Finally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization. Our approach is the first ML approach to any kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescriptions, vague relations, etc.). Finally, these local dependencies al"
W03-1023,W03-2606,1,0.561576,"antecedent is not the flashy sports, but rather flashy sport shows, and thus an important piece of information is omitted. Alternatively, the antecedent is a content-for-container metonymy. Overall, our approach misclassifies antecedents whose relation to the other-anaphor is based on similarity, property-sharing, causality, or is constrained to a specific domain. These relation types are not — and perhaps should not be — encoded in WordNet. 5 Naive Bayes with the Web With its approximately 3033M pages8 the Web is the largest corpus available to the NLP community. Building on our approach in (Markert et al., 2003), we suggest using the Web as a knowledge source for anaphora resolution. In this paper, we show how to integrate Web counts for lexico-syntactic patterns specific to other-anaphora into our ML approach. 5.1 Basic Idea In the examples we consider, the relation between anaphor and antecedent is implicitly expressed, i.e., anaphor and antecedent do not stand in a structural relationship. However, they are linked by a strong semantic relation that is likely to be structurally explicitly expressed in other texts. We exploit this insight by adopting the following procedure: 1. In other-anaphora, a"
W03-1023,P02-1014,0,0.376737,"Missing"
W03-1023,poesio-etal-2002-acquiring,0,0.220534,"ntic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be used as a huge additional source of"
W03-1023,J01-4004,0,0.319187,"Missing"
W03-1023,W02-1040,0,0.0934807,"Missing"
W03-1023,S01-1035,0,\N,Missing
W08-1207,E06-1027,0,0.248493,"ly assigned affective labels (such as emotion, mood or behaviour). In a first step, they manually collect an affective word list and a list of synsets which contain at least one word in this word list. FineAn additional advantage for practical purposes is that subjectivity labels for senses add an additional layer of annotation to electronic lexica and can therefore increase their usability. As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivityambiguous words (such as positive). In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. A potential disadvantage for annotation at the sense level is that it is dependent on a lexical resource for sense distinctions and that an annotation scheme might have to take idiosyncracies of specific resources into account or, ideally, abstract away from them. In this paper, we investigate the reliability of manual subjectivity labeling of word senses. Specifically, we mark up subjectivity/attitude (subjective, objec"
W08-1207,esuli-sebastiani-2006-sentiwordnet,0,0.442217,"source developers. Then they automatically expand the lists by employing WordNet relations which they consider to reliably preserve the involved labels (such as similar-to, antonym, derived-from, pertains-to, and attribute). Our work differs from theirs in three respects. First, they focus on their semi-automatic procedure, whereas we are interested in human judgements. Second, they use a finer-grained set of affect labels. Third, they do not provide agreement results for their annotation. Similarly, SentiWordNet4 is a resource with automatically determined polarity of word senses in WordNet (Esuli and Sebastiani, 2006), produced via bootstrapping from a small manually determined seed set. Each synset has three scores assigned, representing the positive, negative and neutral score respectively. No human annotation study is conducted. There are only two human annotation studies on subjectivity of word senses as far as we are aware. Firstly, the Micro-WNOp corpus is a list of about 1000 WordNet synsets annotated by Cerini et al. (2007) for polarity. The raters manually assigned a triplet of numerical scores to each sense which represent the strength of positivity, negativity, and neutrality respectively. Their"
W08-1207,P97-1023,0,0.156079,"h bias in WordNet sense descriptions. Section 5 compares our annotation to the annotation of a different scheme, followed by conclusions and future work in Section 6. 2 Related Work Osgood et al. (1957) proposed semantic differential to measure the connotative meaning of concepts. They conducted a factor analysis of large collections of semantic differential scales and 2 3 43 Available at http://www.wjh.harvard.edu/ inquirer/ Available at http://www.grammatics.com/appraisal/ Polarity refers to positive or negative connotations associated with a word or sense. In contrast to other researchers (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005), we do not see polarity as a category that is dependent on prior subjectivity assignment and therefore applicable to subjective senses only. Whereas there is a dependency in that most subjective senses have a relatively clear polarity, polarity can be attached to objective words/senses as well. For example, tuberculosis is not subjective — it does not describe a private state, is objectively verifiable and would not cause a sentence containing it to carry an opinion, but it does carry negative associations for the vast majority of people. We allow for the polarity cate"
W08-1207,strapparava-valitutti-2004-wordnet,0,0.132439,"irer which contains 11788 words marked for polarity (1915 positive, 2291 negative and 7582 no-polarity words) seems to use a relatively ad hoc definition of polarity. Thus, for example amelioration is marked as nopolarity whereas improvement is marked as positive. The projects mentioned above center on subjectivity analysis on words and therefore are not good at dealing with subjectivity or polarity-ambiguous words as explained in the Introduction. Work that like us concentrates on word senses includes approaches where the subjectivity labels are automatically assigned such as WordNet-Affect (Strapparava and Valitutti, 2004), which is a subset of WordNet senses with semi-automatically assigned affective labels (such as emotion, mood or behaviour). In a first step, they manually collect an affective word list and a list of synsets which contain at least one word in this word list. FineAn additional advantage for practical purposes is that subjectivity labels for senses add an additional layer of annotation to electronic lexica and can therefore increase their usability. As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for su"
W08-1207,P05-1017,0,0.0807726,". Section 5 compares our annotation to the annotation of a different scheme, followed by conclusions and future work in Section 6. 2 Related Work Osgood et al. (1957) proposed semantic differential to measure the connotative meaning of concepts. They conducted a factor analysis of large collections of semantic differential scales and 2 3 43 Available at http://www.wjh.harvard.edu/ inquirer/ Available at http://www.grammatics.com/appraisal/ Polarity refers to positive or negative connotations associated with a word or sense. In contrast to other researchers (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005), we do not see polarity as a category that is dependent on prior subjectivity assignment and therefore applicable to subjective senses only. Whereas there is a dependency in that most subjective senses have a relatively clear polarity, polarity can be attached to objective words/senses as well. For example, tuberculosis is not subjective — it does not describe a private state, is objectively verifiable and would not cause a sentence containing it to carry an opinion, but it does carry negative associations for the vast majority of people. We allow for the polarity categories positive (P), neg"
W08-1207,P06-1134,0,0.382736,"Missing"
W13-0115,H05-1091,0,0.0453072,"excluded. Additionally, and similarly to ACE/MUC, they do not mark relations which rely on discourse knowledge and restrict annotations to sentence internal relations. Also, rather than annotating full texts they focus on single sentences extracted from web searches. Despite these important distinctions, the similarities mean that many of the methods used are relevant to entity instantiations, including the use of kernel methods. A range of work has applied tree kernels to the RE problem, applying kernels to shallow parses (Zelenko et al., 2003), dependency trees (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) and full constituency parses (Zhang et al., 2006; Zhou et al., 2007; Swampillai and Stevenson, 2011). Refinements include automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing th"
W13-0115,C10-1018,0,0.0135002,"ou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstructured features. Approaches include Bayesian networks (Roth and Yih, 2002), maximum entropy models (Kambhatla, 2004), Support Vector Machines (SVMs) (Zhou et al., 2005) and the inclusion of background knowledge (Chan and Roth, 2010; Sun et al., 2011). Bridging Anaphora. Bridging anaphora are those anaphora which require inference from the reader to bridge the gap between anaphor and antecedent (Clark, 1975). The classical example is in the form of meronymy, as in Example 122 but bridging anaphora can also be connected to their antecedent by set membership, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people"
W13-0115,P04-1054,0,0.0743131,"ed entities and pronouns are excluded. Additionally, and similarly to ACE/MUC, they do not mark relations which rely on discourse knowledge and restrict annotations to sentence internal relations. Also, rather than annotating full texts they focus on single sentences extracted from web searches. Despite these important distinctions, the similarities mean that many of the methods used are relevant to entity instantiations, including the use of kernel methods. A range of work has applied tree kernels to the RE problem, applying kernels to shallow parses (Zelenko et al., 2003), dependency trees (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) and full constituency parses (Zhang et al., 2006; Zhou et al., 2007; Swampillai and Stevenson, 2011). Refinements include automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the"
W13-0115,C92-2082,0,0.289249,"it relation in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Knowledge-based features. WordNet-based features which express synonymy/hyponymy between potentials members/subsets and sets. A feature which searches Freebase (Bollacker et al., 2008), for potential set member/subset NPs and compares the topics (loosely hyponyms) of matching entries to the potential set NP. A Point-wise Mutual Information feature derived from Google hit counts, based on the notion that the pattern “X and other Y ”, where X is a potential set member or subset and Y is a potential set, indicates hyponymy (Hearst, 1992; Markert and Nissim, 2005). A feature which establishes whether the animacy of the two NPs matches. 4.2 Tree Kernels The unstructured features discussed in Section 4.1 are presented to the machine learner as a vector. Tree features are instead presented as structured data, and the learner works directly with this structured form. We used two trees — Shortest Path Enclosed Tree (SPET) and Shortest Path Tree (SPT), which have been previously used for RE (Zhang et al., 2006; Swampillai and Stevenson, 2011). We also included two variations in the lexicalisation of these trees; full delexicalisati"
W13-0115,W09-2415,0,0.0717303,"Missing"
W13-0115,J93-2004,0,0.0462204,"it might cost the world as much as $100 billion between now and the year 2000 to convert to other coolants, foaming agents and solvents and to redesign equipment for these less efficient substitutes, the Montreal Protocol’s legions of supporters say it is worth it. They insist that CFCs are damaging the earth’s stratospheric ozone layer, which screens out some of the sun’s ultraviolet rays. Hence, as they see it, if something isn’t done earthlings will become ever more subject to sunburn and skin cancer. 1 All examples in this paper are taken from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) unless stated otherwise, and are occasionally abbreviated. Sets are highlighted in bold and members or subsets are shown in italics. (4) Peter Teagan, a specialist in heat transfer, is running a project at Arthur D. Little Inc., of Cambridge, Mass., to find alternative technologies that will allow industry to eliminate CFCs. [. . . ] Or so it must seem to Jackie Mason, the veteran Jewish comedian appearing in a new ABC sitcom airing on Tuesday nights (9:30-10 p.m. EDT). Not only is Mr. Mason the star of ”Chicken Soup,” he’s also the inheritor of a comedic tradition dating back to ”Duck Soup,”"
W13-0115,P12-1084,1,0.822311,"13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning"
W13-0115,W03-2606,1,0.538972,"m. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning subtypes of the mediated category, including set membership. Rahman and Ng (2012) use the Switchboard corpus (Nissim et al., 2004), which includes a restricted version of set membership, and employ a"
W13-0115,J05-3004,1,0.807238,"the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Knowledge-based features. WordNet-based features which express synonymy/hyponymy between potentials members/subsets and sets. A feature which searches Freebase (Bollacker et al., 2008), for potential set member/subset NPs and compares the topics (loosely hyponyms) of matching entries to the potential set NP. A Point-wise Mutual Information feature derived from Google hit counts, based on the notion that the pattern “X and other Y ”, where X is a potential set member or subset and Y is a potential set, indicates hyponymy (Hearst, 1992; Markert and Nissim, 2005). A feature which establishes whether the animacy of the two NPs matches. 4.2 Tree Kernels The unstructured features discussed in Section 4.1 are presented to the machine learner as a vector. Tree features are instead presented as structured data, and the learner works directly with this structured form. We used two trees — Shortest Path Enclosed Tree (SPET) and Shortest Path Tree (SPT), which have been previously used for RE (Zhang et al., 2006; Swampillai and Stevenson, 2011). We also included two variations in the lexicalisation of these trees; full delexicalisation, in which all terminal n"
W13-0115,R11-1037,1,0.645438,"ed, and then a member or subset of this set is introduced. We present the first reliably annotated intrasentential entity instantiation corpus, along with an extension to the intersentential annotations in McKinlay and Markert (2011). We then apply tree kernels to both inter- and intrasentential entity instantiations, showing comparable results to an extensive set of unstructured features. The combination of tree kernels and unstructured features leads to significant improvements over either method in isolation. 1 Introduction In a previous paper, we define an entity instantiation as follows (McKinlay and Markert, 2011): An Entity Instantiation is a non-coreferent entity relationship, where a set of entities is mentioned, and then a member or subset is introduced. Examples 1 and 2 show a set membership instantiation and a subset instantiation, respectively1 . (1) a. b. The two lawmakers sparred in a highly personal fashion, violating usual Senate decorum. Their tone was good-natured, with Mr. Packwood saying he intended to offer [. . . ] (2) a. To the extent that the primary duty of personal staff involves local benefit-seeking, this indicates that political philosophy leads congressional Republicans to pay"
W13-0115,E06-1015,0,0.0272309,"Missing"
W13-0115,X93-1003,0,0.148753,"Missing"
W13-0115,nissim-etal-2004-annotation,0,0.0371199,"hip, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, w"
W13-0115,W03-2605,0,0.0294865,"by set membership, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning"
W13-0115,P04-1019,0,0.0159129,"y high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning subtypes of the mediated category, including set membership. Rahman and Ng (2012) use the Switchboard corpus (Nissim et al., 2004), which includes a restricted version of set membership, and employ a feature set based on"
W13-0115,W97-1301,0,0.0146155,"re amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning subtypes of the mediated category, including set membership. Rahman and Ng (2012) use the Switchboard corp"
W13-0115,prasad-etal-2008-penn,0,0.0471782,"mber of mentions of the entity in the document. Syntactic features. Syntactic parallelism and pre- and post-modification of each NP. The modification type includes values that represent apposition, conjunction, pre modification and bare nouns. Contextual features. The Levin class (Levin, 1993) of each NP’s head verb, as well as the verb itself, whether each NP is in a quotation, and an approximation of the discourse relations present in the two sentences by identifying likely discourse connectives and mapping them to their most frequent explicit relation in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Knowledge-based features. WordNet-based features which express synonymy/hyponymy between potentials members/subsets and sets. A feature which searches Freebase (Bollacker et al., 2008), for potential set member/subset NPs and compares the topics (loosely hyponyms) of matching entries to the potential set NP. A Point-wise Mutual Information feature derived from Google hit counts, based on the notion that the pattern “X and other Y ”, where X is a potential set member or subset and Y is a potential set, indicates hyponymy (Hearst, 1992; Markert and Nissim, 2005). A feature which establishes wh"
W13-0115,E12-1081,0,0.0209432,"es (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning subtypes of the mediated category, including set membership. Rahman and Ng (2012) use the Switchboard corpus (Nissim et al., 2004), which includes a restricted version of set membership, and employ a feature set based on unigrams, markables and binary features based on hand-coded rules. Markert et al. (2012) learn fine-grained IS on a portion of OntoNotes corpus. They couple local features with a collective learning model, using links between instances based upon syntactic parent-child and precedence relations. 3 Annotation, Agreement and Corpus Study We created a substantial corpus annotated for both inter- and intrasentential entity instantiations. Our initial corpus stu"
W13-0115,P10-1005,0,0.0215733,". . ] Despite these problems, we still achieved substantial agreement. This is likely due to the genre of the texts involved; the financial-based newswire texts annotated tend to include many sets, subsets and members which are concrete, such as companies, countries and people. Applying this scheme to a genre of texts that contains more generics and less straightforwardly defined NPs, for example a philosophy text, could lead to a more problematic annotation. One possible way to improve agreement would be to introduce a layer of annotation that identified generic NPs, such as that employed by Reiter and Frank (2010), and prevent these generic NPs from participating in instantiations. 3.2 Intersentential Annotation We follow our previous annotation method (McKinlay and Markert, 2011), automatically identifying plural and singular NPs, and separately displaying plural-plural NP pairs for subset annotation and pluralsingular NP pairs for set member annotation. We also remove NPs that are appositions or predicates, and include the option to mark NPs as “Not a mention”, for excluding instances of non-referential it, idiomatic NPs and generic pronouns. The task of the annotator is then to indicate whether each"
W13-0115,C02-1151,0,0.0556388,"e automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstructured features. Approaches include Bayesian networks (Roth and Yih, 2002), maximum entropy models (Kambhatla, 2004), Support Vector Machines (SVMs) (Zhou et al., 2005) and the inclusion of background knowledge (Chan and Roth, 2010; Sun et al., 2011). Bridging Anaphora. Bridging anaphora are those anaphora which require inference from the reader to bridge the gap between anaphor and antecedent (Clark, 1975). The classical example is in the form of meronymy, as in Example 122 but bridging anaphora can also be connected to their antecedent by set membership, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8"
W13-0115,P11-1053,0,0.0132692,"pillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstructured features. Approaches include Bayesian networks (Roth and Yih, 2002), maximum entropy models (Kambhatla, 2004), Support Vector Machines (SVMs) (Zhou et al., 2005) and the inclusion of background knowledge (Chan and Roth, 2010; Sun et al., 2011). Bridging Anaphora. Bridging anaphora are those anaphora which require inference from the reader to bridge the gap between anaphor and antecedent (Clark, 1975). The classical example is in the form of meronymy, as in Example 122 but bridging anaphora can also be connected to their antecedent by set membership, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woma"
W13-0115,R11-1004,0,0.0910297,"course knowledge and restrict annotations to sentence internal relations. Also, rather than annotating full texts they focus on single sentences extracted from web searches. Despite these important distinctions, the similarities mean that many of the methods used are relevant to entity instantiations, including the use of kernel methods. A range of work has applied tree kernels to the RE problem, applying kernels to shallow parses (Zelenko et al., 2003), dependency trees (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) and full constituency parses (Zhang et al., 2006; Zhou et al., 2007; Swampillai and Stevenson, 2011). Refinements include automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstructured features. Approaches include"
W13-0115,J00-4003,0,0.114328,"have non-anaphoric set members and subsets. (12) I looked into the room. The ceiling was very high. (13) I met two people yesterday. The woman told me a story. 2 Examples 12 and 13 are from Clark (1975). The anaphor is in bold, the antecedent is in italics. Theoretical linguistic literature has discussed set membership and subset bridging (Clark, 1975; Prince, 1981), and the phenomenon has been annotated in at least three corpora (Poesio, 2003; Nissim et al., 2004; Markert et al., 2012). Early computational approaches either used hand-crafted rules (Markert et al., 1996; Poesio et al., 1997; Vieira and Poesio, 2000) or focused solely on meronymy-based bridging (Markert et al., 2003; Poesio et al., 2004). More recent work has focused on learning the information status (IS) of an entity, rather than identifying its antecedent. The IS of an entity represents whether it is new to the reader, old because it is coreferent to a prior mention, or can be mediated from prior text, often by bridging. Most relevant to our work is the learning of fine-grained IS, which involves learning subtypes of the mediated category, including set membership. Rahman and Ng (2012) use the Switchboard corpus (Nissim et al., 2004),"
W13-0115,P06-1104,0,0.199454,"do not mark relations which rely on discourse knowledge and restrict annotations to sentence internal relations. Also, rather than annotating full texts they focus on single sentences extracted from web searches. Despite these important distinctions, the similarities mean that many of the methods used are relevant to entity instantiations, including the use of kernel methods. A range of work has applied tree kernels to the RE problem, applying kernels to shallow parses (Zelenko et al., 2003), dependency trees (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) and full constituency parses (Zhang et al., 2006; Zhou et al., 2007; Swampillai and Stevenson, 2011). Refinements include automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has"
W13-0115,P05-1053,0,0.0392348,"007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstructured features. Approaches include Bayesian networks (Roth and Yih, 2002), maximum entropy models (Kambhatla, 2004), Support Vector Machines (SVMs) (Zhou et al., 2005) and the inclusion of background knowledge (Chan and Roth, 2010; Sun et al., 2011). Bridging Anaphora. Bridging anaphora are those anaphora which require inference from the reader to bridge the gap between anaphor and antecedent (Clark, 1975). The classical example is in the form of meronymy, as in Example 122 but bridging anaphora can also be connected to their antecedent by set membership, such as Example 13 (and Example 6). However, not all entity instantiations are bridged — Examples 1, 2, 5, 7, 8, 9, and 11 are amongst those that have non-anaphoric set members and subsets. (12) I looked i"
W13-0115,D07-1076,0,0.0176415,"s which rely on discourse knowledge and restrict annotations to sentence internal relations. Also, rather than annotating full texts they focus on single sentences extracted from web searches. Despite these important distinctions, the similarities mean that many of the methods used are relevant to entity instantiations, including the use of kernel methods. A range of work has applied tree kernels to the RE problem, applying kernels to shallow parses (Zelenko et al., 2003), dependency trees (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) and full constituency parses (Zhang et al., 2006; Zhou et al., 2007; Swampillai and Stevenson, 2011). Refinements include automatically deciding the portion of the tree required to learn the relation (Zhou et al., 2007) and combining unstructured features with tree kernels (Zhou et al., 2007; Swampillai and Stevenson, 2011). Almost all RE research considers solely intrasentential relations. Swampillai and Stevenson (2011), however, apply tree kernels to the problem of intersentential RE. As a constituency parse tree pertains only to a single sentence, they join the two sentences containing the entities under a new ROOT node. Other work has focused on unstruct"
W13-0115,S10-1006,0,\N,Missing
W14-3706,asheghi-etal-2014-designing,1,0.912506,"Editorial Conversational Forum (forum) Biography (bio) Frequently Asked Questions (faq) Review Story Interview Number of web pages websites 304 264 299 244 292 231 332 330 310 280 242 201 266 184 185 288 264 299 215 209 142 116 127 69 106 190 140 179 24 154 # of pages from the same website max min med 9 1 1 9 23 15 8 12 11 11 15 8 15 38 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 7 1 Fleiss’s κ 0.858 0.713 0.953 0.812 0.830 0.871 0.971 0.801 0.877 0.951 0.905 0.915 0.880 0.953 0.905 Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after the genre names. corpus: the original text and the main text corpora. First, we converted web pages to plain text by removing HTML markup using the KrdWrd tool (Steger and Stemle, 2009). This resulted in the original text corpus which contains individual web pages with all the textual elements present on them. Moreover, in order to investigate the influence of boilerplate parts (e.g. advertisements, headers, footers, template materials, navigation menus and lists of links) of the web pages on genre"
W14-3706,J08-1001,0,0.262085,"cal and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respectively. In the normalized fre3.3 Results and Discussion Table 4 shows the result of the different feature sets listed in the previous section on both the original text and the main text corpora. At first glance, we see"
W14-3706,D08-1020,0,0.0325347,"ti-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respecti"
W14-3706,C94-2174,0,0.427039,"logs, search results could be more beneficial. With the aim of enhancing search engines, AGI has attracted a lot of attention (see Section 2). In this paper, we investigate two important open questions in AGI. The first question is: what set 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections"
W14-3706,sharoff-etal-2010-web,1,0.685924,"ushin Rezapour Asheghi Katja Markert School of Modern School of Computing L3S Research Center Languages and Cultures University of Leeds Leibniz Universit¨at Hannover University of Leeds scs5nra@leeds.ac.uk and School of Computing s.sharoff@leeds.ac.uk University of Leeds markert@l3s.de Abstract of features produces the best result in genre classification on the web? The drawbacks of existing genre-annotated web corpora (low inter-coder agreement; false correlations between topic and genre classes) resulted in researchers’ doubt on the outcomes of classification models based on these corpora (Sharoff et al., 2010). Therefore, in order to answer this question, we perform genre classification with a wide range of features on a reliable and source diverse genre-annotated web corpus. The second question that we investigate in this paper is: could we exploit the graph structure of the web to increase genre classification accuracy? With the aim of learning from the neighbouring web pages, we investigated the performance of a semi-supervised graph-based model, which is a novel technique in genre classification. The remainder of this paper is structured as follows. After reviewing related work in Section 2, we"
W14-3706,P97-1005,0,0.395836,"Missing"
W14-3706,C00-2117,0,0.0501058,"application of AGI could be in Information Retrieval. If a user could use the search engine to retrieve web pages from a specific genre such as news articles, reviews or blogs, search results could be more beneficial. With the aim of enhancing search engines, AGI has attracted a lot of attention (see Section 2). In this paper, we investigate two important open questions in AGI. The first question is: what set 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and McDowell, 2012), random graph"
W14-3706,P03-1054,0,0.00997698,"that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser. For web pages for which the justext tool produced empty files, we treated these features as missing values. Moreover, we used the Brown Coherence Toolkit 6 to construct the entity grid for each web page and computed the probability of each entity transition type. This tool needs the parsed version of the text as an input. Therefore, for web pages for which the justext tool produced empt"
W14-3706,N03-1033,0,0.00443519,"tp://code.google.com/p/justext/ Genre php com edu blog shop instruction recipe news editorial forum bio faq review story interview Number of web pages in corpora Original text Main text 304 221 264 190 299 191 244 242 292 221 231 229 332 243 330 320 310 307 280 251 242 242 201 160 266 262 184 184 185 183 quency representation of a document, the value for each feature is the frequency of that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser."
W14-3706,J11-2004,0,\N,Missing
W19-2101,P15-2072,0,0.509626,"of six G20 countries, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more of"
W19-2101,D16-1148,0,0.135425,"in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The data"
W19-2101,S16-1003,0,0.260767,"Missing"
W19-2101,S17-2088,0,0.0471267,"ata. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Russia South-Africa Turkey United States Adj. tweets 281 290 121 227 128 192 Diff. w/ expert 1 0.0"
W19-2101,W10-0214,0,0.0509992,"oni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not un"
W19-2101,D18-1393,0,0.228381,"cly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content is emotionally charg"
W19-2101,L16-1591,0,0.531323,"s, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content"
W19-2101,P15-1157,0,0.0269095,"fer to the conversation partner or as a form of reference to refer to a third party. For reasons described in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than per"
W19-2101,N13-1132,0,0.0262985,"7). We expect it to better capture differences between tweets which are neutral in tone but reflect differently on the president, such as ‘Trump trailing in primaries’ vs ‘Jobs market improving under Trump’. Crucially, the prompt also allows annotators to give different ratings to ‘President Trump visits France’ and ‘Trump visits France’. As in Card et al. (2015), the perspective is anchored to that of a proponent of the target in order to combat the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017). After annotation we used Multi-Annotator Competence Estimation (MACE) (Hovy et al., 2013) to identify and remove the least reliable annotators. We collected an additional two judgH0 Variation in naming and stance are not related. H1 Naming primarily downplays or emphasises the president’s status. Therefore, formality of naming is positively related to stance. H2 Naming primarily conveys the degree of solidarity with the president. Therefore, formality of naming is negatively related to stance. Table 4 gives examples of tweets which can be interpreted to support either H1 or H2, or to support the existence of alternative, context-specific functions of naming, such as sarcasm. We gr"
W19-2101,P11-1016,0,0.0645378,"case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Rus"
