2017.lilt-15.1,W13-5413,1,0.660566,"rated material to a verb can license the presence of an argument or adjunct in the syntax. For example, the incorporated factors of butter, leg, and dance, respectively, are realized syntactically, in the sentences below. (53) a. Mary buttered her bread with margarine. b. John kicked the ball with his left leg. c. Mary and John danced a polka. This follows from the shadow argument principle in (44), where, in each case, the expressed argument is more informative than the lexically encoded factor.9 8 There could be a range of modifiers but adverbs lend themselves readily to this analysis. 9 In Batiukova and Pustejovsky (2013), informativeness is used as a criterion to Lexical Factorization and Syntactic Behavior / 19 Examining the PMI data in Table 2, we notice the same phenomenon: namely, the time factor encoded in the verb glance is actually acting to select specializations of temporal adverbials, rather than block them. This is the behavior we observed in (53). In other words, the verb glance can be viewed as having promoted the incorporated temporal factor as a shadow argument, as illustrated below. (54) glance =df zs ∶time moment y∶phys x∶human[glance(x, y, z)] On this analysis, the temporal adverbials presen"
2017.lilt-15.1,J90-1003,0,0.223568,"properties of how factors encode a word’s meaning and how this impacts the subsequent syntactic behavior of the word in a sentential context; that is, there is a correlation between the information encoded in a verb v, and the expression of factors for a particular situational frame in which it is used. It has long been recognized that correlations between lexical items and the properties of selection can be linked to patterns of linguistic behavior in data (Harris (1957), Firth (1961)), and this discussion relates directly to work done in corpus and distributional linguistics, starting with Church and Hanks (1990), Resnik (1993), Hindle and Rooth (1993), up to the present. Recall from previous discussion that, within a situational frame, a semantic factor, fi , can be either lexically encoded, syntactically realized, or not expressed at all. In fact, it might be expected that lexical and syntactic factorization is inversely correlated in an utterance: that is, if a verb already incorporates a semantic factor, fi , then it is less likely to be expressed syntactically. Likewise, if the verb does not incorporate Lexical Factorization and Syntactic Behavior / 15 such a factor, it might be more common to se"
2017.lilt-15.1,J93-1005,0,0.29397,"’s meaning and how this impacts the subsequent syntactic behavior of the word in a sentential context; that is, there is a correlation between the information encoded in a verb v, and the expression of factors for a particular situational frame in which it is used. It has long been recognized that correlations between lexical items and the properties of selection can be linked to patterns of linguistic behavior in data (Harris (1957), Firth (1961)), and this discussion relates directly to work done in corpus and distributional linguistics, starting with Church and Hanks (1990), Resnik (1993), Hindle and Rooth (1993), up to the present. Recall from previous discussion that, within a situational frame, a semantic factor, fi , can be either lexically encoded, syntactically realized, or not expressed at all. In fact, it might be expected that lexical and syntactic factorization is inversely correlated in an utterance: that is, if a verb already incorporates a semantic factor, fi , then it is less likely to be expressed syntactically. Likewise, if the verb does not incorporate Lexical Factorization and Syntactic Behavior / 15 such a factor, it might be more common to see it appear in the syntax. We refer to t"
2017.lilt-15.1,2017.lilt-15.2,1,0.873866,"paper, we examine the correlation between lexical semantics and the syntactic realization of the di↵erent components of a word’s meaning in natural language. More specifically, we will explore the effect that lexical factorization in verb semantics has on the suppression or expression of semantic features within the sentence. Factorization was a common analytic tool employed in early generative linguistic approaches to lexical decomposition, and continues to play a role in contemporary semantics, in various guises and modified forms. Building on the unpublished analysis of verbs of seeing in Joshi (1972), we argue here that the significance of lexical factorization is twofold: first, current models of verb meaning owe much of their insight to factor-based theories of meaning; secondly, the factorization properties of a lexical item appear to influence, both directly and indirectly, the possible syntactic expressibility of arguments and adjuncts in sentence composition. We argue that this information can be used to compute what we call the factor expression likelihood (FEL) associated with a verb in a sentence. This is the likelihood that the overt syntactic expression of a factor will cooccur"
2017.lilt-15.1,J01-3003,0,0.0219966,"tentionally accidentally PMI results The pointwise mutual information (PMI) compares the probability of a specific factor, fi , and the verb, v, showing up together (jointly), with the probabilities of seeing fi and v independently. As in Church and Hanks (1990), where it is argued “word association norms” can be measured in terms of PMI given enough linguistic data, the association between factor expression and lexical choice is also potentially captured with such a metric. While this is a relatively simple measure, and there have been many suggested improvements and alternatives to it (cf. (Merlo and Stevenson, 2001, Rumshisky, 2008, Pustejovsky et al., 2004), it will suffice to demonstrate our thesis for the remainder of this section. Let us return to the example of the verbs kill and murder first mentioned above. We measure their relative PMI scores for two specific lexicalizations of the intentionality factor, as calculated over a large corpus of English.7 If there is a real correlation between a verb and an adverb of intention, then the PMI score will be significantly larger than 0 (� 0). If, on the other hand, there is no relationship of interest between them, then PMI ≈ 0. What we find in Table 1 i"
2017.lilt-15.1,W04-1908,1,0.480776,"ntwise mutual information (PMI) compares the probability of a specific factor, fi , and the verb, v, showing up together (jointly), with the probabilities of seeing fi and v independently. As in Church and Hanks (1990), where it is argued “word association norms” can be measured in terms of PMI given enough linguistic data, the association between factor expression and lexical choice is also potentially captured with such a metric. While this is a relatively simple measure, and there have been many suggested improvements and alternatives to it (cf. (Merlo and Stevenson, 2001, Rumshisky, 2008, Pustejovsky et al., 2004), it will suffice to demonstrate our thesis for the remainder of this section. Let us return to the example of the verbs kill and murder first mentioned above. We measure their relative PMI scores for two specific lexicalizations of the intentionality factor, as calculated over a large corpus of English.7 If there is a real correlation between a verb and an adverb of intention, then the PMI score will be significantly larger than 0 (� 0). If, on the other hand, there is no relationship of interest between them, then PMI ≈ 0. What we find in Table 1 is that, as intuitively predicted, there appe"
2020.coling-main.373,J12-2006,0,0.0157986,"annotating modality in text. These annotation schemes often address modality and negation together as extra-propositional aspects of meaning, focusing on the tasks of detecting key linguistic markers and mapping their scope (Saurı et al., 2006; Morante and Daelemans, 2012). These tasks can then be leveraged to identify and analyze related concepts such as subjectivity, hedging, evidentiality, uncertainty, committed belief, and factuality (Morante and Sporleder, 2012). Automatic tagging of modality and negation and detection of related concepts has received little, though promising, attention (Baker et al., 2012; Prabhakaran et al., 2012; Marasovi´c and Frank, 2016). The detection of the related concept of hedging (and its scope) was the focus of the CoNLL 2010 Shared Task (Farkas et al., 2010). In the context of human-robot dialogue, modality and related concepts provide the basis for assessing speaker beliefs, commitments, and attitudes, thereby fostering understanding and coherent interaction. For example, the manner in which a speaker employs modal information can be used to assess trustworthiness (Su et al., 2010); this is important both for the human to trust the robot and work collaboratively,"
2020.coling-main.373,W13-2322,0,0.0166809,"th (Condoravdi, 2001). In so doing, it designates how the expression of interest relates to the common ground between the speakers. There are two possible values for TI: (i) Local TI signifies that the utterance applies only to the immediate context; and (ii) Global TI signifies that the utterance adds meaningful, new information to the common ground that speakers should be aware of throughout the dialogue. A good diagnostic for this value is to ask how the subsequent response or action contributes to the understanding of the 3 Dial-AMR augments standard Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to map unconstrained language in natural human instructions to appropriate action specifications in the robot’s limited repertoire. 4226 Utterance Level I: Value Level II: Interpretation Temporal Index I think you are more familiar with objects than I am. (speaker = robot) Can you manipulate objects? (speaker = human) I need your help to decide which objects are important. (speaker = robot) How far would you like me to turn? (speaker = robot) I can move closer to take a picture. (speaker = robot) Please be aware of lag time (speaker = robot) epistemic ability deontic bouletic teleological imp"
2020.coling-main.373,2020.lrec-1.86,1,0.76525,"human-robot dialogue with the goal to make the interpretation of such expressions easily automated in the future. We hypothesize that certain readings and scope preferences for modal operators are more salient in human-robot dialogue because of the unique makeup of the common ground (Poesio, 1993). We provide a mapping from formal semantic theories of modality related to participant beliefs and updates of the common ground (Portner, 2009), to a practical model of speech acts that translates into robot action for search and navigation task-oriented dialogue and an automated NLU and NLG system (Bonial et al., 2020). This mapping is formalized in an annotation scheme in which the use of modal expressions is mapped to their effect in dialogue, providing a model for the robot to learn the meaning of modal expressions (Chai et al., 2018). Our annotation task reveals surprisingly high inter-annotator agreement for a complex scheme; results indicate that our data is highly repetitive in the natural language used, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4222 Proceedings of the 28th International Confere"
2020.coling-main.373,W10-3001,0,0.0213534,"Missing"
2020.coling-main.373,W17-6919,1,0.840651,"establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007). The common ground represents the mutual knowledge, beliefs, and assumptions of the participants that result from co-situatedness, co-perception, and co-intent. Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018). Specifically, this entails outlining three key aspects of common ground interpretation: (i) the situated grounding of expressions in context; (ii) an interpretation of the expression contextualized to the dynamics of the discourse; and (iii) an appreciation of the actions and consequences associated with objects in the environment. Here, we address (ii) first, before moving on to (i) and (iii). 5.2 Dynamic Interpretation of Modal Expressions An account of how modal expressions are used in discourse needs to capture their command-force “context change potential” (CCP), u"
2020.coling-main.373,2020.dmr-1.1,1,0.681525,"nship particular to the human-robot context, in which the human is endowed with more authority; the question or goal under discussion (Ginzburg, 1995); and other properties of the common ground, described in 5.3. The reverse mapping can be formalized, too: ability, bouletic, deontic, and teleological modals can all map on to request speech acts (Table 8), though their logical representations will differ. As Bonial et al. (2020) use AMR to represent their speech act inventory (footnote 3), we plan to extend our work by translating AMR into first-order-logic for simpler mapping to robot action (Lai et al., 2020). Logical differences between modal categories will then be captured in our FOL translations and assist the robot in understanding the mappings between modal expressions and speech acts. 5.3 Situated Grounding and Modal Meaning As noted in Section 2.2, dialogues in SCOUT were collected to mimic the setting of a low-bandwidth reconnaissance or search-and-navigation operation. A participant verbally instructs a robot at a remote location, guiding the robot to explore a physical space. The sensors and video camera on-board the robot populate a map as it moves, enabling it to describe that environ"
2020.coling-main.373,P18-4016,0,0.297596,"be an autonomous robot to complete search and navigation tasks. The domain testbed for this data was collaborative exploration in a low-bandwidth environment, mimicking the conditions of a reconnaissance or search-and-navigation operation. For data collection, two “wizard” experimenters controlled the robot’s dialogue processing and robot navigation capabilities behind the scenes. This design permitted participants to instruct the robot without imposing artificial restrictions on the language used. As more data was collected, increasing levels of automated dialogue processing were introduced (Lukin et al., 2018a). We discuss the impact of further design details in Sections 4 and 5. Table 1 shows an example SCOUT interaction. The dialogues are divided into two conversational floors, each involving two interlocutors: the left conversational floor consists of dialogue between the participant and the dialogue manager (DM), and the right consists of dialogue between the DM and the robot navigator (RN). The participant and RN never speak directly to or hear each other; instead, the DM acts as an intermediary passing communication between the participant and the RN. Of interest to our work, the left conver"
2020.coling-main.373,W18-5012,0,0.251783,"be an autonomous robot to complete search and navigation tasks. The domain testbed for this data was collaborative exploration in a low-bandwidth environment, mimicking the conditions of a reconnaissance or search-and-navigation operation. For data collection, two “wizard” experimenters controlled the robot’s dialogue processing and robot navigation capabilities behind the scenes. This design permitted participants to instruct the robot without imposing artificial restrictions on the language used. As more data was collected, increasing levels of automated dialogue processing were introduced (Lukin et al., 2018a). We discuss the impact of further design details in Sections 4 and 5. Table 1 shows an example SCOUT interaction. The dialogues are divided into two conversational floors, each involving two interlocutors: the left conversational floor consists of dialogue between the participant and the dialogue manager (DM), and the right consists of dialogue between the DM and the robot navigator (RN). The participant and RN never speak directly to or hear each other; instead, the DM acts as an intermediary passing communication between the participant and the RN. Of interest to our work, the left conver"
2020.coling-main.373,W16-1613,0,0.0564101,"Missing"
2020.coling-main.373,W17-2808,0,0.128131,"tics (e.g. a question to the robot, “Can you prevent fire?”) and the interaction of scopal operators (e.g. a robot assertion, “I probably cannot fit there.”). Our work is one step towards aligning participant perceptions of respective environments and both discourse and real-world events. 2.2 Human-Robot Dialogue The data we annotate comes from the Situated Corpus of Understanding Transactions (SCOUT), a collection of dialogues from the robot navigation domain.1 SCOUT was created to explore the natural diversity of communication strategies in situated human-robot dialogue (Marge et al., 2016; Marge et al., 2017). Data collection efforts leveraged “Wizard-of-Oz” experiment design (Riek, 2012), in which participants directed what they believed to be an autonomous robot to complete search and navigation tasks. The domain testbed for this data was collaborative exploration in a low-bandwidth environment, mimicking the conditions of a reconnaissance or search-and-navigation operation. For data collection, two “wizard” experimenters controlled the robot’s dialogue processing and robot navigation capabilities behind the scenes. This design permitted participants to instruct the robot without imposing artifi"
2020.coling-main.373,W12-3807,0,0.0702998,"Missing"
2020.coling-main.373,S15-1009,0,0.0184961,"odality and related concepts provide the basis for assessing speaker beliefs, commitments, and attitudes, thereby fostering understanding and coherent interaction. For example, the manner in which a speaker employs modal information can be used to assess trustworthiness (Su et al., 2010); this is important both for the human to trust the robot and work collaboratively, and for the robot to assess whether or not it should accept human instruction. Additionally, modal information allows both dialogue participants to assess the factuality of events and propositions (Saur´ı and Pustejovsky, 2009; Prabhakaran et al., 2015). Notably this is a complex process that requires the understanding of both fine-grained lexical semantics (e.g. a question to the robot, “Can you prevent fire?”) and the interaction of scopal operators (e.g. a robot assertion, “I probably cannot fit there.”). Our work is one step towards aligning participant perceptions of respective environments and both discourse and real-world events. 2.2 Human-Robot Dialogue The data we annotate comes from the Situated Corpus of Understanding Transactions (SCOUT), a collection of dialogues from the robot navigation domain.1 SCOUT was created to explore th"
2020.coling-main.373,W17-7103,1,0.779686,"g to the local context, or establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007). The common ground represents the mutual knowledge, beliefs, and assumptions of the participants that result from co-situatedness, co-perception, and co-intent. Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018). Specifically, this entails outlining three key aspects of common ground interpretation: (i) the situated grounding of expressions in context; (ii) an interpretation of the expression contextualized to the dynamics of the discourse; and (iii) an appreciation of the actions and consequences associated with objects in the environment. Here, we address (ii) first, before moving on to (i) and (iii). 5.2 Dynamic Interpretation of Modal Expressions An account of how modal expressions are used in discourse needs to capture their command-force “context"
2020.coling-main.373,W19-3303,1,0.841056,"rator. In contrast, we also find utterances where only the proposition is explicit, and the operator implicit (“45 degrees”, “Picture”) These phenomena fall under the umbrella of underspecification, an enduring challenge of creating meaningful natural language representation that must nevertheless be actionable in settings like HRI. Finally, corrected or disjoint scope (“Can you turn 90 degrees left... I mean right”) and coordination (“Can you go back inside and take a picture”) also pose challenges to scope in 4228 dialogue, especially in the context of sentence-based meaning representation (Pustejovsky et al., 2019). Finally, we note some utterances that our annotation scheme alone cannot account for. These include conditional utterances such as “If you can turn around and take a photo so I can have a clear picture” interpreted as commands. We note for now that these utterances exemplify our ambiguity challenge: the modal can has both ability and teleological meanings, while the utterance can function as both a request (given its conditional nature) and a command (given that it is uttered by the human). 5 Towards a Formal Theory of Modality in Human-Robot Dialogue Here, we sketch the beginnings of a form"
2020.coling-main.373,W13-5401,1,0.708509,"teraction, exploration, and manipulation. These are modally contingent actions that a situation presents to an agent by virtue of the objects it encounters. The contextual meaning for many modal expressions will be interpreted relative to such object knowledge. For these reasons, it is useful to think of objects as providing habitats, which are situational contexts 4230 or environments conditioning the object’s affordances, which may be either “Gibsonian” affordances (Gibson et al., 1982) or “Telic” affordances (Pustejovsky, 1995). A habitat specifies how an object typically occupies a space (Pustejovsky, 2013). Affordances are used as attached behaviors, which the object either facilitates by its geometry (Gibsonian) or purposes for which it is intended to be used (Telic). For example, a Gibsonian affordance for [[CUP]] is “grasp,” while its Telic affordance is “drink from.” Similarly, in SCOUT’s environment, a “doorway” affords passage to another room, unless it is blocked by an object or closed. Hence, when asked: “Can you go through the doorway?”, the modal force is taken as a query over its situational (or local) ability, given what the speaker already knows about the robot’s navigation capabil"
2020.coling-main.373,2020.lrec-1.28,0,0.0190061,"naturally occurring modal expressions in task-based human-robot dialogue and to provide information about the use and interpretation of these expressions in context. In addition to modal expressions, we annotate negation and quantification for the purpose of detecting scope relations and meaning in dialogue more broadly in future work. Our approach acknowledges both the semantic richness of how modals are assigned interpretations in context (Rubinstein et al., 2013), as well as the situational grounding of the role an expression is playing in the task-oriented dialogue (Sarathy et al., 2019; Roque et al., 2020; Bonial et al., 2020). For this reason, we have developed a two-level annotation scheme that separates out the basic modal value of an expression from its eventual interpretation within a context. We introduce a number of constraints to help pinpoint the interpretation of modal expressions in dialogue and to make annotation feasible for non-experts. First, we reduce the number of modality type values from Rubinstein et al. (2013) from seven to six, eliminating the circumstantial and combined bouletic/teleological values and adding a value for imperative. Our adaptation forces annotators to se"
2020.coling-main.373,W13-0306,0,0.0308983,"Missing"
2020.coling-main.373,W10-2102,0,0.0413319,"detection of related concepts has received little, though promising, attention (Baker et al., 2012; Prabhakaran et al., 2012; Marasovi´c and Frank, 2016). The detection of the related concept of hedging (and its scope) was the focus of the CoNLL 2010 Shared Task (Farkas et al., 2010). In the context of human-robot dialogue, modality and related concepts provide the basis for assessing speaker beliefs, commitments, and attitudes, thereby fostering understanding and coherent interaction. For example, the manner in which a speaker employs modal information can be used to assess trustworthiness (Su et al., 2010); this is important both for the human to trust the robot and work collaboratively, and for the robot to assess whether or not it should accept human instruction. Additionally, modal information allows both dialogue participants to assess the factuality of events and propositions (Saur´ı and Pustejovsky, 2009; Prabhakaran et al., 2015). Notably this is a complex process that requires the understanding of both fine-grained lexical semantics (e.g. a question to the robot, “Can you prevent fire?”) and the interaction of scopal operators (e.g. a robot assertion, “I probably cannot fit there.”). Ou"
2020.coling-main.373,L18-1017,0,0.0209976,"avigation instruction initiated by the participant (#1), its clarification (#2-4), subsequent translation to a simplified form (Dialogue Manager (DM) to Robot Navigator (RN), #6), and acknowledgement of instructions (#5, 8) and execution by the RN (#7). comprised of several potential modal expressions: “move” as an imperative; “not sure” as a negated epistemic; and “can” expressing a circumstantial ability. All SCOUT speech data (collected from the participant and RN) are transcribed and time-aligned with text messages produced by the DM. SCOUT also includes annotations of dialogue structure (Traum et al., 2018) that allow for the characterization of distinct information states by way of sets of participants, participant roles, turn-taking and floor-holding, and other factors (Traum and Larsson, 2003). In total, SCOUT contains over 80 hours of human-robot dialogue from 83 participants. 2.3 Modal Expressions in Dialogue As we are interested in modal meaning in context, we take a broad approach to the modal expressions we investigate, including modal verbs, attitude verbs, and imperatives. Most theories of modality in natural language take Kratzer (1981) as a starting point. Modal statements are interp"
2020.dmr-1.1,D15-1198,0,0.461939,"on Designing Meaning Representations, pages 1–12 Barcelona, Spain (Online), December 13, 2020 such representations cannot be used directly for drawing valid inferences such as textual entailment or contradiction. Quantifiers, for example, are not given any special semantics, being represented as simple modifiers. Furthermore, AMR does not represent quantifier scope in any way, leaving open the question in (1) of how many talks were listened to in the room. Several proposals have been put forth to provide AMRs with more meaningful logical forms, driven both by parsing and theoretical concerns. Artzi et al. (2015) present a CCG-based grammar induction technique for AMR parsing; a learned joint model integrates classical lambda calculus and underspecification to improve search for compositional and non-compositional elements of AMR, respectively. Semantically speaking, each AMR variable gets its own lambda term (scoped as low as possible) and each AMR role becomes a binary predicate that is applied to those variables. More strictly theoretical work also provides systematic mappings from AMR into logic. Bos (2016) defines a syntax of AMR and provides a recursive translation function into first-order logi"
2020.dmr-1.1,W13-2322,0,0.68337,"from AMR to first order logic using continuation semantics, which allows us to capture the semantic context of an expression in the form of an argument. This is a natural extension of AMR’s original design principles, allowing us to easily model basic projection phenomena such as quantification and negation as well as complex phenomena such as bound variables and donkey anaphora. 1 Introduction Abstract Meaning Representation (AMR) is a general-purpose meaning representation that has become popular for its simple structure, ease of annotation and available corpora, and overall expressiveness (Banarescu et al., 2013; Knight et al., 2019). Specifically, AMR focuses on representing the predicative core of a sentence as an intuitive representation of the semantics of a sentence, advantageous for parsing and matching algorithms. As an example, the AMR for the English sentence “Everyone in the room listened to a talk.” is given in example (1), in three equivalent formats. (1) a. Everyone in the room listened to a talk. b. c. listen-01 ARG0 person ARG1 talk location mod all (l / listen-01 :ARG0 (p / person :mod (a / all) :location (r / room)) :ARG1 (t / talk)) room d. instance(l, listen-01) ∧ instance(p, perso"
2020.dmr-1.1,2020.lrec-1.86,1,0.740784,"9). Our method also does not modify standard AMR nodes, leaves, or edges, which allows us to utilize existing AMR corpora. Formalizing basic projection phenomena in AMR paves the way for more comprehensive meaning representation, allowing simple translation of complex phenomena such as negative raising that evidence speaker belief and intent. Another interesting advantage of the continuation-passing style semantics introduced here for sentence or utterance level expressions, is the natural way it can be extended to model how AMRs have recently 9 been used in human-robot interaction dialogues (Bonial et al., 2020). The model presented here can easily adopt the dynamic semantics of “discourse moves as continuations”, as introduced by de Groote (2001), and extended by Asher and Pogodalla (2010). As previously mentioned, in a continuation semantics, the order of application determines the relative scope of a predicate and each of its arguments. In this paper, we take the out-going roles of a predicate to be ordered, and the order of application to be the order the arguments are written in the AMR. In contrast, Pustejovsky et al. (2019) attach an optional scope node to the predicate, that explicitly marks"
2020.dmr-1.1,W19-3302,0,0.34919,", o) ∧ ARG1(o, d)) → ∃l.love-01(l) ∧ ARG1(l, d) ∧ ARG0(l, f ) 7 Discussion Continuations are a natural solution to the challenge of translating AMR to FOL: by treating the continuation of an expression as an associated argument to the relation associated with that predicate, we maintain AMR’s focus on the predicative core while still allowing valid inferences from projection phenomena to fall out. Our method avoids the pitfalls of underspecification, allowing us to prioritize the most plausible interpretation of a scope ambiguity yet also to capture less common interpretations when necessary (Bos and Abzianidze, 2019). Our method also does not modify standard AMR nodes, leaves, or edges, which allows us to utilize existing AMR corpora. Formalizing basic projection phenomena in AMR paves the way for more comprehensive meaning representation, allowing simple translation of complex phenomena such as negative raising that evidence speaker belief and intent. Another interesting advantage of the continuation-passing style semantics introduced here for sentence or utterance level expressions, is the natural way it can be extended to model how AMRs have recently 9 been used in human-robot interaction dialogues (Bo"
2020.dmr-1.1,J16-3006,0,0.153766,"th more meaningful logical forms, driven both by parsing and theoretical concerns. Artzi et al. (2015) present a CCG-based grammar induction technique for AMR parsing; a learned joint model integrates classical lambda calculus and underspecification to improve search for compositional and non-compositional elements of AMR, respectively. Semantically speaking, each AMR variable gets its own lambda term (scoped as low as possible) and each AMR role becomes a binary predicate that is applied to those variables. More strictly theoretical work also provides systematic mappings from AMR into logic. Bos (2016) defines a syntax of AMR and provides a recursive translation function into first-order logic (FOL) that is able to handle scope phenomena. In a similar vein, Stabler (2017) adds tense and number information into AMR leaves and makes explicit quantificational determiners as a basis for a mapping to a higher order, dynamic logic. Finally, Pustejovsky et al. (2019) add scope itself to AMR graph structure, representing it relationally in the form of a scope node that attaches to the predicative core when necessary. Adding to the discussion about the expressive capacity of AMR, Crouch and Kalouli"
2020.dmr-1.1,S18-2013,0,0.266896,"o logic. Bos (2016) defines a syntax of AMR and provides a recursive translation function into first-order logic (FOL) that is able to handle scope phenomena. In a similar vein, Stabler (2017) adds tense and number information into AMR leaves and makes explicit quantificational determiners as a basis for a mapping to a higher order, dynamic logic. Finally, Pustejovsky et al. (2019) add scope itself to AMR graph structure, representing it relationally in the form of a scope node that attaches to the predicative core when necessary. Adding to the discussion about the expressive capacity of AMR, Crouch and Kalouli (2018) argue that any purely graphical representation of meaning is unable to capture basic natural language semantics such as Boolean operators. Instead, the authors propose layering graphs based on the Resource Description Framework (RDF) (Schreiber and Raimond, 2014) and named graphs (Carroll et al., 2005) such that the interaction between different layers can capture Booleans (such as negation and disjunction), modals and irrealis contexts, distributivity and quantifier scope, co-reference, and sense selection (Kalouli and Crouch, 2018). However, in this work, we will show how we can give AMR a"
2020.dmr-1.1,W18-4912,1,0.877136,"ng these defaults to generate logical forms from AMRs in the future. Although our semantics is able to handle many different kinds of scope phenomena, much more work is needed to capture the plethora of meanings possible in natural language. Extending the translation we present here to logical connectives, conditionals, modality, non-declarative sentences that depend on notions of common ground, and other types of quantifiers (e.g. “most”) are all possible next steps. In recent years, there have been a number of proposals to extend AMR to handle definiteness (Stabler, 2017), tense and aspect (Donatelli et al., 2018), and discourse relations (O’Gorman et al., 2018). We look forward to seeing how these proposals and others can be integrated with our semantics. 8 Conclusion In this paper, we presented a continuation semantics for AMR, building off previous work in translating AMRs to logical forms. We showed that our semantics is powerful enough to handle a wide variety of scope phenomena, including quantification, negation, bound variables, and donkey anaphora. Code for this paper, combining a PENMAN parser based on Goodman (2020), with a computational implementation of our translation function, is availab"
2020.dmr-1.1,2020.acl-demos.35,0,0.0720901,"Missing"
2020.dmr-1.1,W18-1304,0,0.248029,"ing to the discussion about the expressive capacity of AMR, Crouch and Kalouli (2018) argue that any purely graphical representation of meaning is unable to capture basic natural language semantics such as Boolean operators. Instead, the authors propose layering graphs based on the Resource Description Framework (RDF) (Schreiber and Raimond, 2014) and named graphs (Carroll et al., 2005) such that the interaction between different layers can capture Booleans (such as negation and disjunction), modals and irrealis contexts, distributivity and quantifier scope, co-reference, and sense selection (Kalouli and Crouch, 2018). However, in this work, we will show how we can give AMR a semantics expressive enough to handle at least some of these phenomena, namely quantification and negation, without needing to introduce additional graph structure. Here, we build on previous work and present a translation function from AMR to FOL using continuations. A continuation of an expression encodes surrounding contextual information relevant to its interpretation. More specifically, the continuation hypothesis assumes that some natural language expressions denote functions that take their own semantic context as an argument ("
2020.dmr-1.1,C18-1313,0,0.0490395,"Missing"
2020.dmr-1.1,W19-3303,1,0.760815,"y speaking, each AMR variable gets its own lambda term (scoped as low as possible) and each AMR role becomes a binary predicate that is applied to those variables. More strictly theoretical work also provides systematic mappings from AMR into logic. Bos (2016) defines a syntax of AMR and provides a recursive translation function into first-order logic (FOL) that is able to handle scope phenomena. In a similar vein, Stabler (2017) adds tense and number information into AMR leaves and makes explicit quantificational determiners as a basis for a mapping to a higher order, dynamic logic. Finally, Pustejovsky et al. (2019) add scope itself to AMR graph structure, representing it relationally in the form of a scope node that attaches to the predicative core when necessary. Adding to the discussion about the expressive capacity of AMR, Crouch and Kalouli (2018) argue that any purely graphical representation of meaning is unable to capture basic natural language semantics such as Boolean operators. Instead, the authors propose layering graphs based on the Resource Description Framework (RDF) (Schreiber and Raimond, 2014) and named graphs (Carroll et al., 2005) such that the interaction between different layers can"
2020.emnlp-tutorials.5,D19-1215,1,0.883324,"Missing"
2020.emnlp-tutorials.5,kordjamshidi-etal-2010-spatial,1,0.743733,"Missing"
2020.emnlp-tutorials.5,P19-1655,0,0.0284336,"ng the question of reasoning, we (a) point out the role of qualitative and quantitative formal representations in helping spatial reasoning based on natural language and the possibility of learning such representations from data to support compositionality and inference (Hudson and Manning, 2018; Hu et al., 2017); and (b) examine how continuous representations contribute to supporting reasoning and alternative hypothesis formation in learning (Krishnaswamy et al., 2019). We point to the cutting edge research that shows the influence of explicit representation of spatial entities and concepts (Hu et al., 2019; Liu et al., 2019). • Spatial Reasoning – Overview on natural language and visual reasoning tasks and data – Modeling compositionality and spatial reasoning in (Deep) learning models • Downstream tasks – Spatial concepts in dialogue systems – Spatial reasoning for QA and VQA – HRI, navigation and way-finding instructions – Corpus-based GIS systems 3 Prerequisites and reading list Familiarity with machine learning and natural language processing will be helpful for this tutorial. Our selected reading list is as follows. 29 • Qualitative spatial representation and reasoning. Anthony G. Cohn, an"
2020.emnlp-tutorials.5,P06-1131,0,0.0866842,"hidi and Moens, 2015) and extraction that is driven by various target tasks and applications. We discuss machine learning models including structured output prediction models, deep learning architectures and probabilistic graphical models that have been used in the related work. 2 Outline The tutorial will cover the following syllabus: • Spatial Representations Finally, we overview the usage of spatial semantics by various downstream tasks and killer applications including language grounding, navigation, self-driving cars, robotics (Tellex et al., 2011; Kollar et al., 2010), dialogue systems (Kelleher and Kruijff, 2006) and human machine interaction, and geographical information systems and knowledge graphs (Stock et al., 2013; Mai et al., 2020). Spatial semantics is very closely connected and relevant to visualization of natural language and grounding language into perception, central to dealing with configurations in the physical world and motivating a combination of vision and language for richer spatial understanding. The related tasks include: text-to-scene conversion; image captioning; spatial and visual question answering; and spatial understanding in multimodal settings (Rahgooy et al., 2018) for rob"
2020.emnlp-tutorials.5,N18-2124,1,0.831025,"(Kelleher and Kruijff, 2006) and human machine interaction, and geographical information systems and knowledge graphs (Stock et al., 2013; Mai et al., 2020). Spatial semantics is very closely connected and relevant to visualization of natural language and grounding language into perception, central to dealing with configurations in the physical world and motivating a combination of vision and language for richer spatial understanding. The related tasks include: text-to-scene conversion; image captioning; spatial and visual question answering; and spatial understanding in multimodal settings (Rahgooy et al., 2018) for robotics and navigation tasks and language grounding (Thomason et al., 2018). – Linguistic corpora and semantic annotations – Spatial knowledge representation and spatial calculi models – Distributed representations • Spatial Information Extraction – Spatial entity and relation extraction – Spatial ontology population – Considering domain knowledge and pragmatics in spatial extractions • Spatial Semantic Grounding – Combining vision and language (symbolic and multimodal embeddings) – Capturing spatial common sense – Grounding language in 2D and 3D physical worlds – Generating referring ex"
2020.emnlp-tutorials.5,P17-2034,0,0.0288127,"semantics (Cohn et al., 1997). Spatial language meaning representation includes research related to cognitive and linguistically motivated spatial semantic representations, spatial knowledge representation and spatial ontologies, qualitative and quantitative representation models used for formal meaning representation, and various spatial annotation schema and efforts for creating specialized corpora. We discuss various datasets that either focus on spatial annotations or downstream tasks that need spatial language learning and reasoning. Particularly, natural language visual reasoning data (Suhr et al., 2017, 2018). Moreover, continuous meaning representations for spatial concepts is another aspect to be highlighted in the tutorial, e.g., (Collell Talleda and Moens, 2018; Collell Talleda et al., 2018; DeruytThis tutorial provides an overview over the cutting edge research on spatial language understanding. However, we cover some background material from various perspectives given that ACL community has not paid enough attention, in the last two decades, to this topic. There are a few emerging research work very recently looking back into the importance of spatial language in various NLP tasks. On"
2020.lrec-1.684,bird-etal-2008-acl,0,0.0522279,"discuss each replication step in detail and report various challenges. 5.1. Hardware and Software Specification We used a single workstation equipped with 36-core Intel Xeon CPU (2.10GHz), 128GB RAM, and 4 × Geforce TITAN Xp (12 GB VRAM) GPUs, running Redhat Linux 7.4. The system was developed on python 3.6 and tensorflow 2.0. All software dependencies and external libraries are listed in the code repository. 5.2. Data Collection In addition to the datasets provided as a part of the shared task (D1.1, D1.2), in the RHZ paper, the authors combined the ACL Anthology Reference Corpus (ACL-ARC) (Bird et al., 2008) with abstracts from papers posted on arXiv 7 cs.CL papers to train word embeddings (later used for vector representation of text data) and to train a language model (later used for validating synthetically generated data). When attempting to recreate the data set that the authors described, we encountered some challenges. First off, there are multiple versions of the ACL-ARC and the RHZ does not specify which version was used for training or from which source the data was gathered. Instead, the paper reported an approximate number of tokens obtained from the ACL-ARC to be 90 million. The ACLA"
2020.lrec-1.684,P13-1166,0,0.292449,"Missing"
2020.lrec-1.684,S18-1111,0,0.027571,"Missing"
2020.lrec-1.684,S15-2097,0,0.0699367,"Missing"
2020.lrec-1.684,W11-2123,0,0.0169637,"es (non-relations) from T1test sets were not added to T2training set, as that would only worsen the class imbalance in training data. We also synthesized additional training data (Dsyn) using the strategy described in the RHZ, and used them for both T1and T2tranining. That is, by combining entities from test data and tokens-between from training data. Goldstandard labels were also taken from training data. Then we filtered the synthetic sentences using a language model trained with the text data obtained in section 5.2. in training word embeddings, using the KenLM language modeling toolkit13 (Heafield, 2011) (5-gram and the default smoothing options). We found that using the threshold in the RHZ paper (−21 log probability) resulted in far more synthetic instances (∼33,000) than those reported by the RHZ (61). Thus, instead of using a threshold, we chose to rank the resulting instances based on language model probability and keep the top N instances to match the number of synthetic instances in the RHZ (N = 61). For T2, because, after generating negative instances, the class distribution is largely skewed toward NONE, ETH13 version 3.9.2 5573 https://github.com/kpu/kenlm, version 96d303c DS3Lab sy"
2020.lrec-1.684,D18-1349,0,0.0210533,"erent needs. (Ronzano et al., 2016) initialized the Scientific Knowledge Miner Project that focuses on citation characterization and scientific article recommendation. Summarization of scientific papers has also been well studied. The information from citations plays an important role in the summarization task. (Mei and Zhai, 2008) made use of citation context to generate impact-based summaries. (Klampfl et al., 2016) aimed to summarize the relevant text from a reference paper based on another document that cites this reference paper. Abstracts also encode critical information about a paper. (Jin and Szolovits, 2018) presented a new neural approach to sentence classification in medical scientific abstracts utilizing context information. SemEval-2017 Task 10 (Augenstein et al., 2017) called for papers to work on the extraction of keyphrases and relations from scientific papers, which is closely related to our reproduction work. Recent study also shows the effectiveness to use neural approaches for relation extraction. Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) are commonly used neural architectures for the task (Lee et al., 2017; Li and Mao, 2019; Zhang et al., 2018). 3. Task and"
2020.lrec-1.684,D14-1181,0,0.00509677,"tioned in section 5.2.. The size of our text corpus for word embedding training is 88,070,855 tokens. In addition to using pre-trained word embeddings, we also created dense representations for POS tags and relative positions of each token in a sequence. The initial values for all the embeddings except pre-trained word2vec are randomly initialized from a uniform distribution ranging from -1.0 to 1.0. In practice, word tokens that cannot be looked up in the pre-trained embedding table will also be randomly initialized. CNN Classifier The RHZ stated that the structure of their CNN model follows Kim (2014) and Collobert et al. (2011), hence we decided to reproduce the multichannel CNN model by Kim (2014) that was originally for sentence classification, which improved on the state-of-the-art of several text classification tasks. It composed of multiple embedding layers, which is followed by multiple concurrent convolutional layers with different kernel sizes, and max-over-time pooling layers. The intermediate output from pooling layers is concatenated and flattened before fed into a fully connected layer with dropout and softmax layer. Kim (2014) also experimented with different variants of the"
2020.lrec-1.684,W16-1514,0,0.0191344,"earch engine makes it difficult for researchers to acquire high quality information. Various approaches have been applied to extract fine-grained semantic information from scientific text to meet different needs. (Ronzano et al., 2016) initialized the Scientific Knowledge Miner Project that focuses on citation characterization and scientific article recommendation. Summarization of scientific papers has also been well studied. The information from citations plays an important role in the summarization task. (Mei and Zhai, 2008) made use of citation context to generate impact-based summaries. (Klampfl et al., 2016) aimed to summarize the relevant text from a reference paper based on another document that cites this reference paper. Abstracts also encode critical information about a paper. (Jin and Szolovits, 2018) presented a new neural approach to sentence classification in medical scientific abstracts utilizing context information. SemEval-2017 Task 10 (Augenstein et al., 2017) called for papers to work on the extraction of keyphrases and relations from scientific papers, which is closely related to our reproduction work. Recent study also shows the effectiveness to use neural approaches for relation"
2020.lrec-1.684,S17-2171,0,0.0441717,"Missing"
2020.lrec-1.684,P14-5010,0,0.00264388,"tructure for further cleaning and processing. Then, next step is to split sentences. We first experimented with the spaCy (Honnibal and Montani, 2017)10 sentence segmenter, but found that many sentences were being erroneously split on periods and commas found in abbreviations such as ”e.g.” and ”et al.” These errors persisted across experiments with different spaCy pre-trained neural models11 including en core web lg, en trf bertbaseuncased lg, ac10 cessed at 2019-10-24 11 5572 https://spacy.io/ https://spacy.io/models/en-starters and en trf xlnetbasecased lg. Next we tried using the CoreNLP (Manning et al., 2014) 12 sentence splitter module, and found that it performed too conservatively for our data. Ultimately, we chose to use the CoreNLP sentence splitter module configured with a additional regex to split some sentences that were missed by the segmenter. There were at least three instances in which a sentence was incorrectly segmented due to the presence of punctuation within words. We suspect that these errors are present due to errors in original typing or errors from digitization old publications. As POS tags used as features in the neural model, we also run the input data through the CoreNLP PO"
2020.lrec-1.684,P08-1093,0,0.0372241,"from traditional web search engines are too broad. The limited power of the standard search engine makes it difficult for researchers to acquire high quality information. Various approaches have been applied to extract fine-grained semantic information from scientific text to meet different needs. (Ronzano et al., 2016) initialized the Scientific Knowledge Miner Project that focuses on citation characterization and scientific article recommendation. Summarization of scientific papers has also been well studied. The information from citations plays an important role in the summarization task. (Mei and Zhai, 2008) made use of citation context to generate impact-based summaries. (Klampfl et al., 2016) aimed to summarize the relevant text from a reference paper based on another document that cites this reference paper. Abstracts also encode critical information about a paper. (Jin and Szolovits, 2018) presented a new neural approach to sentence classification in medical scientific abstracts utilizing context information. SemEval-2017 Task 10 (Augenstein et al., 2017) called for papers to work on the extraction of keyphrases and relations from scientific papers, which is closely related to our reproductio"
2020.lrec-1.684,R19-1089,0,0.0481385,"Missing"
2020.lrec-1.684,W16-1505,0,0.0152086,"More recently, using advanced semantically-driven NLP technologies to solve various problems in science as well as the humanities is becoming more and more popular (G´abor et al., 2018). Relation extraction and classification allows for enhanced discoverabilty of scientific literature. The results from traditional web search engines are too broad. The limited power of the standard search engine makes it difficult for researchers to acquire high quality information. Various approaches have been applied to extract fine-grained semantic information from scientific text to meet different needs. (Ronzano et al., 2016) initialized the Scientific Knowledge Miner Project that focuses on citation characterization and scientific article recommendation. Summarization of scientific papers has also been well studied. The information from citations plays an important role in the summarization task. (Mei and Zhai, 2008) made use of citation context to generate impact-based summaries. (Klampfl et al., 2016) aimed to summarize the relevant text from a reference paper based on another document that cites this reference paper. Abstracts also encode critical information about a paper. (Jin and Szolovits, 2018) presented"
2020.lrec-1.684,S18-1112,0,0.403673,"20)4 is dedicated to motivating the spread of scientific work on reproduction as a type of a collaborative shared task. The selected tasks for REPROLANG are about reproducing results of a set of published articles that focus on different language technologies, including lexical processing, sentence processing, text processing, applications and language resources. Our work targets Task C.1 of REPROLANG 2020, Relation extraction and classification. In this paper, we aim to reproduce an automatic scientific relation extraction and classification system (henceforth ETH-DS3Lab system) described in Rotsztejn et al. (2018) (henceforth RHZ). The ETH-DS3Lab system was originally submitted to SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers (SE18T7) (G´abor et al., 2018) and the reported results ranked first place for 3 subtasks out of 4. Throughout this paper, we will describe our steps of re-implementing the system and replicating the experiments and results in the most detailed manner. In doing so, we will also show difficulties we faced partly due to the nature of the task itself, and partly due to the under-specified or missing details in the RHZ And finally, we’ll end"
2020.lrec-1.725,N12-1070,0,0.0332486,"train a generation model that can be deployed “live” in a multimodal interaction where the situation encountered at any given time may not cleanly map to a situation from the EMRE dataset. The process of maximizing the contextually-salient information content provided by the linguistic component of the multimodal referring expression, and by extension by all modalities including iconic gesture and action, could be handled by a composing and constructing expressions with a probabilistic grammar. Existing work in multimodal grammars, particularly on gesture and speech (cf. Alahverdzhieva et al. (2012), Alahverdzhieva et al. (2017)) often focuses on timing and aligning the gesture and speech components using edgebased constraints to generated a syntax tree of both speech Figure 6: Action command using gesture-language ensemble • Multimodal dialogue parsing. Given a situation where both gestures and natural language can indicate both objects and actions or events, common ground structures should be helpful in extracting both object and action information separately from each modality and in disambiguating the information provided by one modality with information from the other (see Fig. 6)."
2020.lrec-1.725,W18-6402,0,0.0311998,"c of study in computational linguistics and natural language processing since at least the mid-1990s (Johnston et al., 1997), but has seen increased interest from the CL/NLP communities in recent years. This has been due to a number of factors, including the increase in processing power; the availability of large datasets of text, images, and video; the rise of depth sensors (e.g., Microsoft Kinect); and the availability of GPUs for deep model training. This has resulted in a number of new datasets and approaches to cross-modal linking (Yatskar et al., 2016; Goyal et al., 2017), shared tasks (Barrault et al., 2018), and grounding tasks (Beinborn et al., 2018; Zhou et al., 2018). The most common modalities under study in the CL/NLP communities are text, audio/speech, and images/video, but “modality” can in principle refer to any channel of information. Therefore, multi-channel transmission of information can be separated by channel into the particular information transmitted by each modality (i.e., objects depicted in images with their descriptions in text, or spoken demonstratives with aligned deixis via a gesture). Such disjunct mechanisms allow us to package, quantify, measure, and order our experienc"
2020.lrec-1.725,C18-1197,0,0.0310182,"atural language processing since at least the mid-1990s (Johnston et al., 1997), but has seen increased interest from the CL/NLP communities in recent years. This has been due to a number of factors, including the increase in processing power; the availability of large datasets of text, images, and video; the rise of depth sensors (e.g., Microsoft Kinect); and the availability of GPUs for deep model training. This has resulted in a number of new datasets and approaches to cross-modal linking (Yatskar et al., 2016; Goyal et al., 2017), shared tasks (Barrault et al., 2018), and grounding tasks (Beinborn et al., 2018; Zhou et al., 2018). The most common modalities under study in the CL/NLP communities are text, audio/speech, and images/video, but “modality” can in principle refer to any channel of information. Therefore, multi-channel transmission of information can be separated by channel into the particular information transmitted by each modality (i.e., objects depicted in images with their descriptions in text, or spoken demonstratives with aligned deixis via a gesture). Such disjunct mechanisms allow us to package, quantify, measure, and order our experiences, creating rich conceptual reifications an"
2020.lrec-1.725,P18-1182,0,0.0414735,"Missing"
2020.lrec-1.725,P97-1036,0,0.51371,"esture and language, conditioned on the introduction of content into the common ground between the (computational) speaker and (human) viewer, and demonstrate how these formal features can contribute to training better models to predict viewer judgment of referring expressions, and potentially to the generation of more natural and informative referring expressions. Keywords: multimodality, interfaces, referring expressions, semantics, common ground 1. Introduction Multimodality has been a topic of study in computational linguistics and natural language processing since at least the mid-1990s (Johnston et al., 1997), but has seen increased interest from the CL/NLP communities in recent years. This has been due to a number of factors, including the increase in processing power; the availability of large datasets of text, images, and video; the rise of depth sensors (e.g., Microsoft Kinect); and the availability of GPUs for deep model training. This has resulted in a number of new datasets and approaches to cross-modal linking (Yatskar et al., 2016; Goyal et al., 2017), shared tasks (Barrault et al., 2018), and grounding tasks (Beinborn et al., 2018; Zhou et al., 2018). The most common modalities under stu"
2020.lrec-1.725,W19-8660,0,0.0222882,"l linguistics (Claassen, 1992; Bortfeld and Brennan, 1997; Van Der Sluis and Krahmer, 2001; Krahmer and van der Sluis, 2003; Funakoshi et al., 2004; Viethen and Dale, 2008). Despite this, there has been comparatively little research from the community into the ways that multiple modalities interact during real-time communication and how to replicate such structures computationally. Most work in this area originated in the psychology and cognitive science communities, and has been explored in related communities such as robotics (Petit et al., 2012; Matuszek et al., 2014; Whitney et al., 2016; Kasenberg et al., 2019), but has direct relevance to computational language understanding and generation. McNeill (2000) argues that thought is multimodal, and that the combinatorics of gesture do not correspond to the syntagmatic values that emerge from the combinatorics of speech. Quek et al. (2002), holds that speech and gesture are coexpressive and processed partially independently, and therefore complement each other. Thus, if interlocutors agree that the meaning of a gesture in a description and the meaning of accompanying speech share the same referent, this must be tested to see if 1) the gesture and speech"
2020.lrec-1.725,J12-1006,0,0.076976,"Missing"
2020.lrec-1.725,W03-2307,0,0.229623,"Missing"
2020.lrec-1.725,C16-2012,1,0.861655,"ntiating two similarly colored objects (e.g., by use of “other”), α introduces that she knows the same attribute predicates over b1 and b2 but that b1 and b2 are distinct: Cα = (S |[“other”, b1s , b2s ] ∈ S ∧ b1s = b2s ) → Kα JAtt(b1 ∧ b2 )KM ∧ Kα b1 6= b2 • By distinguishing demonstratives in an ensemble RE, α introduces that she is meaningfully distinguishing between “near” and “far” regions of sf c, the table surface: Cα = (S, G |G = P ointg ∧ “this” ∈ S) → Kα Jnear(sf c)K 6= Jf ar(sf c)KM The visualizations in the EMRE dataset are produced using the VoxSim event/agent simulation platform (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b), which employs the VoxML modeling language, enabling object and event visualization semantics (Pustejovsky and Krishnaswamy, 2016). Because a simulator is an extension of a model checker (Pustejovsky and Krishnaswamy, 2014), a simulation can be evaluated formally. Because a simulator requires numerical parameter values to run (Davis and Marcus, 2016), it can be evaluated quantitatively. Values extracted from the simulator and collated in the dataset may be either real numbers or vector values (e.g., distance values or coordinates) or symbolic (e.g., obje"
2020.lrec-1.725,W19-0507,1,0.879408,"), it is the introduction of such information into the discourse that creates the “shared situated reference” (Pustejovsky et al., 2017) between them, and the introduction of particular information into the common ground may be more or less informative depending not only on the prior contents of the common ground but also the modality through which the new information is introduced. The task is then to assess this, either quantitatively or formally. In this paper, we present an analysis of the common ground structures presented in a dataset of Embodied Multimodal Referring Expressions (EMRE) (Krishnaswamy and Pustejovsky, 2019a). These are references to definite objects performed by an avatar in a simulated world using gesture, language, or both. The appropriateness of each referring technique depicted was then evaluated by annotators on Amazon Mechanical Turk. The virtual environment allows saving a number of quantitative and qualitative parameter values for each depicted referring technique, allowing further analysis, including for our purposes here, of the introduction of elements between the avatar and the annotators (as proxy for the human interlocutors), and the subsequent update to the common ground caused b"
2020.lrec-1.725,J07-2004,0,0.0734263,"Missing"
2020.lrec-1.725,S14-1014,1,0.806966,"distinguishing demonstratives in an ensemble RE, α introduces that she is meaningfully distinguishing between “near” and “far” regions of sf c, the table surface: Cα = (S, G |G = P ointg ∧ “this” ∈ S) → Kα Jnear(sf c)K 6= Jf ar(sf c)KM The visualizations in the EMRE dataset are produced using the VoxSim event/agent simulation platform (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b), which employs the VoxML modeling language, enabling object and event visualization semantics (Pustejovsky and Krishnaswamy, 2016). Because a simulator is an extension of a model checker (Pustejovsky and Krishnaswamy, 2014), a simulation can be evaluated formally. Because a simulator requires numerical parameter values to run (Davis and Marcus, 2016), it can be evaluated quantitatively. Values extracted from the simulator and collated in the dataset may be either real numbers or vector values (e.g., distance values or coordinates) or symbolic (e.g., object labels or qualitative attributes). Thus, we can conduct ablation tests on the effects of formal, symbolic, and quantitative features on the predictive model trained over data extracted from a simulation. Each of these features and others can be extracted from"
2020.lrec-1.725,L16-1730,1,0.831976,"t: Cα = (S |[“other”, b1s , b2s ] ∈ S ∧ b1s = b2s ) → Kα JAtt(b1 ∧ b2 )KM ∧ Kα b1 6= b2 • By distinguishing demonstratives in an ensemble RE, α introduces that she is meaningfully distinguishing between “near” and “far” regions of sf c, the table surface: Cα = (S, G |G = P ointg ∧ “this” ∈ S) → Kα Jnear(sf c)K 6= Jf ar(sf c)KM The visualizations in the EMRE dataset are produced using the VoxSim event/agent simulation platform (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b), which employs the VoxML modeling language, enabling object and event visualization semantics (Pustejovsky and Krishnaswamy, 2016). Because a simulator is an extension of a model checker (Pustejovsky and Krishnaswamy, 2014), a simulation can be evaluated formally. Because a simulator requires numerical parameter values to run (Davis and Marcus, 2016), it can be evaluated quantitatively. Values extracted from the simulator and collated in the dataset may be either real numbers or vector values (e.g., distance values or coordinates) or symbolic (e.g., object labels or qualitative attributes). Thus, we can conduct ablation tests on the effects of formal, symbolic, and quantitative features on the predictive model trained ov"
2020.lrec-1.725,W17-7103,1,0.802835,"Pustejovsky, 2018). Demonstrating such knowledge is needed to ensure a shared understanding between interlocutors, and when one such interlocutor is a computer whose multichannel expressions are quantitatively defined, this allows us to measure certain aspects of the computational common ground created by the computer’s representation of information it has shared with its interlocutors, including humans. When two agents are co-situated and attending to the same situation (co-attending), it is the introduction of such information into the discourse that creates the “shared situated reference” (Pustejovsky et al., 2017) between them, and the introduction of particular information into the common ground may be more or less informative depending not only on the prior contents of the common ground but also the modality through which the new information is introduced. The task is then to assess this, either quantitatively or formally. In this paper, we present an analysis of the common ground structures presented in a dataset of Embodied Multimodal Referring Expressions (EMRE) (Krishnaswamy and Pustejovsky, 2019a). These are references to definite objects performed by an avatar in a simulated world using gesture"
2020.lrec-1.725,W08-1109,0,0.00888122,"ons include neural approaches with high-dimensional word embeddings (Ferreira et al., 2018) and spatial expression generation in human-robot interaction (Wallbridge et al., 2019)— including grounding referring expressions in an environment using visual features and attributives (Shridhar and Hsu, 2018; Cohen et al., 2019; Magassouba et al., 2019). 5919 Studies in the interaction between language and gesture also have a long history in computational linguistics (Claassen, 1992; Bortfeld and Brennan, 1997; Van Der Sluis and Krahmer, 2001; Krahmer and van der Sluis, 2003; Funakoshi et al., 2004; Viethen and Dale, 2008). Despite this, there has been comparatively little research from the community into the ways that multiple modalities interact during real-time communication and how to replicate such structures computationally. Most work in this area originated in the psychology and cognitive science communities, and has been explored in related communities such as robotics (Petit et al., 2012; Matuszek et al., 2014; Whitney et al., 2016; Kasenberg et al., 2019), but has direct relevance to computational language understanding and generation. McNeill (2000) argues that thought is multimodal, and that the com"
2020.lrec-1.725,W16-5906,0,0.0154121,"a formal feature set derived from a common ground structure to predict the naturalness and salient quality of a referring expression associated with that common ground structure. We hypothesize that this is because formal features make the model explainable on a finer-grained level, and that the propositional content extractable from the linguistic utterances used correlates more closely with the quality of the overall referring expressions than lesssymbolically defined features like sentence embeddings. Basic features provide a solid baseline upon which to improve RE classification accuracy (Zhang et al., 2016). Here, however, using sentence embeddings actually seemed to hurt the accuracy. In the data, the purely linguistic “the red block in front of the knife” is more likely to be rated as “average” while “that red block in front of the knife” is multimodal (accompanied by deictic gesture) and more likely to receive a high rating (see Fig. 5). However, the sentence embeddings for these sentences are very similar, due to an alternation of two words (“the”/“that”) that already tend to be similar in distributional semantic space. 5924 “The” vs. “that” captures little of the distinction introduced by t"
2020.lrec-1.725,D18-1400,0,0.0300248,"ing since at least the mid-1990s (Johnston et al., 1997), but has seen increased interest from the CL/NLP communities in recent years. This has been due to a number of factors, including the increase in processing power; the availability of large datasets of text, images, and video; the rise of depth sensors (e.g., Microsoft Kinect); and the availability of GPUs for deep model training. This has resulted in a number of new datasets and approaches to cross-modal linking (Yatskar et al., 2016; Goyal et al., 2017), shared tasks (Barrault et al., 2018), and grounding tasks (Beinborn et al., 2018; Zhou et al., 2018). The most common modalities under study in the CL/NLP communities are text, audio/speech, and images/video, but “modality” can in principle refer to any channel of information. Therefore, multi-channel transmission of information can be separated by channel into the particular information transmitted by each modality (i.e., objects depicted in images with their descriptions in text, or spoken demonstratives with aligned deixis via a gesture). Such disjunct mechanisms allow us to package, quantify, measure, and order our experiences, creating rich conceptual reifications and semantic different"
2020.lrec-1.726,E06-1042,0,0.180706,"the previously presented architecture by Gao et al. (2018). Our code is available at https://github. com/gititkeh/visibility_embeddings. 2. 2.1. Background and Related Work Metaphor Detection Currently, neural methods are dominating the task of Metaphor Detection, with recent state-of-the-art results by Gao et al. (2018) and Mao et al. (2019), using BiLSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im"
2020.lrec-1.726,W18-0911,0,0.0149976,"word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im Walde (2017) and Rei et al. (2017) also proved to work effectively on several annotated datasets. Different types of word embeddings were studied by researchers, including 5928 Figure 1: Example sentences with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The literal sentences (as well as the images) are taken from the Visual Genome dataset (Kr"
2020.lrec-1.726,D18-1060,0,0.233865,"Missing"
2020.lrec-1.726,I17-2018,1,0.725448,"gs. Operationally, we follow Turney et al. (2011) and their adoption of Lakoff and Johnson (1980)’s notion that metaphor is a way to move knowledge from a concrete domain to an abstract one. Hence, there should be a correlation between the “degree of abstractness in a word’s context [...] with the likelihood that the word is used metaphorically” (Turney et al., 2011). Recent studies have suggested that there is a strong correlation between the concreteness scores of words, as annotated by humans, and the visibility of words, as calculated as a function of their occurrences in a visual corpus (Kehat and Pustejovsky, 2017). In the present paper, we take this notion one step further and use visibility of words directly as a feature of the system. More specifically, we further improve on the recently presented results by Gao et al. (2018) on the task of verb classification for metaphor detection. In their work, Gao et al. (2018) used contextual information, in the form of contextualized word embeddings (ELMo) (Peters et al., 2018), as well as the GloVe embeddings (Pennington et al., 2014), both concatenated and fed as an input to a simple BiLSTM. We use a number of popular Vision-Language Datasets to create what"
2020.lrec-1.726,W17-1903,0,0.297961,"Missing"
2020.lrec-1.726,W15-2801,0,0.0168047,"ykowiecka et al., 2018). In our work, we study the effect of using embeddings created from visual datasets, which were shown to be useful in Metaphor Detection (Shutova et al., 2016), as well as in the task of estimating concreteness scores (Kehat and Pustejovsky, 2017). 2.2. Vision-Language datasets The field of Vision and Language has become extremely popular in the last several years. New tasks involving both images and texts were introduced to both the Computer Vision and Natural Language Processing communities, such as Visual Question Answering (Antol et al., 2015) and visual entailment (Krishnamurthy, 2015). This growing interest has led to an explosion of datasets combining visual and textual information, mostly in the form of an image (or segmented regions of an image) and its corresponding or associated textual caption. Many of the most popular vision-language datasets are based on extensive crowdsourcing. The most famous ones to date are the Visual Genome (Krishna et al., 2016) (See examples in Figure 1), Microsoft COCO (Lin et al., 2014), Imagenet (Deng et al., 2009), which is a visual version of WordNet (Miller, 1995), and Flickr30K (Young et al., 2014). Other vision-language datasets, lik"
2020.lrec-1.726,P19-1378,0,0.883181,"are created by a simple sampling technique from visual corpora (the textual part of vision-language datasets, usually in the form of a list of image-caption sentences). We show that these Visibility Embeddings are useful when combined in a simple concatenation manner with the previously presented architecture by Gao et al. (2018). Our code is available at https://github. com/gititkeh/visibility_embeddings. 2. 2.1. Background and Related Work Metaphor Detection Currently, neural methods are dominating the task of Metaphor Detection, with recent state-of-the-art results by Gao et al. (2018) and Mao et al. (2019), using BiLSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Sin"
2020.lrec-1.726,S16-2003,0,0.174276,"w that these Visibility Embeddings are useful when combined in a simple concatenation manner with the previously presented architecture by Gao et al. (2018). Our code is available at https://github. com/gititkeh/visibility_embeddings. 2. 2.1. Background and Related Work Metaphor Detection Currently, neural methods are dominating the task of Metaphor Detection, with recent state-of-the-art results by Gao et al. (2018) and Mao et al. (2019), using BiLSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness s"
2020.lrec-1.726,W18-0916,0,0.482868,"he images) are taken from the Visual Genome dataset (Krishna et al., 2016), and are the captions of the regions highlighted in squares in the respective images. The metaphorical sentences are taken from the MOH-X datasets (Mohammad et al., 2016). Words with significantly higher concreteness scores are highlighted in green, and words that are considered abstract are highlighted in red. embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). In our work, we study the effect of using embeddings created from visual datasets, which were shown to be useful in Metaphor Detection (Shutova et al., 2016), as well as in the task of estimating concreteness scores (Kehat and Pustejovsky, 2017). 2.2. Vision-Language datasets The field of Vision and Language has become extremely popular in the last several years. New tasks involving both images and texts were introduced to both the Computer Vision and Natural Language Processing communities, such as Visual Question Answering (Antol et al., 2015) and visual entailment (Krishnamurthy, 2015). T"
2020.lrec-1.726,D14-1162,0,0.0852219,"Missing"
2020.lrec-1.726,N18-1202,0,0.058861,"g correlation between the concreteness scores of words, as annotated by humans, and the visibility of words, as calculated as a function of their occurrences in a visual corpus (Kehat and Pustejovsky, 2017). In the present paper, we take this notion one step further and use visibility of words directly as a feature of the system. More specifically, we further improve on the recently presented results by Gao et al. (2018) on the task of verb classification for metaphor detection. In their work, Gao et al. (2018) used contextual information, in the form of contextualized word embeddings (ELMo) (Peters et al., 2018), as well as the GloVe embeddings (Pennington et al., 2014), both concatenated and fed as an input to a simple BiLSTM. We use a number of popular Vision-Language Datasets to create what we call Visibility Embeddings. These embeddings are created by a simple sampling technique from visual corpora (the textual part of vision-language datasets, usually in the form of a list of image-caption sentences). We show that these Visibility Embeddings are useful when combined in a simple concatenation manner with the previously presented architecture by Gao et al. (2018). Our code is available at https://"
2020.lrec-1.726,W18-0908,0,0.231439,"LSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im Walde (2017) and Rei et al. (2017) also proved to work effectively on several annotated datasets. Different types of word embeddings were studied by researchers, including 5928 Figure 1: Example sentences with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The literal sentences (as well as the images) are taken fr"
2020.lrec-1.726,D17-1162,0,0.170819,"Missing"
2020.lrec-1.726,N16-1020,0,0.0674124,"he metaphorical sentences are taken from the MOH-X datasets (Mohammad et al., 2016). Words with significantly higher concreteness scores are highlighted in green, and words that are considered abstract are highlighted in red. embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). In our work, we study the effect of using embeddings created from visual datasets, which were shown to be useful in Metaphor Detection (Shutova et al., 2016), as well as in the task of estimating concreteness scores (Kehat and Pustejovsky, 2017). 2.2. Vision-Language datasets The field of Vision and Language has become extremely popular in the last several years. New tasks involving both images and texts were introduced to both the Computer Vision and Natural Language Processing communities, such as Visual Question Answering (Antol et al., 2015) and visual entailment (Krishnamurthy, 2015). This growing interest has led to an explosion of datasets combining visual and textual information, mostly in the form of an image (or segmented regions of an i"
2020.lrec-1.726,W18-0918,0,0.120386,"nces with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The literal sentences (as well as the images) are taken from the Visual Genome dataset (Krishna et al., 2016), and are the captions of the regions highlighted in squares in the respective images. The metaphorical sentences are taken from the MOH-X datasets (Mohammad et al., 2016). Words with significantly higher concreteness scores are highlighted in green, and words that are considered abstract are highlighted in red. embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). In our work, we study the effect of using embeddings created from visual datasets, which were shown to be useful in Metaphor Detection (Shutova et al., 2016), as well as in the task of estimating concreteness scores (Kehat and Pustejovsky, 2017). 2.2. Vision-Language datasets The field of Vision and Language has become extremely popular in the last several years. New tasks involving both images and texts were introduced to both the Computer Vision and Natural Lan"
2020.lrec-1.726,W18-0914,0,0.359004,"ao et al. (2019), using BiLSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im Walde (2017) and Rei et al. (2017) also proved to work effectively on several annotated datasets. Different types of word embeddings were studied by researchers, including 5928 Figure 1: Example sentences with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The literal sentences (as well as"
2020.lrec-1.726,P14-1024,0,0.0833246,"of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im Walde (2017) and Rei et al. (2017) also proved to work effectively on several annotated datasets. Different types of word embeddings were studied by researchers, including 5928 Figure 1: Example sentences with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The literal sentences (as well as the images) are taken from the Visual Genome dataset (Krishna et al., 2016), and are the captions of the regions highli"
2020.lrec-1.726,D11-1063,0,0.726318,"r, we deal with the second task, which more formally takes a sentence w , ...wn and a verb index i as input, and outputs a label for the target verb wi of either “metaphorical” or “literal”, in relation to its role in the sentence (See Figure 1 for examples for non-metaphorical (literal) and metaphorical usages of the same verb in different contexts). In our approach to improve metaphor detection, we follow Black (1979)’s observation that a metaphor is essentially an interaction between two terms, creating an “implicationcomplex” to resolve two incompatible meanings. Operationally, we follow Turney et al. (2011) and their adoption of Lakoff and Johnson (1980)’s notion that metaphor is a way to move knowledge from a concrete domain to an abstract one. Hence, there should be a correlation between the “degree of abstractness in a word’s context [...] with the likelihood that the word is used metaphorically” (Turney et al., 2011). Recent studies have suggested that there is a strong correlation between the concreteness scores of words, as annotated by humans, and the visibility of words, as calculated as a function of their occurrences in a visual corpus (Kehat and Pustejovsky, 2017). In the present pape"
2020.lrec-1.726,W18-0913,0,0.269757,"al. (2018) and Mao et al. (2019), using BiLSTMs and contextualized word embeddings (ELMo) (Peters et al., 2018), demonstrated on a number of popular annotated Metaphor Detection datasets by Mohammad et al. (2016) (MOH-X), Steen et al. (2010) (the VU Amsterdam Metaphor Corpus (VUA)) and Birke and Sarkar (2006) (TroFi). In the recent 2018 VUA Metaphor Detection Shared Task, several neural models with different architectures were introduced. Most of the teams in the task used LSTM’s combined with other linguistic features, such as part-of-speech tags, WordNet data, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and im Walde (2017) showed concreteness scores to be effective for Metaphor Detection. Embedding-based approaches such as in K¨oper and im Walde (2017) and Rei et al. (2017) also proved to work effectively on several annotated datasets. Different types of word embeddings were studied by researchers, including 5928 Figure 1: Example sentences with non-metaphorical (literal) and metaphorical usages of the verbs “pour” and “wrestle”. The lite"
2020.lrec-1.726,Q14-1006,0,0.132958,"t al., 2015) and visual entailment (Krishnamurthy, 2015). This growing interest has led to an explosion of datasets combining visual and textual information, mostly in the form of an image (or segmented regions of an image) and its corresponding or associated textual caption. Many of the most popular vision-language datasets are based on extensive crowdsourcing. The most famous ones to date are the Visual Genome (Krishna et al., 2016) (See examples in Figure 1), Microsoft COCO (Lin et al., 2014), Imagenet (Deng et al., 2009), which is a visual version of WordNet (Miller, 1995), and Flickr30K (Young et al., 2014). Other vision-language datasets, like the SBU dataset (Ordonez et al., 2011) were created automatically by simply querying the web. In our work we use what we call “visual corpora”, which are the text-only parts of vision and language datasets. These texts tend to represent words and ideas of higher concreteness on average, helping us to solve concretenessrelated tasks such as metaphor detection (Kehat and Pustejovsky, 2017). 2.3. Word Concreteness The concreteness of a word commonly refers to what extent the word represents things that can be perceived directly through the five senses (Brysb"
2020.lrec-1.893,P10-4005,0,0.0417135,"chitecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying dat"
2020.lrec-1.893,ide-etal-2014-language,1,0.813,"et al., 2009) and the General Architecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O"
2020.lrec-1.893,W12-2425,0,0.0198493,"Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serialization, as its interchange format; while WebLicht uses its XML-based Text Corpus Format (TCF) (Heid et al., 2010). Additionally, the LAPPS Grid defines a linked data vocabulary that ensures semantic interoperability (Ide et al., 2015). Beyond in-platform interoperability, the LAPPS Grid has established multi-platform interoperability between LAPPS Grid and two CLARIN platforms (Hinrichs et al., 2018) as well as several other platforms (e.g., DKPro (Eckart de Castilho and Gurevych, 2014), PubAnnotation (Kim and Wang, 2012), and INCEpTION (Klie et al., 2018)). Figure 1 shows a visualization of named entity annotations in the LAPPS Grid, using Brat (Stenetorp et al., 2012). All annotations are represented in the LIF format and linked either to offsets within read-only primary data or to other annotation layers. Within the LIF document containing the annotations, each annotation references a name (e.g., PERSON), possibly coupled with additional attributes, that links to a full definition in the LAPPS Grid Web Service Exchange Vocabulary (WSEV)5 . Alternative names used within specific tools are mapped to the WSEV"
2020.lrec-1.893,C18-2002,0,0.0631695,"Missing"
2020.lrec-1.893,P14-5010,0,0.00570971,"stems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying data interchange formats that impose a common I/O format 7230 Figure 1: Named entity annotations in the LAPPS Grid, generated by LINDAT/CLARIN’s NameTag 4 tool and visualized with Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serializatio"
2020.lrec-1.893,W19-2512,1,0.777297,"resent the collections in their client software (e.g., in what order, in which orientation, on what zoom level, etc). Unfortunately, however, the IIIF does not provide detailed specifications for the semantics of the content of the textual annotations. In principle, one could design an independent, adequate data model for textual annotation that can be carried out on IIIF, since the IIF specification is built upon the Open Annotation model and linked-data conformity, but this would involve significant additional effort. MMIF is an interoperable representation format that is used in the CLAMS (Rim et al., 2019) project. CLAMS is a platform of computational analysis tools designed for digital libraries and archives who have to deal with not just textual data, but also audiovisual time-based data. To handle the complexity of multimodal content and semantics of the audiovisual data sources, MMIF is specifically designed to enable alignment of annotations on different modes of the primary data sources. 6 7 https://iiif.io/ https://iiif.io/api/presentation/3.0/ Specifically, multimodal annotations in MMIF are first categorized by the anchor type on which the annotation is placed. That is, an annotation c"
2020.lrec-1.893,schmidt-etal-2008-exchange,0,0.0890206,"not only NLP, but also computer vision (CV), and speech technologies processing audio and video data. The machine learning approach to solving problems is data-driven, and most state-of-the-art applications are based on supervised algorithms which rely on large sets of training data. To ensure the high quality of datasets containing rich multimodal annotations, the CL community has had to move beyond text-only annotation practices, and has tried to establish a common format for multimodal annotations, particularly with regard to annotating speech in audio and gestures in video. For example, (Schmidt et al., 2008) outline a diversity of annotation applications and formats, as well as the community effort to develop an interoperable format that carries complicated, layered multimodal annotation. As a result, 7231 Figure 2: A primary data text collection can be created from an image and can then be input to downstream NLP components. UIMA and Component MetaData Infrastructure (CMDI) (Broeder et al., 2012) have been widely adopted frameworks that provide interoperable multimodal information exchange between computational analysis tools, and for metadata repositories for discoverability, respectively. More"
2020.lrec-1.893,E12-2021,0,0.107914,"Missing"
2020.lrec-1.893,K17-3009,0,0.0570048,"Missing"
2020.nlpcovid19-2.28,W19-4021,1,0.860469,"Missing"
2020.nlpcovid19-2.28,P10-4005,0,0.035952,"re rendered mutually interoperable via transduction to the LAPPS Grid Interchange Format (LIF) (Verhagen et al., 2015) and the Web Service Exchange Vocabulary (WSEV) (Ide et al., 2014b), both designed to capture fundamental properties of existing annotation models in order to serve as a common pivot among them. The LAPPS Grid also provides interoperable, two-way access to the PubAnnotation annotation repository (Kim and Wang, 2012), the INCEpTION machine learning-assisted annotation platform (Klie et al., 2018), along with multi-lingual NLP tools and data in two EU-CLARIN platforms: WebLicht (Hinrichs et al., 2010) and LINDAT/CLARIN (Straka et al., 2016). All LAPPS Grid components are released under the Apache 2.0 open source license. 4 AskMe Overview The AskMe application is an open-source, Solrbased microservice architecture running in a Docker Swarm on Jetstream (Stewart et al., 2015; Towns et al., 2014), an NSF-funded compute cluster. Originally developed to allow for search and retrieval from the PubMed database, it now allows for search of the full set of CORD-19 documents, which are updated nightly. Figure 2: AskMe Query Results In response to a natural language query, AskMe applies several scori"
2020.nlpcovid19-2.28,ide-etal-2014-language,1,0.876929,"Missing"
2020.nlpcovid19-2.28,W14-5204,1,0.868638,"ed to other query engines available to search covid-related publications. 1 Introduction The onset of the coronavirus pandemic has prompted concerted efforts within the field of natural language processing (NLP) to enable researchers and practitioners to easily access and mine the growing body of literature concerned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S."
2020.nlpcovid19-2.28,L18-1327,1,0.850229,"tion The onset of the coronavirus pandemic has prompted concerted efforts within the field of natural language processing (NLP) to enable researchers and practitioners to easily access and mine the growing body of literature concerned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S. National Science Foundation grant NSFEAGER 181123. 3 https://www.kaggle.com/allen-in"
2020.nlpcovid19-2.28,W12-2425,0,0.0245035,"opment engine (Afgan et al., 2018), and may also be accessed directly via SOAP calls or programmatically through Java and Python interfaces. All tools and resources in the LAPPS Grid are rendered mutually interoperable via transduction to the LAPPS Grid Interchange Format (LIF) (Verhagen et al., 2015) and the Web Service Exchange Vocabulary (WSEV) (Ide et al., 2014b), both designed to capture fundamental properties of existing annotation models in order to serve as a common pivot among them. The LAPPS Grid also provides interoperable, two-way access to the PubAnnotation annotation repository (Kim and Wang, 2012), the INCEpTION machine learning-assisted annotation platform (Klie et al., 2018), along with multi-lingual NLP tools and data in two EU-CLARIN platforms: WebLicht (Hinrichs et al., 2010) and LINDAT/CLARIN (Straka et al., 2016). All LAPPS Grid components are released under the Apache 2.0 open source license. 4 AskMe Overview The AskMe application is an open-source, Solrbased microservice architecture running in a Docker Swarm on Jetstream (Stewart et al., 2015; Towns et al., 2014), an NSF-funded compute cluster. Originally developed to allow for search and retrieval from the PubMed database, i"
2020.nlpcovid19-2.28,C18-2002,0,0.0405808,"Missing"
2020.nlpcovid19-2.28,L16-1680,0,0.0227748,"Missing"
2020.nlpcovid19-2.28,2020.nlpcovid19-acl.1,0,0.0282017,"ned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S. National Science Foundation grant NSFEAGER 181123. 3 https://www.kaggle.com/allen-institute-for-ai/CORD19-research-challenge/data 2 primarily as a front end to the LAPPS Grid, one of its most salient features is the ability to further process query results with the large array of NLP tools available in the Grid. Th"
2021.naacl-demos.8,W19-1909,0,0.0217165,"Missing"
2021.naacl-demos.8,2020.emnlp-demos.18,0,0.241329,"ung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated"
2021.naacl-demos.8,D19-1371,0,0.0246983,"89178)] To address the role of angiotensin in lung injury, there is an ongoing clinical trial to examine whether losartan treatment affects outcomes in COVID-19 associated ARDS (NCT04312009). [PMID:32439915 (PMC7242178)] Losartan was also the molecule chosen in two trials recently started in the United States by the University of Minnesota to treat patients with COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from te"
2021.naacl-demos.8,W17-2307,0,0.0224213,"posing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated with Evidence amlodipine and benazepril, and chronic back pain. Sentences [PMID:32081428 (PMC7092824)] On the other hand, many ACE inhibitors are cu"
2021.naacl-demos.8,W13-2001,0,0.0375856,"sults, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Disease"
2021.naacl-demos.8,N19-1145,1,0.895006,"Missing"
2021.naacl-demos.8,Q17-1008,0,0.0252127,"s Factor, and Interleukin-10. We see all of these connections in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many l"
2021.naacl-demos.8,2020.bionlp-1.22,0,0.0299439,"Missing"
2021.naacl-demos.8,D19-6204,1,0.833702,"ctions in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and"
2021.naacl-demos.8,2020.acl-demos.11,1,0.877148,"2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., biomedical scientists) to identify specific drug candidates and molecular targets that are relevant in their biomedical and clinical research aims. The use of our know"
2021.naacl-demos.8,W19-5006,0,0.0162201,"ults, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19"
2021.naacl-demos.8,2020.acl-main.230,1,0.885325,"Missing"
2021.naacl-demos.8,D19-1410,0,0.0136688,"nowledge elements in each path in the KG. Each edge is assigned a salience score by aggregating the scores of paths passing through it. In addition to knowledge elements, we also present related sentences and source information as evidence. We use BioBert (Lee et al., 2020), a pre-trained language model to represent each sentence along with its left and right neighboring sentences as local contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evid"
2021.naacl-demos.8,2020.bionlp-1.21,0,0.0201708,"igation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease"
2021.naacl-demos.8,D18-1230,1,0.819154,"Missing"
2021.naacl-demos.8,2020.bionlp-1.3,0,0.0196518,"h COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., bio"
2021.naacl-demos.8,W16-3104,0,0.0689349,"Missing"
2021.naacl-demos.8,P19-1191,1,0.889692,"Missing"
2021.naacl-demos.8,C14-1149,1,0.831567,"Missing"
2021.naacl-demos.8,2020.acl-demos.8,1,0.900567,"cal contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19. 4 4.1 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 CATB-10270 CATB-1418 CATB-1674 CATB-16A CATB-16D2 CATB-1852 CATB1874 CATB-2744 CATB-3098 CATB-348 CATB-3483 CATB-5880 CATB-84 CATB912 CATD CATHY CATK"
2021.naacl-srw.11,P17-4009,0,0.0217461,"g system for identifying relationships between biomedical entities. It supports template-based queries for structured search and also provides key sentences as the provenance of identified relations. Fabregat et al. (2018) proposed a knowledge base of human pathways and reactions. It supports visualization of event hierarchy and pathway networks. Linguistic visualization research in general is an emerging field of visual analytics for linguistics (Butt et al., 2020). Previous research in this field covers thematic text cluster analysis (Gold et al., 2015), NER-based document content analysis (El-Assady et al., 2017b), multi-party discourse analysis (El-Assady et al., 2017a) and topic modeling visualization (El-Assady et al., 2018). Butt et al. (2020) propose a web framework that consists of various linguistic visualization techniques. However, existing work in this field focuses on the analysis of corpora of conversational text and transcripts, and does not include approaches for analyzing and visualizing semantics of relations. 5 Conclusion We have proposed semantic visualization, a linguistic visual analytic method of multiple steps involving data extraction, parameter reduction, hierarchical structur"
2021.naacl-srw.11,2020.nlpcovid19-acl.1,0,0.0541219,"Missing"
2021.starsem-1.21,2020.figlang-1.32,0,0.0186375,"al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and"
2021.starsem-1.21,2020.figlang-1.30,0,0.0150596,", such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Myko"
2021.starsem-1.21,I17-2018,1,0.849037,"e and no-cost embeddings, are created by checking the occurrence of each word in a set of different visual and non-visual corpora, as a way to estimate its concreteness. They developed the big visual corpus (BVC), which contains the textual parts of multiple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets."
2021.starsem-1.21,2020.figlang-1.17,0,0.0137185,"2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed"
2021.starsem-1.21,2020.lrec-1.726,1,0.618097,"d as one of the following two tasks: (1) Sequence Labeling, in which each token in the sentence is classified as either “metaphorical” or Figure 1: Images from the Visual Genome (Krishna et al., 2016) along with their literal (“L”) description, and a metaphorical (“M”) sentence with a similar verb from the MOH-X dataset (Mohammad et al., 2016) (concrete words in green and abstract words in red). “literal” (multiple outputs per sentence). (2) Classification of a specific target word, usually the main verb (one output per sentence). This task is sometimes called “verb classification”. Recently, Kehat and Pustejovsky (2020) presented the simply constructed Visibility Embeddings (VE), which use references to visual/nonvisual corpora to estimate word concreteness, and applied it to the task of verb-classification. In this paper we apply VE also to the sequence labeling task, and show how they consistently improve the result of a BiLSTM model with BERT. We also discuss possible problems when reporting results 222 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 222–228 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics on very small annotated"
2021.starsem-1.21,W17-1903,0,0.0443909,"Missing"
2021.starsem-1.21,2020.figlang-1.34,0,0.0558957,"Missing"
2021.starsem-1.21,2020.figlang-1.18,0,0.025114,"iple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary"
2021.starsem-1.21,2020.figlang-1.26,0,0.0144356,"pus (BVC), which contains the textual parts of multiple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing d"
2021.starsem-1.21,P19-1378,0,0.122735,"showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Coltheart, 1981) and the 40K list by Brysbaert et al. (2014), either as a reference or for training. 3 Metaphor Detection The current state-of-the-art in metaphor detection is achieved by neural methods, enriched with contextual word embeddings (such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)), and commonly combined with varied linguistic features and metrics. Some notable results are the ones by Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as wel"
2021.starsem-1.21,S16-2003,0,0.0582762,"to another text from the target domain. For example, a single use of a verb like “push” or “leak” can have both literal and metaphorical meanings, in relation to its context (see Figure 1). Technically, the task of metaphor detection at the sentence level is commonly approached as one of the following two tasks: (1) Sequence Labeling, in which each token in the sentence is classified as either “metaphorical” or Figure 1: Images from the Visual Genome (Krishna et al., 2016) along with their literal (“L”) description, and a metaphorical (“M”) sentence with a similar verb from the MOH-X dataset (Mohammad et al., 2016) (concrete words in green and abstract words in red). “literal” (multiple outputs per sentence). (2) Classification of a specific target word, usually the main verb (one output per sentence). This task is sometimes called “verb classification”. Recently, Kehat and Pustejovsky (2020) presented the simply constructed Visibility Embeddings (VE), which use references to visual/nonvisual corpora to estimate word concreteness, and applied it to the task of verb-classification. In this paper we apply VE also to the sequence labeling task, and show how they consistently improve the result of a BiLSTM"
2021.starsem-1.21,W18-0916,0,0.0166263,"2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Coltheart, 1981) and the 40K list by Brysbaert et al. (2014), either as a reference or for training. 3 Metaphor Detection The current state-of-the-art in metaphor detection is achieved by neural methods, enriched with contextual word embeddings (such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)), and commonly combined with varied linguistic f"
2021.starsem-1.21,D14-1162,0,0.0854928,"Missing"
2021.starsem-1.21,2020.figlang-1.3,0,0.0254861,"res that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as well as in the more recent 2020 Shared Task (Leong et al., 2020), with the majority of groups Model Details As a base structure we use the simple BiLSTM architectures presented by Gao et al. (2018). The sequence labeling model (see Figure 2) consists of two layers, a BiLSTM and a feedforward layer, to get a label for each word in the sentence. We implemented the model in Python using the AllenNLP package (Gardner et al., 2017). 1 yet not directly comparable to ours, since they used different train-test separations and evaluation, see Dankers et al. (2020) 223 Figure 2: Simple sequence model with multiple outputs, one per word in the sentence. We use a pret"
2021.starsem-1.21,N18-1202,0,0.0299105,"ifferent dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Coltheart, 1981) and the 40K list by Brysbaert et al. (2014), either as a reference or for training. 3 Metaphor Detection The current state-of-the-art in metaphor detection is achieved by neural methods, enriched with contextual word embeddings (such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)), and commonly combined with varied linguistic features and metrics. Some notable results are the ones by Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models"
2021.starsem-1.21,W18-0907,0,0.0149554,"th contextual word embeddings (such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)), and commonly combined with varied linguistic features and metrics. Some notable results are the ones by Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as well as in the more recent 2020 Shared Task (Leong et al., 2020), with the majority of groups Model Details As a base structure we use the simple BiLSTM architectures presented by Gao et al. (2018). The sequence labeling model (see Figure 2) consists of two layers, a BiLSTM and a feedforward layer, to get a label for each word in the sentence. We implement"
2021.starsem-1.21,W18-0908,0,0.0203935,"Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as well as in the more recent 2020 Shared Task (Leong et al., 2020), with the majority of groups Model Details As a base structure we use the simple BiLSTM architectures presented by Gao et al. (2018). The sequence labeling model (see Figure 2) consists of two layers, a BiLSTM and a feedforward layer, to get a label for each word in the sentence. We implemented the model in Python using the AllenNLP package (Gardner et al., 2017). 1 yet not directly comparable to ours, since they used different train-test separations and evaluation, see Dankers et al. (2020) 2"
2021.starsem-1.21,D17-1162,0,0.0131652,"964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Colthe"
2021.starsem-1.21,W18-0918,0,0.0189388,"t al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Coltheart, 1981) and the 40K list by Brysbaert et al. (2014), either as a reference or for training. 3 Metaphor Detection The current state-of-the-art in metaphor detection is achieved by neural methods, enriched with contextual"
2021.starsem-1.21,2020.figlang-1.35,0,0.023179,"(Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Pre"
2021.starsem-1.21,2020.figlang-1.4,0,0.0149946,"ess. They developed the big visual corpus (BVC), which contains the textual parts of multiple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and"
2021.starsem-1.21,W18-0914,0,0.0120223,"le results are the ones by Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as well as in the more recent 2020 Shared Task (Leong et al., 2020), with the majority of groups Model Details As a base structure we use the simple BiLSTM architectures presented by Gao et al. (2018). The sequence labeling model (see Figure 2) consists of two layers, a BiLSTM and a feedforward layer, to get a label for each word in the sentence. We implemented the model in Python using the AllenNLP package (Gardner et al., 2017). 1 yet not directly comparable to ours, since they used different train-test separations and evaluation, see"
2021.starsem-1.21,2020.figlang-1.27,0,0.0357916,"textual parts of multiple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categori"
2021.starsem-1.21,P14-1024,0,0.0152152,"ef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scores to be effective for Metaphor Detection, however, they all used fix concreteness score lists, such as the MRC (Coltheart, 1981) and the 40K list by Brysbaert et al. (2014), either as a reference or for training. 3 Metaphor Detection The current state-of-the-art in metaphor detection is achieved by neural methods, enriched with contextual word embeddings (such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)), and commonly combined with varied linguistic features and metrics. Some notable results are the ones by Gao e"
2021.starsem-1.21,D11-1063,0,0.119555,"mproved results when visibility embeddings are combined with BERT. 1 Introduction When browsing through vision-language datasets, one can make the intuitive observation that their textual parts (“visual corpora”) contain more physical language, mostly descriptive, which tends to be non-metaphorical by nature (See, for example, typical images from the Visual Genome dataset in Figure 1). Recently, this property was used to build visibility embeddings, which aim to provide a good estimation of a word’s concreteness, a feature that has been long related to metaphoricity (Lakoff and Johnson, 1980; Turney et al., 2011). Many metaphors indeed involve noticeable differences between the abstractness of words constructing them, like “clean conscience” (vs. “clean air”). Metaphors are not created in isolation, commonly do not stand alone as non-literal expressions, and are highly context-dependent in nature. Even the most concrete and physical text can be considered as metaphorical when mentioned in a different context than its original one, or in proximity to another text from the target domain. For example, a single use of a verb like “push” or “leak” can have both literal and metaphorical meanings, in relatio"
2021.starsem-1.21,2020.figlang-1.16,0,0.0157837,"a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et al., 2020; Wan et al., 2020; Dankers et al., 2020). Embedding-based approaches such as in K¨oper and Schulte im Walde (2017) and Rei et al. (2017) proved to work effectively on several annotated datasets. Different types of word embeddings were studied, including embeddings trained on corpora representing different levels of language mastery (Stemle and Onysko, 2018), and embeddings representing different dictionary categories in the form of binary vectors for each word (Mykowiecka et al., 2018). Previous work by Turney et al. (2011), Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2017) showed concreteness scor"
2021.starsem-1.21,W18-0913,0,0.0194351,"trics. Some notable results are the ones by Gao et al. (2018) who used ELMo and BiLSTM, Mao et al. (2019) who also experimented with BERT and features that rely on human metaphor processing, Dankers et al. (2019) who performed joint learning with emotion prediction, and Le et al. (2020) who used graph convolutional neural networks with dependency parse trees. Impressive results1 were presented in the 2018 Metaphor Detection Shared Task (Leong et al., 2018), with most of the groups using neural models with other linguistic elements like POS tags, WordNet features, concreteness scores and more (Wu et al., 2018; Swarnkar and Singh, 2018; Pramanick et al., 2018; Bizzoni and Ghanimifard, 2018), as well as in the more recent 2020 Shared Task (Leong et al., 2020), with the majority of groups Model Details As a base structure we use the simple BiLSTM architectures presented by Gao et al. (2018). The sequence labeling model (see Figure 2) consists of two layers, a BiLSTM and a feedforward layer, to get a label for each word in the sentence. We implemented the model in Python using the AllenNLP package (Gardner et al., 2017). 1 yet not directly comparable to ours, since they used different train-test separ"
2021.starsem-1.21,Q14-1006,0,0.0168994,"bility embeddings (VE) were shown by (Kehat and Pustejovsky, 2020) to be useful for metaphor detection when concatenated to the input of BiLSTM models for the verb classification task. These simple and no-cost embeddings, are created by checking the occurrence of each word in a set of different visual and non-visual corpora, as a way to estimate its concreteness. They developed the big visual corpus (BVC), which contains the textual parts of multiple vision-language datasets, such as Visual Genome (Krishna et al., 2016), ImageNet (Deng et al., 2009), MSCOCO (Lin et al., 2014) and Flick r 30K (Young et al., 2014), as well as a “non-visual” corpus, Brown − BV C, which is the subtraction of the BVC from the Brown corpus (Francis and Kucera, 1964). These two corpora were previously shown (Kehat and Pustejovsky, 2017) to be highly concrete and highly abstract on average, respectively. 2.2 using some variation of BERT in addition to the other features (Su et al., 2020; Gao and Zhang, 2002; Kuo and Carpuat, 2020; Torres Rivera et al., 2020; Kumar and Sharma, 2020; Hall Maudslay et al., 2020; Stemle and Onysko, 2020; Liu et al., 2020; Brooks and Youssef, 2020; Chen et al., 2020; Alnafesah et al., 2020; Li et"
C04-1133,W99-0901,0,0.0456944,"ora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly"
C04-1133,W01-0703,0,0.0127387,"icates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disam"
C04-1133,C02-1112,0,0.0114425,", Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays"
C04-1133,A97-1052,0,0.0608604,"sambiguation, selectional preference acquisition, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as t"
C04-1133,briscoe-carroll-2002-robust,0,0.0343565,"isting semantic classes. For instance, checking if, for a certain percentage of pairs in the candidate set, there already exists a set of which both elements are members. 3 Current Implementation The CPA patterns are developed using the British National Corpus (BNC). The sorted instances are used as a training set for the supervised disambiguation. For the disambiguation task, each pattern is translated into into a set of preprocessing-specific features. The BNC is preprocessed using the Robust Accurate Statistical Parsing system (RASP) and semantically tagged with BSO types. The RASP system (Briscoe and Carroll (2002)) tokenizes, POS-tags, and lemmatizes text, generating a forest of full parse trees for each sentence and associating a probability with each parse. For each parse, RASP produces a set of grammatical relations, specifying the relation type, the headword, and the dependent element. All our computations are performed over the single topranked tree for the sentences where a full parse was successfully obtained. Some of the grammatical relations identified by RASP are shown in (10). (10) subjects: ncsubj, clausal (csubj, xsubj) objects: dobj, iobj, clausal complement modifiers: adverbs, modifiers"
C04-1133,C00-1028,0,0.0127386,"ence in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the def"
C04-1133,N01-1013,0,0.0119248,"hms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current Stat"
C04-1133,P03-1007,0,0.0327247,"over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy"
C04-1133,W97-0808,0,0.0223121,"eference acquisition, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierar"
C04-1133,P99-1014,0,0.010384,"ion, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic"
C04-1133,W98-0701,0,0.0265348,"work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what"
C04-1133,J01-3001,0,0.0156747,"cal features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns with senses, however. In this paper we modify the notion of word sense, and at the same time revise the manner in which senses are encoded. The notion of word sense that has been generally adopted in the literature is an artifact of several factors in the status quo,"
C04-1133,P03-2029,0,0.0121971,"y supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns w"
C04-1133,C92-2070,0,0.104375,"do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages. However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998);"
C04-1133,P95-1026,0,0.0211395,"etween predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages. However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (20"
C04-1133,L02-1000,0,\N,Missing
C08-3012,S07-1025,0,0.0126557,". Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative or positive), and any modal operators which gov"
C08-3012,P07-2044,0,0.270689,"stem described in (Verhagen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatic"
C08-3012,P06-1095,1,0.551204,"agen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative"
C08-3012,H05-1088,1,0.79094,"Missing"
C08-3012,sauri-etal-2006-slinket,1,0.913365,"Missing"
C08-3012,P05-3021,1,0.204565,"n 1970?” since a boolean keyword search cannot distinguish between those documents where the event win is actually anchored to the year 1970 versus those that are not. The TARSQI Project (Temporal Awareness and Reasoning Systems for Question Interpretation) focused on enhancing natural language question answering systems so that temporallybased questions about the events and entities in news articles can be addressed. To explicitly mark the needed temporal relations the project delivered a series of tools for extracting time expressions, events, subordination relations and temporal relations (Verhagen et al., 2005; Mani et al., 2006; Saur´ı et al., 2005; Saur´ı et al., 2006a). But although those tools performed reasonably well, they were not integrated in a principled way. This paper describes the TARSQI Toolkit (TTK), which takes the TARSQI components and integrates them into a temporal parsing framework. The toolkit is different from the system described in (Verhagen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of vis"
C08-3012,S07-1046,0,0.0166616,"ive Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative or positive), and any modal operators which govern the event bein"
C08-3012,S07-1108,0,0.057593,"Missing"
C16-2012,P16-1231,0,0.0230172,"Missing"
C16-2012,P15-1006,0,0.0687741,"Missing"
C16-2012,P13-1104,0,0.0294041,"Missing"
C16-2012,S14-1014,1,0.569558,"nges encountered and describing the solutions implemented. 1 Introduction The expressiveness of natural language is difficult to translate into visuals, and much work in textto-scene generation has focused on creating static images, e.g., Coyne and Sproat (2001) and Chang et. al (2015). Our approach centers on motion verbs, using a rich formal model of events and mapping from an NL expression, through Dynamic Interval Temporal Logic (Pustejovsky and Moszkowicz, 2011), into a 3D animated simulation. Previously, we introduced a method for modeling motion language predicates in three dimensions (Pustejovsky and Krishnaswamy, 2014). This led to VoxML, a modeling language to encode composable semantic knowledge about NL entities (Pustejovsky and Krishnaswamy, 2016), and a reasoner to generate simulations involving novel objects and events (Krishnaswamy and Pustejovsky, 2016). Our system, VoxSim, uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. The latest stable build of VoxSim is available at http://www.voxicon.net. The Unity project and source is at https://github.com/nkrishnaswamy/voxicon. 2 Theoretical Motivations Dynamic interpretations of event"
C16-2012,L16-1730,1,0.484729,"ate into visuals, and much work in textto-scene generation has focused on creating static images, e.g., Coyne and Sproat (2001) and Chang et. al (2015). Our approach centers on motion verbs, using a rich formal model of events and mapping from an NL expression, through Dynamic Interval Temporal Logic (Pustejovsky and Moszkowicz, 2011), into a 3D animated simulation. Previously, we introduced a method for modeling motion language predicates in three dimensions (Pustejovsky and Krishnaswamy, 2014). This led to VoxML, a modeling language to encode composable semantic knowledge about NL entities (Pustejovsky and Krishnaswamy, 2016), and a reasoner to generate simulations involving novel objects and events (Krishnaswamy and Pustejovsky, 2016). Our system, VoxSim, uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. The latest stable build of VoxSim is available at http://www.voxicon.net. The Unity project and source is at https://github.com/nkrishnaswamy/voxicon. 2 Theoretical Motivations Dynamic interpretations of event structures divide motion verbs into “path” and “manner of motion” verbs. Path verbs reassign the moving argument’s position relative to"
C16-2012,W13-5401,1,0.698539,"ath verbs reassign the moving argument’s position relative to a specified location; for manner verbs, position is specified through prepositional adjunct. Thus, The spoon falls and The spoon falls into the cup result in different “mental instantiations,” or “simulations” (Bergen, 2012). In order to visualize events, a computational system must infer path or manner information from the objects involved or from their composition with the predicate. Visual instantiations of lexemes, or “voxemes” (Pustejovsky and Krishnaswamy, 2016), require an encoding of their situational context, or a habitat (Pustejovsky, 2013; McDonald and Pustejovsky, 2014), as well as afforded behaviors that the object can participate in, that are either Gibsonian or telic in nature (Gibson, 1977; Gibson, 1979; Pustejovsky, 1995). For instance, a cup may afford containing another object, or being drunk from. Many event descriptions presuppose such conditions that rarely appear in linguistic data, but a visualization lacking them will make little sense to the observer. This linguistic “dark matter,” conspicuous by its absence, is thus easily exposable through simulation. This work is licenced under a Creative Commons Attribution"
C88-2110,E87-1001,0,0.017527,"ysis, howeverp follows mare closely that position taken in /tligginbotham 1986/. semantics that makes reference to events, just as with nominalization:b 12 we can begin to understand how to analyze 12 For a discussion of the event/resultative distinction in naminalizations, see /Pustejovsky 1 9 ~ / a n d /Pustejovsky 1997/. 521 a knowledge base to a linguistic generation system. Appro(22) a. John is running lisp. priate word choice is, of course, a function of numerous facb. Mary has entered lisp. tors and considerations (/Ward 1988/, /Pustejovsky and c. John knows lisp well. Nirenburg 1987/,/Danlos 1987/), but, in any case, the sed. Mary is writing lisp. lection process makes sense only if the incoming semantic Now, it might be argued that these senses are all slight representation provides for there to be a distinction that variants of one central sense for the nominal, perhaps that is later reflected as a lexical distinction. of lisp as a language. Yet what we know about lisp that Consider now an implicitly relational nominal such makes it different from another language, say FORTRAN, as cigarette. The lexical representation for such a noun is is that it is an environment as well as a lang"
C88-2110,C88-2110,1,0.0522399,"is assigned d. ?This book lasts a week. to a particular paradigm, it assumes the set of behaviors This is certainly surprising if no reference is made to the characteristic of that entire class. So it is with such artype of egent referenced by the object. Within the calcutifactual nominals. There are many such subsystems in lus of aspect outlined in/Pustejovsky 1987/, play(z) and the lexicon, each with their own internM consistency repread(x,y) fall into different event-types, activity and ac.- resented by unique paradigms. complishment, respectively, and license different temporal detail in/Pustejovsky and Anick 1988/. This idc~ is explored in predicates. So, it is not surprising that lexical semantic information is accessible to such processes in the grammar. 4. Computational Implications of Lexical Organization Another interesting application of the notion of hidIn this section we discuss the relevance and implicaden event (or h-event) comes from evaluative predicates. 11 ])'or example, consider the differences between the (a) and tions of the above analysis of nominal semantics for com(b) NPs below: putationM purposes. We will first look at the effect that (16) a. a vinyl record: ~P3x3e[arti/aet(x) A v"
C88-2110,P86-1035,0,0.082703,"Missing"
C88-2110,P87-1028,1,\N,Missing
C88-2110,C88-2149,0,\N,Missing
C88-2110,H86-1013,0,\N,Missing
C88-2110,P87-1025,0,\N,Missing
C90-2002,C88-2153,0,0.0564392,"Missing"
C90-2002,C88-2110,1,0.751515,"rence to the Formal role, while sentence (3) refers to the Constitutive role. Example (2), however, can refer to either the Telic or the Agentive aspects given above. The utility of such knowledge for information retrieval is readily apparent. This theory claims that noun meanings should make reference to related concepts and the relations into which they enter. The qualia structure, thus, can [Agent: information(*y*)] phys-obj ect (*x*)] hold(S,*x*,*y*)] artifact(*x*) ~ write(T,w,*y*)] This simply states that any semantics for tape must logically make reference to the object itself (F), 2See Pustejovsky and Anick (1988) for details. aThis relates to Mel'~uk's lexical functions and the syntactic structures they associate with an element. See Mel'euk (1988) and references therein. Cruse (1986) discusses the foregrounding and backgronnding of information with respect to similar examples. IFor elaboration on this idea and how it applies to various lexical classes, see Pustejovsky (forthcoming). 8 read read read read 2 what it can contain (C), what purpose it serves (T); and how it arises (A). This provides us with a semantic representation which can capture the multiple perspectives which a single lexical item m"
C90-2002,J86-3002,0,\N,Missing
C90-2002,P90-1033,0,\N,Missing
C90-2007,P85-1032,0,0.149399,"Missing"
C90-2007,P86-1035,0,0.0570337,"Missing"
C90-2007,J87-3004,0,\N,Missing
C94-2112,H92-1086,0,0.0437753,"l predicates such as begin is ColMitioned by the event sort of the (lualia a.ssociate(1 with the NI ) itself. Thus, only Nl)s having associated tra.nsi+ tion events will allow coercion a,n([ control. This is not to sly, howew~r, that bcgi?z selects only for transition events. There are, of course, perfectly g r a m m a t i c a l examples of prt)cess COm l)lelnents, as shown in (8) below: (8) a. The snow began to ['all at mi(llfight. b. John 1)egan to feel ill. c. The W~/l]' bega.n to t'each ilttO Bestride. These examples illustrate the use of begin as a raising verb. We will follow Perhnutter [14], in distinguishing between two senses of the. verb begin, distiuguishal)le not I)y the selectional properties given in C o d a r d and .layex. but, rather, con+ forming to the distinction that [1.t] ula(le; namely, ~'~S eit]:ter a Raisi'ng or a (~'o?~lrol v e r b . The analysis is as folk)ws. There are in(leed two grammatical expressions of the verb &:gin, as Raising and Sul)ject-(,'outrol forms: As a control verb, the event sort specified as tim c(maplement iS a TR+ANSI3'ION. As a Raising verb, however, the event may be any sort. This tbllows the' typing assignments below: C o n t r o h (('"
C94-2112,W91-0209,0,\N,Missing
C94-2112,C88-2110,1,\N,Missing
C94-2112,E93-1021,0,\N,Missing
D19-6211,W19-1915,1,0.84372,"r work (Holderness et al., 2018). These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patients psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain. These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work (Holderness et al., 2019) and pretrained on inhouse psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component. The clinical sentiment scores are computed for every note in the admission. Figure 1 details the data analysis pipeline that is employed for the feature extraction. First, a multilayer perceptron (MLP) classifier is trained on EHR sentences (8,000,000 sentences consisting of 340,000,000 tokens) that are extracted from the Research P"
D19-6211,W18-5615,1,0.668564,"in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance: 82 Figure 1: NLP pipeline for feature extraction. the patient during the patient’s evaluation. 3.2 Unstructured Features Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work (Holderness et al., 2018). These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patients psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain. These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work (Holderness et al., 2019) and pre"
E17-5006,P15-1006,0,0.0268615,"minimal model creation. Multimodal simulation of language, particularly motion expressions, brings together a number of existing lines of research from the computational linguistic, semantics, robotics, and formal logic communities, including action and event representation (Di Eugenio, 1991), modeling gestural correlates to NL expressions (Kipp et al., 2007; Neff et al., 2008), and action event modeling (Kipper and Palmer, 2000; Yang et al., 2015). We combine an approach to event modeling with a scene generation approach akin to those found in work by (Coyne and Sproat, 2001; Siskind, 2011; Chang et al., 2015). Mapping natural language expressions through a formal model and a dynamic logic interpretation into a visualization of the event described provides an environment for grounding concepts and referring expressions that is interpretable by both a computer and a human user. This opens a variety of avenues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and s"
E17-5006,P91-1044,0,0.609387,"Missing"
E17-5006,C16-2012,1,0.654918,"c utterance to a simulation in Unity. This knowledge includes specification of object affordances, e.g., what actions are possible or enabled by use an object. VoxML (Pustejovsky and Krishnaswamy, 2016b; Pustejovsky and Krishnaswamy, 2016a) encodes semantic knowledge of real-world objects represented as 3D models, and of events and attributes related to and enacted over these objects. VoxML goes beyond the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a simulation platform such as VoxSim. VoxSim (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b) uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. It uses the Unity game engine for graphics and I/O processing and takes as input a simple natural language utterance. The parsed utterance is semantically interpreted and transformed into a hybrid dynamic logic representation (DITL), and used to generate a minimal simulation of the event when composed with VoxML knowledge. 3D assets and VoxML-modeled nominal objects and events are created with other Unity-based tools, and VoxSim uses t"
E17-5006,S14-1014,1,0.847047,"nues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agen"
E17-5006,L16-1730,1,0.880478,"herent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object over time, e.g., its affordance structure. The former has developed into a semantic notion of situational context, called a habitat (Pustejovsky, 2013a; McDonald and Pustejovsky, 2014), while the latter is addressed by dynamic interpretations of event structure (Pustejovsky and Moszkowicz, 2011; Pustejovsky and Krishnaswamy, 2016b; Pustejovsky, 2013b). The requirements on building a visual simulation from language include several components. We require a rich type system for lexical items and their composition, as well as a language for modeling the dynamics of events, based on Generative Lexicon (GL). Further, a minimal embedding space (MES) for the simulation must be determined. This is the 3D region within which the state is configured or the event unfolds. Object-based attributes for participants in a situation or event also need to be specified; e.g., orientation, relative size, default position or pose, etc. The"
E17-5006,W13-5401,1,0.72348,"computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object"
E17-5006,W13-0705,1,0.703948,"computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object"
E85-1027,P84-1065,0,0.0333145,"Missing"
E85-1027,P85-1012,1,0.850263,"Missing"
E85-1027,P84-1002,1,\N,Missing
E85-1027,J82-2003,0,\N,Missing
H05-1088,N04-1008,0,\N,Missing
H05-1088,W03-1206,0,\N,Missing
H05-1088,C92-1027,0,\N,Missing
H05-1088,A97-1051,0,\N,Missing
H05-1088,W02-1033,0,\N,Missing
H05-1088,P05-3021,1,\N,Missing
H05-1088,P02-1006,0,\N,Missing
H05-1088,N04-1020,0,\N,Missing
H86-1015,E85-1027,1,0.870726,"Missing"
H86-1015,E85-1037,0,0.0493319,"Missing"
H92-1047,C90-1005,0,0.0430275,"Missing"
H92-1047,J93-2005,1,\N,Missing
H92-1047,J90-1003,0,\N,Missing
H92-1047,J86-3002,0,\N,Missing
H92-1047,J93-1005,0,\N,Missing
H92-1047,C88-2110,1,\N,Missing
H92-1047,C90-2002,1,\N,Missing
H92-1047,C90-2007,1,\N,Missing
havasi-etal-2006-bulb,kingsbury-palmer-2002-treebank,0,\N,Missing
havasi-etal-2006-bulb,W04-1908,1,\N,Missing
havasi-etal-2006-bulb,C04-1133,1,\N,Missing
havasi-etal-2006-bulb,meyers-etal-2004-annotating,0,\N,Missing
havasi-etal-2006-bulb,pustejovsky-etal-2006-towards,1,\N,Missing
I17-2018,N12-1094,0,0.0772238,"Missing"
I17-2018,D15-1021,0,0.0598827,"MRC dataset (Coltheart, 1981). K¨oper and im Walde (2017) generated a huge concreteness scores list for 3M words, using 32K pairs from the list by Brysbaert et al. (2014) to train a neural network model with high correlation scores with the existed lists. 2.2 turtle boat milk side symbol clean impossible immortality justification The MRC psycholinguistic database (Coltheart, 1981) contains 4,295 words and concreteness scores (range from 158 to 670), given by human subjects through psychological experiments. 3.1 Descriptions of Corpora Studied Brown corpus (Francis and Kucera, 1964). Following Ferraro et al. (2015), a representative of a non-visual “general”/“balanced” corpus. Visual Genome (Krishna et al., 2016). The largest VL dataset to date, containing 5.4M region descriptions for more than 108K images, visual question answers and more, all created through crowd-sourcing. We used the set of all region descriptions (see Figure 1) as corpus. SBU Captioned Photo Dataset (Ordonez et al., 2011). Another large scale dataset, containing user generated image descriptions for 1M images, created by quering Flickr. As a result, the captions are not necessarily full or accurate (see Figure 1). Flickr 30K (Young"
I17-2018,N16-1020,0,0.0449884,"uitive, give surprisingly good (comparable) results, while not requiring any multimodal processing at all. TSV-SVO sets. The feature vector for each phrase in the sets is simple, consists of our assigned concreteness score for each word in the phrase. For the second set, we divide each triple into two pairs to get 150 “literal” ‘SV’/‘VO’ pairs and 165 “metaphorical” ones. We flipped the feature vector of the ‘VO’ pairs to represent scores in the form of ‘OV’, so that the nouns and verb would appear at consistent places in the vector. As a reference to our results, we bring previous results by Shutova et al. (2016), who used linguistic embedding model, visual embedding model, and a multimodal model that mixed the two (see Table 5). Acknowledgments We would like to thank the three anonymous reviewers for their suggestions and comments. This work was supported by Contract W911NF15-C-0238 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). The TroFi dataset The dataset by Birke and Sarkar (2006) contains annotated “literal” and “non-literal” sentences from the Wall Street Journal for 50 verbs. We follow the exact same algorithm used in Turney et al. (2011) on a"
I17-2018,P14-1024,0,0.249709,"Missing"
I17-2018,D11-1063,0,0.777942,"e event descriptions over abstract concepts and private or mental states (Dodge et al., 2012). In this work, we provide further evidence that visual corpora are indeed less abstract than general corpora, and characterize this as a property of what we term a word’s visibility score. We then Figure 1: Visual Genome (top) with multiple captions, and SBU with a user-generated caption per image. 2.1 Abstractness and concreteness A common notion for the concreteness of a word is to what extent the word represents things that can be perceived directly through the five senses (Brysbaert et al., 2014; Turney et al., 2011), such as tiger and wet. Accordingly, an abstract word 103 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 103–108, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP represents a concept that is far from immediate perception, or alternatively, could be explained only by other words (as opposed to be demonstrated through image, taste, etc.), like fun and truth. Concreteness scores are currently applied in tasks like concept visualization and image description generation, event detection in text and more. Previous methods for measuring wo"
I17-2018,W17-1903,0,0.313132,"Missing"
I17-2018,H89-1033,0,0.668981,"words in visual corpora (the textual components of vision-language datasets) to compute concreteness scores for words. Our simple method does not require hand-annotated concreteness score lists for training, and yields state-of-the-art results when evaluated against concreteness scores lists and previously derived scores, as well as when used for metaphor detection. 1 2 Related Work and Background Introduction One of the most pervasive problems in cognitive science, linguistics, and AI has been establishing the semantic relationship between language and vision (Miller and Johnson-Laird, 1976; Winograd, 1972; Jackendoff, 1983; Waltz, 1993). In recent years, new datasets have emerged that enable researchers to approach this question from a new angle: that of determining both how linguistic expressions are grounded in visual images, and how features of visual images are expressible in language. To this end, large vision and language (VL) datasets have become increasingly popular, mostly used in combined VL tasks, such as visual captioning and question answering, image retrieval and more. However, visual corpora, the language corpora created in the service of image annotation, have properties that h"
I17-2018,Q14-1006,0,0.20933,"2015), a representative of a non-visual “general”/“balanced” corpus. Visual Genome (Krishna et al., 2016). The largest VL dataset to date, containing 5.4M region descriptions for more than 108K images, visual question answers and more, all created through crowd-sourcing. We used the set of all region descriptions (see Figure 1) as corpus. SBU Captioned Photo Dataset (Ordonez et al., 2011). Another large scale dataset, containing user generated image descriptions for 1M images, created by quering Flickr. As a result, the captions are not necessarily full or accurate (see Figure 1). Flickr 30K (Young et al., 2014). 5 captions per image for more than 31K real-world images from Flickr, created through crowd-sourcing. Microsoft COCO (Lin et al., 2014). Includes object segmentation and 5 captions per image for more than 300K images from Flickr. ImageNet (Deng et al., 2009). A dataset matching images and the corresponding WordNet synsets (Miller et al., 1990). We gathered all available annotated synsets as the ImageNet corpus. We also created a set of non-visual Brown corpora by subtracting each of the corpora from the Brown corpus, to each we refer as BrownN V − V C in relation to some visual corpus V C. V"
I17-2018,E06-1042,0,\N,Missing
ide-etal-2014-language,windhouwer-2012-relcat,0,\N,Missing
ide-etal-2014-language,cieri-etal-2014-new,1,\N,Missing
ide-etal-2014-language,piperidis-2012-meta,0,\N,Missing
ide-etal-2014-language,cassidy-etal-2014-alveo,0,\N,Missing
ide-etal-2014-language,J08-3010,0,\N,Missing
ide-etal-2014-language,W14-5211,1,\N,Missing
ide-etal-2014-language,P13-1166,0,\N,Missing
ide-etal-2014-language,W14-5204,1,\N,Missing
J12-2002,W09-3012,0,0.545296,"formance on both versions of the gold standard provides a look into two different aspects of the system. Whereas the original version shows its impact on a standard NLP pipeline, the corrected version puts the proposed algorithm to test by exposing it to complex sentences with several levels of embedding. In order to assess De Facto’s results regarding these two aspects, we generated a baseline from a supervised learning approach, by means of support vector machines (SVM). We followed Prabhakaran, Rambow, and Diab (2010), which is state-of-the-art on automatic tagging of committed belief (cf. Diab et al. 2009b), a notion equivalent to modality and which distinguishes between certain vs. uncertain events. The classiﬁcation that they propose is less ﬁne-grained than ours (certain vs. probable vs. possible), but the information supporting the distinctions is exactly the same, and therefore we adopted the features employed in their best classiﬁer (listed from 1 to 12 in the following example). In addition, we added feature 13 given that our classiﬁer was not aiming at identifying event mentions in the text (contrary to Prabhakaran, Rambow, and Diab’s model), and features 14 and 15 to cope with distinc"
J12-2002,W10-3001,0,0.209272,"Missing"
J12-2002,W10-3107,0,0.0571117,"different versions of the ACE corpus for the Event and Relation recognition task (see, e.g., ACE 2008), in the Penn Discourse TreeBank (Prasad et al. 2007), and in TimeBank (Pustejovsky et al. 2006). In other corpora, factuality information becomes the epicenter of their annotations. For example, Rubin (2007, 2010) is concerned with the notion of certainty, the Language Understanding Annotation Corpus (Diab et al. 2009a) focuses on the author’s committed belief towards what is reported (a notion comparable to the modality axis in event factuality), and the small knowledge-intensive corpus by Henriksson and Velupillai (2010) targets degrees of certainty. In the bioNLP area, factuality and related information is lately becoming a notable area of research and has led to the creation of remarkable corpus resources. The BioScope corpus (Vincze et al. 2008) contains more than 20,000 sentences annotated with speculative and negative key words and their scope. Based on this experience, Dalianis and Skeppstedt (2010) compiled a corpus of Swedish electronic health records with speculation and negative cues marked up, together with the values resulting from their interaction. The corpus presented in Wilbur, Rzhetsky, and S"
J12-2002,W07-1428,0,0.0396298,"judged as not having happened, or as being only possible, are different from those derived from events evaluated as factual. Event factuality is also essential for any task involving temporal ordering, because the plotting of event mentions into a timeline requires different actions depending on their veracity. Karttunen and Zaenen (2005) discuss its relevance for information extraction, and in the area of textual entailment, factuality-related information (modality, intensional contexts, etc.) has been taken as a basic feature in some systems participating in the PASCAL RTE challenges (e.g., Hickl and Bensley 2007). The need for this type of information is also acknowledged in the annotation schemes of corpora devoted to event information, such as the ACE corpus for the Event and Relation recognition task (e.g., ACE 2008), or TimeBank, a corpus annotated with event and temporal information (Pustejovsky et al. 2006). Signiﬁcantly, in the past few years this level of information has been at the focus of much research within the NLP area dedicated to the biomedical domain. Distinguishing between what is reported as a fact versus a possibility in experiment reports or in patient health records is a crucial"
J12-2002,W10-3102,0,0.00968539,"tion Corpus (Diab et al. 2009a) focuses on the author’s committed belief towards what is reported (a notion comparable to the modality axis in event factuality), and the small knowledge-intensive corpus by Henriksson and Velupillai (2010) targets degrees of certainty. In the bioNLP area, factuality and related information is lately becoming a notable area of research and has led to the creation of remarkable corpus resources. The BioScope corpus (Vincze et al. 2008) contains more than 20,000 sentences annotated with speculative and negative key words and their scope. Based on this experience, Dalianis and Skeppstedt (2010) compiled a corpus of Swedish electronic health records with speculation and negative cues marked up, together with the values resulting from their interaction. The corpus presented in Wilbur, Rzhetsky, and Shatkay (2006) tags the polarity and certainty degree of clauses, along with other dimensions. The GENIA Event corpus (Kim, Ohta, and Tsujii 2008) contains 1,000 abstracts with biological events annotated with polarity and degrees of certainty, in addition to other information such as the lexical cues leading to these values (Ohta, Kim, and Tsuji 2007). Such an approach is followed by the c"
J12-2002,de-marneffe-etal-2006-generating,0,0.0110631,"Missing"
J12-2002,W08-0607,0,0.0222269,"ation structures play a crucial role in determining the factuality values of events as well as their relevant sources, but most of the work presented so far addresses the problem of event factuality identiﬁcation by means of classiﬁers fed with linguistic features that are not fully sensitive to sentences’ structural depth and the complex interactions among their constituents. Previous work using subordination syntax to model factuality is the tool for identifying polarity and modality using lexical information and subordinating contexts by Saur´ı, Verhagen, and Pustejovsky (2006). Similarly, Kilicoglu and Bergler (2008) use the data from Medlock and Briscoe (2007) to show the effectiveness of lexically centered syntactic patterns for distinguishing between speculative and nonspeculative sentences. These systems are, however, limited in that they neither account for the effect of multiple embeddings, nor distinguish between different sources. To our knowledge, the ﬁrst system in which factuality-related information is computed applying top–down on a dependency tree, and hence potentially overcoming these limitations, is Nairn, Condoravdi, and Karttunen (2006), who model the percolation of the polarity feature"
J12-2002,W09-1401,0,0.0459724,"main. This interest has resulted in the compilation of domain-speciﬁc corpora devoted particularly to that level of information, such as BioScope (Vincze et al. 2008), and others that include event factivity as a further attribute in the annotation of biomedical events, such as GENIA (Kim, Ohta, and Tsujii 2008). Furthermore, factuality-related information was the main focus in the CoNLL-2010 shared task on Learning to Detect Hedges and their Scope in Natural Language Text (Farkas et al. 2010), and the topic in a subtask of the BioNLP’09 and BioNLP’11 shared task editions on Event Extraction (Kim et al. 2009),1 dedicated to predict whether the biological event is under negation or speculation. The overall goal of this article is to contribute to a better understanding of this particular aspect of speculation. We analyze all the ingredients involved in computing the factuality nature of event mentions in text, and put forward a computational model based on that. As a proof of concept, the model is implemented into De Facto, a factuality proﬁler, and its performance tested against FactBank, a corpus annotated 1 For the 2011 edition, refer to: https://sites.google.com/site/bionlpst/. 262 Saur´ı and P"
J12-2002,W00-0730,0,0.0156919,"Missing"
J12-2002,W04-3103,0,0.0607281,"Missing"
J12-2002,P07-1125,0,0.0511365,"t if the values resulting from these cues and their interactions are not provided. Complementary to this perspective, the second approach to factuality-related information puts the emphasis on identifying speculative degrees (along the lines assumed in this article). Pioneering work within this view is Light, Qiu, and Srinivasan (2004), a paper exploring the use of speculative language in sentences from Medline abstracts. It experiments with a handcrafted list of hedge cues as well as a supervised SVM in order to classify sentences as either certain, high, or low speculative. Drawing on this, Medlock and Briscoe (2007) address the classiﬁcation of sentences into speculative or non-speculative as a weakly supervised machine learning task and perform experiments with SVMs, achieving a precision-recall breakeven point of 0.76. This line of research is further explored by Szarvas (2008). On the other hand, Shatkay et al. (2008) use the corpus developed by Wilbur, Rzhetsky, and Shatkay (2006) to explore machine learning classiﬁers for tagging data along the ﬁve dimensions in which it is marked up, including polarity and degrees of certainty. It is a challenging task in that it involves simultaneous multi-dimensi"
J12-2002,W09-1304,0,0.0156313,"hose prioritizing the identiﬁcation of linguistic structure (that is, speculative cues and their scope); and (b) those focusing on the factuality values that result from these cues and their interaction. The ﬁrst approach mostly revolves around the BioScope corpus, which has become a good catalyzer for research on this topic in the biomedicine domain. Part of it was used for the CoNLL-2010 shared task on Learning To Detect Hedges and their scope in Natural Language Text (Farkas et al. 2010). Moreover, it is at the basis of explorations on hedging and negation cues scope identiﬁcation, such as Morante and Daelemans (2009a, 2009b), which apply a supervised sequence labeling ¨ ur ¨ and Radev (2009) and Velldal, Ovrelid, and Oepen (2010), which approach, or Ozg 293 Computational Linguistics Volume 38, Number 2 combine supervised learning techniques with rule-based systems exploiting syntactic patterns. Identifying modality and polarity cues and their scope is certainly a key aspect for determining the degree of factuality of events, but not sufﬁcient if the values resulting from these cues and their interactions are not provided. Complementary to this perspective, the second approach to factuality-related inform"
J12-2002,W09-1105,0,0.0100202,"hose prioritizing the identiﬁcation of linguistic structure (that is, speculative cues and their scope); and (b) those focusing on the factuality values that result from these cues and their interaction. The ﬁrst approach mostly revolves around the BioScope corpus, which has become a good catalyzer for research on this topic in the biomedicine domain. Part of it was used for the CoNLL-2010 shared task on Learning To Detect Hedges and their scope in Natural Language Text (Farkas et al. 2010). Moreover, it is at the basis of explorations on hedging and negation cues scope identiﬁcation, such as Morante and Daelemans (2009a, 2009b), which apply a supervised sequence labeling ¨ ur ¨ and Radev (2009) and Velldal, Ovrelid, and Oepen (2010), which approach, or Ozg 293 Computational Linguistics Volume 38, Number 2 combine supervised learning techniques with rule-based systems exploiting syntactic patterns. Identifying modality and polarity cues and their scope is certainly a key aspect for determining the degree of factuality of events, but not sufﬁcient if the values resulting from these cues and their interactions are not provided. Complementary to this perspective, the second approach to factuality-related inform"
J12-2002,W06-3907,0,0.748722,"Missing"
J12-2002,W10-3112,0,0.0525494,"Missing"
J12-2002,D09-1145,0,0.0264654,"Missing"
J12-2002,C10-2117,0,0.270223,"Missing"
J12-2002,N07-2036,0,0.0299841,"tuality-related information takes place, thus providing the support for more applied investigations. 6.1 Factuality Information in Corpora In some corpora, factuality-related information is annotated as information complementary to the main phenomenon they target. It is, for instance, contemplated in different versions of the ACE corpus for the Event and Relation recognition task (see, e.g., ACE 2008), in the Penn Discourse TreeBank (Prasad et al. 2007), and in TimeBank (Pustejovsky et al. 2006). In other corpora, factuality information becomes the epicenter of their annotations. For example, Rubin (2007, 2010) is concerned with the notion of certainty, the Language Understanding Annotation Corpus (Diab et al. 2009a) focuses on the author’s committed belief towards what is reported (a notion comparable to the modality axis in event factuality), and the small knowledge-intensive corpus by Henriksson and Velupillai (2010) targets degrees of certainty. In the bioNLP area, factuality and related information is lately becoming a notable area of research and has led to the creation of remarkable corpus resources. The BioScope corpus (Vincze et al. 2008) contains more than 20,000 sentences annotated"
J12-2002,sauri-etal-2006-slinket,1,0.860511,"Missing"
J12-2002,P08-1033,0,0.0104617,"rk within this view is Light, Qiu, and Srinivasan (2004), a paper exploring the use of speculative language in sentences from Medline abstracts. It experiments with a handcrafted list of hedge cues as well as a supervised SVM in order to classify sentences as either certain, high, or low speculative. Drawing on this, Medlock and Briscoe (2007) address the classiﬁcation of sentences into speculative or non-speculative as a weakly supervised machine learning task and perform experiments with SVMs, achieving a precision-recall breakeven point of 0.76. This line of research is further explored by Szarvas (2008). On the other hand, Shatkay et al. (2008) use the corpus developed by Wilbur, Rzhetsky, and Shatkay (2006) to explore machine learning classiﬁers for tagging data along the ﬁve dimensions in which it is marked up, including polarity and degrees of certainty. It is a challenging task in that it involves simultaneous multi-dimensional classiﬁcation and, in some dimensions also, multi-label tagging. They experiment with SVMs and Maximum Entropy classiﬁers, and report very good results (macro-averaged F1 of 0.71 for degrees of certainty and 0.97 for polarity). Resourcing to rich linguistic inform"
J12-2002,W10-3007,0,0.0241764,"Missing"
J12-2002,W08-0606,0,0.210574,"Missing"
J89-3005,C86-1080,0,0.0252732,"ion obtaining between the two objects? These are simply the problems of interpretation and generation of spatial expressions. Most computational approaches to lexical disambiguation involved some mechanism of selection among alternative word senses. Discovering the associations between one sense of a word and the rest of the expression will help in the disambiguation of the overall expression. This can be accomplished in any number of ways, including marker passing (Charniak 1983, Hirst 1987), message passing (Rieger and Small 1979), preference semantics (Wilks 1975), and collative semantics (Fass 1986). Herskovits implicitly assumes that senses are generated and matched according to the context and situational constraints. However, she leaves the particular computational mechanisms for the decoding of spatial expressions underspecified and unclear. It would have been helpful to see how her procedure for obtaining a context-specific interpretation can be made more explicit, so that it can be compared to some of the approaches mentioned above. As a result, the reader is left intrigued but unsatisfied with the details of the computational aspects of the proposal. The discussion of how locative"
J89-3005,H92-1086,0,0.060307,"Furthermore, Winograd and Flores hold that a language user cannot possibly have a mental representation of the world that he or she perceives. Adopting a neo-Heideggerian position on mental activity, they deny that symbol manipulation plays a role in our understanding of the world and our use of language. Herskovits adopts this position to some extent, albeit with some important exceptions. She agrees with many linguists and cognitive psychologists that constructional theory building is essential for revealing generalizations about the data and for a better understanding of the phenomena (cf. Goodman 1951, Carnap 1967, Schlick 1918). Furthermore, she adopts a strong &quot;representational&quot; view on the nature of lexical meaning (closer in spirit to Lakoff (1987) or Jackendoff (1983)), which is embedded within a nonrepresentational theory of pragmatics. From a linguistic perspective, Herskovits makes the following theoretical claims: 1. 2. 188 Spatial prepositions (henceforth SPs) have ideal meanings associated with them in their lexical entries. The ways in which this meaning is conventionally exploitedmthat is, the canonical usage types for a wordmare also stored in the lexicon with the preposition"
J89-3005,P79-1003,0,0.0178664,"ven a situation with two spatial objects, how can we best describe the spatial relation obtaining between the two objects? These are simply the problems of interpretation and generation of spatial expressions. Most computational approaches to lexical disambiguation involved some mechanism of selection among alternative word senses. Discovering the associations between one sense of a word and the rest of the expression will help in the disambiguation of the overall expression. This can be accomplished in any number of ways, including marker passing (Charniak 1983, Hirst 1987), message passing (Rieger and Small 1979), preference semantics (Wilks 1975), and collative semantics (Fass 1986). Herskovits implicitly assumes that senses are generated and matched according to the context and situational constraints. However, she leaves the particular computational mechanisms for the decoding of spatial expressions underspecified and unclear. It would have been helpful to see how her procedure for obtaining a context-specific interpretation can be made more explicit, so that it can be compared to some of the approaches mentioned above. As a result, the reader is left intrigued but unsatisfied with the details of t"
J91-4003,P88-1011,0,0.0223097,"nvolving many different generative factors that account for the way that language users create and manipulate the context under constraints, in order to be understood. Within such a theory, where many separate semantic levels (e.g. lexical semantics, compositional semantics, discourse structure, temporal structure) have independent interpretations, the global interpretation of a &quot;discourse&quot; is a highly flexible and malleable structure that has no single interpretation. The individual sources of semantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences must be globally coherent, a state that is accomplished by processes of cooperation among separate semantic modules. The basic result of such a view is that semantic interpretation proceeds in a principled fashion, always aware of what the source of a particular inference is, and what the certainty of its value is. Such an approach allows the reasoning process to be both tractable and computationally efficient. The representation of lexical semantics, therefore, should be seen as just one of many levels in a richer characterization of contextual str"
J91-4003,W91-0209,0,0.157547,"Missing"
J91-4003,E89-1009,0,0.0491918,"sed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a description of projective inheritance and the notion of &quot;degrees of prototypicality&quot; of predication. I will argue that such degrees of salienc"
J91-4003,P85-1032,0,0.0607716,"of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a description of projective inheritance and the notion of &quot;degrees of prototypicality&quot; of predication. I will argue t"
J91-4003,H92-1086,0,0.0532274,"position is possible if it is p e r f o r m e d generatively. Rather than assuming a fixed set of primitives, let us assume a fixed n u m b e r of generative devices that can be seen as constructing semantic expressions. 5 Just as a formal language is described more in terms of the productions in the g r a m m a r than its accompanying vocabulary, a semantic language is definable b y the rules generating the structures for expressions rather than the vocabulary of primitives itself. 6 4 For further discussion on the advantages and disadvantages to both approaches, see Jackendoff (1983). 5 See Goodman (1951) and Chomsky (1955) for explanations of the method assumed here. 6 This approach is also better suited to the way people write systems in computational linguistics. Different people have distinct primitives for their own domains, and rather than committing a designer 417 Computational Linguistics Volume 17, Number 4 How might this be done? Consider the sentences in. Example 21 again. A minimal decomposition on the word closed is that it introduces an opposition of terms: closed and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are predicated of different subeve"
J91-4003,T87-1006,0,0.231593,"been little attention paid to the other lexical categories (but see Miller and Johnson-Laird [1976], Miller and Fellbaum [1991], and Fass [1988]). That is, we have little insight into the semantic nature of adjectival predication, and even less into the semantics of nominals. Not until all major categories have been studied can we hope to arrive at a balanced understanding of the lexicon and the methods of composition. Stepping back from the lexicon for a moment, let me say briefly what I think the 1 This is still a contentiouspoint and is an issue that is not at all resolvedin the community.Hobbs (1987) and Wilensky(1990), for example,argue that there should be no distinctionbetween commonsenseknowledgeand lexicalknowledge.Nevertheless, I will suggestbelow that there are good reasons,both methodologicaland empirical,for establishingjust such a division.Pustejovskyand Bergler (1991) contains a good survey on how this issue is addressed by the community. 410 James Pustejovsky The Generative Lexicon position of lexical research should be within the larger semantic picture. Ever since the earliest attempts at real text understanding, a major problem has been that of controlling the inferences as"
J91-4003,J87-3004,0,0.350008,"meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure for a lexical item. I will build on Grimshaw&apos;s recent proposals (Grimshaw 1990) for how to define the mapping from the lexicon to syntax. to a particular vocabulary of primitives, a lexical semantics should provide a method for the decomposition and composition of lexical items. 7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles determine a minimal semantic description of a word that has both semantic and grammatical consequences. 418 James Pustejovsky The Generative Lexicon This provides us with an answer to the question of what levels of semantic representation are necessary for a computational lexical semantics. In sum, I will argue that lexical meaning can best be captured by assuming the following levels of representation. 1. Argument Structure: The behavior of a word as a function, with its arity specified. This is the predicate argument structure for a word, which"
J91-4003,P88-1012,0,0.0290891,"ould be viewed as involving many different generative factors that account for the way that language users create and manipulate the context under constraints, in order to be understood. Within such a theory, where many separate semantic levels (e.g. lexical semantics, compositional semantics, discourse structure, temporal structure) have independent interpretations, the global interpretation of a &quot;discourse&quot; is a highly flexible and malleable structure that has no single interpretation. The individual sources of semantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences must be globally coherent, a state that is accomplished by processes of cooperation among separate semantic modules. The basic result of such a view is that semantic interpretation proceeds in a principled fashion, always aware of what the source of a particular inference is, and what the certainty of its value is. Such an approach allows the reasoning process to be both tractable and computationally efficient. The representation of lexical semantics, therefore, should be seen as just one of many levels in a richer charac"
J91-4003,P85-1038,0,0.0386362,"ions includes: 7, negation, _&lt;, temporal precedence, &gt;_, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy,but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwithet al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, ~rl,..., 7rn, generates a sequence of predicates, (Q1,..., Qnl, called the projective expansion of Q1, P(Q1). Definition The projective conclusion space, P(@R), is the set of projective expansions generated from all elements of the conclusion space, ~, on role R of predicate Q: as: P(~R) = {(P(Q1),P(Qn)&gt; [ (QI,...,Qn) E ~R}. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categor"
J91-4003,J88-2003,0,0.0919046,"semantic characterization of a lexical item, but it is a necessary component. 5.2 Event Structure As mentioned above, the theory of decomposition being outlined here is based on the central idea that word meaning is highly structured, and not simply a set of semantic features. Let us assume this is the case. Then the lexical items in a language will essentially be generated by the recursive principles of our semantic theory. One level of semantic description involves an event-based interpretation of a word or phrase. I will call this level the event structure of a word (cf. Pustejovsky 1991; Moens and Steedman 1988). The event structure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure. Because it is recursively defined on the syntax, it is also a property of phrases and sentences. 8 I will assume a sortal distinction between three classes of events: states (eS), processes (8), and transitions (eT). Unlike most previous ,;ortal classifications for events, I will adopt a subeventual analysis or predicates, as argued in Pustejovsky (1991) and i n d e p e n"
J91-4003,J88-2005,0,0.0693578,"Missing"
J91-4003,C88-2110,1,0.908712,"Missing"
J91-4003,J91-4003,1,0.0513221,"n and finish as shown in Example 5. Example 5 a. Mary finished the cigarette. b. Mary finished her beer. The exact meaning of the verb finish varies depending on the object it selects, assuming for these examples the meanings finish smoking or finish drinking. Test for the ambiguity of a word. Distinguish between homonymy and polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and logical aspects of ambiguity. For example, the homonymy between the two senses of bank in Example 6 is accidental.2 Example 6 a. the bank of the river b. the richest bank in the city 2 Cf. Weinreich(1972) distinguishesbetween contrastiveand complementarypolysemy,essentially covering this same distinction.See Section4 for discussion. 412 James Pustejovsky The Generative Lexicon In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich 1972; Lakoff 1987). Example 7 a. The bank raised its interest rates yesterday (i.e. the b. The store is next to the new bank (i.e. the building). • institution). Establish what the compositional nature of a lexical item is when applied to other words. For example, alleged vs. female in Example 8. Example 8 a. the alleged suspect b. the female suspect"
J93-2005,J87-3001,0,0.00878406,"sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the previous techniques to make explicit the implicit information specified by a theory of generative lexicons. Given what we described above for the lexical structures for nominals, we can identify these semantic relations in the OALD and LDOCE by pattern matching on the parse trees of definitions. To illustrate what specific information can be derived by automatic seeding"
J93-2005,E85-1025,0,0.0192452,"Missing"
J93-2005,C90-2002,1,0.807126,"is important to mention the process of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to auto"
J93-2005,E91-1038,1,0.901808,"equire an overt or implicit negation within the immediate discourse context, rather than within the clause. For this reason, we will call such verbs discourse polarity items. For our purposes, the significance of such data is twofold: first, experiments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment; second, if such k n o w l e d g e is in fact so systematic, then it m u s t be at least partially represented in the lexical semantics of the verb. To test w h e t h e r the intuitions s u p p o r t e d b y the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived the statistics reported in Figure 7. Let us assume, on the basis of this preliminary data 16 presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items. The question then 15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980). 16 Overlap between the categories occurs in less than 35 cases. 350 James Pustejovsky et al. Keywords Lexical Semantic Techniques for Corpus"
J93-2005,J87-3002,0,0.0171017,"Missing"
J93-2005,C90-2007,1,0.867444,"Missing"
J93-2005,E91-1039,0,0.00841041,"ith prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/0. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical str"
J93-2005,J87-3003,0,0.0176228,"e proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of infor"
J93-2005,C90-3010,0,0.0115792,"rent sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &quot;licensed violation&quot; of selectional restrictions. For example, while the verb announce selects for a human subject, sentences like The Phantasie Corporation announced thir"
J93-2005,A88-1019,0,0.0254102,"Missing"
J93-2005,J90-1003,0,0.084532,"ampaign investigation process program operation negotiation strike production meeting term visit test construction debate trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus. Corpus studies confirm similar results for &quot;weakly intensional contexts&quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &quot;the proposal to,&quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that al"
J93-2005,M92-1031,0,0.0147224,"Missing"
J93-2005,J86-3002,0,0.0985829,"Missing"
J93-2005,C92-2099,0,0.0248904,"fy the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/0. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical structures seeded by a machine-readable dic"
J93-2005,C90-3025,0,0.00950683,"on of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the previous techniques to make explicit the impl"
J93-2005,P83-1019,0,0.0127306,"aum (1991), such simple oppositional predicates f o r m a central p a r t of our lexicalization of concepts. Semantically m o t i v a t e d collocations such as these extracted f r o m large c o r p o r a can p r o v i d e presuppositional information for w o r d s that w o u l d otherwise be missing f r o m the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, s o m e recent research in related areas is promising. Hindle (1990) reports interesting results of this kind b a s e d on literal collocations, w h e r e he parses the corpus (Hindle 1983) into p r e d i c a t e - a r g u m e n t structures a n d applies a mutual information m e a s u r e (Fano 1961; M a g e r m a n a n d Marcus 1990) to w e i g h the association b e t w e e n the predicate and each of its arguments. For example, as a list of the m o s t frequent objects for the v e r b drink in his corpus, Hindle f o u n d beer, tea, Pepsi, a n d champagne. Based on the distributional hypothesis that the degree of shared contexts is a similarity m e a s u r e for words, he d e v e l o p s a similarity metric for n o u n s b a s e d on their substitutability in certain v e r b"
J93-2005,P90-1034,0,0.156894,"e drive]], [[database management] system]. 2. Generation of taxonomic relationships on the basis of collocational information. Technical sublanguages often express subclass relationships in noun compounds of the form &lt;instance-name> &lt;class-name>, as in &quot;Unix operating system&quot; and &quot;C language.&quot; Unfortunately, noun compounds are also employed to express numerous other relationships, as in &quot;Unix kernel&quot; and &quot;C debugger.&quot; We have found, however, that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships, using a strategy similar to that employed by Hindle (1990) for detecting synonyms. Given a term T, we extract from the phrase database those nouns Ni that appear as the head of any phrase in which T is the immediately preceding term. These nouns represent candidate classes of which T may be a member. We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb/T collocation (cf. Hindle 1990). We do the same for each noun Ni. Under the assumption that instance and class nouns are likely to co-occur with the same verbs, we compute a similarity score between T and each noun Ni, by summing the pr"
J93-2005,P91-1030,0,0.0207828,"program operation negotiation strike production meeting term visit test construction debate trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus. Corpus studies confirm similar results for &quot;weakly intensional contexts&quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &quot;the proposal to,&quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that almost anything can be vetoed,"
J93-2005,P86-1018,0,0.0110737,"Missing"
J93-2005,C88-2098,0,0.014747,"is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation o"
J93-2005,C88-2100,0,0.0862097,"Missing"
J93-2005,J91-4003,1,0.768039,"s most queries tend to be expressed as conjunctions of nouns. From a theoretical perspective, we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs. While Zernik (1989) presents ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus. We will assume, for this paper, the general framework of a generative lexicon as outlined in Pustejovsky (1991). In particular, we make use of the principles of type coercion and qualia structure. This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the language. These devices and the associated dictionary make up a generative lexicon, where semantic information is distributed throughout the lexicon to all categories. The general framework assumes four basic levels of semantic description: argument structure, qualia structure, lexical inheritance structure, and event structure. Connectin"
J93-2005,H92-1047,1,0.927271,"ss of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent pos"
J93-2005,C88-2110,1,0.8174,"se in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions. Example 4 read a book Example 5 read a story in a book Example 6 read a tape Example 7 read the information on the tape Instruments, on the other hand, display classic agent-instrument causative alternations, such as those in Examples 8 through 11 (cf. Fillmore 1968; Lakoff 1968, 1970). Example 8 ... smash the vase with the hammer Example 9 The hammer smashed the vase. Example 10 ... kill him with a gun Example 11 The gun killed him. Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15. These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it. Example 12 John painted the door. Example 13 John walked through the door. 334 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Example14 John is scrubbing the fireplace. Example15 The smoke filled the fireplace. That is, paint and scrub are actions on physical objects while walk through and fill are processes in spaces. These collocational patterns, we argue, are systematically predictable f"
J93-2005,A88-1018,0,0.021045,"Missing"
J93-2005,J92-3003,0,0.0111081,"Computational Linguistics Volume 19, Number 2 it serves (telic), and how it arises (agentive). This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts. Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses. This is the contribution of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an i s - a hierarchy (cf. Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. 7 Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings. Thus for every predicate, Q, we can generate its opposition, =Q. Similarly"
J93-2005,P91-1036,0,0.138492,"e venture Figure 6 Verb-object pairs with prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/0. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to sup"
J93-2005,E91-1040,0,0.011147,"octer, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much"
J93-2005,J88-4003,0,0.0333998,"Missing"
J93-2005,W91-0209,0,\N,Missing
J93-2005,J93-1005,0,\N,Missing
J93-2005,P85-1037,0,\N,Missing
J93-2005,P84-1036,0,\N,Missing
L16-1073,ide-etal-2014-language,1,0.876291,"erability, Workflow management 1. 2 Overview 1 The NSF-SI -funded LAPPS Grid project is a collaborative effort among Brandeis University, Vassar College, Carnegie-Mellon University (CMU), and the Linguistic Data Consortium (LDC) at the University of Pennsylvania, which has developed an open, web-based infrastructure through which massive and distributed resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, evaluated, disseminated and consumed by researchers, developers, and students across a wide variety of disciplines (Ide et al., 2014). The LAPPS Grid is part of a larger multiway international collaboration including key individuals and projects from the U.S., Europe, Australia, and Asia involved with language resource development and distribution and standards-making, who are creating the “The Federated Grid of Language Services” (FGLS) federation (Ishida et al., 2014), a multi-lingual, international network of web service grids and providers. We have also recently entered into a formal partnership with WebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual acce"
L16-1073,P13-1166,0,0.0680149,"Missing"
L16-1073,W09-3034,1,0.6995,"ebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual access to all from any one of the three portals. The key to the success of these partnerships is the interoperability among tools and services that is accomplished via the service-oriented architecture and the development of common vocabularies and multi-way mappings that has involved key researchers from around the world for over a decade2 . 1 http://www.lappsgrid.org E.g., the NSF-funded Sustainable Interoperability for Language Technology (SILT) project (NSF-INTEROP 0753069) (Ide et al., 2009), the EU-funded Fostering Language Resources Network (FLaReNet) project (Calzolari et al., 2009), the International Standards Organization (ISO) committee for Language Resource Management (ISO TC37 SC4), and parallel efforts in Asia and Australia, together with the LAPPS project and international col2 The LAPPS Grid currently includes a wide range of NLP component web services and provides facilities for service discovery, service composition (including automatic format conversion between tools where necessary), performance evaluation (via provision of component-level measures for standard eva"
L16-1730,P15-1006,0,0.0750228,"ntic values. Keywords: Semantics, Cognitive Methods, Simulation, Lexical Semantics, Visualization 1. Introduction In this paper, we describe a modeling language for constructing 3D visualizations of concepts denoted by natural language expressions. This language, VoxML (Visual Object Concept Modeling Language), is being used as the platform for creating multimodal semantic simulations in the context of human-computer communication.1 Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015), and we have endeavored to incorporate dynamic semantics and motion language into our model. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how a"
L16-1730,P13-1104,0,0.0480925,"Missing"
L16-1730,W13-0701,0,0.0270103,"are specified and encoded with a richly typed framework, such as Generative Lexicon, most lexemes will have a representation within VoxML that encodes an interpretation of the word’s semantic content as a visualization. We have followed a strict methodology of specification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005; Pustejovsky et al., 2010) and others in the family of SemAF standards. Further, our work shares many of the goals pursued in (Dobnik et al., 2013; Dobnik and Cooper, 2013), for specifying a rigidly-defined type system for spatial representations associated with linguistic expressions. 4606 In this paper, we describe a specification language for modeling “visual object concepts”, VoxML. The object defined within VoxML will be called a voxeme, and the library of voxemes, a voxicon. 2. Habitats and Affordances a. Atomic Structure (FORMAL): objects expressed as basic nominal types b. Subatomic Structure (CONST): mereotopological structure of objects c. Event Structure (TELIC and AGENTIVE): origin and functions associated with an object d. Macro Object Structure: ho"
L16-1730,W13-3004,0,0.0677807,"Missing"
L16-1730,S14-1014,1,0.784202,"describe a modeling language for constructing 3D visualizations of concepts denoted by natural language expressions. This language, VoxML (Visual Object Concept Modeling Language), is being used as the platform for creating multimodal semantic simulations in the context of human-computer communication.1 Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015), and we have endeavored to incorporate dynamic semantics and motion language into our model. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent"
L16-1730,W13-5401,1,0.639106,"nstructing 3D visualizations of concepts denoted by natural language expressions. This language, VoxML (Visual Object Concept Modeling Language), is being used as the platform for creating multimodal semantic simulations in the context of human-computer communication.1 Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015), and we have endeavored to incorporate dynamic semantics and motion language into our model. In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object o"
L18-1009,W17-2812,1,0.277459,"bNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument structure, thematic roles and semantic representations. The remainder of this paper will describe"
L18-1009,W13-5401,1,0.813638,"s (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument structure, thematic roles and semantic representations. The remainder of this paper will describe the changes being made to VerbNet’s semantic representations and the reasons behind those changes. Section 2 briefly describes the role semantic representations play in VerbNet and breaks down 2. VerbNet and Its Representation of Events Each VerbNet class contains semantic representations that are compatible w"
L18-1009,W13-3004,0,0.0309998,"g a semantic expression evaluated with respect to its opposition: give, open; build: Binary transition (achievement): ¬φ ∈ S1 , and φ ∈ S2 T S1 S2 Complex transition (accomplishment): ¬φ ∈ P , and φ ∈ S T P S The basic event types are the states and processes, which can represent independent events or be combined to derive complex events (transitions). Subevents within an event are ordered by temporal relations and relative prominence or 2 The resulting structure is equivalent to a Labeled Transition System (van Benthem, 1991), and is consistent with the approach developed in (Fernando, 2009; Fernando, 2013). 58 cause(e2 , e3 ) has location(e4 , Theme, ?Destination) relations, i.e., relations from states to states, and hence interpreted over an input/output state-state pairing (cf. (Naumann, 2001)). The model encodes three kinds of representations: (i) predicative content of a frame; (ii) programs that move from frame to frame; and tests that must be satisfied for a program to apply. These include: pre-tests, while-tests, and result-tests. 5. (See sections 4.1-4.3 for further examples.) A more minor adjustment concerns the path rel predicate, which was introduced earlier in the revision process t"
L18-1009,P06-1117,0,0.542098,"erbNet (Kipper et al., 2006) is a promising source of such information. It is a hierarchical, domain-independent verb lexicon that groups verbs into classes based on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is"
L18-1009,kawahara-palmer-2014-single,1,0.220308,"ed on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument stru"
L18-1206,W06-2920,0,0.0607984,"Missing"
L18-1206,W14-5211,0,0.072335,"Missing"
L18-1206,hinrichs-krauwer-2014-clarin,1,0.875661,"Missing"
L18-1206,L16-1262,1,0.886086,"Missing"
L18-1206,L16-1680,1,0.905458,"Missing"
L18-1282,L16-1503,1,0.819942,"nk xml:id=“r2” target=“#m6” pred=“ancient” distr=“individual”/&gt; &lt;adNLink xml:id=“r3” target=“#m7” pred=“chinese” distr=“individual”/&gt; &lt;adNLink xml:id=“r4” target=“#m10” pred=“japanese” distr=“individual”/&gt; &lt;participationLink event=“#e1” participant=“#x1” semRole=“theme” distr=“individual” eventScope=“narrow”/&gt; (see Bunt, 2010). This approach supports the design of alternative user-friendly representations, allowing for example to use tabular forms or other formats that are more convenient for human annotators and researchers than XML representations. This has been exploited in the DialogBank (Bunt et al., 2016), a resource of dialogues annotated according to the ISO 24617-2 annotation scheme, with alternative representations and the possibility to convert from one representation to another. 5.2. QuantML Abstract Syntax, Concrete Syntax, and Semantics presses reference domain involvement. (23) a. hman, some, indef, λz.|z |= 3i b. &lt;entity xml:id=“x1” target=“#m1” type=“student” involvement=“3”/&gt; X c. x |X |= 3, x∈X ⇒ STUDENT(x) (24) IA (hP, 3, indef, Ci) = The annotation structures defined by the QuantML abstract syntax consist of entity structures and link structures. The concrete syntax specifies a"
L18-1282,W15-0201,1,0.354177,"the theme role with individual distributivity, is interpreted as the DRS in (4b), and the sentence “Two men lifted a piano” is interpreted as the DRS (4c), obtained by combining the DRSs for the NPs, the verb, and the semantic role relations. X (4) a. 1 E, Y b. e y ⇒ y∈Y e∈E theme(e,y) X, Y, E c. 2.3. |X |= 2, x ∈ X ⇒ MAN(x), y ∈ Y ⇒ PIANO(y), e ∈ E ⇒ LIFT(e), e y ⇒ agent(e,X) y∈Y theme(e,y) Principles of Semantic Annotation A third pillar of the approach to quantification annotation proposed in this paper is formed by the ISO principles of semantic annotation (ISO standard 24617-6; see also (Bunt, 2015) and (Pustejovsky et al., 201), which require an annotation scheme to have a three-part definition consisting of (1) an abstract syntax that specifies the possible annotation structures in set-theoretical terms, such as pairs and triples of concepts; (2) a concrete syntax, that specifies a representation format of annotation structures as XML expressions; (3) a semantics that specifies the meaning of annotation structures. This formal definition is supported by a metamodel that captures the fundamental concepts used in annotations and the way they are related. This organization ensuress that s"
L18-1335,W00-1423,0,0.223433,", using both modalities complementarily increases human working memory and decreases cognitive load (Dumas et al., 2009). Visual information has been shown to be particularly useful in establishing mutual understanding that enables further communication (Clark and Wilkes-Gibbs, 1986; Clark and Brennan, 1991; Dillenbourg and Traum, 2006; Eisenstein et al., 2008a; Eisenstein et al., 2008b). We will hereafter refer to this type of shared understanding as “common ground,” which can be expressed in multiple modalities. Coordination between humans using non-verbal communication (cf. Cassell (2000), Cassell et al. (2000)) can be adapted to the HCI domain, particularly in the context of shared visual workspaces (Fussell et al., 2000; Kraut et al., 2003; Fussell et al., 2004; Gergle et al., 2004). Allowing for shared gaze has been shown to increase performance in spatial tasks in shared collaborations (Brennan et al., 2008), and the co-involvement of gaze and speech have also been studied in interaction with robots and avatars (Mehlmann et al., 2014; Skantze et al., 2014; Andrist et al., 2017). In the context of shared physical tasks in a common workspace, shared perception creates the context for the conversat"
L18-1335,L16-1551,0,0.0287932,"esture often refers to interfaces with multimodal displays, such as those on mobile devices (Oviatt, 2003; Lemmel¨a et al., 2008; Johnston, 2009). Evaluation of embodied virtual agents is often focused on the agent’s “personality” or nonverbal actions, to help overcome the “uncanny valley” effect (Kr¨amer et al., 2007). However, recent developments in multimodal technology and robotics provide resources on formally evaluating the success of multimodal grounding operations (e.g., Declerck et al. (2010), Hough and Schlangen (2016), Zarrieß and Schlangen (2017)), or of interactive systems (e.g., Fotinea et al. (2016)). Many of the newest methods rely on datasets gathered using high-end technology and complex experimental setups, including motion capture, multiple depth cameras, rangefinding sensors, or geometrically-calibrated accelerometry (systems rarely rely on all of these as that would be prohibitive). Our evaluation scheme is intended to be situationagnostic and relies solely on logging the time and nature of interactions between interlocutors, conditioning on semantic elements during post-processing. In addition, using a suite of gesture-recognition software running on Titan X/Xp GPUs, the experime"
L18-1335,gebre-etal-2012-towards,0,0.0336925,"Missing"
L18-1335,W16-3637,0,0.0215985,"on multimodal evaluation also includes evaluation of gestural usage, although in this case gesture often refers to interfaces with multimodal displays, such as those on mobile devices (Oviatt, 2003; Lemmel¨a et al., 2008; Johnston, 2009). Evaluation of embodied virtual agents is often focused on the agent’s “personality” or nonverbal actions, to help overcome the “uncanny valley” effect (Kr¨amer et al., 2007). However, recent developments in multimodal technology and robotics provide resources on formally evaluating the success of multimodal grounding operations (e.g., Declerck et al. (2010), Hough and Schlangen (2016), Zarrieß and Schlangen (2017)), or of interactive systems (e.g., Fotinea et al. (2016)). Many of the newest methods rely on datasets gathered using high-end technology and complex experimental setups, including motion capture, multiple depth cameras, rangefinding sensors, or geometrically-calibrated accelerometry (systems rarely rely on all of these as that would be prohibitive). Our evaluation scheme is intended to be situationagnostic and relies solely on logging the time and nature of interactions between interlocutors, conditioning on semantic elements during post-processing. In addition,"
L18-1335,W13-4030,0,0.0315761,"ned objects, events, and actions, and use the VoxML-based simulation implementation VoxSim to create the environment in which a multimodal interaction involving natural language and gesture takes place. This allows us to exercise VoxML object and event semantics to assess conditions on the success or failure of the interaction. 2. Multimodal Interaction A wealth of prior work exists on the role of gestural information in human-computer interaction. “Put-that-there” (Bolt, 1980) included deixis for disambiguation, and inspired a community surrounding multimodal integration (Dumas et al., 2009; Kennington et al., 2013; Turk, 2014). As speech and gesture are processed partially independently (Quek et al., 2002), using both modalities complementarily increases human working memory and decreases cognitive load (Dumas et al., 2009). Visual information has been shown to be particularly useful in establishing mutual understanding that enables further communication (Clark and Wilkes-Gibbs, 1986; Clark and Brennan, 1991; Dillenbourg and Traum, 2006; Eisenstein et al., 2008a; Eisenstein et al., 2008b). We will hereafter refer to this type of shared understanding as “common ground,” which can be expressed in multipl"
L18-1335,C16-2012,1,0.93544,"own to increase performance in spatial tasks in shared collaborations (Brennan et al., 2008), and the co-involvement of gaze and speech have also been studied in interaction with robots and avatars (Mehlmann et al., 2014; Skantze et al., 2014; Andrist et al., 2017). In the context of shared physical tasks in a common workspace, shared perception creates the context for the conversation between interlocutors (Lascarides and Stone, 2006; Lascarides and Stone, 2009b; Clair et al., 2010; Matuszek et al., 2014), and it is this shared space that gives many gestures, such as pointing, their meaning (Krishnaswamy and Pustejovsky, 2016a). Dynamic computation of discourse (Asher and Lascarides, 2003) becomes more complex with multiple modalities but embodied actions (such as coverbal gestures) fortunately do not seem to violate coherence relations (Lascarides and Stone, 2009a). Prior work on multimodal evaluation also includes evaluation of gestural usage, although in this case gesture often refers to interfaces with multimodal displays, such as those on mobile devices (Oviatt, 2003; Lemmel¨a et al., 2008; Johnston, 2009). Evaluation of embodied virtual agents is often focused on the agent’s “personality” or nonverbal action"
L18-1335,W17-6919,1,0.869962,"ural events with both actions and words. VoxSim is built on the platform created by VoxML “voxemes,” or semantic visual objects, and therefore allows the direct interpretation of gestures mapped through dynamic semantics. A sample voxeme for a gesture is given above in Section 2.1. The system also accepts speech input for simple directions, answers to yes/no questions, and object disambiguation by attributive adjective. Further information about voxeme properties is laid out in Pustejovsky and Krishnaswamy (2016). 2.3. Scenario The sample interaction is adopted from functionality presented in Krishnaswamy et al. (2017). In this scenario, a human and an avatar in the VoxSim environment must collaborate to complete a simple construction task using virtual blocks that are manipulated by the avatar. The human has a plan or goal configuration that they must instruct the avatar to reach using a combination of gestures and natural language instructions. The avatar in turn communicates through gestures and natural language output to request clarification of ambiguous instructions or present its interpretation of the human’s commands. The human may indicate (point to) blocks and instruct the avatar to slide and move"
L18-1335,L16-1730,1,0.784034,"eryday use, naive users will come to expect their interactions to approximate what they are familiar with when communicating with another human, multimodally. With increased interest in multimodal interaction comes a need to evaluate the performance of a multimodal system on all levels with which it engages the user. Such evaluation should be modality-agnostic and assess the success of communication between human and computer, based on the semantics of objects, events, and actions situated within the shared context created by the human-computer interaction. We use the modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) as the platform for modeling the aforementioned objects, events, and actions, and use the VoxML-based simulation implementation VoxSim to create the environment in which a multimodal interaction involving natural language and gesture takes place. This allows us to exercise VoxML object and event semantics to assess conditions on the success or failure of the interaction. 2. Multimodal Interaction A wealth of prior work exists on the role of gestural information in human-computer interaction. “Put-that-there” (Bolt, 1980) included deixis for disambiguation, and inspired a community surrounding"
L18-1335,D17-1100,0,0.0293298,"o includes evaluation of gestural usage, although in this case gesture often refers to interfaces with multimodal displays, such as those on mobile devices (Oviatt, 2003; Lemmel¨a et al., 2008; Johnston, 2009). Evaluation of embodied virtual agents is often focused on the agent’s “personality” or nonverbal actions, to help overcome the “uncanny valley” effect (Kr¨amer et al., 2007). However, recent developments in multimodal technology and robotics provide resources on formally evaluating the success of multimodal grounding operations (e.g., Declerck et al. (2010), Hough and Schlangen (2016), Zarrieß and Schlangen (2017)), or of interactive systems (e.g., Fotinea et al. (2016)). Many of the newest methods rely on datasets gathered using high-end technology and complex experimental setups, including motion capture, multiple depth cameras, rangefinding sensors, or geometrically-calibrated accelerometry (systems rarely rely on all of these as that would be prohibitive). Our evaluation scheme is intended to be situationagnostic and relies solely on logging the time and nature of interactions between interlocutors, conditioning on semantic elements during post-processing. In addition, using a suite of gesture-reco"
M93-1015,C92-2099,0,0.0640773,"Missing"
M93-1015,J91-4003,1,0.853549,"count of the tokens in the initial corpora wa s used to highlight those words which should be targeted, essentially determining what core vocabular y elements are of rnost importance . In general, the lexical structures used by the system can be though t of as providing for the shallowest possible semantic decomposition while still capturing significan t generalizations about how words relate conceptually to one another . 168 Deriving the Lexicon from Machine-Readable Resource s The lexical knowledge base consists of lexical items called generative lexical structures (GLSs), afte r Pustejovsky[9] . This model of semantic knowledge associated with words is based on a system o f generative devices which is able to recursively define new word senses for lexical items in the language . For this reason, the algorithm and associated dictionary is called a generative lexicon . The lexical structures contain conventional syntactic and morphological information along with detailed typin g information about arguments The creation of the GLS lexicon begins with the printer 's tape of the Longman Dictionary of Contemporary English (LDOCE), Proctor[8] . This was parsed and analysed by the CRL lexi"
M93-1015,H92-1047,1,0.861818,"Missing"
M93-1015,J93-2005,1,0.781633,"rmation about arguments The creation of the GLS lexicon begins with the printer 's tape of the Longman Dictionary of Contemporary English (LDOCE), Proctor[8] . This was parsed and analysed by the CRL lexical grou p to give a tractable formatted version of LDOCE called LEXBASE[5] . LEXBASE contains syntacti c codes, inflectional variants, and boxcodes, selectional information for verbs and nouns, indicatin g generally what kind of arguments are well-formed with that lexical item . A GLS entry is automaticall y derived from LEXBASE by parsing the LEXBASE format for specific semantic information [11] . Th e most novel aspect of this conversion involves parsing the example sentences as well as parenthetical texts in the definition . This gives a much better indication of argument selection for an item than d o the the boxcodes alone . For example, the verb market is converted into the following GLS entry as a result of this initial mapping . gls(market , syn([type(v) , code (gcode_t1) , eventstr([]) , ldoce_id(market_1_1) , caseinfo([subcatl(A1) , subcat2(A2) , case(A1,np) , case(A2,np)]) , inflection([ing(marketing) , pastp(marketed) , plpast(marketed) , singpastl(marketed) , singpast2(ma"
M93-1015,P92-1022,0,0.0125926,"basic transitive) . That is, the cospec encodes explicit information regarding the linear positioning of arguments, as well as semantic constraints on the arguments as imposed by the typing information i n the qualia . The syntactic representation of a word's environment may appear flat, but the semanti c interpretation is based on a unification-like algorithm which creates a much richer functional structure . Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) . Lexically Encoding Idiomatic and Phrasal Structure s One of the advantages to the highly lexical approach being taken here is the ability to encode idiomati c expressions and phrasal expressions as part of the lexicon proper, where motivated by statistical confir mation of the collocations . For example, in the English JV corpus, it so happens that reporting verb s such as announce very often appear with tensed clause complements carrying pronominal subjects, a s below : IBM ; announced that it; had e"
P05-3021,W97-0810,0,0.041464,"Missing"
P05-3021,W01-1315,0,0.0329764,"(Sang and D´ej´ean, 2001); they are currently being implemented as sequences of finite-state transducers along the lines of (A¨ıtMokhtar and Chanod, 1997). Evaluation results are not yet available. 6 SputLink SputLink is a temporal closure component that takes known temporal relations in a text and derives new 83 implied relations from them, in effect making explicit what was implicit. A temporal closure component helps to find those global links that are not necessarily derived by other means. SputLink is based on James Allen’s interval algebra (1983) and was inspired by (Setzer, 2001) and (Katz and Arosio, 2001) who both added a closure component to an annotation environment. Allen reduces all events and time expressions to intervals and identifies 13 basic relations between the intervals. The temporal information in a document is represented as a graph where events and time expressions form the nodes and temporal relations label the edges. The SputLink algorithm, like Allen’s, is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations. For example, if A precedes B and B precedes C, then we can compose the two relations"
P05-3021,N03-2019,1,0.852311,"each event. For example, a past tense non-stative verb followed by a past perfect non-stative verb, with grammatical aspect maintained, suggests that the second event precedes the first. GUTenLINK uses default rules for ordering events; its handling of successive past tense nonstative verbs in case (iii) will not correctly order sequences like Max fell. John pushed him. GUTenLINK is intended as one component in a larger machine-learning based framework for ordering events. Another component which will be developed will leverage document-level inference, as in the machine learning approach of (Mani et al., 2003), which required annotation of a reference time (Reichenbach, 1947; Kamp and Reyle, 1993) for the event in each finite clause. 1 TimeBank is a 200-document news corpus manually annotated with TimeML tags. It contains about 8000 events, 2100 time expressions, 5700 TLINKs and 2600 SLINKs. See (Day et al., 2003) and www.timeml.org for more details. An early version of GUTenLINK was scored at .75 precision on 10 documents. More formal Precision and Recall scoring is underway, but it compares favorably with an earlier approach developed at Georgetown. That approach converted eventevent TLINKs from"
P05-3021,W01-0708,0,0.0278935,"Missing"
P05-3021,P00-1010,1,\N,Missing
P06-1095,P92-1030,0,0.0591899,"of rules derived from human intuitions. 1 Introduction The growing interest in practical NLP applications such as question-answering and text summarization places increasing demands on the processing of temporal information. In multidocument summarization of news articles, it can be useful to know the relative order of events so as to merge and present information from multiple news sources correctly. In questionanswering, one would like to be able to ask when an event occurs, or what events occurred prior to a particular event. A wealth of prior research by (Passoneau 1988), (Webber 1988), (Hwang and Schubert 1992), (Kamp and Reyle 1993), (Lascarides and Asher 1993), (Hitzeman et al. 1995), (Kehler 2000) and others, has explored the different knowledge sources used in inferring the temporal ordering of events, including temporal adverbials, tense, aspect, rhetorical relations, pragmatic conventions, and background knowledge. For example, the narrative convention of events being described in the order in which they occur is followed in (1), but overridden by means of a discourse relation, Explanation in (2). (1) Max stood up. John greeted him. (2) Max fell. John pushed him. In addition to discourse relat"
P06-1095,N04-1020,0,0.0106106,"Missing"
P06-1095,P04-1074,0,0.124056,"Missing"
P06-1095,N03-2019,1,0.411076,"Missing"
P06-1095,P00-1010,1,0.81515,"Missing"
P06-1095,J88-2005,0,0.0930769,"Missing"
P06-1095,H05-1088,1,0.378139,"Missing"
P06-1095,setzer-gaizauskas-2000-annotating,0,0.0424773,"54 33.33 25.00 84.29 66.37 60.86 82.54 94.20 38.60 88.23 97.26 68.29 83.87 90.90 47.72 89.30 96.09 Event-Time 88.25 (62.3) Prec Rec F 0 0 0 76.47 79.31 73.68 86.07 90.16 74.28 77.97 56.00 80.78 93.56 75.36 78.62 63.63 83.34 91.83 Table 2. Machine learning results using unclosed and closed data 3.2 Expanding Training Data using Temporal Reasoning To expand our training set, we use a temporal closure component SputLink (Verhagen 2004), that takes known temporal relations in a text and derives new implied relations from them, in effect making explicit what was implicit. SputLink was inspired by (Setzer and Gaizauskas 2000) and is based on Allen’s interval algebra, taking into account the limitations on that algebra that were pointed out by (Vilain et al. 1990). It is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations in a document. SputLink’s transitivity table is represented by 745 axioms. An example axiom: If relation(A, B) = BEFORE && relation(B, C) = INCLUDES then infer relation(A, C) = BEFORE Once the TLINKs in each document in the corpus are closed using SputLink, the same vector generation procedure and feature represe"
P06-1095,E06-1049,0,\N,Missing
P06-1095,J88-2006,0,\N,Missing
P06-1095,E95-1035,0,\N,Missing
P06-1095,W04-3205,0,\N,Missing
P06-1095,W01-1309,0,\N,Missing
P85-1012,E85-1037,0,0.0403318,"Missing"
P85-1012,E85-1027,1,\N,Missing
P87-1024,P85-1038,0,0.0340052,"Missing"
P87-1024,P85-1009,0,0.0371857,"Missing"
P87-1028,J82-2002,0,0.0767905,"Missing"
P87-1028,W98-1400,0,0.130925,"Missing"
P87-1028,E85-1027,1,0.88243,"Missing"
P87-1028,J85-4002,0,\N,Missing
pustejovsky-etal-2006-towards,W91-0217,0,\N,Missing
pustejovsky-etal-2006-towards,bel-etal-2000-simple,0,\N,Missing
pustejovsky-etal-2006-towards,havasi-etal-2006-bulb,1,\N,Missing
pustejovsky-moszkowicz-2012-role,C08-2024,1,\N,Missing
pustejovsky-moszkowicz-2012-role,pustejovsky-etal-2010-iso,1,\N,Missing
pustejovsky-moszkowicz-2012-role,W11-0409,0,\N,Missing
pustejovsky-moszkowicz-2012-role,W03-0804,0,\N,Missing
pustejovsky-moszkowicz-2012-role,saint-dizier-2006-prepnet,0,\N,Missing
pustejovsky-yocum-2014-image,W09-3716,1,\N,Missing
pustejovsky-yocum-2014-image,W10-0707,0,\N,Missing
pustejovsky-yocum-2014-image,W10-0721,0,\N,Missing
pustejovsky-yocum-2014-image,D13-1128,0,\N,Missing
pustejovsky-yocum-2014-image,P08-1032,0,\N,Missing
pustejovsky-yocum-2014-image,W03-0804,0,\N,Missing
pustejovsky-yocum-2014-image,W09-3009,0,\N,Missing
pustejovsky-yocum-2014-image,kordjamshidi-etal-2010-spatial,0,\N,Missing
Q14-1012,J86-2003,0,0.0356834,"l subdomain. 3 Interpreting ‘Event’ and Temporal Expressions in the Clinical Domain Much prior work has been done on standardizing the annotation of events and temporal expressions in text. The most widely used approach is the ISOTimeML specification (Pustejovsky et al., 2010), an ISO standard that provides a common framework for annotating and analyzing time, events, and event relations. As defined by ISO-TimeML, an E VENT refers to anything that can be said “to obtain or hold true, to happen or to occur”. This is a broad notion of event, consistent with Bach’s use of the term “eventuality” (Bach, 1986) as well as the notion of fluents in AI (McCarthy, 2002). Because the goals of the THYME project involve automatically identifying the clinical timeline for a patient from clincal records, the scope of what should be admitted into the domain of events is interpreted more broadly than in ISO-TimeML3 . Within the THYME-TimeML guideline, an E VENT is anything relevant to the clinical timeline, i.e., anything that would show up on a detailed timeline of the patient’s care or life. The best single-word syntactic head for the E VENT is then used as its span. For example, a diagnosis would certainly"
Q14-1012,S13-2002,1,0.783065,"rk A BEFORE C, then an equivalent inferred TLINK would be used to match it. E VENT and T IMEX 3 IAA was generated based on exact and overlapping spans, respectively. These results are reported in Table 3. The THYME corpus also differs from ISOTimeML in terms of E VENT properties, with the addition of DocTimeRel, ContextualModality and ContextualAspect. IAA for these properties is in Table 4. 7.3 Baseline Systems To get an idea of how much work will be necessary to adapt existing temporal information extraction systems to the clinical domain, we took the freely available ClearTK-TimeML system (Bethard, 2013), 151 which was among the top performing systems in TempEval 2013 (UzZaman et al., 2013), and evaluated its performance on the THYME corpus. ClearTK-TimeML uses support vector machine classifiers trained on the TempEval 2013 training data, employing a small set of features including character patterns, tokens, stems, part-of-speech tags, nearby nodes in the constituency tree, and a small time word gazetteer. For E VENTs and T IMEX 3s, the ClearTK-TimeML system could be applied directly to the THYME corpus. For DocTimeRels, the relation for an E VENT was taken from the TLINK between that E VENT"
Q14-1012,W13-1903,1,0.813475,"ation for these events, (3) the interaction of general and domain-specific events and their importance in the final timeline, and, more generally, (4) the importance of rough temporality and narrative containers as a step towards finer-grained timelines. We have several avenues of ongoing and future work. First, we are working to demonstrate the utility of the THYME corpus for training machine learning models. We have designed support vector machine models with constituency tree kernels that were able to reach an F1-score of 0.737 on an E VENT-T IMEX 3 narrative container identification task (Miller et al., 2013), and we are working on training models to identify events, times and the remaining types of temporal relations. Second, as per our motivating use cases, we are working to integrate this annotation data with timeline visualization tools and to use these annotations in quality-of-care research. For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al., under review)]. Finally, we plan to explore the application of our notion of an event (anything that should be visible on a domain-appropriate timel"
Q14-1012,miltsakaki-etal-2004-penn,0,0.0158666,"h the number of events and times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force"
Q14-1012,W04-0210,0,0.0401525,"d times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force annotations to"
Q14-1012,W11-0419,1,0.93622,"s, as well as to develop state-of-the-art algointerventions and diagnostics which have been thus rithms to train and test on this dataset. far attempted. In other sections, the doctor may outDeriving timelines from news text requires the conline her current plan for the patient’s treatment, then crete realization of context-dependent assumptions later describe the patient’s specific medical history, about temporal intervals, orderings and organization, allergies, care directives, and so forth. underlying the explicit signals marked in the text Most critically for temporal reasoning, each clin(Pustejovsky and Stubbs, 2011). Deriving patient ical note reflects a single time in the patient’s treathistory timelines from clinical notes also involves ment history at which all of the doctor’s statements these types of assumptions, but there are special deare accurate (the D OCTIME), and each section tends mands imposed by the characteristics of the clinical to describe events of a particular timeframe. For narrative. Due to both medical shorthand practices example, ‘History of Present illness’ predominantly and general domain knowledge, many event-event describes events occuring before D OCTIME, whereas relations are"
Q14-1012,pustejovsky-etal-2010-iso,1,0.785939,"e can 2 The Nature of Clinical Documents be both domain-specific and complex, and are often we have been examining left implicit, requiring significant domain knowledge In the THYME corpus, 1 notes from a large healthcare 1,254 de-identified to accurately detect and interpret. In this paper, we discuss the demands on accurately practice (the Mayo Clinic), representing two distinct annotating such temporal information in clinical fields within oncology: brain cancer, and colon cannotes. We describe an implementation and extension cer. To date, we have principally examined two difof ISO-TimeML (Pustejovsky et al., 2010), devel- ferent general types of clinical narrative in our EHRs: oped specifically for the clinical domain, which we clinical notes and pathology reports. Clinical notes are records of physician interactions refer to as the “THYME Guidelines to ISO-TimeML” (“THYME-TimeML”), where THYME stands for with a patient, and often include multiple, clearly “Temporal Histories of Your Medical Events”. A sim- delineated sections detailing different aspects of the plified version of these guidelines formed the basis patient’s care and present illness. These notes are for the 2012 i2b2 medical-domain tempo"
Q14-1012,S13-2001,1,0.830816,"Missing"
Q14-1012,W08-0606,0,0.0123269,"for situations where doctors proffer a diagnosis, but do so cautiously, to avoid legal liability for an incorrect diagnosis or for overlooking a correct one. For example: (3) a. The signal in the MRI is not inconsistent with a tumor in the spleen. b. The rash appears to be measles, awaiting antibody test to confirm. These H EDGED E VENTs are more real than a hypothetical diagnosis, and likely merit inclusion on a timeline as part of the diagnostic history, but must not be conflated with confirmed fact. These (and other forms of uncertainty in the medical domain) are discussed extensively in (Vincze et al., 2008). In contrast, G ENERIC E VENTs do not refer to the patient’s illness or treatment, but instead discuss illness or treatment in general (often in the patient’s specific demographic). For example: (4) In other patients without significant comorbidity that can tolerate adjuvant chemotherapy, there is a benefit to systemic adjuvant chemotherapy. These sections would be true if pasted into any patient’s note, and are often identical chunks of text repeatedly used to justify a course of action or treatment as well as to defend against liability. Contextual Aspect (to distinguish from grammatical as"
rumshisky-etal-2012-word,D08-1027,0,\N,Missing
rumshisky-etal-2012-word,S07-1001,0,\N,Missing
rumshisky-etal-2012-word,N06-2015,0,\N,Missing
rumshisky-etal-2012-word,P06-1014,0,\N,Missing
rumshisky-etal-2012-word,W10-0731,0,\N,Missing
rumshisky-etal-2012-word,W11-0409,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0807,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0813,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-1908,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,C04-1133,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,briscoe-carroll-2002-robust,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,pustejovsky-etal-2006-towards,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0834,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0828,0,\N,Missing
S07-1014,P06-1095,1,0.245989,"ovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006; Boguraev et al., forthcoming). An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expressions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemed more effective. Thus we here present an initial evaluation exercise based on three limited tasks that we bel"
S10-1005,J05-1004,0,0.277012,"in argument selection. For example, in (2) below, the sense annotation for the verb enjoy should arguably assign similar values to both (2a) and (2b). Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then 27 Proceedings of"
S10-1005,burchardt-etal-2006-salsa,0,0.0282901,"nnotation for the verb enjoy should arguably assign similar values to both (2a) and (2b). Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then 27 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010"
S10-1005,W04-1908,1,0.864145,"Missing"
S10-1005,W09-3716,1,0.701906,". Train: Algorithm is trained over a corpus annotated with the target feature set; Test: Algorithm is tested against held-out data; Evaluate: Standardized evaluation of results; Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), and TimeBank (Pustejovsky et al., 2005). 1 This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 28 e. Hear, sense perceive physical sound : HUMAN hear SOUND We used a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). Types were selected for their prevalence in manually identified selection context patterns developed for several hundred English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The types used for annotation were: ABSTRACT ENTITY, ANIMATE , ARTIFACT, A"
S10-1005,J91-4003,1,0.586536,"whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Valeria Quochi ILC-CNR Pisa, Italy (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selectio"
S10-1005,W08-1206,1,0.854565,"rchitecture 4.1 Data Set Construction Phase: English For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: This set of types is purposefully shallow and non-hierarchical. For example, HUMAN is a subtype of both ANIMATE and PHYSICAL OB JECT , but annotators and system developers were instructed to choose the most relevant type (e.g., HUMAN) and to ignore inheritance. 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 3. A set of sentences was randomly extracted for each target verb from the BNC (Burnard, 1995). The extracted sentences were parsed automatically, and the sentences organized according to the grammatical relation the target verb was involved in. Sentences were excluded from the set if the target argument was expressed as anaphor, or was not present in the sentence. The semantic he"
S10-1005,bel-etal-2000-simple,0,0.23492,"nformare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. HUMAN ARTIFACT 7. The Italian training data contained 1466 instances, 381 of which are coercions; the test data had 1463 instances, with 384 coercions. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates were adapted from the Italian Pattern Dictionary (Hanks and Jezek, 2007) and mapped to their SIMPLE equivalents (Lenci et al., 2000). a. arrivare, sense reach a location: [prep] LOCATION → HUMAN (accusare, annunciare) → HUMAN (annunciare, avvisare) EVENT → LOCATION (arrivare, raggiungere) ARTIFACT → EVENT (cominciare, completare) EVENT → DOCUMENT (leggere, divorare) HUMAN → DOCUMENT (leggere, divorare) EVENT → SOUND (ascoltare, echeggiare) ARTIFACT → SOUND (ascoltare, echeggiare) LOCATION 5 Data Format The test and training data were provided in XML. The relation between the predicate (viewed as a function) and its argument were represented by composition link elements (CompLink), as arriva 30 shown below. The test data di"
S10-1005,verhagen-2010-brandeis,0,0.0257747,"en argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: 4. Word sense disambiguation of the target predicate was performed manually on each extracted sentence, matching the target against the sense inventory and the corresponding type templates as described above. The appropriate senses were then saved into the database along with the associated type template. 5. The sentences containing coercive senses of the target verbs were loaded into the Brandeis Annotation Tool (Verhagen, 2010). Annotators were presented with a list of sentences and asked to determine whether the argument in the specified grammatical relation to the target belongs to the type associated with that sense in the corresponding template. Disagreements were resolved by adjudication. a. Arrive (at), sense reach a destination or goal : HU MAN arrive at LOCATION b. Cancel, sense call off : HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PROPOSITION d. Finish, sense complete an activity: HUMAN finish EVENT 29 Coerion Type EVENT → LOCATION ARTIFACT → EVENT EVENT → PROPO"
S10-1005,S07-1007,0,0.0214918,"in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Valeria Quochi ILC-CNR Pisa, Italy (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly. In fact, the metonymy example in (1) is an instance of a much more pervasive phen"
S10-1005,W04-2705,0,0.0983243,"Missing"
S10-1005,miltsakaki-etal-2004-penn,0,\N,Missing
S10-1005,S07-1016,0,\N,Missing
S10-1005,E06-2001,0,\N,Missing
S10-1005,C04-1133,1,\N,Missing
S10-1005,ide-suderman-2004-american,0,\N,Missing
S10-1005,ohara-2008-lexicon,0,\N,Missing
S10-1010,verhagen-2010-brandeis,1,0.752179,"B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: X precision recall f -measure Table 1: Corpus size and relation tasks All corpora include event and timex annotation. The French corpus contained a subcorpus with temporal relations but these relations were not split into the four tasks C through F. Annotation proceeded in two phases: a dual annotation phase where two annotators annotate each document and an adjudication phase where a judge resolves disagreements between the annotators. Most languages used BAT, the Brandeis Annotation Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3 Evaluation Metrics = = = tp/(tp + f p) tp/(tp + f n) 2 ∗ (P ∗ R)/(P + R) Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the"
S10-1010,taule-etal-2008-ancora,0,0.0880496,"Missing"
S10-1010,S07-1014,1,0.681823,"ons in the text and is identical to the TIMEX3 tag in TimeML. Times can be expressed syntactically by adverbial or prepositional phrases, as shown in the following example. (1) a. on Thursday b. November 15, 2004 c. Thursday evening d. in the late 80’s e. later this afternoon Introduction The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text. However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate. The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. In the rest of this paper, we first introduce the data that we are dealing with. Which gets us in a position to present the lis"
S13-2001,derczynski-gaizauskas-2010-analysing,1,0.706855,"ems (silver). The TempEval-3 platinum evaluation corpus was annotated/reviewed by the organizers, who are experts in the area. This process used the TimeML Annotation Guidelines v1.2.1 (Saur´ı et al., 2006). Every file was annotated independently by at least two expert annotators, and a third was dedicated to adjudicating between annotations and merging the final result. Some annotators based their work on TIPSem annotation suggestions (Llorens et al., 2012b). The GATE Annotation Diff tool was used for merging (Cunningham et al., 2013), a custom TimeML validator ensured integrity,3 and CAVaT (Derczynski and Gaizauskas, 2010) was used to determine various modes of TimeML mis-annotation and inconsistency that are inexpressable via XML schema. Post-exercise, that corpus (TempEval-3 Platinum with around 6K tokens, on completely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The Temp"
S13-2001,S10-1063,1,0.726158,"13 shows the results from event extraction. In this case, the previous state-of-the-art is not improved. Table 14 only shows the results obtained in temporal awareness by the state-of-the-art system since there were not participants on this task. We observe that TIPSemB-F approach offers competitive results, which is comparable to results obtained in TE3 English test set. 6.1 Comparison with TempEval-2 TempEval-2 Spanish test set is included as a subset of this TempEval-3 test set. We can therefore compare the performance across editions. Furthermore, we can include the full-featured TIPSem (Llorens et al., 2010), which unlike TIPSemB-F used the AnCora (Taul´e et al., 2008) corpus annotations as features including semantic roles. For timexes, as can be seen in Table 15, the original TIPSem obtains better results for timex extraction, which favours the hypothesis that machine learning systems are very well suited for this task (if the training data is sufficiently representative). However, for normalization (value F1), HeidelTime – a rule-engineered system – obtains better results. This indicates that rule-based approaches have the upper hand in this task. TIPSem uses FSS-TimEx TIPSemB-F TIPSem F1 59.0"
S13-2001,padro-stanilovsky-2012-freeling,0,0.0212522,"Missing"
S13-2001,S10-1062,1,0.680883,"ompletely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The TempEval-3 silver evaluation corpus is a 600K word corpus collected from Gigaword (Parker et al., 2011). We automatically annotated this corpus by TIPSem, TIPSem-B (Llorens et al., 2013) and TRIOS (UzZaman and Allen, 2010). These systems were retrained on the corrected TimeBank and AQUAINT corpus to generate the original TimeML temporal relation set. We then merged these three state-of-the-art system outputs using our merging algorithm (Llorens et al., 2012a). In our selected merged configuration all entities and relations suggested by the best system (TIPSem) are added in the merged output. Suggestions from other systems (TRIOS and TIPSem-B) are added in the merged output, only if they are also supported by another system. The weights considered in our configuration are: TIPSem 0.36, TIPSemB 0.32, TRIOS 0.32."
S13-2001,P11-2061,1,0.562387,"the system identified both the entity and attribute (attr) together. Attribute Recall = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Refentity | Attribute Precision = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Sysentity | Attribute F1-score = 2∗p∗r p+r Attribute (Attr) accuracy, precision and recall can be calculated as well from the above information. Attr Accuracy = Attr F1 / Entity Extraction F1 Attr R = Attr Accuracy * Entity R Attr P = Attr Accuracy * Entity P 4.2 Temporal Relation Processing To evaluate relations, we use the evaluation metric presented by UzZaman and Allen (2011).5 This metric captures the temporal awareness of an annotation in terms of precision, recall and F1 score. Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. Unlike TempEval2 relation score, where only categorization is evaluated for relations, this metric evaluates how well pairs of entities are identified, how well the relations are categorized, and how well the events and temporal expressions are extracted. + |Sys− ∩"
S13-2001,S10-1010,1,\N,Missing
S13-2001,taule-etal-2008-ancora,0,\N,Missing
S14-1014,P07-2009,0,0.0337119,"sing it to extract the constituent parts of a predicate/argument representation, and using that output to prompt a behavior in software as a dynamic event structure. More robust parsing will afford us the opportunity to expand the diversity of predicates that the software can handle as well (McDonald and Pustejovsky, 2014). While currently limited to unary and binary predicates, we need to extend the capability to ternary predicates and predicates of greater arity, including the use of adjunct phrases and indirect objects. We are in the process of developing an implementation that uses Boxer (Curran et al., 2007) so that we can create first-order models from the dynamic expressions used here. tact with the floor at all times, the simulated motion would still be considered a “roll” (if rotating around an axis parallel to the floor), or a “slide” (if not), regardless of what the precise direction of motion is. Minimal pairs in a model have to be compared and contrasted in a discriminative way, and thus in modeling a slide predicate versus a roll predicate, knowing that the distinction is one of rotation parallel to the surface is enough to distinguish the two predicates in a model. In a simulation, the"
S14-1014,W13-3004,0,0.0883082,"s reference to values on a scale, C, and ensures that it continues in an order-preserving change through the iterations. When this test references the values on a scale, C, we call this a directed ν-transition (~ν ), e.g., x 4 y, x < y: C? x ν (6) ~ν =df ei −→ ei+1 . (7) loc(z) = x e0 loc(z) = yn ~ ν −→ loc(z) = y1 e1 ~ ν −→ . . . en This now provides us with our dynamic interpretation of directed manner of motion verbs, such as slide, swim, roll, where we have an iteration of assignments of locations, undistinguished except 1 This is consistent with the approach developed in (Fernando, 2009; Fernando, 2013). This approach to a dynamic interpretation of change in language semantics is also inspired by Steedman (2002). 2 Cf. Groenendijk and Stokhof (1990) for dynamic updating, and Naumann (2001) for a related analysis. 101 that the values are order-preserving according to a scalar constraint. This is quite different from the dynamic interpretation of path predicates. Following (Galton, 2004; Pustejovsky and Moszkowicz, 2011), path predicates such as arrive and leave make reference to a “distinguished location”, not an arbitrary location. For example, the ball enters the room is satisified when the"
S14-1014,W13-5401,1,0.441915,"hed in the model. Further, the simulation must specify how they are distinguished, the analogue to informativeness. In this paper, we argue that traditional lexical modeling can benefit greatly from examining how semantic interpretations are contextually and conceptually grounded. We explore a dynamic interpretation of the lexical semantic model developed in Generative Lexicon Theory (Pustejovsky, 1995; Pustejovsky et al., 2014). Specifically, we are interested in using model building (Blackburn 100 makes reference to orientational as well as configurational factors, a view that is pursued in Pustejovsky (2013b). This forces us to look at the various spatio-temporal regions associated with the event participants, and the interactions between them. These issues are relevant to our present concerns, because in order to construct a simulation, a motion event must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the event localization, while optionally including room enough for a frame of reference visualization of the event (the viewer’s perspective). We return to this issue later in the paper when constructing our simulation from the semantic interpretation as"
S14-1014,W13-0705,1,0.559175,"hed in the model. Further, the simulation must specify how they are distinguished, the analogue to informativeness. In this paper, we argue that traditional lexical modeling can benefit greatly from examining how semantic interpretations are contextually and conceptually grounded. We explore a dynamic interpretation of the lexical semantic model developed in Generative Lexicon Theory (Pustejovsky, 1995; Pustejovsky et al., 2014). Specifically, we are interested in using model building (Blackburn 100 makes reference to orientational as well as configurational factors, a view that is pursued in Pustejovsky (2013b). This forces us to look at the various spatio-temporal regions associated with the event participants, and the interactions between them. These issues are relevant to our present concerns, because in order to construct a simulation, a motion event must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the event localization, while optionally including room enough for a frame of reference visualization of the event (the viewer’s perspective). We return to this issue later in the paper when constructing our simulation from the semantic interpretation as"
S14-1014,rumshisky-etal-2012-word,1,0.83263,"onsistent between a minimal pair of behaviors, we can evaluate the quantitative and qualitative differences between the values that do change. As simulations require values to be assigned to variables that can be left unspecified in an ordinary modeling process, simulations expose presuppositions about the semantics of motion verbs and of compositions that would not be necessary in a model alone. In order to evaluate the appropriateness of a given simulation, we are currently experimenting with a strategy often used in classification and annotation tasks, namely pairwise similarity judgments (Rumshisky et al., 2012; Pustejovsky and Rumshisky, 2014). This involves presenting a user with a simple discrimination task that has a reduced cognitive load, comparing the similarity of the example to the target instances. In the present context, a subject is shown a specific simulation resulting from the translation from textual input, through DITL, to the visualization. A set of activity or event descriptions is given, and the subject is then asked to select which best describes the simulation shown; e.g., “Is this a sliding?”, “Is this a rolling?”. The results of this experiment are presently being evaluated. A"
S15-2134,S13-2002,0,0.110539,"arated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented wit"
S15-2134,Q14-1022,1,0.272596,"lar participants, optimized for task: • HITSZ-ICRC4 . rule-based timex module, SVM (liblinear) for event and relation detection and classification • hlt-fbk-ev1-trel1. SVM, separated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-s"
S15-2134,S10-1063,1,0.913075,"• hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented with a postprocessing step. The goal is to analyze how a general time expression reasoner could improve results. The TREFL component is straightforward: resolve all ti"
S15-2134,S13-2001,1,0.797947,"Missing"
S15-2134,S07-1014,1,0.893706,"Missing"
S15-2136,N13-3004,0,0.0310777,"RE P OST E XP or S ET 4. Adjudicators revised and finalized the temporal relations More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a). Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.1 The event, time and temporal relation annotations were distributed separately from the text, in an open source repository2 using the Anafora standoff format (Chen and Styler, 2013). • Identifying event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The spans (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not"
S15-2136,W11-0419,1,0.496993,"AL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not completed in time. • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations in the THYME corpus – Narrative container relations (Pustejovsky and Stubbs, 2011) between events and/or times, represented by T LINK annotations with T YPE =C ONTAINS in the THYME corpus The evaluation was run in two phases: 1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : P = |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is th"
S15-2136,Q14-1012,1,0.497138,"Missing"
S15-2136,P11-2061,0,0.0272505,"ated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A C ON - TAINS B and B C ONTAINS C, so A C ONTAINS C): Pclosure = Rclosure |S ∩ closure(H)| |S| 6 |closure(S) ∩ H| = |H| Fclosure = Participating Systems Three research teams submitted a total of 13 runs: 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 was predicted to be a narrative container, containing only the closest event expression to it in the text. Ba"
S15-2136,S13-2001,1,0.954536,"l of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea. And output annotations over the text that capture the following kinds of information: Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understan"
S15-2136,S07-1014,1,0.916093,"Missing"
S15-2136,S10-1010,1,\N,Missing
S15-2149,W14-0150,0,0.0161128,"HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3 UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final submitted run. 4 These baseline classifiers were developed at Brandeis University by Aaron Levine and Zachary Yocum. Cf. Section 5.1 for full description. 5 This system was developed at Brandeis Un"
S15-2149,J14-1004,0,0.00559209,"d rapid training and model inspection. The hypotheses were written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6 We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Lexical Syntactic Semantic Sparser Id word[-1,0,1] isupper[-1,0,1] wordlen[-1,0,1] pos[-1,0,1] ner[-1,0,1] CATEGORY[-1,0,1] FORM[-1,0,1] LCATEGORY[-1,0,1] LFORM[-1,0,1] Value string binary ternary7 POS tag NER tag Sparser category Sparser form Sparser category Sparser form Table 4: Top Ten Positive Feature Weights Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear"
S15-2149,P05-1045,0,0.0275864,"and F1. c. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (P"
S15-2149,S13-2044,1,0.317352,"that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relation"
S15-2149,kordjamshidi-etal-2010-spatial,1,0.728017,"Missing"
S15-2149,S12-1048,1,0.515402,"tion, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the li"
S15-2149,P14-5010,0,0.010339,"a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial rol"
S15-2149,D14-1162,0,0.119259,"and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants o"
S15-2149,W13-0503,1,0.756586,"ial relations between spatial signals and spatial elements (connected, unconnected, part-of, etc.). b. Identify their attributes. Spatial Orientation Identification (OLink) a. Identify orientational relations between spatial signals and spatial elements (above, under, in front of, etc.). b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE"
S15-2149,W11-0416,0,0.0287912,"1577 61 3 148 34 69 15 16 39 17 19 14 14 Table 1: Corpus Statistics Phase 2 Extent tag adjudication. Phase 3 Link tag argument and attribute annotation. Phase 4 Link tag adjudication. Phases 2 and 4 produced gold standards from annotations in the preceding annotation phases. This annotation strategy ensured that the intermediate gold standard extent tag set was adjudicated before any link tag annotations were performed. The annotation and adjudication effort was conducted at Brandeis University using Multidocument Annotation Environment (MAE) and Multi-annotator Adjudication Interface (MAI) (Stubbs, 2011). We used MAE to perform each phase of the annotation procedure and MAI to adjudicate and produce gold standard standoff annotations in XML format. In addition to the ISO-Space annotation tags and attributes, as a post-process, we also provided sentence and lexical tokenization as a separate standoff annotation layer in the XML data for the training and test sets. Each document was covered by a minimum of three annotators for each annotation phase (though not necessarily the same annotators per phase). As such, we report inter-annotator agreement (IAA) as a mean Fleiss’s κ coefficient for all"
S15-2149,N03-1033,0,0.0146901,"ink.a precision, recall, and F1. c. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellba"
S15-2149,W09-1124,0,0.0506481,"Missing"
S15-2149,R09-1083,0,0.0331692,"Missing"
S15-2149,J05-1004,0,\N,Missing
S15-2149,J14-4004,0,\N,Missing
S16-1165,S16-1195,0,0.0501484,"Missing"
S16-1165,S16-1196,0,0.0406396,"Missing"
S16-1165,S15-2136,1,0.542156,"Missing"
S16-1165,S16-1193,0,0.0677741,"Missing"
S16-1165,N13-3004,1,0.0547857,"Missing"
S16-1165,S16-1192,0,0.064286,"Missing"
S16-1165,S16-1198,0,0.0757468,"Missing"
S16-1165,S16-1190,0,0.0552623,"Missing"
S16-1165,S16-1200,0,0.0379618,"Missing"
S16-1165,S16-1201,0,0.0940692,"Missing"
S16-1165,S16-1199,0,0.0614168,"Missing"
S16-1165,P14-5010,1,0.0104397,"Missing"
S16-1165,W11-0419,1,0.613193,"event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The span (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations. – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. The evaluation was run in two phases: 1. Systems were provided access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were provided access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R)"
S16-1165,Q14-1012,1,0.456145,"Missing"
S16-1165,P11-2061,0,0.0570732,"w a comparison across systems for assigning attribute values, even when different systems produce different numbers of events and times. This metric is calculated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ONTAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against."
S16-1165,S13-2001,1,0.208832,"on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; Bethard et al., 2015) to broaden our understanding of the language of time beyond newswire expressions and structure. Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, rel"
S16-1165,S07-1014,1,0.223174,"Missing"
S17-2093,S15-2136,1,0.731438,"Missing"
S17-2093,N13-3004,0,0.0211527,"Missing"
S17-2093,S17-2177,0,0.026199,"tor machines and structured perceptrons with features including words and part-of-speech tags. For domain adaptation, KULeuven-LIIR tried assigning higher weight to the brain cancer training data, and representing unknown words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) al"
S17-2093,S17-2179,0,0.0278089,"words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) also competed, but did not submit a system description. 8 Evaluation Results Tables 2 to 4 show the results of the evaluation. In all tables, the best system score from each column is in bold. Systems marked with † were submitt"
S17-2093,S17-2181,0,0.0428032,"Missing"
S17-2093,S17-2178,0,0.0383126,"Missing"
S17-2093,S17-2180,0,0.0292967,"on to it in the text. 7 Participating Systems 11 teams submitted a total of 28 runs, 10 for the unsupervised domain adaptation phase, and 18 for the supervised domain adaptation phase. All participating systems trained some form of supervised classifiers, with common features including character n-grams, words, part-of-speech tags, and Unified Medical Language System (UMLS) concept types. Below is a brief description of each participating team, and a note if they performed any more elaborate domain adaptation than simply adding the extra 30 brain cancer notes to their training data. 568 GUIR (MacAvaney et al., 2017) combined conditional random fields, rules, and decision tree ensembles, with features including character n-grams, words, word shapes, word clusters, word embeddings, part-of-speech tags, syntactic and dependency tree paths, semantic roles, and UMLS concept types. Team GUIR KULeuven-LIIR LIMSI-COT ULISBOA Hitachi baseline WuHanNLP Hitachi (P R et al., 2017) combined conditional random fields, rules, neural networks, and decision tree ensembles, with features including character n-grams, word n-grams, word shapes, word embeddings, verb tense, section headers, and sentence embeddings. GUIR LIMS"
S17-2093,S17-2176,0,0.0310189,"Missing"
S17-2093,W11-0419,1,0.216445,"data. All colon cancer data was released as part of Clinical TempEval 2015 and 2016. The Train-10 column is the data from the first 10 patients of the brain cancer Train data, which was the only additional training data released in Clinical TempEval 2017. tested on the annotations of the brain cancer Test set. Systems were again free to use all the raw brain cancer text if they had a way to do so. Note that across all phases, the only brain cancer data released was the Train-10 set. The remainder of the brain cancer data was reserved for future evaluations. 3 – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : Tasks Nine tasks were included (the same as those of Clinical TempEval 2015 and 2016), grouped into three categories: P = • Identifying time expressions (TIMEX3 annotations in the THYME corpus) consisting of the following components: |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is the set of items predi"
S17-2093,S17-2098,0,0.0529925,"Missing"
S17-2093,P11-2061,0,0.101468,"ns were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ON TAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| To predict with the model, the raw text of the test data was searched for all exact character matches of any of the memorized phrases, preferring longer phrases when multiple matches overlapped. Wherever a phrase match was found, an event or time with the memorized (most frequent) attribute values was predicted. Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of systempredicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Human Agreement We also provide two types of human agreement on the tasks, measured with the same evaluation"
S17-2093,S13-2001,1,0.94542,"s were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce when given the text: Data The C"
S17-2093,S07-1014,1,0.86365,"cer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce wh"
sauri-etal-2006-slinket,H05-1088,1,\N,Missing
sauri-etal-2006-slinket,W02-1033,0,\N,Missing
sauri-etal-2006-slinket,P05-3021,1,\N,Missing
sauri-etal-2006-slinket,W04-3103,0,\N,Missing
sauri-etal-2006-slinket,C96-1079,0,\N,Missing
verhagen-etal-2006-annotation,A97-1051,0,\N,Missing
verhagen-etal-2006-annotation,P05-3021,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,C08-3012,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1062,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1075,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1071,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P00-1010,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,H05-1088,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1025,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1108,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1098,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P06-1095,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P05-3021,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,pustejovsky-etal-2010-iso,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P09-1068,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1063,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,R11-1084,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,ide-romary-2006-representing,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1070,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,verhagen-etal-2006-annotation,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,sauri-etal-2006-slinket,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,W10-1920,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,W11-0419,1,\N,Missing
vogel-etal-2012-atlis,S07-1005,0,\N,Missing
vogel-etal-2012-atlis,W03-0907,0,\N,Missing
W02-0312,C96-1021,0,\N,Missing
W02-0312,A00-1026,0,\N,Missing
W02-0312,P83-1019,0,\N,Missing
W04-0208,W01-1605,0,0.0899404,"Missing"
W04-0208,J95-2003,0,0.133007,"can see that even for interpreting such relatively simple discourses, a system might require a variety of sources of linguistic knowledge, including knowledge of tense, aspect, temporal adverbials, discourse relations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle,"
W04-0208,W90-0117,0,0.033397,"l as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambigu"
W04-0208,kingsbury-palmer-2002-treebank,0,0.0616809,"elations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapped to other discourse representations used in computational linguistics. A TDM tree can be converted to a first-order temporal logic representation (where temporal ordering and inclusion operators are added) by expanding the properties of the nodes. These properties include any additional predications made explicitly about the event, e.g., information from thematic arguments and adjuncts. In other words, a full predicate argument representation, e.g., as might be found in the PropBank (Kingsbury and Palmer 2002), can be associated with each node. TDMs can also be mapped to Discourse Representation Structures (DRS) (which in turn can be mapped to a logical form). Since TDMs represent events as pairs of time points (which can be viewed as intervals), and DRT represents events as primitives, we can reintroduce time intervals based on the standard DRT approach (e ⊆ t for events, e O t for states, except for present tense states, where t ⊆ e). Consider an example from the Discourse Representation Theory (DRT) literature (from Kamp and Reyle 1993): (3) a. A man entered the White Hart. b. He was wearing a b"
W04-0208,N03-2019,1,0.824933,"se, time expressions are flagged, and their values normalized, so that Thursday in He left on Thursday would get a resolved ISO time value depending on context (TIMEX2 2004). Finally, temporal relations between events and time expressions (e.g., that the leaving occurs during Thursday) are recorded by means of temporal links (TLINKs) that express Allen-style interval relations (Allen 1984). Several automatic tools have been developed in conjunction with TimeML, including event taggers (Pustejovsky et al. 2003), time expression taggers (Mani and Wilson 2000), and an exploratory link extractor (Mani et al. 2003). Temporal reasoning algorithms have also been developed, that apply transitivity axioms to expand the links using temporal closure algorithms (Setzer and Gaizauskas 2001), (Pustejovsky et al. 2003). However, TimeML is inadequate as a temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating a complex graph that links primitive events and times. 4 Related Frameworks Since the relations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapped to other discourse representations used in computat"
W04-0208,W99-0307,0,0.0257067,"rresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear. Although (Marcu et al. 1999) (Carlson et al. 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. While we appreciate the importance of representing rhetorical relations in order to carry out temporal inferences about event ordering, we believe that there are substantial advantages in isolating the temporal aspects and modeling them separately as TDMs. This greatly simplifies the representation, which we discuss next. 2 Temporal Discourse Models A TDM is a tree-struc"
W04-0208,W01-1311,0,0.0679626,"ime points. So, E0 is an abstract node representing a top-level story, and E1 is an abstract node representing an embedded story. Note that the mentioned events are ordered left to right in text order for notational convenience, but no temporal ordering is directly represented in the tree. Since the nodes in this representation are at a semantic level, the tree structure is not necessarily isomorphic to a representation at the text level, although T1 happens to be isomorphic. Ee Ec Ed C2 = {Ea < Ee, Eb < Ec} Note that the partial ordering C can be extended using T and temporal closure axioms (Setzer and Gaizauskas 2001), (Verhagen 2004), so that in the case of <T2, C2>, we can infer, for example, that Eb < Ed, Ed < Ee, and so forth. In representing states, we take a conservative approach to the problems of ramification and change (McCarthy and Hayes 1969). This is the classic problem of recognizing when states (the effects of actions) change as a result of actions. Any tensed stative predicate will be represented as a node in the tree (progressives are here treated as stative). Consider an example like John walked home. He was feeling great. Here we represent the state of feeling great as being minimally a p"
W04-0208,P00-1010,1,\N,Missing
W04-0208,J03-4002,0,\N,Missing
W04-0208,J88-2006,0,\N,Missing
W04-0208,E95-1035,0,\N,Missing
W04-1908,briscoe-carroll-2002-robust,0,0.0330228,"lements are members. Each feature may be realized by a number of RASP relations. For instance, a feature dealing with objects would take into account RASP relations ’dobj’, ’obj2’, and ’ncsubj’ (for passives). 3 Current Implementation The CPA patterns are developed using the British National Corpus (BNC). The sorted instances are used as a training set for the supervised disambiguation. For the disambiguation task, each pattern is translated into into a set of preprocessing-specific features. The BNC is preprocessed with the RASP parser and semantically tagged with BSO types. The RASP system (Briscoe and Carroll (2002)) generates full parse trees for each sentence, assigning a probability to each parse. It also produces a set of grammatical relations for each parse, specifying the relation type, the headword, and the dependent element. All our computations are performed over the single top-ranked tree for the sentences where a full parse was successfully obtained. Some of the RASP grammatical relations are shown in (10). (10) subjects: ncsubj, clausal (csubj, xsubj) objects: dobj, iobj, clausal complement modifiers: adverbs, modifiers of event nominals We use endocentric semantic typing, i.e., the headword"
W04-1908,W97-0808,0,\N,Missing
W04-1908,C02-1112,0,\N,Missing
W04-1908,C00-1028,0,\N,Missing
W04-1908,C92-2070,0,\N,Missing
W04-1908,N01-1013,0,\N,Missing
W04-1908,A97-1052,0,\N,Missing
W04-1908,P95-1026,0,\N,Missing
W04-1908,J01-3001,0,\N,Missing
W04-1908,P03-1007,0,\N,Missing
W04-1908,P99-1014,0,\N,Missing
W04-1908,P03-2029,0,\N,Missing
W04-1908,W01-0703,0,\N,Missing
W04-1908,W98-0701,0,\N,Missing
W04-1908,L02-1000,0,\N,Missing
W05-0302,P98-1013,0,0.0768075,"Missing"
W05-0302,W99-0302,0,0.0733028,"Missing"
W05-0302,W01-1514,0,0.0273335,"k, NomBank and Coreference, the core predicate argument structures and referents for the arguments. One possible representation format would be to convert each annotation into features and values to be added to a larger feature structure. 1 The resulting feature structure would combine stand alone and offset annotation – it would include actual words and features from the text as well as special features that point to the actual text (character offsets) and, perhaps, syntactic trees (offsets along the lines of PropBank/NomBank). Alternative global annotation schemes include annotation graphs (Cieri & Bird, 2001), and MATE (Carletta, et. al., 1999). There are many areas in which the boundaries between these annotations have not been clearly defined, such as the treatment of support constructions and light verbs, as discussed below. Determining the most suitable format for the merged representation should be a top priority. NomBank would add arguments for report, trial, launch and beginning as follows: According to [Rel_report.01 reports], [Arg1 [ArgM-LOC sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a patrol boat] developed by Kazakhstan] are being conducted and the [ArgM-MNR formal] [Rel_lau"
W05-0302,hajicova-kucerova-2002-argument,0,0.0418506,"Missing"
W05-0302,W04-2705,1,0.895192,"Missing"
W05-0302,W04-0413,1,0.854761,"Missing"
W05-0302,miltsakaki-etal-2004-penn,0,0.0711906,"Missing"
W05-0302,J05-1004,1,0.327965,"fforts. This level could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than producing a single unified representation like Head-driven Phrase Structure Grammar (Pollard and Sag 1994) or the Prague Dependency Tectogramatical Representation (Hajicova & Kucerova, 2002). PropBank (Palmer et al, 2005) annotates predicate argument structure anchored by verbs. NomBank (Meyers, et. al., 2004a) annotates predicate argument structure anchored by nouns. TimeBank (Pustejovsky et al, 2003) 2. The Component Annotation Schemata We describe below existing independent annotation efforts, each one of which is focused on a specific aspect of the semantic representation task: semantic role labeling, 5 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5–12, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tuned to a hiring scenario (MUC-6, 1995),"
W05-0302,W99-0309,0,0.0605805,"esolved in order to bring these different layers together seamlessly. Most of these approaches have annotated the same type of data, Wall Street Journal text, so it is also important to demonstrate that the annotation can be extended to other genres such as spoken language. The demonstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community conse"
W05-0302,J98-2001,1,0.810969,"monstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community consensus on coreference. PropBank: The Penn Proposition Bank focuses on the argument structure of verbs, and provides a corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. An important goal is to provide consistent semantic role label"
W05-0302,W04-2709,0,\N,Missing
W05-0302,W04-2704,1,\N,Missing
W05-0302,J93-2004,0,\N,Missing
W05-0302,W04-0210,1,\N,Missing
W05-0302,C98-1013,0,\N,Missing
W05-0302,poesio-kabadjov-2004-general,1,\N,Missing
W05-0302,W04-2327,1,\N,Missing
W05-1302,W02-1001,0,\N,Missing
W05-1302,W02-0312,1,\N,Missing
W05-1302,P02-1045,0,\N,Missing
W05-1302,W02-0303,0,\N,Missing
W05-1302,O04-1011,0,\N,Missing
W07-1517,miltsakaki-etal-2004-penn,0,0.0398621,"Missing"
W07-1517,J05-1004,0,0.0425206,"a use case. With MAIS, we adopt the following requirements for the interoperability of syntactic and semantic annotations: 1. Each annotation scheme has its own philosophy and is independent from the other annotations. Simple and generally available interfaces provide access to the content of each annotation scheme. 2. Interactions between annotations are not defined a priori, but based on use cases. 3. Simple tree-based and one-directional merging of annotations is useful for visualization of overlap between schemes. The annotation schemes currently embedded in MAIS are the Proposition Bank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and TimeBank (Pustejovsky et al., 2003). Other linguistics annotation schemes like the opinion annotation 2 Interoperability of Annotations Our goal is not to define a static merger of all annotation schemes. Rather, we avoid defining a potentially complex interlingua and instead focus on how information from different sources can be combined pragmatically. A high-level schematic representation of the system architecture is given in figure 1. PropBank NomBank annotation TimeBank initializers PropBank NomBank TimeBank interface interface interface case-based inte"
W07-1517,W04-2705,0,\N,Missing
W07-1517,J93-2004,0,\N,Missing
W09-2414,burchardt-etal-2006-salsa,0,0.0272909,"the task. 1 (1) a. John reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this cas"
W09-2414,ide-suderman-2004-american,0,0.0149133,"l be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) H UMAN deny E NTITY to H UMAN H UMAN deny H UMAN E NTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. Semantic head for the target grammatical relation will be identified in each case. is saved into the database, along with the associated type template. In the second subtask, the"
W09-2414,W04-2705,0,0.0985323,"Missing"
W09-2414,miltsakaki-etal-2004-penn,0,0.0594706,"Missing"
W09-2414,ohara-2008-lexicon,0,0.0191172,"reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial p"
W09-2414,J05-1004,0,0.34099,"the problem in detail and describe the data preparation for the task. 1 (1) a. John reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semant"
W09-2414,S07-1016,0,0.0504028,"Missing"
W09-2414,W04-1908,1,0.906371,"identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying semantic type of the target argument. Sense inventories for the verbs and the type templates associated with different syntactic frames will be provided to the participants. Figure 2: Corpus Development Architecture 3.1 Semantic Types 4 In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). The BSO types were selected for their prevalence in manually identified selection context patterns developed for several hundreds English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The following list of types is currently being used for annotation: Preparing the data for this task will be done in two phases: the data set construction phase and the annotation phase. The first phase consists of (1) selecting the target verbs to be annotated and compiling a sense inventory for each target, and (2)"
W09-2414,W09-3716,1,0.551497,"e House (LOCATION → HUMAN) denied this statement. c. The Boston office called with an update (EVENT → INFO ). b. Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; 89 PropBank (Palmer et al., 2005) NomBank (Meyers et al., 2004) TimeBank (Pustejovsky et al., 2005) Opinion Corpus (Wiebe et al., 2005) Penn Discourse TreeBank (Miltsakaki et al., 2004) 1 This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). The definition of coercion will be extended to include instances of type-shifting due to what we term the qua-relation. (7) a. You can crush the pill (P HYSICAL O BJECT) between two spoons. (Selection) b. It is always possible to crush imagination (A BSTRACT E NTITY qua P HYSICAL O BJECT) under the weight of numbers. (Coercion/qua-relation) In order to determine whether type-shifting has taken place, the classification task must then involve the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on"
W09-2414,J91-4003,1,0.527846,"the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the"
W09-2414,W08-1206,1,0.84921,"E and PHYSICAL OBJECT, the system should simply choose the most relevant type (i.e. HUMAN) and not be concerned with type inheritance. The present set of types may be revised if necessary as the annotation proceeds. 90 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) H UMAN deny E NTITY to H UMAN H UMAN deny H UMAN E NTITY The set of type templates for each verb will be built using"
W09-2414,S07-1007,0,\N,Missing
W09-2414,C04-1133,1,\N,Missing
W09-2418,S07-1014,1,0.624748,"es. The data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a handbuilt gold standard of annotated texts using the TimeML markup scheme.1 The data sets included sentence boundaries, TIMEX 3 tags (including the special document creation time tag), and EVENT tags. For tasks A and B, a restricted set of events was used, namely those events that occur more than 5 times in TimeBank. For all three tasks, the relation labels used were BEFORE, AFTER, OVER LAP , BEFORE - OR - OVERLAP , OVERLAP - OR - AFTER and VAGUE.2 For a more elaborate description of TempEval-1, see (Verhagen et al., 2007; Verhagen et al., 2009). 1 See www.timeml.org for details on TimeML, TimeBank is distributed free of charge by the Linguistic Data Consortium (www.ldc.upenn.edu), catalog number LDC2006T08. 2 Which is different from the set of 13 labels from TimeML. The set of labels for TempEval-1 was simplified to aid data preparation and to reduce the complexity of the task. Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112–116, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics There were six systems competing in"
W09-3716,burchardt-etal-2006-salsa,0,0.0604023,"xplain what each task includes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or P"
W09-3716,W03-1901,0,0.0247673,"from the database2 : Sir Nicholas Lyell, Attorney General, denies a cover-up. <SELECTOR sid=&quot;s1&quot;&gt;denies</SELECTOR&gt; a <NOUN nid=&quot;n1&quot;&gt;cover-up</NOUN&gt; . <CompLink cid=&quot;cid1&quot; sID=&quot;s1&quot; relatedToNoun=&quot;n1&quot; gramRel=&quot;dobj&quot; compType=&quot;COERCION&quot; sourceType=&quot;EVENT&quot; targetType=&quot;PROPOSITION&quot;/&gt; 3.2 Qualia Selection in Modification Constructions For this task, the relevant semantic relations are defined in terms of the qualia structure. We examine two kinds of constructions in this task: adjectival modification of nouns and nominal compounds3 . 2 While we present these examples as an inline annotation, a LAF (Ide and Romary, 2003) compliant offset annotation is fully compatible with GLML. 3 Since target nouns have already been selected for these two tasks, it is also possible to annotate qualia selection in verb-noun contexts such as Can you shine the lamp over here? (TELIC). However, here we focus solely on the modification contexts mentioned here. 175 3.2.1 Adjectival Modification of Nouns This task involves annotating how particular noun qualia values are bound by the adjectives. Following Pustejovsky (2000), we assume that the properties grammatically realized as adjectives ”bind into the qualia structure of nouns,"
W09-3716,ohara-2008-lexicon,0,0.118901,"cludes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or PropBank (Palm"
W09-3716,J05-1004,0,0.0619913,"ns; (iii) Type selection in modification of dot objects. We explain what each task includes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any se"
W09-3716,J91-4003,1,0.832111,"election. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or PropBank (Palmer et al., 2005). (1) a. Mary called yesterday. b. The Boston office called yesterday. While this has been treated as type coercion or metonymy in the literature (cf. Hobbs et al., 1993 , Pustejovsky, 1991, Nunberg, 1979, Egg, 2005), the point here is that an annotation using frames associated with verb senses should treat the sentences on par with one another. Yet this is not possible if the entity typing given to the subject in (1a) is HUMAN and that given for (1b) is ORGANIZATION. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organiza"
W09-3716,W08-1206,1,\N,Missing
W09-3716,S07-1007,0,\N,Missing
W09-3716,W04-1908,1,\N,Missing
W09-3716,C04-1133,1,\N,Missing
W11-0224,ide-romary-2006-representing,0,0.030159,"aintained in any serious fashion after 2004. Developers of the system dispersed over the world and the Medstract server fatally crashed in 2007. Here, we describe the resurrection of Medstract. One goal was that code should be open source and that installation should not depend on idiosyncraJames Pustejovsky Computer Science Department Brandeis University, Waltham, USA jamesp@cs.brandeis.edu cies of the developer’s machine, which was a problem with the inherited code base. Reusability and extendability are ensured by following the principles embodied in the Linguistic Annotation Format (LAF) (Ide and Romary, 2006). In LAF, source data are untouched, annotations are grouped in layers that can refer to each other and to the source, and each layer is required to be mappable to a graph-like pivot format. For MedstractPlus, each component is set up to be independent from other layers, although of course each layer may need access to certain types of information in order to create non-trivial output. This allows us to swap in alternative modules, making it easier to experiment with different versions of the tagger and chunker for example. We now proceed to describe the system in section 2 and finish with the"
W11-0224,W02-0312,1,0.740297,"Missing"
W11-0419,miltsakaki-etal-2004-penn,0,0.0253441,"ever, strategies that we can adopt to make this labeling task more tractable. First we need to distinguish the domains over which ordering relations are performed. Temporal ordering relations in text are of three kinds: (2) a. A relation between two events; b. A relation between two times; c. A relation between a time and an event. TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. But a human reader of a text does make a distinction, based on the discourse relations established by the author of the narrative (Miltsakaki et al., 2004; Poesio, 2004). Temporal expressions denoting the local Narrative Container in the text act as embedding intervals within which events occur. Within TimeML, these are event-time anchoring relations (TLINKs). Discourse relations establish how events relate to one another in the narrative, and hence should constrain temporal relations between two events. Thus, one of the most significant constraints we can impose is to take advantage of the discourse structure in the document before event-event ordering relations are identified. Although, in principle, during an annotation a temporal relation c"
W11-0419,W04-0210,0,0.0592245,"can adopt to make this labeling task more tractable. First we need to distinguish the domains over which ordering relations are performed. Temporal ordering relations in text are of three kinds: (2) a. A relation between two events; b. A relation between two times; c. A relation between a time and an event. TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. But a human reader of a text does make a distinction, based on the discourse relations established by the author of the narrative (Miltsakaki et al., 2004; Poesio, 2004). Temporal expressions denoting the local Narrative Container in the text act as embedding intervals within which events occur. Within TimeML, these are event-time anchoring relations (TLINKs). Discourse relations establish how events relate to one another in the narrative, and hence should constrain temporal relations between two events. Thus, one of the most significant constraints we can impose is to take advantage of the discourse structure in the document before event-event ordering relations are identified. Although, in principle, during an annotation a temporal relation can be specified"
W11-0419,prasad-etal-2008-penn,0,0.00414462,"w, two sources of informativeness in how events are temporally ordered relative to each other in a text: (a) externally and (b) internally. Consider first external informativeness. This is information derived from relations outside the temporal relation constraint set, e.g., as coming from explicit discourse relations between events (and hence is associated with the relations in (2a) above). For example, we will assume that, for two events, e1 and e2 , in a text, the temporal relation between them is more informative if they are also linked through a discourse relation, e.g., a PDTB relation (Prasad et al., 2008). Making such an assumption will allow us to focus in on the temporal relations that are most valuable without having to exhaustively annotate all event pairs. Now consider internal informativeness. This is information derived from the nature of the relation itself, as defined largely by the algebra of relations (Allen, 1984; Vilain et al., 1986). First, we assume that, for two events, e1 and e2 , a temporal relation R1 is more informative than R2 if R1 entails R2 . More significantly, however, as noted above, is to capitalize on the relations that inhere between events and the times that anch"
W11-0419,W10-1840,0,\N,Missing
W13-0503,C08-2024,1,0.899387,"Missing"
W13-0503,pustejovsky-etal-2010-iso,1,0.90313,"1, figure=pl1, ground=pl2, trigger=s1, relType=“NORTH”, frame type=ABSOLUTE, referencePt=“NORTH”, projective=TRUE) Motion Tag The ISO-Space MOTION tag is a species of TimeML event that involves a change of location or spatial configuration. Table 5 lists the attributes of the MOTION tag. b. [The dogse1 ] is [in front ofs2 ] [the couchse2 ]. figure=sne1, ground=sne2, trigger=s2, relType=“FRONT”, frame type=INTRINSIC, referencePt=sne2, projective=FALSE) Attribute id motion type motion class Measure Link Measurement relationships are captured with the MLINK tag, as first proposed for ISO-TimeML (Pustejovsky et al., 2010). Currently, this tag describes either the relationship between two spatial objects or the dimensions of a single object (cf. Table 4). Attribute id figure ground relType val endPoint1 endPoint2 Movement The treatment of movement in ISO-Space draws heavily from the foundations of lexical semantics in (Talmy, 1985) and the motion-event classifications in (Muller, 1998) and (Pustejovsky and Moszkowicz, 2008). There are two ISO-Space tags which capture movement: MOTION and MOVELINK. Table 3: OLINK Attributes. OLINK (id=ol2, (id=ml6, relType=general dimen, figure=m1, ground=m1, val=me4) motion sen"
W13-0503,W03-0804,0,\N,Missing
W13-0509,W06-1805,0,0.187299,"University jamesp@cs.brandeis.edu Abstract a human frame-of-reference, as lexically encoded in the language, such the following classes:1 DIMEN SION , PHYSICAL PROPERTY , COLOR , EMOTIONS , TEMPORAL SPATIAL VALUE , MANNER . As intuitive as these classes might be for organizing aspects of the lexis of a language, they fail to provide a coherent guide to the inferential patterns associated with adjectival modification. An alternative approach is to adopt a conceptually conservative but more formally descriptive and operational distinction, one which groups adjectives into inferential classes. (Amoia and Gardent, 2006) and (Amoia et al., 2008), following (Kamp, 1975) and (Kamp and Partee, 1995), make just such a move, adopting a four class distinction based on inferential properties of the adjective, as illustrated below: In this paper we report on an ongoing multiinstitution effort to encode inferential patterns associated with adjective modification in English. We focus here on a subset of intensional adjectives typically referred to as “non-subsective” predicates. This class includes adjectives such as alleged, supposed, so-called, and related modally subordinating predicates. We discuss the initial resu"
W13-0509,amoia-gardent-2008-test,0,\N,Missing
W13-5401,J99-4009,0,0.301475,"Missing"
W14-5204,cassidy-etal-2014-alveo,0,0.05905,"se grids by users of any one of them and, perhaps most importantly, facilitate adding additional grids and service platforms to the federation. Currently, the European META-NET initiative is committed to joining the federation in the near future. In addition to the projects listed above, we are also collaborating with several groups on technical solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD format, and a corollary development of an ontology of web service types. These collaborators include the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project, and the Lider project29 . We actively seek collaboration with others in order to move closer to achieving a “global laboratory” for language applications. 6 Conclusion In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WSEV), which provides a terminology for a core of linguistic objects and features exchanged among NLP tools that consume and produce linguistically annotated data. The goal is to bring the field closer to achieving semantic interoperability among NLP data, tools, and services. We are actively working to both e"
W14-5204,P13-1166,0,0.106613,"Missing"
W14-5204,W09-3034,1,0.689856,"Grid provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide general support for service discovery, composition, and reuse. The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College, Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope and goals to ongoing projects such as The Language Grid15 , PANACEA/MetaNET16 , LinguaGrid17 , and CLARIN18 , which also provide web service access to basic NLP processing tools and resources and enable pipelining these tools to create custom NLP applications and composite services such as question answering and machine translation, as well as access to language resources such as mono- and multilingual corpora and lexicons that support NL"
W14-5204,ide-etal-2014-language,1,0.862088,"Missing"
W14-5204,windhouwer-2012-relcat,0,\N,Missing
W14-5204,W14-5211,0,\N,Missing
W14-5206,declerck-2006-synaf,0,0.0328252,"igure 3), it is generally referred to as the model of annotation. For example, GATE has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that brid"
W14-5206,declerck-2008-framework,0,0.0223904,"has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE"
W14-5206,W07-1505,0,0.0205734,"collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009). Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4) will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will"
W14-5206,W07-1501,0,0.224868,"dard for text encoding and interchange, which also enables meta-information description. Concerning the main part (see dashdotted-line part of Figure 3), it is generally referred to as the model of annotation. For example, GATE has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also ha"
W14-5206,W09-3004,0,0.0211468,"and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009). Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4) will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will continue to be necessary. In the following sections, we consider the issue of the continuing availability of such conversion functions, and whether the current realization of those two conversion strategies is sufficien"
W14-5206,W02-0109,0,0.0209259,"data management, a unified interface for data exchange, and a light-weight serialization for data visualization. In addition, we propose a semantic mapping-based pipeline composition, which allows experts to interactively exchange data between heterogeneous components. 1 Introduction The recent work on open infrastructures for human language technology (HLT) research and development has stressed the important role that interoperability should play in developing Natural Language Processing (NLP) pipelines. For example, GATE (Cunningham et al., 2002), UIMA (Ferrucci and Lally, 2004), and NLTK (Loper and Bird, 2002) all allow integrating components from different categories based on common XML, or object-based (e.g., Java or Python) data presentation. The major categories of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc. Pipelined NLP applications can be built by composing several components; for example, a text analysis application such as “relationship analysis from medical records” can be composed by Sentence Splitter, Tokenizer, POS Tagger, NER, and C"
W14-5206,W06-2714,0,0.083658,"Missing"
W14-5206,W12-3610,0,0.0169187,"the Stanford NLP NER. This particular semantic mapping between JSON serialization of a NLTK POS Tagger and the standoff ontology of annotation of POS Tagger, and between the standoff ontology of annotation of POS Tagger and the UIMA annotation of Stanford NLP NER will be reused in the NLP application created by other end-users. Our conceptual framework does not exclusively rely on the above interoperability design. Our conceptual framework (see Figure 5) should integrate existing knowledge of various annotation frameworks, for example, the alignment knowledge from the Open Annotation models (Verspoor and Livingston, 2012) and the pivot bridge knowledge from the GrAF (Ide and Suderman, 2007) under the Linguistic Annotation Framework (LAF). Thus, existing pivot conversion solutions and XSLT-based middleware solutions can also be applied. Our interactive ontology mapping design provides a more flexible choice for linguistic experts to build up NLP pipeline applications on top of heterogeneous components, without online help from engineers. Below we present varying levels of online NLP applications, according to what kind of extra support would be needed for composing different NLP components: • Components are int"
W14-5206,wright-2014-restful,0,0.0286024,"are wrapped into Restful services so that they are operated through HTTP GET protocol, and the XML serialization of UIMA annotation is applied for input and output, each NLP components will have the same interface and data structure. Once an internationalized resource identifier (IRI) is given, all the input and output of tools can be distributed and ubiquitously identified. Moreover, a PUT/GET/POST/DELETE protocol of restful data management is equivalent to an SQL-like CRUD data management interface. For example, an IRI can be defined by a location identifier and the URL of the data service (Wright, 2014). In addition, a lightweight serialization of stand-off annotation can benefit the online visualization of data, which will be easy for experts to read, judge, or edit. For example, the XML serialization of UIMA annotation can be transferred into JSON serialization, which is preferred for online reading or editing. NLP tool services will be available by applying restful wrapping (see Figure 5). However, structural interoperability based on the restful wrapping is not enough for conceptual interoperability. For example, if an OpenNLP tokenizer is wrapped using HTTP GET protocol and GATE annotat"
W14-5206,P06-4018,0,\N,Missing
W14-5206,W03-0804,0,\N,Missing
W14-6004,D07-1114,0,0.0682028,"Missing"
W14-6004,W97-0313,0,0.276147,"Missing"
W14-6004,2009.mtsummit-wpt.4,0,0.0257841,"discover relationships, and facilitate patent searches. One of the indicators of new technology emergence is the coinage, adoption and spread of new terms; hence the identification and tracking of technical terminology over time is of particular interest to researchers designing tools to support analysts engaged in technology forecasting (e.g., Woon, 2009; deMiranda, 2006) For the most part, research into terminology extraction has either (1) focused on the identification of keywords within individual patents or corpora without regard to the roles played by the keywords within the text (e.g., Sheremetyeva, 2009) or, (2) engaged in fine-grained analysis of the semantics of narrow domains (e.g., Yang, 2008). In this paper we strive towards a middle ground, using a highlevel classification suitable for all domains, inspired in part by recent work on sentiment analysis (Liu, 2012). In aspect-based sentiment analysis, natural language reviews of specific target entities, such as restaurants or cameras, are analyzed to extract aspects, i.e., features of the target entities, along with the sentiment expressed toward those features. In the restaurant domain, for example, aspects might include the breadth of"
W14-6004,W02-1028,0,0.185093,"Missing"
W14-6004,N03-1033,0,0.0164202,"Missing"
W14-6004,H05-1044,0,0.0343327,"proach, using statistics to determine domain relevance and consensus is very similar to that adopted here. We have also drawn inspiration from sentiment analysis, proposing an ontology for patents that reflects their review-like qualities (Liu, 2012). Most relevant is the work on discovering aspects and opinions relating to a particular subject such as a camera or restaurant (Kobayashi, 2007). There are many subtleties that have been studied in opinion mining research that we have finessed in our research here, such as detecting implicit sentiment and attributes not expressed as noun phrases. Wilson et al (2005, 2009) addressed the larger problem of determining contextual polarity for subjective expressions in general, putting considerable effort into the compilation of subjectivity clues and annotations. In contrast, our aim was to test whether we could substantially reduce the annotation effort when the task is focused on polarity labeling of attributes within patents. We hypothesized that the specialized role of patents might permit a more lightweight approach amenable to bootstrapping from a very small set of annotations and feature types. 37 Bootstrapping has been successfully applied to develo"
W14-6004,J09-3003,0,0.10052,"Missing"
W14-6004,I13-1188,0,0.046619,"Missing"
W15-0204,D13-1128,0,0.0364188,"and automatic keyword annotation, where, given an image, the system provides the appropiate (or potential) labels that describe its content (Li and Fei-Fei, 2007; Luo et al., 2009; Feng and Lapata, 2010). On the other hand, efforts in the task of image caption generation have experienced a growth due to the advances in object recognition. Here, objects as well as relations among them have to be identified, and the output must be a grammatical (and, if possible, natural) sentence that correctly describes the image content (Kiros et al., 2014). Approaches include those of Farhadi et al. (2010); Elliott and Keller (2013); Kiros et al. (2014) and Karpathy and Fei-Fei (2014), among many others. The current MPEG-7 format encodes several dimensions of information about image structure (visual features, spatio-temporal structure, decomposition in regions or shots, etc.) and semantic content by means of its descriptors (Martinez, 2004). Semantic annotation with MPEG-7 captures events represented in the image as well as participants (objects and agents), the time, location, etc., and annotation and retrieval tools based on this format were presented in Lux et al. (2003); Lux and Granitzer (2005) and Lux (2009). The"
W15-0204,N10-1125,0,0.0237193,"n is becoming increasingly important in the context of algorithms that allow for efficient access and retrieval of images from large datasets; for this reason, it has become an active topic of research in both the computer vision and natural language processing communities. Keyword annotation (tagging) approaches include interactive annotation games (Von Ahn and Dabbish, 2004; Von Ahn et al., 2006; Ho et al., 2009) and automatic keyword annotation, where, given an image, the system provides the appropiate (or potential) labels that describe its content (Li and Fei-Fei, 2007; Luo et al., 2009; Feng and Lapata, 2010). On the other hand, efforts in the task of image caption generation have experienced a growth due to the advances in object recognition. Here, objects as well as relations among them have to be identified, and the output must be a grammatical (and, if possible, natural) sentence that correctly describes the image content (Kiros et al., 2014). Approaches include those of Farhadi et al. (2010); Elliott and Keller (2013); Kiros et al. (2014) and Karpathy and Fei-Fei (2014), among many others. The current MPEG-7 format encodes several dimensions of information about image structure (visual featur"
W15-0204,kordjamshidi-etal-2010-spatial,0,0.0534495,"Missing"
W15-0204,pustejovsky-yocum-2014-image,1,0.828814,"riched to include aspects that go beyond the basic categories addressed so far (location, time, event, participants), such as: the spatial relations between participants, the motion of objects, the semantic role of participants, their orientation and frame of reference, the relations among events in the image, or the characterization of the image as a whole as prototypical, given the event in question. These aspects can be included following text annotation schemes such as SpatialML (Mani et al., 2010), ISOspace (Pustejovsky et al., 2011) and Spatial Role Labeling (Kordjamshidi et al., 2010). Pustejovsky and Yocum (2014) in fact adapt ISOspace to the annotation of the spatial configuration of objects in image captions, in particular to distinguish the way captions refer to the structure versus the content of the image. In this paper, we introduce ImageML for this purpose, and we describe how this richer information concerning the image can be incorporated as a self-contained layered annotation1 , making explicit reference to several embedded specifications, i.e., ISO-TimeML and ISOspace (ISO/TC 37/SC 4/WG 2 (2014); Pustejovsky et al. (2010)).2 2 Problems Posed by Images Text-based image search assumes that im"
W15-0204,pustejovsky-etal-2010-iso,1,\N,Missing
W15-0204,W03-0804,0,\N,Missing
W16-3807,P15-1006,0,0.127318,"monolingual and multilingual language applications. Recently, however, several factors have motivated researchers to look more closely at the relationship between both spoken and written language and the expression of meaning through other modalities. Specifically, there are at least three areas of CL research that have emerged as requiring significant cross-modal or multimodal lexical resource support. These are: • Language visualization and simulation generation: Creating images from linguistic input; generating dynamic narratives in simulation environments from action-oriented expressions;(Chang et al., 2015; Coyne and Sproat, 2001; Siskind, 2001; Pustejovsky and Krishnaswamy, 2016; Krishnaswamy and Pustejovsky, 2016) • Visual Question-Answering and image content interpretation: QA and querying over image datasets, based on the vectors associated with the image, but trained on caption-image pairings in the data; (Antol et al., 2015; Chao et al., 2015a; Chao et al., 2015b) • Gesture interpretation: Understanding integrated spoken language with human or avatargenerated gestures; generating gesture in dialogue to supplement linguistic expressions;(Rautaray and Agrawal, 2015; Jacko, 2012; Turk, 2014;"
W16-3807,N16-1022,0,0.0607131,"oth the authors pre-assumptions and annotators attentio; and underdefined visual actions/concepts. The last problem, which is perhaps the most challenging of all, is related to the fact that in the majority of datasets, verbs and actions are considered the same. However, in reality, one verb can describe multiple different visually defined actions, and the same visual action can be matched to more than one verb. While most of the existed datasets do not distinguish between the two, there are new attempts to solve this inherent ambiguity, as well as to define what a visually defined action is (Gella et al., 2016; Ronchi and Perona, 2015; Yatskar et al., 2016). 5 Conclusion and Future Directions We have described our initial steps towards the design and development of a multimodal lexical resource, based on a modeling language that admits of multiple representations from different modalities. These are not just linked lists of modal expressions but are semantically integrated and interpreted representations from one modality to another. The language VoxML and the resource Voxicon are presently being used to drive simulations using multiple modalities within the DARPA Communicating with Computers progr"
W16-3807,S14-1014,1,0.833016,"ents denoted by natural language expressions. The Dynamic Event Model (DEM) encodes events as programs in a dynamic logic with an operational semantics, while the language VoxML, Visual Object Concept Modeling Language, is being used as the platform for multimodal semantic simulations in the context of human-computer communication, as well as for image- and video-related content-based querying. Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015). In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment, Unity. The goal of that work was to 41 Proceedings of the Workshop on Grammar and Lexicon: Interactions and Interfaces, pages 41–47, Osaka, Japan, December 11 2016. evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational cha"
W16-3807,L16-1730,1,0.850134,"y, however, several factors have motivated researchers to look more closely at the relationship between both spoken and written language and the expression of meaning through other modalities. Specifically, there are at least three areas of CL research that have emerged as requiring significant cross-modal or multimodal lexical resource support. These are: • Language visualization and simulation generation: Creating images from linguistic input; generating dynamic narratives in simulation environments from action-oriented expressions;(Chang et al., 2015; Coyne and Sproat, 2001; Siskind, 2001; Pustejovsky and Krishnaswamy, 2016; Krishnaswamy and Pustejovsky, 2016) • Visual Question-Answering and image content interpretation: QA and querying over image datasets, based on the vectors associated with the image, but trained on caption-image pairings in the data; (Antol et al., 2015; Chao et al., 2015a; Chao et al., 2015b) • Gesture interpretation: Understanding integrated spoken language with human or avatargenerated gestures; generating gesture in dialogue to supplement linguistic expressions;(Rautaray and Agrawal, 2015; Jacko, 2012; Turk, 2014; Bunt et al., 1998) To meet the demands for a lexical resource that can hel"
W16-3807,W13-5401,1,0.871799,"ressions. The Dynamic Event Model (DEM) encodes events as programs in a dynamic logic with an operational semantics, while the language VoxML, Visual Object Concept Modeling Language, is being used as the platform for multimodal semantic simulations in the context of human-computer communication, as well as for image- and video-related content-based querying. Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015). In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment, Unity. The goal of that work was to 41 Proceedings of the Workshop on Grammar and Lexicon: Interactions and Interfaces, pages 41–47, Osaka, Japan, December 11 2016. evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of ho"
W16-3807,W13-0705,1,0.918445,"ressions. The Dynamic Event Model (DEM) encodes events as programs in a dynamic logic with an operational semantics, while the language VoxML, Visual Object Concept Modeling Language, is being used as the platform for multimodal semantic simulations in the context of human-computer communication, as well as for image- and video-related content-based querying. Prior work in visualization from natural language has largely focused on object placement and orientation in static scenes (Coyne and Sproat, 2001; Siskind, 2001; Chang et al., 2015). In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment, Unity. The goal of that work was to 41 Proceedings of the Workshop on Grammar and Lexicon: Interactions and Interfaces, pages 41–47, Osaka, Japan, December 11 2016. evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of ho"
W16-5202,P13-1166,0,0.0336972,"Missing"
W16-5202,gilmanov-etal-2014-swift,0,0.0168661,"allowing for their immediate inclusion in workflows supporting sophisticated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to ap"
W16-5202,W14-5204,1,0.848346,"s in views, where each view contains metadata that spells out the information contained in that view, including information necessary to determine compatibility with other tools and data. Semantic interoperability is achieved via references to definitions in the Web Services Exchange Vocabulary (WSEV). The WSEV has been built bottom up, driven by the needs of components in the LAPPS Grid and closely following standard practice in the field as well as adopting, where possible, existing terminology and type systems. Both LIF and the WSEV are described in detail elsewhere (Verhagen et al., 2016; Ide et al., 2014; Ide et al., 2016). Another distinctive feature of the LAPPS Grid is its Open Advancement (OA) Evaluation system, a sophisticated environment that was used to develop IBM’s Jeopardy-winning Watson. OA can be simultaneously applied to multiple variant workflows involving alternative tools for a given task, and the results are evaluated and displayed so that the best possible configuration is readily apparent. Similarly, the weak links in a chain are easily detected, which can lead to module-specific improvements that affect the entire process. The inputs, tools, parameters and settings used fo"
W16-5202,N16-3019,0,0.0252091,"considerable programming effort to add or modify components. Similarly, the Natural Language Toolkit (NLTK) requires Python programming and effectively limits the user to the tools that are built-in to the system. In contrast, modules can be easily added to the LAPPS Grid by wrapping them as a service, using provided templates; and, more importantly, no programming experience or technical expertise is required, since workflows are constructed using the Galaxy project’s workflow management framework. This makes the LAPPS Grid ideal for instructional use. The recently introduced Kathaa system (Mohanty et al., 2016) provides functionality similar to the LAPPS Grid, but allows modules to be interfaced only if compatible with one another–i.e., there is no attempt to standardize inputs and outputs among modules, so that mixing and matching of different tools that perform the same function is limited. The LAPPS Grid’s Open Advancement evaluation modules, which exploit the ability to construct alternative pipelines in order produce statistics identifying the most effective tool sequence and/or components accounting for the largest proportion of error, are also unique; Kathaa in contrast has only basic evaluat"
W16-5202,L16-1645,0,0.0188181,"histicated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to apply sophisticated HLT text mining methods to the HathiTrust Resea"
W17-6919,W00-1423,0,0.876791,"Missing"
W17-6919,W13-4030,0,0.234903,"Missing"
W17-6919,C16-2012,1,0.858375,"Missing"
W17-6919,W13-5401,1,0.786419,"Missing"
W17-6919,L16-1730,1,0.707072,"Missing"
W17-7103,W17-6919,1,0.758799,"ge VoxML, that encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. We illustrate this with a walk-through of multimodal communication in a shared task. 1 Introduction In this paper, we discuss a developing approach towards modeling peer-to-peer communication using multiple modalities, e.g., language, gesture, vision, and action. This platform integrates a multimodal model of semantics (Multimodal Semantic Simulations, MSS) (Pustejovsky and Krishnaswamy, 2016; Krishnaswamy et al., 2017) with a realtime vision system for recognizing human gestures (Wang et al., 2017). This framework assumes both a richer formal model of events and their participants, as well as a modeling language for constructing 3D visualizations of objects and events denoted by linguistic expressions. We position this approach in the context of the questions posed by the workshop organizers, and provide more detail into the architecture that integrates the multiple sources of knowledge within the shared context of communication. To begin with, let us distinguish between experience and action: two individua"
W17-7103,C16-2012,1,0.874885,"Missing"
W17-7103,W13-5401,1,0.86246,"s until an unambiguous interpretation is achieved. Examples of this are discussed in Section 4. 2 2.1 Multimodal Communication Language Our system uses VoxSim, an open-source language-driven event simulator (Krishnaswamy and Pustejovsky, 2016a,b) to operationalize events in real time, mapping from natural language input using a dynamic semantics and the modeling language VoxML (Pustejovsky and Krishnaswamy, 2016). VoxML semantic typing extends that provided within Generative Lexicon Theory (Pustejovsky, 1995), as well as the approach taken in Asher (2011), to enable a semantics of embodiment (Pustejovsky, 2013; Pustejovsky and Krishnaswamy, 2014). Embodiment, a requirement on and facilitated by the shared virtual environment, allows us to operationalize gestures as programs enacted within the 3D environment with a direct mapping to linguistic description. This enactment is performed by a virtual avatar, in the vein of prior work studying human-computer communication (Kopp et al., 2003; Sowa and Kopp, 2003; Kopp et al., 2005; Kr¨amer et al., 2007; Weitnauer et al., 2008). 2.2 Gesture Gestures and actions are the primary modes of communication between a person and an avatar in our system. The human u"
W17-7103,S14-1014,1,0.848921,"ous interpretation is achieved. Examples of this are discussed in Section 4. 2 2.1 Multimodal Communication Language Our system uses VoxSim, an open-source language-driven event simulator (Krishnaswamy and Pustejovsky, 2016a,b) to operationalize events in real time, mapping from natural language input using a dynamic semantics and the modeling language VoxML (Pustejovsky and Krishnaswamy, 2016). VoxML semantic typing extends that provided within Generative Lexicon Theory (Pustejovsky, 1995), as well as the approach taken in Asher (2011), to enable a semantics of embodiment (Pustejovsky, 2013; Pustejovsky and Krishnaswamy, 2014). Embodiment, a requirement on and facilitated by the shared virtual environment, allows us to operationalize gestures as programs enacted within the 3D environment with a direct mapping to linguistic description. This enactment is performed by a virtual avatar, in the vein of prior work studying human-computer communication (Kopp et al., 2003; Sowa and Kopp, 2003; Kopp et al., 2005; Kr¨amer et al., 2007; Weitnauer et al., 2008). 2.2 Gesture Gestures and actions are the primary modes of communication between a person and an avatar in our system. The human user stands at one end of a table, fac"
W17-7103,L16-1730,1,0.937893,"tion is built on the modeling language VoxML, that encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. We illustrate this with a walk-through of multimodal communication in a shared task. 1 Introduction In this paper, we discuss a developing approach towards modeling peer-to-peer communication using multiple modalities, e.g., language, gesture, vision, and action. This platform integrates a multimodal model of semantics (Multimodal Semantic Simulations, MSS) (Pustejovsky and Krishnaswamy, 2016; Krishnaswamy et al., 2017) with a realtime vision system for recognizing human gestures (Wang et al., 2017). This framework assumes both a richer formal model of events and their participants, as well as a modeling language for constructing 3D visualizations of objects and events denoted by linguistic expressions. We position this approach in the context of the questions posed by the workshop organizers, and provide more detail into the architecture that integrates the multiple sources of knowledge within the shared context of communication. To begin with, let us distinguish between experien"
W17-7415,S12-1048,0,0.0175553,"originally from an interest in tracking objects in motion, as described in texts, and then linking them to maps or other visual geographic displays (Pustejovsky and Moszkowicz, 2008). As such, this results in a conflation of two kinds of information structures: (i) a relation between a motion and the mover in the motion; and (ii) all of the semantic roles that are involved in a motion event. This has the unintended consequence of creating a link structure that overlaps with efforts to annotate semantic roles generally, i.e., SemAF-SR (24617-4, 2014), and specifically within spatial language (Kordjamshidi et al., 2012). Moreover, it is unlike the other relational structures in ISOspace, in that it identifies no actual relation type, independent of the motion event itself. In this sense, it fails to conform to the definition of a link structure, as proposed in Bunt et al. (2016). For these reasons, following Lee (2016), we propose to simplify the structure of the MOVELINK tag as a relation between a MOVER and the path created by the movement, namely the EVENT PATH. 3 The Return of EVENT PATH In ISOspace version 1.3e, Pustejovsky et al. (2010) introduced an additional tag to the elements listed in Section 1 a"
W17-7415,C08-2024,1,0.763277,"entity that moves along the path identifier of a place, path, spatial named entity or event that the mover’s motion is relative to TRUE , FALSE , UNCERTAIN identifier of a path that is equivalent to the one described by the MOVELINK identifier of the spatial signal that participates in the link Table 1: Attributes for MOVELINK 2 Problems with MOVELINK It is perhaps important to understand that the motivation for the MOVELINK tag in ISOspace comes originally from an interest in tracking objects in motion, as described in texts, and then linking them to maps or other visual geographic displays (Pustejovsky and Moszkowicz, 2008). As such, this results in a conflation of two kinds of information structures: (i) a relation between a motion and the mover in the motion; and (ii) all of the semantic roles that are involved in a motion event. This has the unintended consequence of creating a link structure that overlaps with efforts to annotate semantic roles generally, i.e., SemAF-SR (24617-4, 2014), and specifically within spatial language (Kordjamshidi et al., 2012). Moreover, it is unlike the other relational structures in ISOspace, in that it identifies no actual relation type, independent of the motion event itself."
W17-7415,W13-0503,1,0.818261,"wing Lee (2016), we propose to simplify the structure of the MOVELINK tag as a relation between a MOVER and the path created by the movement, namely the EVENT PATH. 3 The Return of EVENT PATH In ISOspace version 1.3e, Pustejovsky et al. (2010) introduced an additional tag to the elements listed in Section 1 above, namely an EVENT PATH. The original intuition behind this type was to have a record of the movement as carried out by the mover: that is, to encode the path created by the traversal of an entity in motion. In order to make this more transparent, following Mani and Pustejovsky (2012), Pustejovsky and Yocum (2013) introduce two axioms of motion into the abstract syntax of ISOspace, given below. (7) a. Axiom 1: Mover Participants Every motion-event involves a mover. 8e9x[motion-event(e) ! mover(x, e)] b. Axiom 2: Event Paths Every motion-event involves an event-path. 8e9p[motion-event(e) ! [event-path(p) ^ loc(e, p)]] These axioms presuppose the following definitions1 : (8) MOVER: participant in a motion-event that undergoes a change in its location.2 PATH : non-null sequence of locations (places). 1 See Pustejovsky and Yocum (2013). Langacker (2008) (p.356) introduces mover as one of the six archetypal"
W18-4301,W14-2907,0,0.067945,"Missing"
W18-4301,P09-1068,0,0.0314198,"tats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed. 1 Introduction There has been significant research on the interpretation of events in text, particularly news articles (UzZaman et al., 2013; Pustejovsky et al., 2003; Aguilar et al., 2014). While identifying events and their participants has received much attention in the field, the construction of narratives, stories, scripts, and globally coherent relations between these events, is much more difficult and remains a challenging task (Chambers and Jurafsky, 2009; Rospocher et al., 2016). In this position paper, rather than focus on the semantics associated with event-denoting expressions in language, we discuss the contributions made by object participants in these events, and how these can influence or determine the global narrative event semantics of the text. The semantic content of events is most often anchored to the matrix predicate of a sentence and the associated event participants, expressed as verbal arguments. Further complicating the interpretation of events is the fact that, while all entities are usually realized as nominal expressions,"
W18-4301,P07-1112,0,0.0168307,"the TELIC roles and affordances associated with objects, as expressed in text and images. These include the Flikr30k (Young et al., 2014; Plummer et al., 2015); the VisualGenome (Krishna et al., 2017); and a subset of the images used for in the Visual Question Answering task from MS COCO (Lin et al., 2014). While there have been some efforts to identify affordances with objects (Chao et al., 2015), it remains a challenging issue to create object-latent event associations at scale. We believe a combination of manual annotation together with automatic extraction techniques for qualia relations (Cimiano and Wenderoth, 2007; Claveau and 4 S´ebillot, 2013) will help in constructing a multimodal lexical resource that reflects the narrative structure of objects. Acknowledgements The authors would like to thank the reviewers for their helpful comments. We would also like Tuan Do, Kyeongmin Rim, Marc Verhagen, and David McDonald for discussion on the topic. This work is supported by a contract with the US Defense Advanced Research Projects Agency (DARPA), Contract CwC-W911NF-15-C-0238. Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official pol"
W18-4301,L16-1730,1,0.861318,"ch an event must be satisfied, e.g., it has to be oriented properly, have fuel, it is air-worthy, etc., as well as be situated such that it can take off from a source, cruise in a trajectory, and land at a destination. An enriched lexical representation for such information of plane would involve far more operational and procedural knowledge than typically associated with the semantics of lexical items, going beyond the normal purview of qualia structure. For this reason, in order to more richly represent this knowledge structure computationally, we are exploiting the modeling language VoxML (Pustejovsky and Krishnaswamy, 2016; Krishnaswamy and Pustejovsky, 2016), which was initially developed in the context of 3D modeling of language in “multimodal semantic simulations,” wherein a computational system can render its interpretation of an event visually, for evaluation or to interact with a human. The VoxML equivalent of the above habitat structure, accounting for placement of the parameters within the embedding space E is given below: 3  plane     HABITAT (2)      =   I NTR  AFFORD STR =   SRC = y1 ∈ E    DEST = y2 ∈ E   [1]  TOP = top(+Y )   DIR = align(Z, Evec(y2 −y1 )) i    h = A1"
W18-4301,W13-5401,1,0.80868,"Yatskar et al. (2016) Both spray cans and hoses have canonical uses, which is to spray some substance (paint/water); knowing the canonical use of an object allows a human, as a reasoner, to infer what event or subsuming event is being depicted. Similar remarks hold for a tool such as a chainsaw (Figure 2a), independent of its role in an explicit cutting event (Figure 2b). Figure 2: Latent Event (left) and Active Event (right) In cases where an object is depicted as violating its canonical or typical use, the implied narrative becomes yet more interesting. The general “scenario localization” (Pustejovsky, 2013b) and particular types of textual or image narrative connotation can be either encoded or subverted by the presence and depiction/description of objects denoted/depicted within them: subverting the inherent narrative encoded into a particular object introduces a new narrative, vis-`a-vis how the object came to be in the situation where is it depicted or described. Take as an example the events associated with the object denoted by the artifactual nominal plane. The prototypical “fly” event can be broken down into the subevents “take off(a),” “translocation(a,b),” “land(b),” (encoded as Genera"
W18-4301,W13-0705,1,0.888064,"Yatskar et al. (2016) Both spray cans and hoses have canonical uses, which is to spray some substance (paint/water); knowing the canonical use of an object allows a human, as a reasoner, to infer what event or subsuming event is being depicted. Similar remarks hold for a tool such as a chainsaw (Figure 2a), independent of its role in an explicit cutting event (Figure 2b). Figure 2: Latent Event (left) and Active Event (right) In cases where an object is depicted as violating its canonical or typical use, the implied narrative becomes yet more interesting. The general “scenario localization” (Pustejovsky, 2013b) and particular types of textual or image narrative connotation can be either encoded or subverted by the presence and depiction/description of objects denoted/depicted within them: subverting the inherent narrative encoded into a particular object introduces a new narrative, vis-`a-vis how the object came to be in the situation where is it depicted or described. Take as an example the events associated with the object denoted by the artifactual nominal plane. The prototypical “fly” event can be broken down into the subevents “take off(a),” “translocation(a,b),” “land(b),” (encoded as Genera"
W18-4301,S13-2001,1,0.863508,"Missing"
W18-4301,Q14-1006,0,0.26774,"image. 1 Proceedings of the Workshop on Events and Stories in the News, pages 1–6 Santa Fe, New Mexico, USA, August 20, 2018. 2 Linguistic Interpretation of Images The body of work on text and image analysis relies on a number of techniques, e.g., semantic annotation of video; statistical classification for feature detection; heuristic, Markovian, and Bayesian methods for classification of composite events, among many others (Ballan et al., 2011). State of the art includes a variety of metrics to evaluate the robustness of caption generation systems (Anderson et al., 2016), event description (Young et al., 2014), and scene description (Aditya et al., 2015). Previous work in visual semantic role labeling (e.g., Gupta and Malik (2015); Yatskar et al. (2016)) often involves determining the main activity and participants in an image or scene. In many cases, the activity is closely linked to one of the objects in the scene, and some canonical property of it. For example, in Figure 1 (taken from Yatskar et al. (2016)), we see two examples of a spraying event, both closely associated with one particular object in the scene—a spray can or a hose. Figure 1: Spraying event with role labels, taken from Yatskar"
W18-4704,W15-0207,1,0.887767,"Missing"
W19-0507,P06-1131,0,0.0334728,"riptions (Bangerter, 2004). Co-temporal/overlapping speech and gesture (or an “ensemble” (Pustejovsky, 2018)) often involves deixis to ground the location, and language to specify further information (Sluis and Krahmer, 2004). As a task’s natural language requirements grow more complex, subjects rely on other modalities to carry semantic load, particularly as the need for immediate interpretation grows (Whitney et al., 2016). Studies in this area have a long history in computational linguistics/semantics (e.g., Claassen (1992); Krahmer and van der Sluis (2003)), human-robot interaction (e.g., Kelleher and Kruijff (2006); Foster et al. (2008)), and computational and human discourse studies (e.g., Bortfeld and Brennan (1997); Funakoshi et al. (2004); Viethen and Dale (2008)). Following these, we seek to build models for generating, recognizing, and classifying referring expressions that are both natural and useful to the human interlocutors of computational dialogue systems. Here, we present a novel dataset of Embodied Multimodal Referring Expressions (EMRE), blending gesture and natural language (English text-to-speech), used by an avatar in a human-computer interaction (HCI) scenario. We describe raw data ge"
W19-0507,W03-2307,0,0.803709,"Missing"
W19-0507,W17-6919,1,0.861203,"natural in context. 2 Data Gathering As our goal is to train models which a system can use to generate and interpret naturalistic multimodal referring expressions during interaction with a human, we gathered data using such a system—specifically VoxSim, a semantically-driven visual event simulator based on the VoxML semantic modeling language (Pustejovsky and Krishnaswamy, 2016), that facilitates data gathering using Monte-Carlo parameter setting to simulate motion predicates in 3D space (Krishnaswamy and Pustejovsky, 2016). We created a variant on the Human-Avatar-Blocks World (HAB) system (Krishnaswamy et al., 2017; Narayana et al., 2018), in which VoxSim visualizes the actions taken by an avatar in the 3D world as she interprets gestural and spoken input from a human interlocutor.1 A shortcoming of the HAB system is the asymmetry between the language that the system’s avatar is capable of recognizing and interpreting, and the English utterances it can generate (Krishnaswamy and Pustejovsky, 2018). Specifically, the avatar can 1) produce complete sentences of structures that it cannot entirely parse and 2) properly interpret spatial terms and relations between objects, but cannot fluently use them to re"
W19-0507,L18-1335,1,0.835515,"6), that facilitates data gathering using Monte-Carlo parameter setting to simulate motion predicates in 3D space (Krishnaswamy and Pustejovsky, 2016). We created a variant on the Human-Avatar-Blocks World (HAB) system (Krishnaswamy et al., 2017; Narayana et al., 2018), in which VoxSim visualizes the actions taken by an avatar in the 3D world as she interprets gestural and spoken input from a human interlocutor.1 A shortcoming of the HAB system is the asymmetry between the language that the system’s avatar is capable of recognizing and interpreting, and the English utterances it can generate (Krishnaswamy and Pustejovsky, 2018). Specifically, the avatar can 1) produce complete sentences of structures that it cannot entirely parse and 2) properly interpret spatial terms and relations between objects, but cannot fluently use them to refer to objects or the relations between them. Improvements to the first asymmetry are under development separately, and here we present data for creating a robust model of referring techniques in all available modalities, to help rectify the second asymmetry, for more fluent interaction in this and other HCI systems. The gesture semantics in VoxSim   are largely based on the formalisms"
W19-0507,L16-1730,1,0.690924,"uter interaction (HCI) scenario. We describe raw data generation, annotation and evaluation, preliminary analysis, and expected uses in training machine learning models for generating referring expressions in real-time that are appropriate, salient, and natural in context. 2 Data Gathering As our goal is to train models which a system can use to generate and interpret naturalistic multimodal referring expressions during interaction with a human, we gathered data using such a system—specifically VoxSim, a semantically-driven visual event simulator based on the VoxML semantic modeling language (Pustejovsky and Krishnaswamy, 2016), that facilitates data gathering using Monte-Carlo parameter setting to simulate motion predicates in 3D space (Krishnaswamy and Pustejovsky, 2016). We created a variant on the Human-Avatar-Blocks World (HAB) system (Krishnaswamy et al., 2017; Narayana et al., 2018), in which VoxSim visualizes the actions taken by an avatar in the 3D world as she interprets gestural and spoken input from a human interlocutor.1 A shortcoming of the HAB system is the asymmetry between the language that the system’s avatar is capable of recognizing and interpreting, and the English utterances it can generate (Kr"
W19-0507,W08-1109,0,0.268594,"language to specify further information (Sluis and Krahmer, 2004). As a task’s natural language requirements grow more complex, subjects rely on other modalities to carry semantic load, particularly as the need for immediate interpretation grows (Whitney et al., 2016). Studies in this area have a long history in computational linguistics/semantics (e.g., Claassen (1992); Krahmer and van der Sluis (2003)), human-robot interaction (e.g., Kelleher and Kruijff (2006); Foster et al. (2008)), and computational and human discourse studies (e.g., Bortfeld and Brennan (1997); Funakoshi et al. (2004); Viethen and Dale (2008)). Following these, we seek to build models for generating, recognizing, and classifying referring expressions that are both natural and useful to the human interlocutors of computational dialogue systems. Here, we present a novel dataset of Embodied Multimodal Referring Expressions (EMRE), blending gesture and natural language (English text-to-speech), used by an avatar in a human-computer interaction (HCI) scenario. We describe raw data generation, annotation and evaluation, preliminary analysis, and expected uses in training machine learning models for generating referring expressions in re"
W19-1915,D18-2029,0,0.0132733,"positive sense. We evaluated three classification models. Our baseline model is a majority vote approach using the Pattern sentiment lexicon employed by McCoy (2015) and Waudby-Smith (2018). The second and third models use fully supervised and semi-supervised multilayer perceptron (MLP) architectures, respectively. Since positive and negative clinical sentiment can differ across each domain, we train a suite of seven models, one for each risk factor domain. The training and test data were vectorized at the sentence level using the pretrained Universal Sentence Encoder (USE) embedding module (Cer et al., 2018) that is available through TensorFlow Hub and is designed specifically for transfer learning tasks. Although USE is trained on a large volume of web-based, generaldomain data, we have found in prior work that the embeddings lead to higher accuracy on downThese corpora are available to other researchers upon request. Table 2 details the distribution of the training and test data. The imbalance of training examples across the three sentiment classes reflects the natural distribution of sentiment reflected in EHRs, as certain risk factor domains (e.g. substance use) will rarely be reflected in a"
W19-1915,W12-3712,0,0.0122073,"f several Boston-area hospitals in the Partners HealthCare network, including Massachusetts General Hospital and Brigham & Women’s Hospital. We used our risk factor domain topic extraction model to automatically identify relevant sentences, which were then manually validated by one of the clinicians involved in this project to ensure they did not involve multiple domains in the same examRelated Works Although there has been some work on clinical adaptation of sentiment analysis using healthcarerelated data extracted from web forums, biomedical texts, or social media postings (See for example (Smith and Lee, 2012; Niu et al., 2005; SalasZ´arate et al., 2017; Nguyen et al., 2014)), there has been minimal work on sentiment analysis when applied to actual EHR data. McCoy et al. (2015) used a corpus of psychosis patient discharge summaries and the 3,000 word Pattern lexical opinion mining dictionary (Smedt and Daelemans, 2012) to classify the associated sentiment of documents using a majority vote classifier. Results of their Cox regression models showed that greater positive sentiment was associated with a reduction in inpatient readmission risk. Waudby-Smith et al. (2018) applied the same Pattern sentim"
W19-1915,W18-5615,1,0.306802,"assifier to predict clinical sentiment at the sentence level. This classifier, which in future works will be integrated in a pipeline for predicting readmission risk, is clinically useful for targeting treatments and aiding in decision making. 2 3 Methods In this work, we define psychiatric clinical sentiment as a clinician’s attitudes (positive, negative, or neutral) towards a patient’s prognosis with regards to seven readmission risk factor domains (appearance, mood, interpersonal relations, substance use, thought content, thought process, and occupation) that were identified in prior work (Holderness et al., 2018). The scope of our current definition is intentionally narrow such that the sentiment of a given sentence is considered in isolation without any prior knowledge. Three clinicians participated in an annotation project that focused on identifying the clinical sentiment associated with psychiatric EHR texts at the sentence level. In total, two corpora of clinical narratives from institutional EHRs, one containing 3,500 sentences (training dataset) and the other 1,650 (test dataset) were annotated using the definition established in the annotation scheme. The training dataset consisted exclusively"
W19-1915,D13-1170,0,0.00545115,"nt for many associated linguistic challenges such as negation handling, scope, sarcasm, qualified statements, and out-of-vocabulary terms. As such, research groups have moved towards approaching the problem from a corpus-based machine learning perspective. This approach has the added benefit of model flexibility depending on the training data and can capture more syntactic nuance. Most state-of-the-art performances on sentiment analysis benchmarks are currently achieved with deep learning sequence models that are trained on syntactically parsed corpora such as the Stanford Sentiment Treebank (Socher et al., 2013). In clinical and medical domains, however, sentiment analysis has not yet been well studied. Yet retrieving subjective clinical attitudes (sentiment) from EHR narratives has the potential to faciliRecently natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs). Sentiment analysis, although widely used in non-medical areas for improving decision making, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of sentiment analysis to psy"
W19-1915,J11-2001,0,0.0607573,"omains such as reactions to stock market prediction or political trends (M¨antyl¨a et al., 2018). With the rise of social media and other user-generated web content, sentiment analysis has been adopted by many industries as a way of monitoring opinions towards their products, reputations, and for identifying opportunities for improvement. Traditionally, sentiment analysis has been approached with a lexicon-based majority vote approach, where a dictionary of terms and their associated sentiments (e.g. SentiWordnet, Pattern, SO-CAL, VADER) are queried to determine the sentiment of a given text (Taboada et al., 2011). However, this approach fails to account for many associated linguistic challenges such as negation handling, scope, sarcasm, qualified statements, and out-of-vocabulary terms. As such, research groups have moved towards approaching the problem from a corpus-based machine learning perspective. This approach has the added benefit of model flexibility depending on the training data and can capture more syntactic nuance. Most state-of-the-art performances on sentiment analysis benchmarks are currently achieved with deep learning sequence models that are trained on syntactically parsed corpora su"
W19-2512,P14-5010,0,0.00398219,"ltural heritage artifacts come alive in the classroom setting (e.g. Ott and Pozzi, 2011; Antonaci et al., 2013). Recently, with the advent of large digital storage, there have been many large-scale projects aimed at the massdigitization of books (Christenson, 2011), newspa1 http://www.clams.ai 91 Proc. of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 91–97 c Minneapolis, MN, USA, June 7, 2019. 2019 Association for Computational Linguistics levels of state-of-the-art NLP tools for textual data, such as CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), but also implement open source SDK for tool developers to promote adoption. These workflow engines can operate different tools which are separately developed only because of the underlying data interchange formats that impose common I/O language between those tools. For such an interchange format, The LAPPS Grid uses LAPPS Interchange Format (LIF) rooted on JSON-LD serialization (Verhagen et al., 2015), while the WebLicht uses XML-based Text Corpus Format (TCF) (Heid et al., 2010). Additionally the LAPPS Grid defines a semantic linked data vocabulary that ensures sem"
W19-2512,heid-etal-2010-corpus,0,0.0272683,"Linguistics levels of state-of-the-art NLP tools for textual data, such as CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), but also implement open source SDK for tool developers to promote adoption. These workflow engines can operate different tools which are separately developed only because of the underlying data interchange formats that impose common I/O language between those tools. For such an interchange format, The LAPPS Grid uses LAPPS Interchange Format (LIF) rooted on JSON-LD serialization (Verhagen et al., 2015), while the WebLicht uses XML-based Text Corpus Format (TCF) (Heid et al., 2010). Additionally the LAPPS Grid defines a semantic linked data vocabulary that ensures semantic interoperability (Ide et al., 2015). Having implemented in-platform interoperability has led to a multi-platform collaboration between LAPPS and CLARIN (Hinrichs et al., 2018). flow engine that provides a common interchange format ensuring syntactic and semantic interoperability between these tools. 2 Prior Work Multilingual Access to Large Spoken Archives (MALACH) (Oard et al., 2002) was one of the early studies that used computational linguistics tools to build an automatic metadata extraction syste"
W19-2512,P10-4005,0,0.457643,"e applications. In the computational linguistics (CL) community, UIMA (Ferrucci et al., 2009) and GATE (Cunningham et al., 2013) have been longstanding popular tool-chaining platforms for researchers and NLP developers. Particularly, UIMA provides an extremely general model of type systems and annotations that can be applied upon multimedia source data. However, there is stiff learning curve behind its high generality, combined with its tight binding with XML syntax and Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) provide user friendly web interfaces. Particularly, these web-based platforms not only offer tool repositories of various 3 Project Description Figure 1 shows the overall structure of the platform in a working environment as delivered to an archive. As a platform, the primary goals of CLAMS are 1) to develop an interchange format between multimodal annotations that allows analysis tools for different modalities to work together when chained into a single workflow, and 2) to provide libraries and archivists a portable workflow engine software with a user-friendly interface to select available"
W19-2512,ide-etal-2014-language,1,0.819026,"Missing"
W19-2512,L16-1073,1,0.901632,"Missing"
W19-3303,C18-1313,0,0.198296,"Missing"
W19-3303,E17-2039,0,0.0583464,"Missing"
W19-3303,J05-1004,0,0.351856,": by focusing on the predicative core of a sentence, it is an intuitive representation for both interpreting the semantics of a sentence, and perhaps more importantly, for use in annotation efforts. An AMR represents the meaning of a sentence with a single-rooted, directed, acyclic graph with nodes labeled with concepts and edges labeled with relations. The primary component of an AMR is the predicate-argument structure, with the predicate being a concept that takes a number of - ARG0 afford-01 time ARG0 person ARG1 moment car name name op “John” Propositions in an AMR are sensedisambiguated (Palmer et al., 2005). In the example above, “possible-01” refers to the first sense of “possible” while “afford-01” represents the first sense of “afford”. A predicate can take a number of core arguments (ARG0, ARG1, etc.) as well as adjunct arguments (e.g., time). The semantic roles for the core arguments are defined with respect to each sense of a predicate 28 Proceedings of the First International Workshop on Designing Meaning Representations, pages 28–33 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics and they are drawn from the PropBank frame files 1 . For example, the sema"
W19-3303,W13-2322,0,0.749115,"appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible. 1 (1) John can’t afford a car at the moment. (2) a. (p / possible-01 :ARG0 (a / afford-01 :ARG0 (p2 / person :name (n / name :op ""John"")) :ARG1 (c /car) :time (m / moment)) :polarity -) b. possible-01 Abstract Meaning Representations polarity Abstract Meaning Representations (AMRs) have recently become popular as a strategy for encoding a kind of canonical meaning for natural language sentences (Banarescu et al., 2013). They differ significantly from other encoding schemes used in NLP—e.g., minimal recursion semantics (MRS)—in terms of their expressiveness for several semantic phenomena in natural language (Copestake et al., 2005). Still, in spite of such shortcomings, there is a major attraction to the general philosophy of this approach: by focusing on the predicative core of a sentence, it is an intuitive representation for both interpreting the semantics of a sentence, and perhaps more importantly, for use in annotation efforts. An AMR represents the meaning of a sentence with a single-rooted, directed,"
W19-3303,W15-1841,0,0.0271802,"poral expressions and their relative scope over event variables, rather than quantified arguments to the verb. An example is that shown in (14). not ARG0 afford-01 time ARG0 person ARG1 moment car name name op “John” The graph-interpretation function continues walking down the tree, and expands the Skolemized form for ‘car’ into a quantified expression, inside the scope of the modal, as shown below. (14) a. John golfed every Sunday. b. ∀t[Sunday(t) → ∃e[golf(e, j) ∧ on(e, t)]] (16) ¬3[∃x[car(x)∧∃e[afford(e, j, x)∧@(e, N)]] This can be compared to the first-order modal expression generated by (Bos, 2015; Bos et al., 2017) for the sentence as shown below in (17). The strategy taken by (Bos et al., 2017), followed here as well, is to scope temporal expressions over the events they govern. Now let us see how the scope relation can be deployed to handle negation and modality in UMR. Consider first the treatment of modals in AMR. As seen in (2) above, modals are treated as predicative nodes. Hence, from (p / possible-01 :ARG0 phi), we can derive the equivalent propositional modal expression, 3φ. However, in (2) we need to translate the polarity over the modal appropriately: ¬3φ. (17) ¬∃x[car(x) ∧"
W19-3303,J16-3006,0,0.0510979,"(d / fail-01 :ARG0 (s / student :mod (a / all)) :polarity -) The attraction of AMR-style representations and annotations is the adoption of a predicative core element along with its arguments: e.g., an event and its participants. This, in turn, leads to an event-rooted graph that has many advantages for parsing and matching algorithms. As can be seen from the example, the predicate-argument structure is front and center in AMR, and we consider this to be one of its strengths. However, as it currently stands, AMR does not represent quantification or its interaction with modality and negation (Bos, 2016). The challenge is to maintain the focus on the predicate-argument structure while also adequately accounting for linguistic phenomena that operate above the level of the core AMR representation, in particular quantification and modality. 2 b. fail-01 ARG0 - polarity student mod all The sentence is ambiguous, however, between the readings “for every student, that student did not fail” and “it is not the case that every student failed”. While MRS and other flattened semantic representations provide a solution to these issues, giving faithful translations of scope with typed expressions, there a"
W19-3303,L18-1282,1,0.843096,"Missing"
W19-3303,E09-1001,0,0.0315359,"that student did not fail” and “it is not the case that every student failed”. While MRS and other flattened semantic representations provide a solution to these issues, giving faithful translations of scope with typed expressions, there are several drawbacks to these approaches. Flat representations reveal no semantic core. Hence, as annotations, the resulting structures are difficult to interpret and inspect. Furthermore, quantifier scope is often underspecified even when it can be disambiguated in context. Dependency MRS (DMRS) is one exception to this in the MRS family of representations (Copestake, 2009), where dependency relations link argument heads to the major predicator of the sentence. In our research, we propose to represent scope relationally, while maintaining both the centrality of the predicative core of the sentence (e.g., listen, drink), as well as the syntactic integrity of the quantified expression (e.g., every person). A relational interpretation for scope provides a first-order interpretation: it references two specific nodes in the graph, and orders one relative to the other. This operates over generalized quantifiers (some book, most people), negation (not, no), as well as"
W19-3318,P17-1044,0,0.0313199,"and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions def"
W19-3318,N06-2015,1,0.264637,"system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions defined in the VN class. To seAcknowledgments We gratefully acknowledge the support of DTRAl -16-1-0002/Project 1553695, eTASC - Empirical Evidence for a Theoretical Approach to Semantic Components and DARPA 15-18-CwC-FP-032 Communicating with Comp"
W19-3318,W08-2222,0,0.0110709,"00 class: has possession(E, Pivot, Theme), or During(E), as for the contiguous location-47.8 class (Italy borders France): contact(During(E), Theme, CoTheme). Most classes having to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg ("
W19-3318,kawahara-palmer-2014-single,1,0.85287,"e, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg (2006). A previous revision of the VerbNet semantic representations made the correspondence of these patterns to constructions more explicit by using a common predicate (i.e., path rel) for all caused-motion cons"
W19-3318,kipper-etal-2006-extending,1,0.611019,"er analysis and planning systems. For applications like robotics or interactions with avatars, commonsense inferences needed to understand human language directions or interactions are often not derivable directly from the utterance. Tracking intrinsic and extrinsic states of entities, such as their existence, location or functionality, currently requires explicit statements with precise temporal sequencing. In this paper, we describe new semantic representations for the lexical resource VerbNet that provide this sort of information for thousands of 2 Background The language resource VerbNet (Kipper et al., 2006) is a hierarchical, wide-coverage verb lexicon that groups verbs into classes based on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet includes a set of member verbs, the thematic roles used in the predicate-argument structure of these members (Bonial et al., 2011), and the class-specific selectional preferences for those roles. The class also provides a set of typical syntactic patterns and corresponding semantic representations. A verb can be a member of multiple classes; for example, run is a member of 8 VerbNet classes, including the run-51.3.2"
W19-3318,L18-1009,1,0.783127,"s or intervals. This Dynamic Event Model (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013) explicitly labels the transitions that move an event from frame to frame. Although people infer that an entity is no longer at its initial location once motion has begun, computers need explicit mention of this fact to accurately track the location of an entity. Similarly, some states hold throughout an event, while others do not. Our new representations make these distinctions clear, where pre-event, while-event, and post-event conditions are distinguished formally in the representation. Elsewhere (Brown et al., 2018), we discuss in more detail the Dynamic Event Model, show the effect of the new subevent structure on the interpretation of the role of the Agent, and give further examples of the new change of location and change of state representations. Applying the Dynamic Event Model to VerbNet semantic representations allowed us refine the event sequences by expanding the previous tripartite division of Start(E), During(E), and End(E) to an indefinite number of subevents. These numbered subevents allow very precise tracking of participants across time and a nuanced representation of causation and action"
W19-3318,P12-1028,1,0.823006,"ing to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg (2006). A previous revision of the VerbNet semantic representations made the correspondence of these patterns to constructions more explicit by using a common predicate (i.e., path rel"
W19-3318,W17-2812,1,0.836296,"onstruction frames(Hwang, 2014). At the request of some users, we are substituting more specific predicates for the general path rel predicate, such as has location, has state and has possession, although the subevent patterns continue to show the commonality across (1) The rabbit hopped across the lawn. Theme V Trajectory motion(during(E), Theme) path rel(start(E), Theme, ?Initial location1 , CH OF LOC , prep) path rel(during(E), Theme, Trajectory, CH OF LOC , prep) path rel(end(E), Theme, ?Destination, CH OF LOC , prep) Efforts to use VerbNet’s semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017), however, indicated a need for greater consistency and expressiveness. We have addressed consistency on several fronts. First, all necessary participants are accounted for in the representations, whether they are instantiated in the syntax, incorporated in the verb itself (e.g., to drill), or simply logically necessary (e.g., all entities that change location begin in an initial location, whether it is commonly mentioned or not). 1 A question mark in front of a thematic role indicates a role that appears in the syntax in some frames for the class but not in this particular frame. 155 troduced"
W19-3318,J02-3001,0,0.100875,"rsing To facilitate immediate use of the new VerbNet semantic representations, we are releasing a semantic parser that predicts the updated semantic representations from events in natural language input sentences. For a given predicative verb in a sentence, we define VerbNet semantic parsing as the task of identifying the VN class, associated thematic roles, and corresponding semantic representations linked to a frame within the class. We approach VerbNet semantic parsing in three distinct steps: 1. Sense disambiguation to identify the appropriate VN class, 2. PropBank semantic role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) an"
W19-3318,P06-1117,0,0.0400665,"indicated with either a bare E, as for the own-100 class: has possession(E, Pivot, Theme), or During(E), as for the contiguous location-47.8 class (Italy borders France): contact(During(E), Theme, CoTheme). Most classes having to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument"
W19-3318,J05-1004,1,0.198198,"te use of the new VerbNet semantic representations, we are releasing a semantic parser that predicts the updated semantic representations from events in natural language input sentences. For a given predicative verb in a sentence, we define VerbNet semantic parsing as the task of identifying the VN class, associated thematic roles, and corresponding semantic representations linked to a frame within the class. We approach VerbNet semantic parsing in three distinct steps: 1. Sense disambiguation to identify the appropriate VN class, 2. PropBank semantic role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018)"
W19-3318,N18-1202,0,0.0462361,"Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions defined in the VN class. To"
W19-3318,W13-5401,1,0.740109,"tates represented with a simple e, processes as a sequence of states characterizing values of some attribute, e1 ...en , and transitions describing the opposition inherent in achievements and accomplishments. In subsequent work within GL, event structure has been integrated with dynamic semantic models in order to more explicitly represent the attribute modified in the course of the event (the location of the moving entity, the extent of a created or destroyed entity, etc.) as a sequence of states related to time points or intervals. This Dynamic Event Model (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013) explicitly labels the transitions that move an event from frame to frame. Although people infer that an entity is no longer at its initial location once motion has begun, computers need explicit mention of this fact to accurately track the location of an entity. Similarly, some states hold throughout an event, while others do not. Our new representations make these distinctions clear, where pre-event, while-event, and post-event conditions are distinguished formally in the representation. Elsewhere (Brown et al., 2018), we discuss in more detail the Dynamic Event Model, show the effect of the"
W19-3318,zaenen-etal-2008-encoding,1,0.719911,"r all caused-motion construction frames(Hwang, 2014). At the request of some users, we are substituting more specific predicates for the general path rel predicate, such as has location, has state and has possession, although the subevent patterns continue to show the commonality across (1) The rabbit hopped across the lawn. Theme V Trajectory motion(during(E), Theme) path rel(start(E), Theme, ?Initial location1 , CH OF LOC , prep) path rel(during(E), Theme, Trajectory, CH OF LOC , prep) path rel(end(E), Theme, ?Destination, CH OF LOC , prep) Efforts to use VerbNet’s semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017), however, indicated a need for greater consistency and expressiveness. We have addressed consistency on several fronts. First, all necessary participants are accounted for in the representations, whether they are instantiated in the syntax, incorporated in the verb itself (e.g., to drill), or simply logically necessary (e.g., all entities that change location begin in an initial location, whether it is commonly mentioned or not). 1 A question mark in front of a thematic role indicates a role that appears in the syntax in some frames for the class but not in this pa"
X93-1021,C92-2099,0,0.0526046,"Missing"
X93-1021,J91-4003,1,0.918749,"ticular role of a tie-up (the newly formed venturein particular) are also recognized and resolved. For example, a phrase which refers to the child entity, such as &apos;the new company&apos; or &apos;the venture&apos;, will be recognized ann merged with the child of the tie-up event in focus. A stack of entities found in the text is maintained. f(time,[after,&apos;B50i&apos;],wj), f(entity_relationship,l,inf), f(entity_relationship,3,inf)]). final_entity(9, &apos;UNSPEC&apos;), f(entiCy_type,~COMPANY&apos;,&apos;UNSPEC&apos;), f(name,[~&apos;,&apos;~,&apos;,&apos;~&apos;,~Jz&apos;],&apos;UNSPEC&apos;), f(entity_relationship,l,inf), f(entity_relationship,3,inf)]). f i n a l _ r e l ( 1, [9,2], &apos;U N S P E C &apos;, &apos;P A R T N E R &apos;, &apos;U B S P E C &apos; ). Definite noun phrases can only be used for local reference. So they can only be used to refer to entities involved in the tie-up event which is in focus. On the contrary, names can be used for both local and global reference, so they can refer to any entity referred to before in the t e x t . When a reference relation between two entities is resolved they are merged to create one single entity which contains all the information about that particular entity. Since a tie-up is generally referenced by an entire sentence rather than a single no"
X93-1021,H92-1047,1,0.896541,"Missing"
X93-1021,J93-2005,1,0.842458,"Missing"
X93-1021,P92-1022,0,0.0224668,"Missing"
X93-1021,P85-1022,0,0.0572839,"Missing"
