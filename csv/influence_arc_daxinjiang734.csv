2020.acl-main.539,D13-1160,0,0.0230982,"-module is generated, which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50."
2020.acl-main.539,P18-1071,0,0.0989351,"Missing"
2020.acl-main.539,D18-1192,0,0.0336961,"ate). In LPA settings, (Weighted) Voting means assigning each program with (score-weighted) equal weight to vote for the final result. Ranking means using the result generated by the top program ranked by the discriminator. As shown in Figure 3, a program in TABFACT is structural and follows a grammar with over 50 functions. To effectively capture the structure of the program and also generate legitimate programs following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig"
2020.acl-main.539,P17-1003,0,0.0467597,"which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50.5 50.4 56.2 57.0 65."
2020.acl-main.539,D19-1603,0,0.11077,"Missing"
2020.acl-main.539,D18-2002,0,0.014426,"grams following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Foll"
2020.acl-main.539,D18-1010,0,0.0610233,"used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fa"
2020.acl-main.539,D14-1162,0,0.0848898,"-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Following Chen et al. (2019), we employ the label of veracity to guide the learning process of the semantic parser. We also employ programs produced by LPA (Latent Program Algorithm) for comparison, which is provided by Chen et al. (2019). In the training process, we train the semantic parser and the claim verification model separately. The training of semantic parser includes two steps: candidate search and sequence-to-action learning. For candidate search, we closely follow LPA by first collecting"
2020.acl-main.539,D17-1317,0,0.036168,"ost influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Kry´sci´nski et al., 2019), as well as the use of this factual accuracy as a reward"
2020.acl-main.539,P13-1045,0,0.0675626,"gical form, in a semantic parsing manner (Liang, 2016). Then, our system builds a heterogeneous graph to capture the connections among the statement, the table and the program. Such connections reflect the related context of each token in the graph, which are used to define attention masks in a Transformer-based (Vaswani et al., 2017) framework. The attention masks are used to learn graph-enhanced contextual representations of tokens1 . We further develop a program-guided neural module network to capture the structural and compositional semantics of the program for semantic compositionality. (Socher et al., 2013; Andreas et al., 2015). Graph nodes, whose representations are computed using the contextual representations of their constituents, are considered as arguments, and logical operations are considered as modules to recursively produce representations of higher level nodes along the program. Experiments show that our system outperforms previous systems and achieves the state-of-the-art verification accuracy. The contributions of this paper can be summarized as follows: • We propose LogicalFactChecker, a graphbased neural module network, which utilizes logical operations for fact-checking. 1 Here"
2020.acl-main.539,D19-1216,0,0.0218259,"nction of difference time, which is not covered by the current set. 5 Related Work There is a growing interest in fact checking in NLP with the rising importance of assessing the truthfulness of texts, especially when pre-trained language models (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019) are more and more powerful in generating fluent and coherent texts. Previous studies in the field of fact checking differ in the genres of supporting evidence used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al.,"
2020.acl-main.539,N18-1074,0,0.15998,"Missing"
2020.acl-main.539,D19-6601,0,0.0280888,"a et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural"
2020.acl-main.539,D19-5316,0,0.0844217,": An example of table-based fact checking. Given a statement and a table as the input, the task is to predict the label. Program reflects the underlying meaning of the statement, which should be considered for fact checking. Introduction Fact checking for textual statements has emerged as an essential research topic recently because of the unprecedented amount of false news and rumors spreading through the internet (Thorne et al., 2018; ∗ Work done while this author was an intern at Microsoft Research. Chen et al., 2019; Goodrich et al., 2019; Nakamura et al., 2019; Kry´sci´nski et al., 2019; Vaibhav et al., 2019). Online misinformation may manipulate people’s opinions and lead to significant influence on essential social events like political elections (Faris et al., 2017). In this work, we study fact checking, with the goal of automatically assessing the truthfulness of a textual statement. The majority of previous studies in fact checking mainly focused on making better use of the meaning of words, while rarely considered symbolic reasoning about logical operations (such as “count”, “superlative”, “aggregation”). However, modeling logical operations is an essential step towards the modeling of compl"
2020.acl-main.539,P17-1041,0,\N,Missing
2020.acl-main.539,D18-1266,0,\N,Missing
2020.acl-main.544,2020.acl-main.23,0,0.0747168,"Missing"
2020.acl-main.544,2021.ccl-1.108,0,0.0750586,"Missing"
2020.acl-main.544,P18-2119,0,0.0142462,"at focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recentl"
2020.acl-main.544,D16-1031,0,0.027836,"and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-cen"
2020.acl-main.544,N16-1098,0,0.0228979,"ated text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP com"
2020.acl-main.544,P18-1043,0,0.387556,"two kinds of reasonable inferences for the event under different background knowledge that is absent in the dataset. Introduction Inferential text generation aims to understand dailylife events and generate texts about their underlying causes, effects, and mental states of event participants, which is crucial for automated commonsense reasoning. Taking Figure 1 as an example, given an event “PersonX reads PersonY’s diary”, the cause of the participant “PersonX” is to “obtain Person Y’s secrets” and the mental state of “PersonX” is “guilty”. Standard approaches for inferential text generation (Rashkin et al., 2018; Sap et al., 2019; Bosselut et al., 2019; Du et al., 2019) typically only ∗ Work done while this author was an intern at Microsoft Research. take the event as the input, while ignoring the background knowledge that provides crucial evidence to generate reasonable inferences. For example, if the background knowledge of this example is “PersonY invites PersonX to read his diary”, the outputs should be different. In this paper, we present an evidence-aware generative model, which ﬁrst retrieves relevant evidence from a large text corpus and then leverages retrieved evidence to guide the generati"
2020.acl-main.544,L16-1233,0,0.0270969,"y using better semantic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and"
2020.acl-main.544,D17-1006,0,0.0182539,"ic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Gr"
2020.acl-main.544,D18-1009,0,0.022466,"different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 20"
2020.acl-main.544,D16-1050,0,0.0283868,"erential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse"
2020.acl-main.544,P17-1061,0,0.0188185,"of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-centered If-Then reasoning is the most related to our work, which introduces an additio"
2020.acl-main.599,P19-1620,0,0.156639,"nswer” should be given if there is no suitable short answer. 2.2 • We achieve state-of-the-art performance on both long and short answer leaderboard of NQ at the time of submission (Jun. 25th, 2019), and our model surpasses single human performance on the development dataset at both long and short answer criteria. Preliminary Data Preprocessing Since the average length of the documents in NQ is too long to be considered as one training instance, we first split each document into a list of document fragments with overlapping windows of tokens, like in the original BERT model for the MRC tasks (Alberti et al., 2019b; Devlin et al., 2019). Then we generate an instance from a document fragment by concatenating a “[CLS]” token, tokenized question, a “[SEP]” token, tokens from the content of the doc6709 Output Layer Document Fragment Add & Norm Paragraph Feed-Forward Sentence Add & Norm Concatenate Token Graph Integration Token-Level Self-Attention Sentence-Level Self-Attention N× Paragraph-Level Self-Attention Figure 4: The graph on the left is an illustration of the graph integration layer. The graph on the right shows the incoming information when updating a paragraph node. The solid lines represent the"
2020.acl-main.599,P17-1171,0,0.270549,"al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. According to Kwiatkowski et al. (2019), a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer. For instance,"
2020.acl-main.599,P16-1046,0,0.238693,"agraphs 6708 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained joi"
2020.acl-main.599,P17-1147,0,0.0527221,"dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al"
2020.acl-main.599,N18-2075,0,0.0309702,"normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani e"
2020.acl-main.599,Q19-1026,0,0.165659,"ar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches,"
2020.acl-main.599,N18-2078,0,0.0307711,"k et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different"
2020.acl-main.599,P18-1078,0,0.295567,"ides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answe"
2020.acl-main.599,P17-1055,1,0.862058,"ns that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extrac"
2020.acl-main.599,N18-1158,0,0.0177026,"ion for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other. At inference time, we use a pipeline s"
2020.acl-main.599,D16-1244,0,0.153285,"Missing"
2020.acl-main.599,N16-1174,0,0.432339,"of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote eac"
2020.acl-main.599,D16-1264,0,0.0512332,"ong and short answer criteria. 1 Figure 1: An example from NQ dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to selec"
2020.acl-main.599,K19-1074,0,0.0605159,"Missing"
2020.acl-main.599,D16-1103,0,0.0241031,"Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have s"
2020.acl-main.599,N18-2074,0,0.0339776,"graph integration layer and pass it to the feedforward layer. 6711 3.3.4 Feed-Forward Layer Following the inner structure of the transformer (Vaswani et al., 2017), we also utilize an additional fully connected feed-forward network at the end of our graph encoder. It consists of two linear transformations with a GELU activation in between. GELU is Gaussian Error Linear Unit activation (Hendrycks and Gimpel, 2016), and we use GELU as the non-linear activation, which is consistent with BERT. 3.3.5 Inspired by positional encoding in Vaswani et al. (2017) and relative position representations in Shaw et al. (2018), we introduce a novel relational embedding on our constructed graph, which aims at modeling the relative position information between nodes on the multi-granularity document structure. We make the edges in our document modeling graph to embed relative positional information. We modify equation 1 and 2 for eij and z i to introduce our relational embedding as follows: eij = zi = X αij  h0i = j∈Ni ,oj +1=oi  h0j + aij + boi , Output Layer The objective function is defined as the negative sum of the log probabilities of the predicted distributions, averaged over all the training instances. The"
2020.acl-main.599,D18-1246,0,0.0199332,"evel encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different 6715 types of edges in the graph. 6 Jaco"
2020.acl-main.599,P18-1030,0,0.0200399,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D18-1244,0,0.031553,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D15-1167,1,0.734635,"Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibil"
2020.acl-main.599,P18-1158,0,0.0352041,"l., 2017; Lai et al., 2017; Trischler et al., 2017; Yang et al., 2018). Lots of work has begun to build end-to-end deep learning models and has achieved good results (Seo et al., 2017; Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network m"
2020.acl-main.599,W17-2623,0,\N,Missing
2020.acl-main.599,D17-1082,0,\N,Missing
2020.acl-main.599,D18-1259,0,\N,Missing
2020.acl-main.599,N19-1423,0,\N,Missing
2020.acl-main.604,P19-1620,0,0.0781126,"e amount of text, one key observation is that most answers are only related to a few words in one paragraph; (b) The final paragraph representation can be used naturally for predicting long answers. 2 NQ provides some visual examples of the data at https://ai.google.com/research/ NaturalQuestions/visualization. 6762 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6762–6771 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We describe the details of DPDA reader in § 3.1. For the second challenge, unlike prior works on NQ dataset (Alberti et al., 2019b; Pan et al., 2019) that only predict the short answer and directly select its paragraph as long answer, RikiNet employs a multi-level cascaded answer predictor which jointly predict the short answer span, the long answer paragraph, and the answer type in a cascaded manner. Another key intuition motivating our design is that even if the relevant documents are not given, humans can easily judge that some questions have no short answers (Borschinger et al., 2019). Take this question as a motivating example:“What is the origin of the Nobel prize?” The answer should be based on a long story, whic"
2020.acl-main.604,P17-1171,0,0.108717,"Since most submissions on the NQ leaderboard are ensemble models, we also report the results of our ensemble model, which consists of three RikiNet-RoBERTa large models with different hyper-parameters. At the time of submission (29 Nov. 2019), the NQ leaderboard shows that our ensemble model achieves the best performance on both LA (F1 76.1) and SA (F1 61.3). 4 The single RikiNet-BERTlarge model was submitted to the NQ public leaderboard on 7 Nov. 2019. 6767 LA Dev R F1 P P LA Test R F1 P SA Dev R F1 P SA Test R F1 DocumentQA (Clark and Gardner, 2018) DecAtt (Parikh et al., 2016) + DocReader (Chen et al., 2017) BERTjoint (Alberti et al., 2019b) BERTlarge + 4M synth NQ (Alberti et al., 2019a) BERTjoint (Alberti et al., 2019b) + RoBERTa large (Liu et al., 2019) ‡ BERTlarge + SQuAD2 PT + AoA (Pan et al., 2019)† BERTlarge + SSPT (Glass et al., 2019)† RikiNet-BERTlarge RikiNet-RoBERTa large ‡ 47.5 52.7 61.3 62.3 65.6 73.2 74.3 44.7 57.0 68.4 70.0 69.1 74.5 76.4 46.1 54.8 64.7 65.9 67.3 68.2 65.8 73.9 75.3 48.9 54.3 64.1 65.2 74.2 - 43.3 55.7 68.3 68.4 74.4 - 45.7 55.0 66.2 66.8 74.3 - 38.6 34.3 59.5 60.7 60.9 61.1 61.4 33.2 28.9 47.3 50.4 51.0 54.7 57.3 35.7 31.4 52.7 55.1 55.5 57.2 54.2 57.7 59.3 40.6 3"
2020.acl-main.604,P18-1078,0,0.156481,"arge . These results demonstrate the effectiveness of our RikiNet. Since most submissions on the NQ leaderboard are ensemble models, we also report the results of our ensemble model, which consists of three RikiNet-RoBERTa large models with different hyper-parameters. At the time of submission (29 Nov. 2019), the NQ leaderboard shows that our ensemble model achieves the best performance on both LA (F1 76.1) and SA (F1 61.3). 4 The single RikiNet-BERTlarge model was submitted to the NQ public leaderboard on 7 Nov. 2019. 6767 LA Dev R F1 P P LA Test R F1 P SA Dev R F1 P SA Test R F1 DocumentQA (Clark and Gardner, 2018) DecAtt (Parikh et al., 2016) + DocReader (Chen et al., 2017) BERTjoint (Alberti et al., 2019b) BERTlarge + 4M synth NQ (Alberti et al., 2019a) BERTjoint (Alberti et al., 2019b) + RoBERTa large (Liu et al., 2019) ‡ BERTlarge + SQuAD2 PT + AoA (Pan et al., 2019)† BERTlarge + SSPT (Glass et al., 2019)† RikiNet-BERTlarge RikiNet-RoBERTa large ‡ 47.5 52.7 61.3 62.3 65.6 73.2 74.3 44.7 57.0 68.4 70.0 69.1 74.5 76.4 46.1 54.8 64.7 65.9 67.3 68.2 65.8 73.9 75.3 48.9 54.3 64.1 65.2 74.2 - 43.3 55.7 68.3 68.4 74.4 - 45.7 55.0 66.2 66.8 74.3 - 38.6 34.3 59.5 60.7 60.9 61.1 61.4 33.2 28.9 47.3 50.4 51.0"
2020.acl-main.604,P17-1055,0,0.046996,"atkowski et al. (2019) adapt Document-QA (Clark and Gardner, 2018) for NQ, and also utilizes DecAtt (Parikh et al., 2016) for paragraph selection and DocReader (Chen et al., 2017) for answer prediction. BERTjoint (Alberti et al., 2019b) modifies BERT for NQ. Besides, some works focus on using data augmentation to improve the MRC models on NQ. Alberti et al. (2019a) propose a synthetic QA corpora generation method based on roundtrip consistency. Glass et al. (2019) propose a span selection method for BERT pre-training (SSPT). More recently, Pan et al. (2019) introduce attention-over-attention (Cui et al., 2017) into the BERT model. Pan et al. (2019) also propose several techniques of data augmentation and model ensemble to further improve the model performance on NQ. Although the use of data augmentation and other advanced pre-trained language models (Lan et al., 2019) may further improve model performance, as this is not the main focus of this paper, we leave them as our future work. Our RikiNet is a new MRC model designed tailored to the NQ challenges and can effectively represent the document and question at multi-levels to jointly predict the answers, which significantly outperforms the above me"
2020.acl-main.604,N19-1240,0,0.0572071,"Missing"
2020.acl-main.604,Q19-1026,0,0.198824,"ale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is significantly longer compared to other datasets. Secondly, NQ task not only requires the model to find an answer span (called short answer) to the question like previous MRC tasks but also asks the model to find a paragraph that contains the information required to answer the q"
2020.acl-main.604,2021.ccl-1.108,0,0.252894,"Missing"
2020.acl-main.604,D16-1244,0,0.113496,"Missing"
2020.acl-main.604,P18-2124,0,0.253676,"ard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is sig"
2020.acl-main.604,D16-1264,0,0.100039,"formance on the official NQ leaderboard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives a"
2020.acl-main.604,Q19-1016,0,0.0332106,"ng comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is significantly longer compared to o"
2020.acl-main.604,D19-1169,0,0.0252877,"s the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and shortanswer tasks, achieving the best performance on the official NQ leaderboard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ com"
2020.acl-main.604,P19-1218,0,0.0618401,"f the i-th paragraph, D(T ) [j, :] is the representation of the j-th token at last DPDA Φ(t) [k] denotes the score of the k-th token at tth block, K is a hyperparameter, and SΦ (t) is the set that includes the index of the selected top-K tokens. This attention mask lets the paragraph representation concentrate on the selected key tokens. The final scaled dot-product attention weight A(t) ∈ Rm×m of the multi-head self-attention sublayer (Vaswani et al., 2017) in Eq. (1) with two proposed attention masks can be written as:    C DC > D(t) (t) L . √ A(t) = Softmax MΦ (t) + M + h 3 Following Zhuang and Wang (2019), our implementation pads the unselected token representations with zero embeddings and adds the scorer representation with the linear transformation to D(t) to avoid gradient vanishing for scorer training. 6765 3.2 Multi-level Cascaded Answer Predictor Due to the nature of the NQ tasks, a short answer is always contained within a long answer, and thus it makes sense to use the prediction of long answers to facilitate the process of obtaining short answers. As shown on the right in Fig. 1, we design a cascaded structure to exploit this dependency. This predictor takes the token representation"
2020.acl-main.604,N19-1423,0,\N,Missing
2020.acl-main.87,D16-1264,0,\N,Missing
2020.acl-main.87,P17-1147,0,\N,Missing
2020.acl-main.87,D17-1215,0,\N,Missing
2020.acl-main.87,P17-1132,0,\N,Missing
2020.acl-main.87,P18-1076,0,\N,Missing
2020.acl-main.87,L18-1437,0,\N,Missing
2020.acl-main.87,P19-1139,0,\N,Missing
2020.acl-main.87,P19-1493,0,\N,Missing
2020.acl-main.87,D19-1249,0,\N,Missing
2020.acl-main.87,D19-1169,0,\N,Missing
2020.coling-main.244,D18-1269,0,0.169357,"g data for low-resource languages (Asai et al., 2018; Lee et al., 2018). However, CLMRC is severely restricted by translation quality (Cui et al., 2019). Recently, large-scale pre-trained language models (PLM) (Devlin et al., 2018; Yang et al., 2019; Sun et al., 2019) are shown effective in NLU related tasks. Inspired by the success of PLM, multilingual PLM (Lample and Conneau, 2019; Huang et al., 2019; Liang et al., 2020) are developed by leveraging large-scale multilingual corpuses for cross-lingual pre-training. Those powerful multilingual PLM are capable of zero-shot or few-shot learning (Conneau et al., 2018; Castellucci et al., 2019), and are effective to transfer from rich-resource languages to low-resource languages. Although those methods gain significant improvements on sentence-level tasks, such as sentence classification (Conneau et al., 2018), there is still a big gap between the performance of CLMRC in rich-resource languages and that in low-resource languages, since CLMRC requires high quality fine-grained representation at the phaselevel (Yuan et al., 2020). Several studies combine multilingual PLM with translation data to improve the CLMRC performance by either data augmentation using"
2020.coling-main.244,D19-1169,0,0.077794,"ce languages to enrich ∗ Equal contribution. Work was done when Junhao Liu was an intern at Microsoft STCA. Min Yang and Daxin Jiang are corresponding authors. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. † License details: http:// 2710 Proceedings of the 28th International Conference on Computational Linguistics, pages 2710–2721 Barcelona, Spain (Online), December 8-13, 2020 training data for low-resource languages (Asai et al., 2018; Lee et al., 2018). However, CLMRC is severely restricted by translation quality (Cui et al., 2019). Recently, large-scale pre-trained language models (PLM) (Devlin et al., 2018; Yang et al., 2019; Sun et al., 2019) are shown effective in NLU related tasks. Inspired by the success of PLM, multilingual PLM (Lample and Conneau, 2019; Huang et al., 2019; Liang et al., 2020) are developed by leveraging large-scale multilingual corpuses for cross-lingual pre-training. Those powerful multilingual PLM are capable of zero-shot or few-shot learning (Conneau et al., 2018; Castellucci et al., 2019), and are effective to transfer from rich-resource languages to low-resource languages. Although those me"
2020.coling-main.244,D19-1607,0,0.0790048,"he performance of CLMRC in rich-resource languages and that in low-resource languages, since CLMRC requires high quality fine-grained representation at the phaselevel (Yuan et al., 2020). Several studies combine multilingual PLM with translation data to improve the CLMRC performance by either data augmentation using translation (Singh et al., 2019) or auxiliary tasks (Yuan et al., 2020) (see Section 2 for some details). Those studies take two alternative approaches. First, they may just leverage translated data in target languages as new training data to directly train target language models (Hsu et al., 2019). The performance of such models is still limited by the translation issues (i.e, the noise introduced by the translation processing). Second, they may strongly rely on language-specific external corpuses, which are not widely or easily accessible (Yuan et al., 2020). According to the generalized cross-lingual transfer result (Lewis et al., 2019), the best cross-lingual performance is often constrained by the passage language, rather than the question language. In other words, the passage language plays an important role in CLMRC. The intuition is that the goal of MRC to pinpoint the exact ans"
2020.coling-main.244,D18-1232,0,0.0177952,"2020). Knowledge distillation from multiple teachers is also proposed (You et al., 2017; Yang et al., 2020), where the relative dissimilarity of feature maps generated from diverse teacher models can provide more appropriate guidance in student model training. Knowledge distillation is effective in transfer learning in those applications. In this paper, on top of translation, we propose a novel approach of language branch training to obtain several language-specific teacher models. We further propose a novel multilingual multi-teacher distillation framework. In contrast to the previous work (Hu et al., 2018; Yuan et al., 2020), our proposed method can greatly reduce the noise introduced by translation systems without relying on external 2712 LBMRC Dataset for English English MRC Dataset Question_en: What is in front of the Notre Dame Main Building? Question_en Passage_en: … Immediately in front of the Main Building and facing it, is ([a copper statue of Christ)] with arms upraised … Question_de Passage_en: … Immediately in front of the Main Building and facing it, is ([a copper statue of Christ)] with arms upraised … Translate English Dataset into Non-English 1 Question_es: ¿Qué hay frente al ed"
2020.coling-main.244,D19-1252,1,0.794979,"vecommons.org/licenses/by/4.0/. † License details: http:// 2710 Proceedings of the 28th International Conference on Computational Linguistics, pages 2710–2721 Barcelona, Spain (Online), December 8-13, 2020 training data for low-resource languages (Asai et al., 2018; Lee et al., 2018). However, CLMRC is severely restricted by translation quality (Cui et al., 2019). Recently, large-scale pre-trained language models (PLM) (Devlin et al., 2018; Yang et al., 2019; Sun et al., 2019) are shown effective in NLU related tasks. Inspired by the success of PLM, multilingual PLM (Lample and Conneau, 2019; Huang et al., 2019; Liang et al., 2020) are developed by leveraging large-scale multilingual corpuses for cross-lingual pre-training. Those powerful multilingual PLM are capable of zero-shot or few-shot learning (Conneau et al., 2018; Castellucci et al., 2019), and are effective to transfer from rich-resource languages to low-resource languages. Although those methods gain significant improvements on sentence-level tasks, such as sentence classification (Conneau et al., 2018), there is still a big gap between the performance of CLMRC in rich-resource languages and that in low-resource languages, since CLMRC req"
2020.coling-main.244,P17-1147,0,0.0176697,"s, which saves the cost of training, inference, and maintenance for multiple models. Extensive experiments on two CLMRC benchmarks clearly show the effectiveness of our proposed method. 1 Introduction Machine Reading Comprehension (MRC) is a central task in natural language understanding (NLU) with many applications, such as information retrieval and dialogue generation. Given a query and a text paragraph, MRC extracts the span of the correct answer from the paragraph. Recently, as a series of largescale annotated datasets become available, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), the performance of MRC systems has been improved dramatically (Xiong et al., 2017; Hu et al., 2017; Yu et al., 2018; Wang et al., 2016; Seo et al., 2016). Nevertheless, those large-scale, high-quality annotated datasets often only exist in rich-resource languages, such as English, French and German. Correspondingly, the improvement of MRC quality can only benefit those rich-source languages. Annotating a large MRC dataset with high quality for every language is very costly and may even be infeasible (He et al., 2017). MRC in low-resource languages still suffers from the lack of large amounts"
2020.coling-main.244,L18-1437,0,0.0922231,"proposed, where translation systems are used to translate datasets from rich-source languages to enrich ∗ Equal contribution. Work was done when Junhao Liu was an intern at Microsoft STCA. Min Yang and Daxin Jiang are corresponding authors. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. † License details: http:// 2710 Proceedings of the 28th International Conference on Computational Linguistics, pages 2710–2721 Barcelona, Spain (Online), December 8-13, 2020 training data for low-resource languages (Asai et al., 2018; Lee et al., 2018). However, CLMRC is severely restricted by translation quality (Cui et al., 2019). Recently, large-scale pre-trained language models (PLM) (Devlin et al., 2018; Yang et al., 2019; Sun et al., 2019) are shown effective in NLU related tasks. Inspired by the success of PLM, multilingual PLM (Lample and Conneau, 2019; Huang et al., 2019; Liang et al., 2020) are developed by leveraging large-scale multilingual corpuses for cross-lingual pre-training. Those powerful multilingual PLM are capable of zero-shot or few-shot learning (Conneau et al., 2018; Castellucci et al., 2019), and are effective to t"
2020.coling-main.244,P19-1493,0,0.028819,"anslation data, which may not be available for some low-resource languages. Recently, large-scale pre-trained language models have shown effective in many natural language processing tasks, which prompt the development of multilingual language models, such as multilingual BERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019), and Unicoder (?). These language models aim to learn language agnostic contextual representations by leveraging large-scale monolingual and parallel corpuses, which show great potential on cross-lingual tasks, such as sentence classification tasks (Hsu et al., 2019; Pires et al., 2019; Conneau et al., 2018). However, there is still a big gap between the performance of CLMRC in rich-resource languages and that in low-resource languages, since CLMRC requires the capability of fine-grained representation at the phase-level (Yuan et al., 2020). To further boost the performance of multilingual PLM on CLMRC task, Yuan et al. (2020) propose two auxiliary tasks mixMRC and LAKM on top of multilingual PLM. Those auxiliary tasks improve the answer boundary detection quality in low-resource languages. mixMRC first uses a translation system to translate the English training data into o"
2020.coling-main.244,D16-1264,0,0.643208,"model can apply to all target languages, which saves the cost of training, inference, and maintenance for multiple models. Extensive experiments on two CLMRC benchmarks clearly show the effectiveness of our proposed method. 1 Introduction Machine Reading Comprehension (MRC) is a central task in natural language understanding (NLU) with many applications, such as information retrieval and dialogue generation. Given a query and a text paragraph, MRC extracts the span of the correct answer from the paragraph. Recently, as a series of largescale annotated datasets become available, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), the performance of MRC systems has been improved dramatically (Xiong et al., 2017; Hu et al., 2017; Yu et al., 2018; Wang et al., 2016; Seo et al., 2016). Nevertheless, those large-scale, high-quality annotated datasets often only exist in rich-resource languages, such as English, French and German. Correspondingly, the improvement of MRC quality can only benefit those rich-source languages. Annotating a large MRC dataset with high quality for every language is very costly and may even be infeasible (He et al., 2017). MRC in low-resource languages still suff"
2020.coling-main.244,2020.acl-main.324,0,0.0241325,"s conducted. The mixMRC task may still be limited by the translation quality and LAKM requires a large amount of external corpus, which is not easily accessible. Knowledge Distillation is initially adopted for model compression (Buciluˇa et al., 2006), where a small and light-weight student model learns to mimic the output distribution of a large teacher model. Recently, knowledge distillation has been widely applied to many tasks, such as person re-identification (Wu et al., 2019), item recommendation (Tang and Wang, 2018), and neural machine translation (Tan et al., 2019; Zhou et al., 2019; Sun et al., 2020). Knowledge distillation from multiple teachers is also proposed (You et al., 2017; Yang et al., 2020), where the relative dissimilarity of feature maps generated from diverse teacher models can provide more appropriate guidance in student model training. Knowledge distillation is effective in transfer learning in those applications. In this paper, on top of translation, we propose a novel approach of language branch training to obtain several language-specific teacher models. We further propose a novel multilingual multi-teacher distillation framework. In contrast to the previous work (Hu et"
2020.coling-main.244,2020.acl-main.87,1,0.706423,"ilingual corpuses for cross-lingual pre-training. Those powerful multilingual PLM are capable of zero-shot or few-shot learning (Conneau et al., 2018; Castellucci et al., 2019), and are effective to transfer from rich-resource languages to low-resource languages. Although those methods gain significant improvements on sentence-level tasks, such as sentence classification (Conneau et al., 2018), there is still a big gap between the performance of CLMRC in rich-resource languages and that in low-resource languages, since CLMRC requires high quality fine-grained representation at the phaselevel (Yuan et al., 2020). Several studies combine multilingual PLM with translation data to improve the CLMRC performance by either data augmentation using translation (Singh et al., 2019) or auxiliary tasks (Yuan et al., 2020) (see Section 2 for some details). Those studies take two alternative approaches. First, they may just leverage translated data in target languages as new training data to directly train target language models (Hsu et al., 2019). The performance of such models is still limited by the translation issues (i.e, the noise introduced by the translation processing). Second, they may strongly rely on"
2020.coling-main.5,2020.acl-main.398,0,0.0366929,"Missing"
2020.coling-main.5,2020.tacl-1.5,0,0.0137301,"onnection to each node of the graph and building the corresponding degree and adjacency matrix. After two rounds of convolution, L(2) denotes the node features updated. Graph prediction is derived by a mean pooling operation on the nodes of the graph, followed by an MLP, that is, y = M LP (P ooling(L(2) )), where y is the predicted QA match score for the corresponding input query Q and data example S. 3.4 Pre-training Strategy Pre-training (Erhan et al., 2010) has become a new paradigm of natural language processing, and various pre-training techniques have been proposed (Devlin et al., 2018; Joshi et al., 2020). However, most previous pre-training techniques were designed for plain text. Due to the structural characteristics of Web semi-structured data, they cannot be applied directly to such data. In this paper, we propose a novel 55 pre-training method that allows the model to learn representations from semantics embedded in both text and structures of tables and lists. Following the successful pre-training experience of transformerbased models (Devlin et al., 2018), we used two pre-training objectives designed specifically for semistructured data. Whole Cell Masking (WCM). We follow the masked la"
2020.coling-main.5,D19-1603,0,0.0580227,"Missing"
2020.coling-main.5,C18-1165,0,0.0514465,"Missing"
2020.coling-main.5,2020.acl-main.745,0,0.0287927,"received increasing interest. Nishida et al. (2017) propose to apply convolutional models to Web table. The rationale is to consider a Web table as a matrix of text, analogous to an image of pixels. However, their model does not show strong performance, partly because the semantic relationship among neighbor cells in tables may be far more complex than the simple adjacency relation among neighbor pixels in an image. Herzig et al. (2020) propose TAPAS, a weakly supervised table parsing method. TAPAS models the structure information of tables by explicitly encoding rows and columns. Similarly, Yin et al. (2020) propose TABERT, which focuses on pre-training methods for the table QA task. The authors design a pipeline for learning row-level and column-level representations. M¨uller et al. (2019) also builds a graph representation on tables cells, focusing on optimizing cell answer selection. However, These works only model the row/column relations among table cells, without considering other relations including caption-content relation, header-cell relation and subject-attribute relation. In this work, we give a thorough categorization of the relations among all components in semi-structured data, and"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-main.484,D19-1252,1,0.908102,"LUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu"
2020.emnlp-main.484,D17-1302,0,0.176422,"e select 11 cross-lingual tasks in XGLUE, which are categorized into 3 groups: single-input understanding tasks, pair-input understanding tasks, and generation tasks. For each task, training set is only available in English. In order to obtain a good performance on XGLUE, a model should be able to learn how to do a task well using its English training set, and then transfer this ability to test sets in other languages. Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks. 2 https://commoncrawl.org/. Single-input Understanding Tasks POS Tagging (POS) Following (Kim et al., 2017), we select a subset of Universal Dependencies (UD) Treebanks (v2.5) (Zeman et al., 2019), which covers 18 languages. Accuracy (ACC) of the predicted POS tags is used as the metric. News Classification (NC) This task aims to predict the category given a news article. It covers 5 languages, including English, Spanish, French, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tas"
2020.emnlp-main.484,2020.acl-main.703,0,0.0930192,"Missing"
2020.emnlp-main.484,P19-4007,0,0.0660349,"Missing"
2020.emnlp-main.484,2021.ccl-1.108,0,0.046651,"Missing"
2020.emnlp-main.484,D18-1269,0,0.125055,"rench, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad titl"
2020.emnlp-main.484,N19-1423,0,0.68133,"oder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microso"
2020.emnlp-main.484,W03-0419,0,0.651554,"Missing"
2020.emnlp-main.484,D19-1382,0,0.266447,"understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microsoft/Unicoder where an NLP task often h"
2020.emnlp-main.484,N19-1131,0,0.0455297,"t News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad title, ad description, label>. The label indicates whether the ad is relevant to the query (Good), or not (Bad). We con6009 Task # of Languages |Train|en |Dev|avg"
2020.emnlp-main.484,2020.findings-emnlp.217,1,\N,Missing
2020.emnlp-main.505,D18-1443,0,0.0163097,"cle to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims"
2020.emnlp-main.505,P16-1154,0,0.168923,". All sub-layers are interconnected with residual connections (He et al., 2016) and layer normalization (Ba et al., 2016). Similarly, the decoder is also composed of a stack of N identical block. In addition to the two sub-layers in each encoder block, the decoder contains a third sub-layer which performs multi-head 6243 attention over the output of the encoder. Figure 2 (a) shows the architecture of the block in the decoder. BASE uses the pre-trained BERT-base model (Devlin et al., 2018) to initialize the parameters of the encoder. Also, it uses the transformer decoder with a copy mechanism (Gu et al., 2016), whose hidden size, the number of multi-head h, and the number of blocks N are the same as its encoder. 3.2.2 Keyphrase-Aware Headline Generation Model In order to explore more effective ways of incorporating keyphrase information into BASE, we design 5 variants of multi-source Transformer decoders. Article + Keyphrase. The basic idea is to add the keyphrase into the decoder directly. The keyphrase Xkey is represented as a sequence of word embeddings. As shown in Figure 2 (b), we add an extra sub-layer that performs multi-head attention over the Xkey in each block of the decoder. (n+1) Xdec ="
2020.emnlp-main.505,C18-1148,0,0.0418051,"Missing"
2020.emnlp-main.505,N16-1014,0,0.280999,"agnostic baselines as follows. (10) PT-NET, the original pointergenerator network (See et al., 2017) , which are widely used in text summarization and headline generation tasks. (11) SEASS (Zhou et al., 2017b), the GRU-based (Cho et al., 2014) sequence-tosequence model with selective encoding mechanism, which is widely used in text summarization. (12) Transformer + Copy (Vaswani et al., 2017; Gu et al., 2016), which has the same architecture hyperparameters as BASE, the only difference is that it does not use BERT to initialize the encoder. (13) BASE + Diverse, which applies diverse decoding (Li et al., 2016b) in beam search to BASE during inference to improve the generation diver6245 Method PT-GEN SEASS Transformer + Copy BASE BASE + Diverse BASE + Filter BASE + KEY BASE + AddFuse BASE + ParallelFuse BASE + StackFuse BASE + AddFuse + KEY BASE + ParallelFuse + KEY BASE + StackFuse + KEY K=1 35.66 31.20 38.91 42.09 39.44 43.53 44.30 43.74 43.97 43.12 43.09 43.87 ROUGE-1 K=3 39.82 34.52 43.80 45.40 45.83 41.52 47.07 47.36 47.28 47.63 46.82 47.70 47.71 K=5 41.59 35.98 45.72 47.21 47.89 43.89 49.08 49.46 49.69 49.74 49.16 49.84 49.96 K=1 19.80 14.82 21.85 24.10 20.82 25.27 25.98 25.20 25.32 24.66 24."
2020.emnlp-main.505,D15-1044,0,0.126297,"Missing"
2020.emnlp-main.505,P17-1099,0,0.177725,"(c) original article to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multih"
2020.emnlp-main.505,D16-1112,0,0.0197807,"est to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims to generate multiple independent headlines, which allows us to recommend news with different news headlines based on the interests of users. Besides, multi-headline generation can provide multiple hints for human news editors to assist them in writing news headlines. However, most existing methods (Takase et al., 2016; Ayana et al., 2016; Murao et al., 2019; Colmenares et al., 2019; Zhang et al., 2018) focus on single-headline generation. The headline generation process is treated as an one-to-one mapping (the input is an article and the output is a headline), which trains and tests the models without any additional guiding information or constraints. We argue that this may lead to two problems. Firstly, since it is reasonable to generate multiple headlines for the news, training to generate the single ground-truth might result in a lack of more detailed guidance. Even worse, a single ground-truth without"
2020.emnlp-main.505,W17-6001,0,0.029021,"e essential substring from a question and use this substring as the headline of the forums. Zhang et al. (2018) propose a method for question headline generation, which designs a dual-attention seq2seq model. However, most previous headline generation methods focus on one-to-one mapping, and the headline generation process is not controllable. In this work, we focus on the news multi-headline generation problem and design a keyphrase-aware headline generation method. Different information aware methods have been successfully used in natural language generation tasks (Zhou et al., 2017a, 2018; Wang et al., 2017), such as responses generation in the dialogue system. Similar to our task, responses generation in a dialogue system is also a one-to-many problem, Zhou et al. (2017a) propose a mechanism-aware seq2seq model for controllable response generation. They model different mechanisms as latent embeddings and learn the latent 6248 embeddings in their seq2seq model. Incorporating these mechanisms, their model can generate controllable responses. Zhou et al. (2018) propose a commonsense knowledge aware conversation generation method. More concretely, in the first stage, their model retrieves subgraphs"
2020.emnlp-main.505,N19-2011,0,0.0143952,"52.60 84.26 83.18 30.13 59.60 65.13 53.82 81.32 84.05 63.91 87.04 89.08 Table 1: Keyphrase Generation Results (2) Seq2Seq. Since our KeyAware News corpus contains the article-keyphrase pairs, we treat the keyphrase generation as a sequence-to-sequence learning task. We train the model BASE with article-keyphrase pairs. During inference, we use beam search with length penalty to generate ngrams (n = 2, 3, and 4) as the keyphrases. (3) Slot Tagging. Because the keyphrases also appear in the news articles, we can formulate the keyphrase generation task as a slot tagging task (Zhang et al., 2016; Williams, 2019). We finetune the BERT-base model to achieve that. Concretely, we use the output sequence of the model to predict the beginning and end position of the keyphrase in the article. During inference, we follow the answer span prediction method used in Seo et al. (2017) to predict n-grams (n = 2, 3, and 4) with the highest probabilities as the keyphrases. 4 Experiments 4.1 Keyphrase Generation In the first experiment, we evaluate the performance of three keyphrase generation methods: (a) unsupervised TF-IDF Ranking, (b) supervised sequence-to-sequence model (SEQ2SEQ), and (c) supervised slot taggin"
2020.emnlp-main.505,D16-1080,1,0.831187,"5 42.05 78.45 76.94 52.60 84.26 83.18 30.13 59.60 65.13 53.82 81.32 84.05 63.91 87.04 89.08 Table 1: Keyphrase Generation Results (2) Seq2Seq. Since our KeyAware News corpus contains the article-keyphrase pairs, we treat the keyphrase generation as a sequence-to-sequence learning task. We train the model BASE with article-keyphrase pairs. During inference, we use beam search with length penalty to generate ngrams (n = 2, 3, and 4) as the keyphrases. (3) Slot Tagging. Because the keyphrases also appear in the news articles, we can formulate the keyphrase generation task as a slot tagging task (Zhang et al., 2016; Williams, 2019). We finetune the BERT-base model to achieve that. Concretely, we use the output sequence of the model to predict the beginning and end position of the keyphrase in the article. During inference, we follow the answer span prediction method used in Seo et al. (2017) to predict n-grams (n = 2, 3, and 4) with the highest probabilities as the keyphrases. 4 Experiments 4.1 Keyphrase Generation In the first experiment, we evaluate the performance of three keyphrase generation methods: (a) unsupervised TF-IDF Ranking, (b) supervised sequence-to-sequence model (SEQ2SEQ), and (c) super"
2020.emnlp-main.505,P19-1100,0,0.0128883,"se-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims to generate multiple"
2020.emnlp-main.505,P17-1101,0,0.0957522,"ith stack-fusing, addition-fusing, and parallel-fusing mechanism, respectively. Based on BASE + StackFuse, BASE + AddFuse, and BASE + ParallelFuse, we further use the keyphrase as their additional inputs, like BASE + KEY. Then we obtain three additional variants (7) BASE + StackFuse + KEY, (8) BASE + AddFuse + KEY, and (9) BASE + ParallelFuse + KEY. In addition to BASE, We also compare four other keyphrase-agnostic baselines as follows. (10) PT-NET, the original pointergenerator network (See et al., 2017) , which are widely used in text summarization and headline generation tasks. (11) SEASS (Zhou et al., 2017b), the GRU-based (Cho et al., 2014) sequence-tosequence model with selective encoding mechanism, which is widely used in text summarization. (12) Transformer + Copy (Vaswani et al., 2017; Gu et al., 2016), which has the same architecture hyperparameters as BASE, the only difference is that it does not use BERT to initialize the encoder. (13) BASE + Diverse, which applies diverse decoding (Li et al., 2016b) in beam search to BASE during inference to improve the generation diver6245 Method PT-GEN SEASS Transformer + Copy BASE BASE + Diverse BASE + Filter BASE + KEY BASE + AddFuse BASE + Paralle"
2020.emnlp-main.505,W04-1013,0,\N,Missing
2020.emnlp-main.505,D14-1179,0,\N,Missing
2020.emnlp-main.505,N19-2010,0,\N,Missing
2020.emnlp-main.505,N19-1423,0,\N,Missing
2020.findings-emnlp.11,P04-3031,0,0.210715,"rved that DiffKSFus performs a bit better on WoW while DiffKSDis on Holl-E. We conjecture that it is because in Holl-E, the golden selected knowledge among different turns usually has high contextual dependency (for example, they may be continuous sentences in the document), which makes it feasible to predict the next selected knowledge simply conditioned on the differential information. 4.5 tively, and adopted the same copy mechanism in SLKS as in our models. 4.3 Implementation Details All the models were implemented with PyTorch (Paszke et al., 2017). The sentences were tokenized with NLTK (Bird and Loper, 2004). We set the vocabulary size to 20K for WoW and 16K for HollE and used the 300-dimensional word embeddings initialized by GloVe (Pennington et al., 2014) or from a standard normal distribution N (0, 1). We applied a dropout rate of 0.5 on word embeddings. The hidden sizes were set to 200 for the encoders (totally 400 for two directions) and to 400 for the decoder. We adopted the ADAM (Kingma and Ba, 2015) optimizer with the initial learning rate set to 0.0005. The batch size was set to 8 dialogues. All the models share the same hyperparameter setting Automatic Evaluation Human Observational Ev"
2020.findings-emnlp.11,D14-1179,0,0.0106621,"Missing"
2020.findings-emnlp.11,N19-1423,0,0.00952322,"the context and the posterior one on both the context and the golden response, and their KL divergence is minimized during training. The knowledge selection of PostKS is supervised by a BOW loss. We also evaluated two variants, where one uses LKS instead of the BOW loss to supervise knowledge selection (PostKS+), and the other is further equipped with copy mechanism (PostKS++). SLKS (Kim et al., 2020) improves PostKS by using two separate GRUs to update the states of dialog history and previously selected knowledge sentences respectively. For fair comparison, we replaced the pretrained BERT (Devlin et al., 2019) encoder and the Transformer (Vaswani et al., 2017) decoder in SLKS with BiGRU and GRU respec119 Models ACC BLEU-2/4 and were trained for 20 epochs on one NVIDIA Titan Xp GPU. The checkpoints of our reported results were selected according to BLEU-4 on the Dev sets. ROUGE-2 WoW Seen MemNet +LKS PostKS +LKS +Copy SLKS 13.2** 18.4** 13.8** 22.5** 21.9** 23.4** 6.6** 7.2** 6.9** 7.5** 9.9** 11.3 1.8** 1.9** 1.8** 2.3** 4.5** 5.5 3.2** 3.3** 3.2** 3.7** 5.6** 6.5 DiffKSFus DiffKSDis 25.5 24.7 11.6 11.3 5.7 5.7 6.8 6.8 4.4 WoW Uneen MemNet +LKS PostKS +LKS +Copy SLKS 12.8** 15.9** 13.6** 15.8** 14."
2020.findings-emnlp.11,W04-1013,0,0.0139932,"5** 4.5** 4.3** 3.9** 8.2** 22.4** 24.5** 8.9** 9.0** 8.6** 13.1** 23.1** 24.9* DiffKSFus DiffKSDis 33.0 33.5 29.5 29.9 25.5 25.9 25.9 26.4 Table 1: Automatic evaluation results. The best results are in bold. Significance tests were conducted between the best results and other competitors, with sign test for ACC, bootstrap resampling (Koehn, 2004) for BLEU, and Student’s t-test for ROUGE. */** indicate p-value &lt; 0.05/0.005 respectively. We used several automatic metrics: ACC, the accuracy of knowledge selection on the whole test set, corpus-level BLEU-2/4 (Papineni et al., 2002), and ROUGE-2 (Lin, 2004). As shown in Table 13 , our method outperforms significantly all the baselines in all the metrics on three test sets (except BLEU and ROUGE on WoW Seen compared with SLKS), which indicates its superiority in selecting proper knowledge and generating informative responses. Compared to the baseline models, our models also demonstrate a stronger ability of generalization from in-domain (WoW Seen) to out-of-domain data (WoW Unseen). It is worth noting that on WoW Unseen, our DiffKSFus obtains a higher accuracy (19.7) of knowledge selection even than the BERT-enhanced SLKS in their original paper"
2020.findings-emnlp.11,P16-1154,0,0.0437195,"l Selector simply looks for st = GRUD (st−1 , [e (yt−1 ) ; hk ]) , s0 = WD [hc ; hk ] + bD , (13) (14) where WD and bD are trainable parameters, and e (yt−1 ) denotes the embedding of the word yt−1 generated in the last time step. Then, the decoder 118 2 The model is trained with teacher forcing, where the golden selected knowledge hτk,iτ ∗ is used during training. outputs the generation probability over the vocabulary (without normalization): φG (yt = w) = wT (WG st + bG ) , (15) where WG and bG are trainable parameters, and w is the one-hot vector of the word w. Meanwhile, a copy mechanism (Gu et al., 2016) is adopted to output additional copy probability of the words in the selected knowledge sentence kbi (without normalization):   X φC (yt = w) = (st )T H hk,bi,j , (16) j:kbi,j =w where H is a fully connected layer activated with tanh. The final probability distribution is computed as follows: P (yt = w) = 1  φG (yt =w) φC (yt =w)  e +e , (17) Z where Z is the normalization term. Then we select the word from vocabulary with the highest probability, saying: yt = arg maxw P(yt = w). 3.5 WoW (Dinan et al., 2019) contains multi-turn knowledge-grounded conversations, collected by wizard-apprent"
2020.findings-emnlp.11,W04-3250,0,0.269559,"6.6** 7.9** 8.7** 1.2** 1.3** 1.2** 1.5** 3.2** 3.7** 2.3** 2.3** 2.1** 2.6** 3.9** 4.6** DiffKSFus DiffKSDis 19.7 18.3* 10.0 9.6 4.7 4.5 5.6 5.3 Holl-E MemNet +LKS PostKS +LKS +Copy SLKS 5.1** 25.1** 6.1** 29.5** 28.0** 28.6** 8.0** 7.7** 6.9** 15.9** 26.5** 28.5** 4.5** 4.3** 3.9** 8.2** 22.4** 24.5** 8.9** 9.0** 8.6** 13.1** 23.1** 24.9* DiffKSFus DiffKSDis 33.0 33.5 29.5 29.9 25.5 25.9 25.9 26.4 Table 1: Automatic evaluation results. The best results are in bold. Significance tests were conducted between the best results and other competitors, with sign test for ACC, bootstrap resampling (Koehn, 2004) for BLEU, and Student’s t-test for ROUGE. */** indicate p-value &lt; 0.05/0.005 respectively. We used several automatic metrics: ACC, the accuracy of knowledge selection on the whole test set, corpus-level BLEU-2/4 (Papineni et al., 2002), and ROUGE-2 (Lin, 2004). As shown in Table 13 , our method outperforms significantly all the baselines in all the metrics on three test sets (except BLEU and ROUGE on WoW Seen compared with SLKS), which indicates its superiority in selecting proper knowledge and generating informative responses. Compared to the baseline models, our models also demonstrate a st"
2020.findings-emnlp.11,P19-1002,0,0.0526079,"that our method significantly outperforms strong baselines in terms of knowledge selection and can generate more informative responses. 2 2.1 existing studies have been mainly devoted to addressing two research problems: (1) knowledge selection: selecting appropriate knowledge given the dialog context and previously selected knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020; Kim et al., 2020); and (2) knowledge-aware generation: injecting the required knowledge to generate meaningful and informative responses (Ghazvininejad et al., 2018; Zhou et al., 2018a; Li et al., 2019; Qin et al., 2019; Yavuz et al., 2019; Zhao et al., 2020). Since selecting the appropriate knowledge is a precursor to the success of knowledge grounded dialog systems, we focus on the knowledge selection problem in this paper. 2.2 The non-sequential selection models capture the relationship between the current context and background knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020). For instance, PostKS (Lian et al., 2019) estimates a posterior distribution over candidate knowledge sentences, which is based on both the context and the golden response, and"
2020.findings-emnlp.11,P18-1138,0,0.0858333,"long to non-sequential selection models. Different from our work and Lian et al. (2019); Kim et al. (2020) that select knowledge from candidate knowledge sentences, their methods are devised for selecting important text spans or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states"
2020.findings-emnlp.11,D17-2014,0,0.046796,"Missing"
2020.findings-emnlp.11,D18-1255,0,0.467457,"(2020) that select knowledge from candidate knowledge sentences, their methods are devised for selecting important text spans or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because"
2020.findings-emnlp.11,P19-1081,0,0.0191341,"r methods are devised for selecting important text spans or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because we also utilize the previously selected knowledge. However, we expl"
2020.findings-emnlp.11,P02-1040,0,0.106956,"* 8.0** 7.7** 6.9** 15.9** 26.5** 28.5** 4.5** 4.3** 3.9** 8.2** 22.4** 24.5** 8.9** 9.0** 8.6** 13.1** 23.1** 24.9* DiffKSFus DiffKSDis 33.0 33.5 29.5 29.9 25.5 25.9 25.9 26.4 Table 1: Automatic evaluation results. The best results are in bold. Significance tests were conducted between the best results and other competitors, with sign test for ACC, bootstrap resampling (Koehn, 2004) for BLEU, and Student’s t-test for ROUGE. */** indicate p-value &lt; 0.05/0.005 respectively. We used several automatic metrics: ACC, the accuracy of knowledge selection on the whole test set, corpus-level BLEU-2/4 (Papineni et al., 2002), and ROUGE-2 (Lin, 2004). As shown in Table 13 , our method outperforms significantly all the baselines in all the metrics on three test sets (except BLEU and ROUGE on WoW Seen compared with SLKS), which indicates its superiority in selecting proper knowledge and generating informative responses. Compared to the baseline models, our models also demonstrate a stronger ability of generalization from in-domain (WoW Seen) to out-of-domain data (WoW Unseen). It is worth noting that on WoW Unseen, our DiffKSFus obtains a higher accuracy (19.7) of knowledge selection even than the BERT-enhanced SLKS"
2020.findings-emnlp.11,D14-1162,0,0.0833471,"among different turns usually has high contextual dependency (for example, they may be continuous sentences in the document), which makes it feasible to predict the next selected knowledge simply conditioned on the differential information. 4.5 tively, and adopted the same copy mechanism in SLKS as in our models. 4.3 Implementation Details All the models were implemented with PyTorch (Paszke et al., 2017). The sentences were tokenized with NLTK (Bird and Loper, 2004). We set the vocabulary size to 20K for WoW and 16K for HollE and used the 300-dimensional word embeddings initialized by GloVe (Pennington et al., 2014) or from a standard normal distribution N (0, 1). We applied a dropout rate of 0.5 on word embeddings. The hidden sizes were set to 200 for the encoders (totally 400 for two directions) and to 400 for the decoder. We adopted the ADAM (Kingma and Ba, 2015) optimizer with the initial learning rate set to 0.0005. The batch size was set to 8 dialogues. All the models share the same hyperparameter setting Automatic Evaluation Human Observational Evaluation We conducted human observational evaluation with pair-wise comparison, where our two models were compared with PostKS++ and SLKS. 100 dialogues"
2020.findings-emnlp.11,P19-1539,0,0.0386284,"ignificantly outperforms strong baselines in terms of knowledge selection and can generate more informative responses. 2 2.1 existing studies have been mainly devoted to addressing two research problems: (1) knowledge selection: selecting appropriate knowledge given the dialog context and previously selected knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020; Kim et al., 2020); and (2) knowledge-aware generation: injecting the required knowledge to generate meaningful and informative responses (Ghazvininejad et al., 2018; Zhou et al., 2018a; Li et al., 2019; Qin et al., 2019; Yavuz et al., 2019; Zhao et al., 2020). Since selecting the appropriate knowledge is a precursor to the success of knowledge grounded dialog systems, we focus on the knowledge selection problem in this paper. 2.2 The non-sequential selection models capture the relationship between the current context and background knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020). For instance, PostKS (Lian et al., 2019) estimates a posterior distribution over candidate knowledge sentences, which is based on both the context and the golden response, and only uses the con"
2020.findings-emnlp.11,D19-1194,0,0.0137048,"ed for selecting important text spans or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because we also utilize the previously selected knowledge. However, we explicitly compute the"
2020.findings-emnlp.11,P18-2118,0,0.0646039,"Missing"
2020.findings-emnlp.11,P19-1369,0,0.102069,"portant text spans or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because we also utilize the previously selected knowledge. However, we explicitly compute the difference betwee"
2020.findings-emnlp.11,2020.acl-main.166,0,0.0136441,"020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because we also utilize the previously selected knowledge. However, we explicitly compute the difference between knowledge selected at different turns, while SLKS only encodes the already selected knowledge in an implicit way. 116 k&quot;! In addition, recently there emerge a number of works that propose RL-based models to select a path in structured knowledge graph (KG) (Xu et al., 2020a,b), which also select knowledge in a sequential way. While our method is designed to ground the conversation to unstructured knowledge text, we will leave as future work the application of our method to such KG-grounded dialog generation tasks (Wu et al., 2019; Moon et al., 2019; Zhou et al., 2020). 3 3.1 Knowledge Encoder Context Encoder 3.3 Task Formulation ?&quot;# Response Decoder Copy Difference-aware Knowledge Selection In order to select proper knowledge, our model gets aware of the difference between the current candidate knowledge sentences and the previously selected knowledge. To make"
2020.findings-emnlp.11,W19-5917,0,0.1384,"rforms strong baselines in terms of knowledge selection and can generate more informative responses. 2 2.1 existing studies have been mainly devoted to addressing two research problems: (1) knowledge selection: selecting appropriate knowledge given the dialog context and previously selected knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020; Kim et al., 2020); and (2) knowledge-aware generation: injecting the required knowledge to generate meaningful and informative responses (Ghazvininejad et al., 2018; Zhou et al., 2018a; Li et al., 2019; Qin et al., 2019; Yavuz et al., 2019; Zhao et al., 2020). Since selecting the appropriate knowledge is a precursor to the success of knowledge grounded dialog systems, we focus on the knowledge selection problem in this paper. 2.2 The non-sequential selection models capture the relationship between the current context and background knowledge (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020). For instance, PostKS (Lian et al., 2019) estimates a posterior distribution over candidate knowledge sentences, which is based on both the context and the golden response, and only uses the context to estimate a p"
2020.findings-emnlp.11,2020.acl-main.635,1,0.830287,"s or fragments from the background knowledge document that will be used in generation. Therefore these works have a different task setting from ours. 2.3 Related Work Knowledge-grounded Dialog Generation Recently, a variety of neural models have been proposed to facilitate knowledge-grounded conversation generation (Zhu et al., 2017; Young et al., 2018; Zhou et al., 2018a; Liu et al., 2018). The research topic is also greatly advanced by many corpora (Zhou et al., 2018b; Moghe et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Moon et al., 2019; Tuan et al., 2019; Wu et al., 2019; Zhou et al., 2020). As surveyed in Huang et al. (2020), Non-sequential Knowledge Selection Sequential Knowledge Selection The sequential selection models additionally make use of previously selected knowledge to facilitate knowledge selection (Kim et al., 2020). For instance, Kim et al. (2020) propose a Sequential Latent Knowledge Selection (SLKS) model. It keeps track of the hidden states of dialog history and previously selected knowledge sentences. Our method is parallel to SLKS because we also utilize the previously selected knowledge. However, we explicitly compute the difference between knowledge selected"
2020.findings-emnlp.11,D18-1076,0,0.406741,"rd they are fantasy novels written by Brandon Sanderson. Figure 1: An example of difference-aware knowledge selection. The blue 4 denotes that the corresponding knowledge has little difference from or is identical to the previously selected one, and selecting it may lead to repetitive responses. The red × denotes that the difference is too large, and selecting it could make the response incoherent with the context. Introduction Knowledge-grounded conversation generation aims at generating informative responses based on both discourse context and external knowledge (Ghazvininejad et al., 2018; Zhou et al., 2018a), where selecting appropriate knowledge is critical to the success of the task. Existing knowledge selection models generally fall into two types. One type is solely based on the context (Lian et al., 2019; Zhang et al., 2019; Meng et al., 2020; Ren et al., 2020), which we call non-sequential selection because knowledge selection at different turns is independent. The other type sequentially selects knowledge additionally conditioned on previously selected knowledge (Kim et al., 2020), which we ∗ * Corresponding author. call sequential selection. As shown in Kim et al. (2020), such a sequent"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2020.findings-emnlp.370,P17-1171,0,0.490746,"es/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at bot"
2020.findings-emnlp.370,P84-1044,0,0.185057,"Missing"
2020.findings-emnlp.370,P18-1078,0,0.283266,"and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and pass"
2020.findings-emnlp.370,D18-1456,0,0.0158676,"mance isn’t as good as Reflection model. 5 Related Work Machine Reading Comprehension: Machine reading comprehension (Hermann et al., 2015; Chen et al., 2017; Rajpurkar et al., 2016; Clark and Gardner, 2018) is mostly based on the attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) that take as input hquestion, paragraphi, compute an interactive representation of them and predict the start and end positions of the answer. When dealing with no-answer cases, popular method is to jointly model the answer position probability and no-answer probability by a shared softmax normalizer (Kundu and Ng, 2018; Clark and Gardner, 2018; Devlin et al., 2019), or independently model the answerability as a binary classification problem (Hu et al., 2019; Yang et al., 2019; Liu et al., 2019). For long document processing, there are pipeline approaches of IR + Span Extraction (Chen et al., 2017), DecAtt + DocReader (Kwiatkowski et al., 2019), sliding window approach (Alberti et al., 2019) and recently proposed long sequence handling Transformers (Kitaev et al., 2020; Guo et al., 2019; Beltagy et al., 2020) Answer Verifier: Answer verifier (Tan et al., 2018; Hu et al., 2019) is proposed to validate the leg"
2020.findings-emnlp.370,P17-1055,0,0.0312392,"uding no-answer, yes/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search"
2020.findings-emnlp.370,Q19-1026,0,0.195755,"ure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover, there are richer answer types in the NQ task. In addition to indicating textual answer spans (long and short), the ‡ Corresponding author. https://ai.google.com/research/ NaturalQuestions/lead"
2020.findings-emnlp.370,N19-1423,0,0.557435,"this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover, there are richer an"
2020.findings-emnlp.370,2020.acl-main.604,1,0.724645,"Missing"
2020.findings-emnlp.370,2021.ccl-1.108,0,0.106043,"Missing"
2020.findings-emnlp.370,P18-2124,0,0.0424119,"We transform head features by scale to [0, 1], sqrt, log, minus mean then divided by standard deviation. for RoBERTa; then use sliding window approach to slice document into instances as described in Section 2.1. For NQ, since the document is quite long, we add special atomic markup tokens to indicate which part of the document the model is reading. 3.1 Implementation Our implementation is based on Huggingface Transformers (Wolf et al., 2019). All the pretrained models are large version (24 layers, 1024 hidden size, 16 heads, etc.). For MRC model training, we firstly finetune it on squad2.0 (Rajpurkar et al., 2018) data and then continue to finetune on NQ data. For Reflection model, we firstly leverage the MRC model to generate training data, and then finetune Reflection model which is initialized by MRC model parameters. We use one MRC model to deal with all answer types in NQ, but two Reflection models, one for long answer, the other for short. We manually tune the hyperparameters based on dev data F1 and submit best models to NQ organizer for leaderboard, list the best setting in Appendices. Experiments are performed on 4 NVIDIA Tesla P40 24GB cards, both MRC and Reflection model can be trained withi"
2020.findings-emnlp.370,D16-1264,0,0.0630444,"ning target are quite different from MRC model, further training will hurt the accuracy of answer prediction. This configuration save a lot memory and computation cost of prediction: all the data only need to pass through one Transformer. The results show it can improve most of the metrics. However, the [cls] representation in MRC model targets at answer types classification which include no-answer but not predicted wrong-ans, the performance isn’t as good as Reflection model. 5 Related Work Machine Reading Comprehension: Machine reading comprehension (Hermann et al., 2015; Chen et al., 2017; Rajpurkar et al., 2016; Clark and Gardner, 2018) is mostly based on the attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) that take as input hquestion, paragraphi, compute an interactive representation of them and predict the start and end positions of the answer. When dealing with no-answer cases, popular method is to jointly model the answer position probability and no-answer probability by a shared softmax normalizer (Kundu and Ng, 2018; Clark and Gardner, 2018; Devlin et al., 2019), or independently model the answerability as a binary classification problem (Hu et al., 2019; Yang et al., 2019; L"
2020.findings-emnlp.370,P18-1158,0,0.0201174,"paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover"
2020.findings-emnlp.6,P19-1051,0,0.171914,"Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each sentence with two different Aspect te"
2020.findings-emnlp.6,P17-1036,0,0.0454591,"Missing"
2020.findings-emnlp.6,P19-1048,0,0.0149936,"is carried out 10 epochs on 8 NVIDIA Tesla V100 GPU. We use fp16 to speed up training and to reduce memory usage. The pretraining process takes more than 5 days. 3.3 extraction. We use the BERT+GRU for DL and BERT+SAN for DR as our baselines due to their best-reported performance. Besides, we produce the results on DT with BERT+SAN keeping the settings the same as on DR 8 . We compare our model with the above baselines on DL , DR , and DT , and compare it with the following baselines on DL , DR-14 , DR-15 , and DR-16 because of the common datasets reported by the official implementation. IMN (He et al., 2019) uses an interactive architecture with multi-task learning for end-to-end ABSA tasks. It contains aspect term and opinion term extraction besides aspect-level sentiment classification. DREGCN (Liang et al., 2020a) designs a dependency syntactic knowledge augmented interactive architecture with multi-task learning for end-toend ABSA. DREGCN is short for the official DREGCN+CNN+BERT due to its better performance. WHW (Peng et al., 2020) develops a two-stage framework to address aspect term extraction, aspect sentiment classification, and opinion extraction. TAS-BERT (Wan et al., 2020) proposes a"
2020.findings-emnlp.6,P11-1013,0,0.0979624,"Missing"
2020.findings-emnlp.6,P19-1356,0,0.0142943,"the representation of each layer of BERT. It varies from the 1st layer to the L-th layer. L is the max layer of BERT, e.g., 12 in BERT-Base. He ∈ R(ˆn+2)×h is the representation HL belonging to the last layer, in which two extra embeddings belong to special tokens [CLS] and [SEP], and the labels of them are set to ‘O’ in the experiments. h is the hidden size, n ˆ is the length of S after tokenizing by the wordpiece vocabulary. Different layers of BERT capture different levels of information, e.g., phrase-level information in the lower layers and linguistic information in intermediate layers (Jawahar et al., 2019). The higher layers are usually task-related. Thus, a shared BERT between ATE and ASC tasks is the right choice. We extract the representation Hc for ASC task from the l-th layer of BERT: Hc = Hl . Gc = Transformer-Decoder(Hc , Q), (4) where Q represents the aspect term labels generated by the ATE module (ground-truth labels in the training phase). The vocab size is |T e |in the word embedding of the Transformer-Decoder. Note that the Transformer-Decoder here is not the same as the original transformer decoder. The difference is that we use Multi-Head Attention instead of Masked Multi-Head Att"
2020.findings-emnlp.6,D16-1060,0,0.0605274,"Missing"
2020.findings-emnlp.6,P18-1234,0,0.0378307,"Missing"
2020.findings-emnlp.6,P14-1033,0,0.0691541,"Missing"
2020.findings-emnlp.6,P19-1052,0,0.0323022,"Missing"
2020.findings-emnlp.6,2020.acl-main.340,0,0.0361081,"Missing"
2020.findings-emnlp.6,D19-5505,0,0.0691065,"ost-pretraining BERT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each"
2020.findings-emnlp.6,S15-2082,0,0.0723186,"Missing"
2020.findings-emnlp.6,S16-1002,0,0.0243253,"nsion. wh is a trainable weight matrix. f (·) is the ReLU function. We use hi to calculate loss as Eq. (5) and Eq. (9). It is a consistent strategy to generate sentiment labels, although it cannot improve the performance in our preliminary experiments. 3 #NEG 963 1,751 977 451 581 271 come from the SemEval challenges, and the last comes from an English Twitter dataset, as shown in Table 1. DL contains laptop reviews from SemEval 2014 (Pontiki et al., 2014), and DR are restaurant reviews merged from SemEval 2014 (DR-14 ), SemEval 2015 (DR-15 ) (Pontiki et al., 2015), and SemEval 2016 (DR-16 ) (Pontiki et al., 2016). We keep the official data division of these datasets for the training set, validation set, and testing set. The reported results of DL and DR are average scores of 5 runs. DT consists of English tweets. Due to a lack of standard train-test split, we report the ten-fold cross-validation results of DT as done in (Li et al., 2019b; Luo et al., 2019b). The evaluation metrics are precision (P), recall (R), and F1 score based on the exact match of aspect term and its polarity. where Le and Lc are calculated by Eq. (9), denote the loss for aspect term and polarity, respectively. LVAT denotes the VA"
2020.findings-emnlp.6,S14-2004,0,0.109383,"Missing"
2020.findings-emnlp.6,J11-1002,0,0.167574,"Missing"
2020.findings-emnlp.6,C16-1311,0,0.0804478,"Missing"
2020.findings-emnlp.6,P19-1056,1,0.29574,"RT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each sentence with two d"
2020.findings-emnlp.6,D18-1504,0,0.0328348,"Missing"
2020.findings-emnlp.6,2020.acl-main.295,0,0.0233536,"Missing"
2020.findings-emnlp.6,P19-1344,0,0.0340413,"Missing"
2020.findings-emnlp.6,P18-1088,0,0.0505874,"Missing"
2020.findings-emnlp.6,D16-1059,0,0.0440344,"Missing"
2020.findings-emnlp.6,D13-1171,0,0.0663062,"Missing"
2020.findings-emnlp.6,D16-1058,0,0.0666723,"Missing"
2020.findings-emnlp.6,P18-2094,0,0.0293293,"Missing"
2020.findings-emnlp.6,N19-1242,0,0.0460036,"Missing"
2020.findings-emnlp.6,D15-1073,0,0.119076,"r Computational Linguistics tag sets: aspect term tags and polarity tags. The subsequent one uses collapsed labels as the tags set, e.g., “B-POS” and “I-POS”, in which each tag indicates the aspect term boundary and its polarity. Except for the joint and collapsed approaches, a pipelined approach first labels the given sentence using aspect term tags, e.g., “B” and “I” (the beginning and inside of an aspect term), and then feeds the aspect terms into a classifier to obtain their corresponding polarities. Several related works have been published in these approaches. Mitchell et al. (2013) and Zhang et al. (2015) found that the joint and collapsed approaches are superior to the pipelined approach on named entities and their sentiments co-extraction. Li et al. (2019b) proposed a unified model with the collapsed approach to do aspect term-polarity co-extraction. Hu et al. (2019) solved this task with a pipelined approach. Luo et al. (2019b) adopted the joint approach to do such a co-extraction. We follow the joint approach in this paper, and believe that it has a more apparent of responsibilities than the collapsed approach through learning parallel sequence labels. However, previous works on the joint"
2021.acl-demo.28,2021.naacl-main.211,0,0.177047,"human evaluation as in (Zhao et al., 2020). We randomly collect 500 single-turn and 500 multi-turn context-response pairs from the online logs of the real-word dialog system Xiaoice. Then, we recruit 3 native speakers as human annotators. The annotators have to judge which response is better, based on informativeness, consistency, and fluency of the responses. If an annotator cannot tell which response is better, he/she is required to label a “Tie”. With the Models Seq2Seq (Vinyals and Le, 2015) Transformer (Vaswani et al., 2017) RoBERTa (Liu et al., 2019) CodeBERT (Feng et al., 2020) PLBART (Ahmad et al., 2021) Prophetnet-Code Ruby 9.64 11.18 11.17 12.16 14.11 14.37 Javascript 10.21 11.59 11.90 14.90 15.56 16.60 Go 13.98 16.38 17.72 18.07 18.91 18.43 Python 15.93 15.81 18.14 19.06 19.30 17.87 Java 15.09 16.26 16.47 17.65 18.45 19.39 PHP 21.08 22.12 24.02 25.16 23.58 24.57 overall 14.32 15.56 16.57 17.83 18.32 18.54 Table 9: Results of ProphetNet-Code on CodeXGLUE for code-to-text summarization task. Numbers in this table are smoothed BLEU-4 scores. Method LSTM (Bahdanau et al., 2014) Transformer (Vaswani et al., 2017) MASS (Song et al., 2019) BART (Lewis et al., 2019) ProphetNet-En R-1 37.3 39.5 42."
2021.acl-demo.28,2020.acl-main.9,0,0.146754,"C (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovanov et al., 2019) PLATO w/o latent (Bao et al., 2020) PLATO (Bao et al., 2020) ProphetNet-Dialog-En B-1 0.336 0.309 0.405 0.397 0.461 DailyDialog B-2 D-1 D-2 0.238 0.03 0.128 0.249 0.029 0.25 0.322 0.046 0.246 0.311 0.053 0.291 0."
2021.acl-demo.28,P19-1608,0,0.0400526,"Missing"
2021.acl-demo.28,D15-1229,0,0.0821035,"ream task finetuning scripts, including ProphetNet-En pre-trained with 160GB English raw text, ProphetNet-Zh pre-trained with 160GB Chinese raw text, ProphetNet-Multi with 101GB Wiki-100 corpus and 1.5TB Common Crawl2 data, ProphetNet-Dialog-En with 60 million sessions Reddit open-domain dialog corpus, ProphetNetDialog-Zh with collected Chinese dialog corpus over 30 million sessions, and ProphetNet-Code pre-trained with 10 million codes and documents. ProphetNet-X achieves new state-of-the-art results on 10 benchmarks, including Chinese summarization (MATINF-SUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015)), Chinese question answering (MATINF-QA (Xu et al., 2020a)), cross-lingual generation (XGLUE NTG (Liang et al., 2020) and † 1 Corresponding Author. https://github.com/microsoft/ProphetNet 2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share th"
2021.acl-demo.28,P19-4007,0,0.0224534,"Missing"
2021.acl-demo.28,D19-1407,0,0.0536767,"Missing"
2021.acl-demo.28,2020.acl-main.703,0,0.0861707,"Missing"
2021.acl-demo.28,I17-1099,0,0.321053,"2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share the same model structure and cover various languages and domains. XGLUE QG (Liang et al., 2020)), English summarization (MSNews (Liu et al., 2020a)), English dialog generation (DailyDialog (Li et al., 2017), PersonaChat (Zhang et al., 2018), and DSTC7AVSD (Alamri et al., 2019)), and code summarization (CodeXGLUE (Lu et al., 2021)). Users can simply download the ProphetNet-X repository and find corresponding pre-trained model with downstream task finetuning scripts. The main contributions of ProphetNet-X can be described as follows: • We provide a family of pre-trained models named ProphetNet-X, with six models including English and Chinese natural language generation in open-domain and dialog, multilingual generation, and code generation. • All the pre-trained ProphetNet-X models share the same"
2021.acl-demo.28,P18-2027,0,0.0228438,"Missing"
2021.acl-demo.28,2021.findings-acl.36,1,0.805985,"Missing"
2021.acl-demo.28,D19-1387,0,0.0557333,"Missing"
2021.acl-demo.28,2020.tacl-1.47,0,0.406675,"updating more pre-training models and finetuning scripts. 1 Introduction In recent years, quite a few natural language generation pre-training models are proposed (Qi et al., 2020; Lewis et al., 2019; Song et al., 2019; Brown et al., 2020). Downstream generation tasks benefit from these large scale pre-training models greatly in fluency and accuracy. Researchers also extend these general pre-training works into specific domains such as DialoGPT (Zhang et al., 2019) is ∗ Work is done during internship at Microsoft Research Asia. extended from GPT (Brown et al., 2020) for dialog system, mBART (Liu et al., 2020b) is extended from BART (Lewis et al., 2019) for multi-lingual generation, CodeBERT (Feng et al., 2020) is extended from BERT (Devlin et al., 2018) for programming language modeling, etc. Although there are pre-trained models for some specific domains, it is not convenient for users to find them and set them up. Besides, even some models in the same pre-training family with the same model structure and pre-training tasks, their codes and details vary a lot because of different implementation and backends selection. ProphetNet (Qi et al., 2020) is firstly proposed as an English text pre-traini"
2021.acl-demo.28,2021.ccl-1.108,0,0.116891,"Missing"
2021.acl-demo.28,D15-1166,0,0.0212423,"Missing"
2021.acl-demo.28,N18-1018,0,0.0341321,"Missing"
2021.acl-demo.28,W04-3252,0,0.0754182,"s 2. Both the max sequence lengths of the input and output are set to 512. For ProphetNet-En, ProphetNet-Zh, ProphetNetMulti, ProphetNet-Dialog-En, and ProphetNetCode, we carry out un-supervised pre-training with masked span prediction task. Spans of continuous tokens are masked out from the encoder input sentences and predicted from the decoder side. We masked continuous 9 tokens in every 64 tokens from the encoder side, and predict the 9 tokens on the decoder side. In other words, for maximum 512 encoder sequence length, totally 8(spans) × 9(tokens per span) = 72 tokens 234 Method TextRank (Mihalcea and Tarau, 2004) LexRank (Erkan and Radev, 2004) Seq2Seq (Sutskever et al., 2014) Seq2Seq+Att (Luong et al., 2015) WEAN (Ma et al., 2018) Global Encoding (Lin et al., 2018) BertAbs (Liu and Lapata, 2019) MTF-S2Ssingle (Xu et al., 2020a) MTF-S2Smulti (Xu et al., 2020a) ProphetNet-Zh MATINF-QA R-1 R-2 R-L 16.62 4.53 10.37 19.62 5.87 13.34 20.28 5.94 13.52 21.66 6.58 14.26 24.18 6.38 15.47 MATINF-SUMM R-1 R-2 R-L 35.53 25.78 36.84 33.08 23.31 34.96 23.05 11.44 19.55 43.05 28.03 38.58 34.63 22.56 28.92 49.28 34.14 47.64 57.31 44.05 55.93 43.02 28.05 38.55 48.59 35.69 43.28 58.82 44.96 54.26 R-1 24.38 22.15 33.80"
2021.acl-demo.28,2020.findings-emnlp.217,1,0.882562,"PLG (Programming Language Generation) model ProphetNet-Code to show the generation performance besides NLG (Natural Language Generation) tasks. In our experiments, ProphetNet-X models achieve new state-of-the-art performance on 10 benchmarks. All the models of ProphetNet-X share the same model structure, which allows users to easily switch between different models. We make the code and models publicly available1 , and we will keep updating more pre-training models and finetuning scripts. 1 Introduction In recent years, quite a few natural language generation pre-training models are proposed (Qi et al., 2020; Lewis et al., 2019; Song et al., 2019; Brown et al., 2020). Downstream generation tasks benefit from these large scale pre-training models greatly in fluency and accuracy. Researchers also extend these general pre-training works into specific domains such as DialoGPT (Zhang et al., 2019) is ∗ Work is done during internship at Microsoft Research Asia. extended from GPT (Brown et al., 2020) for dialog system, mBART (Liu et al., 2020b) is extended from BART (Lewis et al., 2019) for multi-lingual generation, CodeBERT (Feng et al., 2020) is extended from BERT (Devlin et al., 2018) for programming"
2021.acl-demo.28,D16-1264,0,0.0250842,"conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovanov et al., 2019) PLATO w/o latent (Bao et al., 2020) PLATO (Bao et al., 2020) ProphetNet-Dialog"
2021.acl-demo.28,D15-1044,0,0.0440847,"2017) for chit-chat generation, Persona-Chat (Zhang et al., 2018) for knowledge grounded conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovano"
2021.acl-demo.28,P15-1152,0,0.0173033,"t-Zh, we evaluate our pre-trained model with MATINF-QA (Xu et al., 2020a) for generative question answering task, MATINFSUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015) for summarization task. For ProphetNet-Multi, we follow UnicoderF N P to evaluate on XGLUE (Liang et al., 2020) for 235 For ProphetNet-Dialog-En, we carry out finetuning on DailyDialog (Li et al., 2017) for chit-chat generation, Persona-Chat (Zhang et al., 2018) for knowledge grounded conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) Prop"
2021.acl-demo.28,2020.acl-main.330,0,0.126352,"re-trained models with downstream task finetuning scripts, including ProphetNet-En pre-trained with 160GB English raw text, ProphetNet-Zh pre-trained with 160GB Chinese raw text, ProphetNet-Multi with 101GB Wiki-100 corpus and 1.5TB Common Crawl2 data, ProphetNet-Dialog-En with 60 million sessions Reddit open-domain dialog corpus, ProphetNetDialog-Zh with collected Chinese dialog corpus over 30 million sessions, and ProphetNet-Code pre-trained with 10 million codes and documents. ProphetNet-X achieves new state-of-the-art results on 10 benchmarks, including Chinese summarization (MATINF-SUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015)), Chinese question answering (MATINF-QA (Xu et al., 2020a)), cross-lingual generation (XGLUE NTG (Liang et al., 2020) and † 1 Corresponding Author. https://github.com/microsoft/ProphetNet 2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework"
2021.acl-demo.28,P18-1205,0,0.128826,"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share the same model structure and cover various languages and domains. XGLUE QG (Liang et al., 2020)), English summarization (MSNews (Liu et al., 2020a)), English dialog generation (DailyDialog (Li et al., 2017), PersonaChat (Zhang et al., 2018), and DSTC7AVSD (Alamri et al., 2019)), and code summarization (CodeXGLUE (Lu et al., 2021)). Users can simply download the ProphetNet-X repository and find corresponding pre-trained model with downstream task finetuning scripts. The main contributions of ProphetNet-X can be described as follows: • We provide a family of pre-trained models named ProphetNet-X, with six models including English and Chinese natural language generation in open-domain and dialog, multilingual generation, and code generation. • All the pre-trained ProphetNet-X models share the same model structure. Users only need t"
2021.acl-demo.28,2020.emnlp-main.279,1,0.890954,"Missing"
2021.acl-long.285,N19-1423,0,0.604718,"ncelevel representation. Then, all utterances in a conversation are concatenated and a [SEP] token is inserted at the end of the whole sequence. It is notable that these three tasks share the same form of input data. Thus, the input only needs to be encoded once by BERT while the output can be fed into three tasks, which is computation-efficient. As shown in Figure 1, a task-dependent non-linear transformation layer is placed on top of BERT in order to adapt the output of BERT to different tasks. We will describe the details of these tasks as follows. 3.2.1 Model Overview In this paper, BERT (Devlin et al., 2019) is chosen as the backbone of our PLM for MPC. Thus, we name it MPC-BERT. It is worth noting that our proposed self-supervised tasks for training MPCBERT can also be applied to other types of PLMs. We first give an overview of the input representations and the overall architectures of MPC-BERT. When constructing the input representations, in order to consider the speaker information of each utterance, speaker embeddings (Gu et al., 2020) are introduced as shown in Figure 1. Considering that the set of interlocutors are inconsistent in different conversations, a position-based interlocutor embe"
2021.acl-long.285,D19-1199,0,0.159628,"conversations and multi-party conversations (MPC). In this paper, we study MPC. In addition to predicting utterances, identifying the speaker and recognizing the addressee of an utterance are also important tasks for MPC. Ouchi and Tsuboi (2016) first proposed the task of addressee and response selection and created an MPC corpus for studying this task. Zhang et al. (2018a) proposed SI-RNN, which updated speaker embeddings role-sensitively for addressee and response selection. Meng et al. (2018) proposed a task of speaker classification as a surrogate task for speaker modeling. Le et al. 3683 (2019) proposed a who-to-whom (W2W) model to recognize the addressees of all utterances. Hu et al. (2019) proposed a graph-structured network (GSN) to model the graphical information flow for response generation. Wang et al. (2020) proposed to track the dynamic topic for response selection. Generally speaking, previous studies on MPC cannot unify the representations of interlocutors and utterances effectively. Also, they are limited to each individual task, ignoring the complementary information among different tasks. To the best of our knowledge, this paper makes the first attempt to design various"
2021.acl-long.285,W15-4640,0,0.0303581,"ate response either in a generation-based (Shang et al., ∗ † Work done during the internship at Microsoft. Corresponding author. Utterance Addressee How can I setup if I want add new server at xchat? From places, network servers, work group, his computer, and then I I.1 clicked on the shared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng"
2021.acl-long.285,D19-1193,1,0.82954,"g the internship at Microsoft. Corresponding author. Utterance Addressee How can I setup if I want add new server at xchat? From places, network servers, work group, his computer, and then I I.1 clicked on the shared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance (Ouchi and Tsuboi,"
2021.acl-long.285,L18-1496,0,0.0702752,"2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019) are unique and important issues in MPC. An instance of MPC always contains complicated interactions between interlocutors, between utterances and between an interlocutor and an utterance. Therefore, it is challenging to model the conversation flow and fully understand the dialogue content. Existing studies on MPC learn the representations of interlocutors and utterances with neural networks, and their representation 3682 Proceedings of the 59th Annual Meeting of the Association for Computat"
2021.acl-long.285,2020.acl-main.740,0,0.0138965,"tation for calculating the matching score mij , i.e., the probability of ci and cj sharing the same parent node. Finally, the pretraining objective of this task is to minimize the cross-entropy loss as Lsnd = −[yij log(mij ) + (1 − yij )log(1 − mij )], (9) where yij = 1 if ci and cj share the same parent node and yij = 0 otherwise. 3.4 Multi-task Learning In addition, we also adopt the tasks of masked language model (MLM) and next sentence prediction (NSP) in original BERT pre-training (Devlin et al., 2019), which have been proven effective for incorporating domain knowledge (Gu et al., 2020; Gururangan et al., 2020). Finally, MPCBERT is trained by performing multi-task learning that minimizes the sum of all loss functions as L = Lrur + Liss + Lpcd + Lmsur + Lsnd + Lmlm + Lnsp . 4 4.1 (10) Downstream Tasks Addressee Recognition Given a multi-party conversation where part of the addressees are unknown, Ouchi and Tsuboi (2016) and Zhang et al. (2018a) recognized an addressee of the last utterance. Le et al. (2019) recognized addressees of all utterances in a conversation. In this paper, we follow the more challenging setting in Le et al. (2019). Formally, models are asked to predict {ˆ an }N n=1 N given {(s"
2021.acl-long.285,D16-1231,0,0.0743782,"hared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019) are unique and important issues in MPC. An instance of MPC always contains complicated interactions between interlocutors, between utterances and between an interlocutor and"
2021.acl-long.285,P15-1152,0,0.0739948,"Missing"
2021.acl-long.285,2020.acl-demos.30,0,0.216497,"Missing"
2021.acl-long.285,D19-1011,0,0.0251879,"conversations and multi-party conversations (MPC). In this paper, we study MPC. In addition to predicting utterances, identifying the speaker and recognizing the addressee of an utterance are also important tasks for MPC. Ouchi and Tsuboi (2016) first proposed the task of addressee and response selection and created an MPC corpus for studying this task. Zhang et al. (2018a) proposed SI-RNN, which updated speaker embeddings role-sensitively for addressee and response selection. Meng et al. (2018) proposed a task of speaker classification as a surrogate task for speaker modeling. Le et al. 3683 (2019) proposed a who-to-whom (W2W) model to recognize the addressees of all utterances. Hu et al. (2019) proposed a graph-structured network (GSN) to model the graphical information flow for response generation. Wang et al. (2020) proposed to track the dynamic topic for response selection. Generally speaking, previous studies on MPC cannot unify the representations of interlocutors and utterances effectively. Also, they are limited to each individual task, ignoring the complementary information among different tasks. To the best of our knowledge, this paper makes the first attempt to design various"
2021.acl-long.285,P18-1103,0,0.0171851,"based (Shang et al., ∗ † Work done during the internship at Microsoft. Corresponding author. Utterance Addressee How can I setup if I want add new server at xchat? From places, network servers, work group, his computer, and then I I.1 clicked on the shared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the address"
2021.acl-long.285,P19-1001,1,0.836308,", ∗ † Work done during the internship at Microsoft. Corresponding author. Utterance Addressee How can I setup if I want add new server at xchat? From places, network servers, work group, his computer, and then I I.1 clicked on the shared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance"
2021.acl-long.285,2020.emnlp-main.533,0,0.0615756,"Missing"
2021.acl-long.285,P17-1046,0,0.0182146,"in a generation-based (Shang et al., ∗ † Work done during the internship at Microsoft. Corresponding author. Utterance Addressee How can I setup if I want add new server at xchat? From places, network servers, work group, his computer, and then I I.1 clicked on the shared folder. It did not allow you to see the files? I.2 It prompts for authentication and I don’t know what to put. I tried guest I.3 with no password. Put proper authentication in, then? I.2 I think you had kde on suse? I.2 2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and"
2021.acl-long.396,N18-1144,0,0.0163552,"ict its location at steps when its state is changed (i.e., the predicted state is create or move4 ). And the locations of an entity with unchanged states can be inferred according to its locations in previous steps. Such pipeline fashion 4 The location of an entity will be None if its state is destroy. Therefore, we do not need to predict its location when an entity is destroyed. 5104 can increase consistency between states and locations of an entity than inferring location and state simultaneously. 4 Experiments This section describes the evaluation results of R EAL on two datasets (ProPara (Dalvi et al., 2018) and Recipes (Bosselut et al., 2018)). We also provide ablation study and case analysis to illustrate the effectiveness of graph-based reasoning. 4.1 Datasets and Evaluation Metrics Statistics ProPara Recipes #sentences #para #train/#dev/#test avg. #entities per para avg. #sentences per para 3.3K 488 391/43/54 4.17 6.7 7.6K 866 693/86/87 8.57 8.8 Table 1: Statistics of ProPara and Recipes dataset. ProPara contains procedural texts about scientific processes, e.g., photosynthesis, fossil formulation. It contains about 1.9k instances (one entityparagraph pair as an instance) written and annotate"
2021.acl-long.396,D19-1457,0,0.0130885,"to predict consistent state sequence. For example, NCET (Gupta and Durrett, 2019b) tracks the entity in a continuous space and leverages a conditional random field (CRF) to keep a consistent prediction sequence. Other models inject knowledge from external data sources to complement missing knowledge. ProStruct (Tandon et al., 2018) introduces commonsense constraints to refine the probability space, while KOALA (Zhang et al., 2020) leverages Bert Encoder pre-trained on related corpus from Wiki, and injects the ConceptNet (Speer et al., 2017) knowledge. Besides, a few models (Das et al., 2019; Dalvi et al., 2019) are proposed to build graphs on the procedural text. For instance, KG-MRC (Das et al., 2019) constructs dynamic knowledge graphs between entities and locations. However, these methods can not systematically capture the relations among entities, actions, and locations, and entity-action and entity-entity relations are ignored. Graph Reasoning in Language Understanding. Graph-based reasoning methods (Zeng et al., 2020; Zhong et al., 2020; Zheng and Kordjamshidi, 2020) are widely used in natural language understanding tasks to enhance performance. For example, Zeng et al. (2020) constructs a dou"
2021.acl-long.396,N19-1423,0,0.15941,"ence “The trashbags are thrown into a large outdoor trashcan”, the verb thrown has two arguments, the trashbags and into a large outdoor trashcan, then we connect the two mention nodes trashbags and trashcans. We also create edges between mentions of the same entity in different semantic structures. For example, in Figure 2, the entity bones are mentioned in two sentences, which correspond to two entity mention nodes. We connect these two nodes to propagate information from one to the other during graph-based reasoning. 3.2 Nodes Representation. We first feed the entire paragraph to the BERT (Devlin et al., 2019) 2 1 Graph-based Representation Learning 3 We will use step and sentence interchangeably. 5102 https://github.com/flairNLP/flair https://github.com/allenai/allennlp. decompose leaving decay soft tissue bone bone S2: Soft tissues quickly decompose leaving the hard bones or shells behind. S1 Action node Entity/Loc mention node seep replace … organic material mineral S3: As the encased bones decay, minerals seep in replacing the organic material. Intra-structure edge Inter-structure edge Verb-centric structure Sn Sentence structure Figure 2: An example of entity-action-location graph, constructed"
2021.acl-long.396,N19-1244,0,0.0363907,"Missing"
2021.acl-long.396,P15-1030,0,0.011753,"es the representation of entity e in sentence St , het denotes the representation of the entity mention node ne in sentence St , hvt denotes the representation of the action node na connected with ne in sentence St . If entity e is not mentioned in sentence St , we use zero vector as representation of St for e. Note if there are multiple mention nodes for the entity e in sentence St , we take the mean pooling over all mention nodes as het . And we take similar approach for multiple actions. We utilize a BiLSTM layer on the sequence of sentence embeddings. And a conditional random field (CRF) (Durrett and Klein, 2015) is applied on the top of the BiLSTM to make the final prediction. The loss function for the state tracking module is defined as X Lstate = − (e,P )∈D T  1X logP yets |P, e; θG , θst , T t=1 (6) where D is the training collection containing entity paragraph pairs, P yets |P, e; θG , θst represents the predicted probability of gold state yets in sentence St given the entity e and paragraph P , θG are parameters for graph reasoning and the text encoder, and θst are parameters in state tracking module. Softmax Softmax … Softmax xlt = [het khlt ], (7) where het and hlt denotes the representation"
2021.acl-long.396,D19-1070,0,0.212951,"re associated with trashcans. Then, in the following sentence “The trashcan is emptied by a large trash truck.”, although trashbags are not explicitly mentioned, their loca5100 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5100–5109 August 1–6, 2021. ©2021 Association for Computational Linguistics tions are changed by the association with trashcan. Recent works on procedural text understanding have achieved remarkable progress (Tandon et al., 2018; Bosselut et al., 2018; Gupta and Durrett, 2019b; Du et al., 2019; Das et al., 2019; Gupta and Durrett, 2019a). However, the existing methods do not systematically model the relations among entities, actions, and locations. Instead, most methods either leverage inherent constraints on entity states or exploit external knowledge to make predictions. For example, Gupta and Durrett (2019b) propose a structural neural network to track each entity’s hidden state and summarize the global state transitions with a CRF model. Tandon et al. (2018) inject commonsense knowledge into a neural model with soft and hard constraints. Although Das et al. (2"
2021.acl-long.396,W19-1502,0,0.175251,"re associated with trashcans. Then, in the following sentence “The trashcan is emptied by a large trash truck.”, although trashbags are not explicitly mentioned, their loca5100 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5100–5109 August 1–6, 2021. ©2021 Association for Computational Linguistics tions are changed by the association with trashcan. Recent works on procedural text understanding have achieved remarkable progress (Tandon et al., 2018; Bosselut et al., 2018; Gupta and Durrett, 2019b; Du et al., 2019; Das et al., 2019; Gupta and Durrett, 2019a). However, the existing methods do not systematically model the relations among entities, actions, and locations. Instead, most methods either leverage inherent constraints on entity states or exploit external knowledge to make predictions. For example, Gupta and Durrett (2019b) propose a structural neural network to track each entity’s hidden state and summarize the global state transitions with a CRF model. Tandon et al. (2018) inject commonsense knowledge into a neural model with soft and hard constraints. Although Das et al. (2"
2021.acl-long.396,D18-1006,0,0.0604834,"ags are thrown into trashcans.”, trashbags are associated with trashcans. Then, in the following sentence “The trashcan is emptied by a large trash truck.”, although trashbags are not explicitly mentioned, their loca5100 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5100–5109 August 1–6, 2021. ©2021 Association for Computational Linguistics tions are changed by the association with trashcan. Recent works on procedural text understanding have achieved remarkable progress (Tandon et al., 2018; Bosselut et al., 2018; Gupta and Durrett, 2019b; Du et al., 2019; Das et al., 2019; Gupta and Durrett, 2019a). However, the existing methods do not systematically model the relations among entities, actions, and locations. Instead, most methods either leverage inherent constraints on entity states or exploit external knowledge to make predictions. For example, Gupta and Durrett (2019b) propose a structural neural network to track each entity’s hidden state and summarize the global state transitions with a CRF model. Tandon et al. (2018) inject commonsense knowledge into a neural model with s"
2021.acl-long.396,2020.emnlp-main.127,0,0.0357556,"et al., 2020) leverages Bert Encoder pre-trained on related corpus from Wiki, and injects the ConceptNet (Speer et al., 2017) knowledge. Besides, a few models (Das et al., 2019; Dalvi et al., 2019) are proposed to build graphs on the procedural text. For instance, KG-MRC (Das et al., 2019) constructs dynamic knowledge graphs between entities and locations. However, these methods can not systematically capture the relations among entities, actions, and locations, and entity-action and entity-entity relations are ignored. Graph Reasoning in Language Understanding. Graph-based reasoning methods (Zeng et al., 2020; Zhong et al., 2020; Zheng and Kordjamshidi, 2020) are widely used in natural language understanding tasks to enhance performance. For example, Zeng et al. (2020) constructs a double graph design for the document-level Relation Extraction (RE) task, Zhong et al. (2020) constructs the retrieved evidence sentences as a graph for Fact-Checking task. Compared with these works, the entity-actionlocation graph in our approach copes better with procedural text understanding task since it precisely defines concepts we are concerned within the task and captures the rich and expressive relations among"
2021.acl-long.396,2020.emnlp-main.714,0,0.0410475,"e-trained on related corpus from Wiki, and injects the ConceptNet (Speer et al., 2017) knowledge. Besides, a few models (Das et al., 2019; Dalvi et al., 2019) are proposed to build graphs on the procedural text. For instance, KG-MRC (Das et al., 2019) constructs dynamic knowledge graphs between entities and locations. However, these methods can not systematically capture the relations among entities, actions, and locations, and entity-action and entity-entity relations are ignored. Graph Reasoning in Language Understanding. Graph-based reasoning methods (Zeng et al., 2020; Zhong et al., 2020; Zheng and Kordjamshidi, 2020) are widely used in natural language understanding tasks to enhance performance. For example, Zeng et al. (2020) constructs a double graph design for the document-level Relation Extraction (RE) task, Zhong et al. (2020) constructs the retrieved evidence sentences as a graph for Fact-Checking task. Compared with these works, the entity-actionlocation graph in our approach copes better with procedural text understanding task since it precisely defines concepts we are concerned within the task and captures the rich and expressive relations among them. 3 Model Task Definition. The procedural text"
2021.acl-long.396,2020.acl-main.549,0,0.0414749,"ages Bert Encoder pre-trained on related corpus from Wiki, and injects the ConceptNet (Speer et al., 2017) knowledge. Besides, a few models (Das et al., 2019; Dalvi et al., 2019) are proposed to build graphs on the procedural text. For instance, KG-MRC (Das et al., 2019) constructs dynamic knowledge graphs between entities and locations. However, these methods can not systematically capture the relations among entities, actions, and locations, and entity-action and entity-entity relations are ignored. Graph Reasoning in Language Understanding. Graph-based reasoning methods (Zeng et al., 2020; Zhong et al., 2020; Zheng and Kordjamshidi, 2020) are widely used in natural language understanding tasks to enhance performance. For example, Zeng et al. (2020) constructs a double graph design for the document-level Relation Extraction (RE) task, Zhong et al. (2020) constructs the retrieved evidence sentences as a graph for Fact-Checking task. Compared with these works, the entity-actionlocation graph in our approach copes better with procedural text understanding task since it precisely defines concepts we are concerned within the task and captures the rich and expressive relations among them. 3 Model Task D"
2021.acl-long.435,2020.acl-main.9,0,0.0256561,"on of four kinds of embeddings, including token-level, turn-level, position-level, and segment-level. Then, we concatenate all the input representations to one sequence for model training. Token-Level The token-level embeddings are the concatenation of (Ow , Qw , Cw , Rw ), which denote the token embedding sequence of visual objects, visual concepts, contexts and response respectively. Note that Ow is the object embedding transformed by a linear layer into the same dimension as word embedding. Turn-Level Since the dialog is multi-turn, we encode this turn order with a relative turn embedding (Bao et al., 2020). Specifically, the turn number is counted from the last utterance of the dialogue to the beginning. Note that as for the tokens corresponding to O and Q, we simply set them the same as the first utterance of C. Position-Level Positional embedding encodes the signal of the token order in the total input sequence, which is the same as positional encoding of the original transformer (Vaswani et al., 2017). Segment-Level Segment embedding is employed to differentiate which segment the token is in, i.e., O, Q, C or R. 4.3.3 Masked Concept Prediction Due to the inherent gap between visual modality"
2021.acl-long.435,2020.emnlp-main.703,0,0.0343251,"Missing"
2021.acl-long.435,N19-1423,0,0.00612897,"set and utilize it to construct the (C, R, V ) triple data. Modeling To improve the efficiency of crossmodal retrieval model on large-scale dialog corpus and image dataset, we adopt a two-tower architecture (Lu et al., 2019) to accelerate the retrieval process where the image features can be pre-extracted offline. The model takes a sentence T and an image V as input, and predicts the relevance score s(T, V ) between the sentence and the image. We use a text encoder and an image encoder to produce the representations of T and V , respectively. The text encoder is a pre-trained BERT-base model (Devlin et al., 2019) and we use the hidden state of special token [CLS] as the embedding of T : et = BERT (T ) (1) Then a Multi-Layer Perceptron (MLP) projects the sentence embedding into the cross-modal space. We follow Tan and Bansal (2020) to perform L2normalization on the last output features, by which we can simplify the nearest neighbor search problem in the euclidean space to the Maximum Inner Product problem (Mussmann and Ermon, 2016): ft (T ) = Ht (et ) kHt (et )k (2) Similarly, the image encoder is composed of a pretrained ResNeXt backbone (Xie et al., 2017) and a MLP with L2 normalization: fv (V ) = Hv"
2021.acl-long.435,W19-4103,0,0.401061,"ted 4-tuples are used to train the visualknowledge-grounded response generator, which is built on the top of a multi-layer Transformer architecture (Vaswani et al., 2017). To effectively inject the visual knowledge into the response generator, we carry out the Masked Concept Prediction and Visual Knowledge Bias besides the response generation objective. The former aims to align the semantic representations between textual words and image regions, while the latter tries to provide more visual knowledge to facilitate the dialog generation. The experimental results on Reddit Conversation Corpus (Dziri et al., 2019a) demonstrate that Maria significantly outperforms previous state-of-the-art methods, and can generate informative responses with visual commonsense of our physical world. Overall, the contributions of this paper are summarized as follows: 1 https://developers.google.com/ knowledge-graph/ 2 We calculate the co-occurrence distribution of object tags from the images in MS-COCO dataset. More examples could be found in Appendices. 5597 • We explore the task of image-grounded dialog generation under a fully open-ended setting where no specific image-dialog pairs are assumed available, i.e., zero-r"
2021.acl-long.435,N16-1014,0,0.0671302,"Missing"
2021.acl-long.435,W04-1013,0,0.0646094,"sent 5601 113.2K/5K/5K samples as training/validation/test set, respectively. After the retrieval model is trained, we fetch 500K images from the Open Images dataset as the image index, and then retrieve images from it by dialog context and response to construct the training data for response generator. 5.2 Evaluation Metrics Both automatic metrics and human evaluation are employed to assess the performance of Maria and baselines. Automatic metrics include: (1) Fluency: perplexity (PPL) measures the confidence of the generated responses; (2) Relevance: BLEU-1 (Papineni et al., 2002), Rouge-L (Lin, 2004), and we follow Serban et al. (2017) to utilize Embedding Average cosine similarity, Vector Extrema cosine similarity, and Embedding Greedy Matching score. All this metrics are calculated by running the public NLG evaluation script4 ; (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) are defined as the number of distinct uni-grams or bi-grams divided by the total amount of words. In human evaluation, we randomly select 100 dialogue contexts and the corresponding generated responses for Maria and compared baselines. Three human annotators are asked to score the respon"
2021.acl-long.435,I17-1047,0,0.0726611,"tags on images reflects some commonsense of our physical world, e.g., “pizza” is usually on the “dining table”, people usually use “knife” when eating “pizza”. Interestingly, we found the “pizza” also co-occurs with “cell phone” and even “plotted plant”. This indicates when people eat pizza, they sometimes would put their cell phones aside on the table, or there might exist some plotted plants in the restaurant. Thus, empowering conversational agents to have the visual perception ability about the physical world is a key way for them to exhibit the human-like intelligence. The existing works (Mostafazadeh et al., 2017; Huber et al., 2018; Shuster et al., 2020) focus on exploring the multimodal dialog models that ground the conversation on a given image. Recently, Yang et al. (2020) propose to learn the dialog generation model with both image-grounded dialogs and textual dialogs by resorting to text-to-image synthesis techniques (Xu et al., 2018; Qiao et al., 2019) to restore a latent image for the text-only dialog. Even so, these works are still constrained by the assumption that the dialog is conducted center around a given (or synthesized) image. In this paper, we take a step further to extend the assump"
2021.acl-long.435,P02-1040,0,0.110234,"rd.edu/people/ karpathy/deepimagesent 5601 113.2K/5K/5K samples as training/validation/test set, respectively. After the retrieval model is trained, we fetch 500K images from the Open Images dataset as the image index, and then retrieve images from it by dialog context and response to construct the training data for response generator. 5.2 Evaluation Metrics Both automatic metrics and human evaluation are employed to assess the performance of Maria and baselines. Automatic metrics include: (1) Fluency: perplexity (PPL) measures the confidence of the generated responses; (2) Relevance: BLEU-1 (Papineni et al., 2002), Rouge-L (Lin, 2004), and we follow Serban et al. (2017) to utilize Embedding Average cosine similarity, Vector Extrema cosine similarity, and Embedding Greedy Matching score. All this metrics are calculated by running the public NLG evaluation script4 ; (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) are defined as the number of distinct uni-grams or bi-grams divided by the total amount of words. In human evaluation, we randomly select 100 dialogue contexts and the corresponding generated responses for Maria and compared baselines. Three human annotators are aske"
2021.acl-long.435,P15-1152,0,0.027718,"in this area include MS-COCO (Lin et al., 2014), VisDial (Das et al., 2017a) and Visual Genome (Krishna et al., 2017). Visual dialog is a task to answer the questions about the factual content of the image in a multi-turn manner. Differently, image-grounded conversation studies how to reply to a dialog context and a given image with proper responses in an open-ended way. Dialog Generation Encouraged by the success of the neural sequence-to-sequence architecture (Sutskever et al., 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature. Recently, there is an emerging trend towards grounding the dialog generation models on the external knowledge, such as knowledge graphs (Zhou et al., 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020). Different from the previous work on knowledge-grounded conversa"
2021.acl-long.435,2020.acl-main.219,0,0.222427,"physical world, e.g., “pizza” is usually on the “dining table”, people usually use “knife” when eating “pizza”. Interestingly, we found the “pizza” also co-occurs with “cell phone” and even “plotted plant”. This indicates when people eat pizza, they sometimes would put their cell phones aside on the table, or there might exist some plotted plants in the restaurant. Thus, empowering conversational agents to have the visual perception ability about the physical world is a key way for them to exhibit the human-like intelligence. The existing works (Mostafazadeh et al., 2017; Huber et al., 2018; Shuster et al., 2020) focus on exploring the multimodal dialog models that ground the conversation on a given image. Recently, Yang et al. (2020) propose to learn the dialog generation model with both image-grounded dialogs and textual dialogs by resorting to text-to-image synthesis techniques (Xu et al., 2018; Qiao et al., 2019) to restore a latent image for the text-only dialog. Even so, these works are still constrained by the assumption that the dialog is conducted center around a given (or synthesized) image. In this paper, we take a step further to extend the assumption of image-grounded conversation to a fu"
2021.acl-long.435,N15-1020,0,0.0742334,"Missing"
2021.acl-long.435,P19-1538,1,0.842298,"Visual dialog is a task to answer the questions about the factual content of the image in a multi-turn manner. Differently, image-grounded conversation studies how to reply to a dialog context and a given image with proper responses in an open-ended way. Dialog Generation Encouraged by the success of the neural sequence-to-sequence architecture (Sutskever et al., 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature. Recently, there is an emerging trend towards grounding the dialog generation models on the external knowledge, such as knowledge graphs (Zhou et al., 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020). Different from the previous work on knowledge-grounded conversation that connects dialogs with unpaired document knowledge (Li et al., 2020), our work lies in the research of imag"
2021.acl-long.435,P19-1362,0,0.0179682,"he Adam Optimizer (Kingma and Ba, 2015) with a learning rate 3e-5 to train the response generation model. The training is conducted on 4 Nvidia Tesla P40 24G GPU cards for 20 epochs. 5.4 Baselines We compare the following baselines in the experiments: (1) Seq2Seq: A standard Sequence to Seqence model with attention mechanism (Bahdanau et al., 2015). (2) HRED: A Hierarchical Recurrent Encoder-Decoder neural network (Serban et al., 2016). (3) VHRED: A variation of HRED that introduces latent variables into the generation (Serban et al., 2017). (4) ReCoSa: A hierarchical transformer-based model (Zhang et al., 2019) that achieves the state-of-the-art performance on benchmarks of dialog generation. (5) ImgVAE: A dialog generation model (Yang et al., 2020) that is trained on both textual dialogs and image-grounded dialogs by recovering a latent image behind the textual dialog within a conditional variational autoencoding framework. (6) DialoGPT: An opendomain dialog model (Zhang et al., 2020) that fine-tunes GPT-2 (Radford et al., 2019) on massive Reddit data. Since DialoGPT is a dialog generation model trained on the text-only corpus, we introduce it as an auxiliary baseline. For a fair comparison, we cho"
2021.acl-long.435,2020.acl-demos.30,0,0.523971,"cs and human evaluation, and can generate informative responses that have some visual commonsense of the physical world. 1 Human-A: Cool! did you play beach volleyball with your friends? (Human-A: Cool, have you had a BBQ with your friends on the beach? The grilled fish was great!) Human-B: Nope, but it sounds great. Maybe next time. Figure 1: An example of human conversations. When human-B talks about vacation on the beach of Hawaii, human-A recalls his/her past experience of playing volleyball or having BBQ on the beach. (Adiwardana et al., 2020), Blender (Roller et al., 2020) and DialoGPT (Zhang et al., 2020), have shown the compelling performance, they are still lack of the perception ability to our physical world. A recent study (Bisk et al., 2020) points out the successful linguistic communication relies on a shared experience of the world that makes language really meaningful. The visual perception is a rich signal for modeling a vastness of experiences in the world that cannot be documented by text alone (Harnad, 1990). On the other hand, human-human conversations involve their understandings of context, the background knowledge they had, and perhaps most importantly the experiences of the wo"
2021.acl-long.435,2020.emnlp-main.272,1,0.793623,"uence-to-sequence architecture (Sutskever et al., 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature. Recently, there is an emerging trend towards grounding the dialog generation models on the external knowledge, such as knowledge graphs (Zhou et al., 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020). Different from the previous work on knowledge-grounded conversation that connects dialogs with unpaired document knowledge (Li et al., 2020), our work lies in the research of image-grounded conversation where a response is generated with a dialog context and a given image. Existing works (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020) in this direction assume there is a given (or synthesized) image for the dialog and explore the multimodal dialog models. In contrast to th"
2021.acl-long.442,P17-1152,0,0.0331223,"enforcing similar objects to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score"
2021.acl-long.442,D17-1070,0,0.0259629,"logy In this section, we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The r"
2021.acl-long.442,2020.findings-emnlp.139,1,0.917023,"x in fragments[1:]) Label: With the growing population of software developers, natural language code search, which improves the productivity of the development process via retrieving semantically relevant code given natural language queries, is increasingly important in both communities of software engineering and natural language processing (Allamanis et al., 2018; Liu et al., 2020a). The key challenge is how to effectively measure the semantic similarity between a natural language query and a code. There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space. However, these models are ∗ 0 Figure 1: Two examples in CoSQA. A pair of a web query and a Python function with documentation is annotated with “1” or “0”, representing whether the code answers the query or not. Introduction Work done during internship at Microsoft Research Asia. The CoSQA data and leaderboard are available at https://github.com/microsoft/CodeXGLUE/tree/main/TextCode/NL-code-search-WebQuery. The code is available at https://github.com/Jun-jie-Huang/CoCLR 1 1 mostly trained on"
2021.acl-long.442,2020.acl-main.758,0,0.0222518,"ring. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague queries and (7) others. Basically, queries in (2)-(7) categories are not likely to be answered only by a code function, since they may nee"
2021.acl-long.442,2021.ccl-1.108,0,0.0812067,"Missing"
2021.acl-long.442,I17-2053,0,0.0462943,"Missing"
2021.acl-long.442,P16-2022,0,0.016286,"ts to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score s(i,i) can be viewed"
2021.acl-long.442,D19-1410,0,0.0129035,"he model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed lines denote the augmented e"
2021.acl-long.442,W18-3022,0,0.0178495,"we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed"
2021.acl-long.442,2020.findings-emnlp.361,0,0.0315257,"Gu et al., 2018; Cambronero Related Work In this part, we describe existing datasets and methods on code search and code question answering. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague que"
2021.acl-long.442,2020.emnlp-main.36,0,0.0559045,"Missing"
2021.emnlp-main.259,2020.acl-main.747,0,0.0303517,"vely. 2 Related Work The cross-lingual spoken language understanding methods can be divided into two main categories: the model transfer methods and the data transfer methods. The model transfer methods build on pre-trained cross-lingual models to learn language agnostic repIn order to filter out noisy instances, we adopt resentations, such as MUSE (Lample et al., 2017), a co-training mechanism, which uses selected CoVE (McCann et al., 2017), mBERT (Wu and instances from the other models to train the Dredze, 2019), XLM (Lample and Conneau, 2019), 3227 Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020). The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages (Liu et al., 2020b; Schuster et al., 2019; Qin et al., 2020; Upadhyay et al., 2018; Li et al., 2020a). To better align embeddings between source and target languages, Liu et al. (2019) use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. Liu et al. (2020b) and Qin et al. (2020) use parallel word pairs to construct code-switching data for fine-tuning. Their methods"
2021.emnlp-main.259,N13-1073,0,0.208038,"ental results show models can improve the results of cross-lingual that our method outperforms the existing state SLU substantially when no golden-labeled trainof the art by 3.05 and 4.24 percentage points ing data in target languages is available. For exon two benchmark datasets, respectively. The ample, machine translation can be employed to code will be made open sourced on github. translate the training data in English into target 1 Introduction languages, and some alignment methods, such as attention weights (Schuster et al., 2019), fastalSpoken language understanding (SLU) is a key ign (Dyer et al., 2013) or giza++ (Och and Ney, component in task-oriented dialogue systems. SLU 2003), can be further applied to label the translated consists of two subtasks: intent detection and slot tagging (Wang et al., 2005; Tur and De Mori, 2011). data. Another approach to alleviate the problem of data scarcity is to automatically generate trainAlthough promising progress has been achieved on SLU in English (Liu and Lane, 2016; Peng et al., ing data. Recently, some methods for monolingual SLU (Anaby-Tavor et al., 2020; Kumar et al., 2020; 2020; Huang et al., 2020), those methods need Zhao et al., 2019; Peng e"
2021.emnlp-main.259,P18-1082,0,0.0707709,"Missing"
2021.emnlp-main.259,D19-1252,1,0.845587,"MUSE (Lample et al., 2017), a co-training mechanism, which uses selected CoVE (McCann et al., 2017), mBERT (Wu and instances from the other models to train the Dredze, 2019), XLM (Lample and Conneau, 2019), 3227 Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020). The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages (Liu et al., 2020b; Schuster et al., 2019; Qin et al., 2020; Upadhyay et al., 2018; Li et al., 2020a). To better align embeddings between source and target languages, Liu et al. (2019) use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. Liu et al. (2020b) and Qin et al. (2020) use parallel word pairs to construct code-switching data for fine-tuning. Their methods encourage the model to align similar words in different languages into the same space and attend to keywords. Liu et al. (2020c) propose a regularization approach to align word-level and sentence-level representations across languages without any external resource. The data transfer methods construct pseudolabeled data in target"
2021.emnlp-main.259,2020.coling-main.310,0,0.0407545,"ken language understanding (SLU) is a key ign (Dyer et al., 2013) or giza++ (Och and Ney, component in task-oriented dialogue systems. SLU 2003), can be further applied to label the translated consists of two subtasks: intent detection and slot tagging (Wang et al., 2005; Tur and De Mori, 2011). data. Another approach to alleviate the problem of data scarcity is to automatically generate trainAlthough promising progress has been achieved on SLU in English (Liu and Lane, 2016; Peng et al., ing data. Recently, some methods for monolingual SLU (Anaby-Tavor et al., 2020; Kumar et al., 2020; 2020; Huang et al., 2020), those methods need Zhao et al., 2019; Peng et al., 2020) automatically large amounts of training data, and thus cannot be label domain-specific data or use pre-trained lanapplied to low-resource languages where zero or guage models to generate additional data. few training data is available. However, the synthesized training data derived In this paper, we target at the extreme setting for from both the translation approach and the data cross-lingual SLU where no labeled data in target generation approach may be quite noisy and may languages is assumed, which is critical for industry contain"
2021.emnlp-main.259,2020.lifelongnlp-1.3,0,0.0746166,"r et al., 2019), fastalSpoken language understanding (SLU) is a key ign (Dyer et al., 2013) or giza++ (Och and Ney, component in task-oriented dialogue systems. SLU 2003), can be further applied to label the translated consists of two subtasks: intent detection and slot tagging (Wang et al., 2005; Tur and De Mori, 2011). data. Another approach to alleviate the problem of data scarcity is to automatically generate trainAlthough promising progress has been achieved on SLU in English (Liu and Lane, 2016; Peng et al., ing data. Recently, some methods for monolingual SLU (Anaby-Tavor et al., 2020; Kumar et al., 2020; 2020; Huang et al., 2020), those methods need Zhao et al., 2019; Peng et al., 2020) automatically large amounts of training data, and thus cannot be label domain-specific data or use pre-trained lanapplied to low-resource languages where zero or guage models to generate additional data. few training data is available. However, the synthesized training data derived In this paper, we target at the extreme setting for from both the translation approach and the data cross-lingual SLU where no labeled data in target generation approach may be quite noisy and may languages is assumed, which is cri"
2021.emnlp-main.259,2020.tacl-1.47,0,0.103347,"thods. The model transfer methods build on pre-trained cross-lingual models to learn language agnostic repIn order to filter out noisy instances, we adopt resentations, such as MUSE (Lample et al., 2017), a co-training mechanism, which uses selected CoVE (McCann et al., 2017), mBERT (Wu and instances from the other models to train the Dredze, 2019), XLM (Lample and Conneau, 2019), 3227 Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020). The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages (Liu et al., 2020b; Schuster et al., 2019; Qin et al., 2020; Upadhyay et al., 2018; Li et al., 2020a). To better align embeddings between source and target languages, Liu et al. (2019) use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. Liu et al. (2020b) and Qin et al. (2020) use parallel word pairs to construct code-switching data for fine-tuning. Their methods encourage the model to align similar words in different languages into the same space and attend to keywords. Liu et al. (2020c) propose a regularization approach"
2021.emnlp-main.259,D19-1129,0,0.0234323,"MUSE (Lample et al., 2017), a co-training mechanism, which uses selected CoVE (McCann et al., 2017), mBERT (Wu and instances from the other models to train the Dredze, 2019), XLM (Lample and Conneau, 2019), 3227 Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020). The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages (Liu et al., 2020b; Schuster et al., 2019; Qin et al., 2020; Upadhyay et al., 2018; Li et al., 2020a). To better align embeddings between source and target languages, Liu et al. (2019) use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. Liu et al. (2020b) and Qin et al. (2020) use parallel word pairs to construct code-switching data for fine-tuning. Their methods encourage the model to align similar words in different languages into the same space and attend to keywords. Liu et al. (2020c) propose a regularization approach to align word-level and sentence-level representations across languages without any external resource. The data transfer methods construct pseudolabeled data in target"
2021.emnlp-main.259,2020.emnlp-main.587,0,0.159315,"thods. The model transfer methods build on pre-trained cross-lingual models to learn language agnostic repIn order to filter out noisy instances, we adopt resentations, such as MUSE (Lample et al., 2017), a co-training mechanism, which uses selected CoVE (McCann et al., 2017), mBERT (Wu and instances from the other models to train the Dredze, 2019), XLM (Lample and Conneau, 2019), 3227 Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020). The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages (Liu et al., 2020b; Schuster et al., 2019; Qin et al., 2020; Upadhyay et al., 2018; Li et al., 2020a). To better align embeddings between source and target languages, Liu et al. (2019) use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. Liu et al. (2020b) and Qin et al. (2020) use parallel word pairs to construct code-switching data for fine-tuning. Their methods encourage the model to align similar words in different languages into the same space and attend to keywords. Liu et al. (2020c) propose a regularization approach"
2021.emnlp-main.259,J03-1002,0,0.0292725,"ethods encourage the model to align similar words in different languages into the same space and attend to keywords. Liu et al. (2020c) propose a regularization approach to align word-level and sentence-level representations across languages without any external resource. The data transfer methods construct pseudolabeled data in target languages. These methods usually employ machine translators to translate training instances in a source language into target languages and then apply alignment methods, such as attention weights (Schuster et al., 2019), fastalign (Dyer et al., 2013), or giza++ (Och and Ney, 2003), to project slot labels to the target language side. The derived training instances are combined with the training data in the source language to fine-tune the pre-trained cross-lingual models. Previous studies (Upadhyay et al., 2018; Schuster et al., 2019; Li et al., 2020a) show that adding translated training data can significantly improve the model performance, especially on languages which are distant from the source language. 3 Method In this section, we define the problem and then propose our method. 3.1 Problem Definition and Solution Framework The SLU task aims to parse user queries i"
2021.emnlp-main.259,2020.findings-emnlp.33,0,0.0281401,"nsists of two major modules, the data augmentation module and the denoising module. The data generation approaches can also construct additional training data. Some methods (Wang and Yang, 2015; Marivate and Sefara, 2020; Gao et al., 2020) make slight changes to the original training instances through word replacement or paraphrases. More sophisticated methods generate training data through large-scale neural networks, such as generative adversarial networks (Goodfellow et al., 2020), variational au3.2 The Data Augmentation Module toencoders (Doersch, 2016; dos Santos Tanaka and Aranha, 2019; Russo et al., 2020), and pre-trained In this module, we augment training data in target language models (Wu et al., 2019; Anaby-Tavor languages via translation and generation. The left et al., 2020; Kumar et al., 2020; Peng et al., 2020). part of Figure 1 shows the details. 3228 Figure 1: The overall framework of data augmentation module (left) and denoising module (right). 3.2.1 Translation We use Google Translator to translate the training corpus Dsrc in the source language (English) to the target languages. In addition to translation, we also need some word alignment methods to project the slot labels to the"
2021.emnlp-main.259,N19-1380,0,0.415753,"erformance on intent detection. The results on though various data augmentation approaches have been proposed to synthesize training slot tagging, however, are often unsatisfactory, esdata in low-resource target languages, the augpecially for distant languages, which are dramatimented data sets are often noisy, and thus imcally different from English in scripts, morphology, pede the performance of SLU models. In this or syntax (Upadhyay et al., 2018; Schuster et al., paper we focus on mitigating noise in aug2019; Li et al., 2020a). mented data. We develop a denoising training Several studies (Schuster et al., 2019; Liu et al., approach. Multiple models are trained with data produced by various augmented methods. 2020b; Li et al., 2020a) show that adding transThose models provide supervision signals to lated data into the fine-tuning process of pretrained each other. The experimental results show models can improve the results of cross-lingual that our method outperforms the existing state SLU substantially when no golden-labeled trainof the art by 3.05 and 4.24 percentage points ing data in target languages is available. For exon two benchmark datasets, respectively. The ample, machine translation can"
2021.emnlp-main.259,D15-1306,0,0.0150878,"i (x; Θ), that is, pI (x; Θ) = softmax(W I · h0 + bI ) S S pS i (x; Θ) = softmax(W · hi + b ) (1) I S where pI ∈ R1×|C |, pSi ∈ R1×|C |, C I is the set of intent labels, C S is the set of slot labels under the BIO annotation schema (Ramshaw and Marcus, I S 1999), W I ∈ R|C |×d and W S ∈ R|C |×d are the output matrices, and bI and bS are the biases. The overall architecture of our proposed method is shown in Figure 1. It consists of two major modules, the data augmentation module and the denoising module. The data generation approaches can also construct additional training data. Some methods (Wang and Yang, 2015; Marivate and Sefara, 2020; Gao et al., 2020) make slight changes to the original training instances through word replacement or paraphrases. More sophisticated methods generate training data through large-scale neural networks, such as generative adversarial networks (Goodfellow et al., 2020), variational au3.2 The Data Augmentation Module toencoders (Doersch, 2016; dos Santos Tanaka and Aranha, 2019; Russo et al., 2020), and pre-trained In this module, we augment training data in target language models (Wu et al., 2019; Anaby-Tavor languages via translation and generation. The left et al.,"
2021.emnlp-main.259,D19-1077,0,0.012724,"may be quite noisy and may languages is assumed, which is critical for industry contain errors in label. For the translation approach, practice, since annotating a large SLU dataset with high quality for every language is simply infeasible. both the translation process and the alignment process may generate errors (Xu et al., 2020; Li et al., Existing cross-lingual transfer learning methods mainly build on pre-trained cross-lingual word em- 2020b). For the data generation approach, it is beddings (Ruder et al., 2019) or contextual mod- often hard to control a right tradeoff between generels (Wu and Dredze, 2019; Huang et al., 2019; ating correct but less diverse data and generalizing more diverse data but with more noise. Moreover, * Work is done during internship at NLP Group, Microsoft generating synthetic training data across languages STCA. † Corresponding author. further adds challenges to the robustness of the 3226 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3226–3237 c November 7–11, 2021. 2021 Association for Computational Linguistics generation methods. To filter out noise in the synthesized training data, a few methods are proposed, such as"
2021.emnlp-main.259,2020.emnlp-main.410,0,0.0230351,"Missing"
2021.emnlp-main.259,D19-1375,0,0.026539,"y ign (Dyer et al., 2013) or giza++ (Och and Ney, component in task-oriented dialogue systems. SLU 2003), can be further applied to label the translated consists of two subtasks: intent detection and slot tagging (Wang et al., 2005; Tur and De Mori, 2011). data. Another approach to alleviate the problem of data scarcity is to automatically generate trainAlthough promising progress has been achieved on SLU in English (Liu and Lane, 2016; Peng et al., ing data. Recently, some methods for monolingual SLU (Anaby-Tavor et al., 2020; Kumar et al., 2020; 2020; Huang et al., 2020), those methods need Zhao et al., 2019; Peng et al., 2020) automatically large amounts of training data, and thus cannot be label domain-specific data or use pre-trained lanapplied to low-resource languages where zero or guage models to generate additional data. few training data is available. However, the synthesized training data derived In this paper, we target at the extreme setting for from both the translation approach and the data cross-lingual SLU where no labeled data in target generation approach may be quite noisy and may languages is assumed, which is critical for industry contain errors in label. For the translation a"
2021.emnlp-main.617,D19-1189,0,0.26478,"TRD. movies but is in the mood for comedies, would likely get a failed recommendation. 1 Introduction In recent years, there is an emerging trend towards building the recommender dialogue sysBuilding an intelligent dialogue system that can tem, i.e., Conversational Recommendation System freely converse with human, and fulfill complex (CRS), which aims to recommend precise items tasks like movie recommendation, travel planning to users through natural conversations. Existing and etc, has been one of longest standing goals of natural language processing (NLP) and artifi- works (Li et al., 2018; Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020) on this line usually cial intelligence (AI). Thanks to the breakthrough consist of two major components, namely a recom∗ Equal contribution. Work performed when Zujie Liang mender module and a dialogue module. The recomwas an intern at Microsoft STCA. † Corresponding author: djiang@microsoft.com. mender module aims at retrieving a subset of items 7821 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7821–7833 c November 7–11, 2021. 2021 Association for Computational Linguistics that meet the user’s interest fro"
2021.emnlp-main.617,P16-1154,0,0.223732,"or: djiang@microsoft.com. mender module aims at retrieving a subset of items 7821 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7821–7833 c November 7–11, 2021. 2021 Association for Computational Linguistics that meet the user’s interest from the item pool by conversation history, while the dialogue module generates free-form natural responses to proactively seek user preference, chat with users, and provide the recommendation. To incorporate the recommended items into the responses, a switching network (Gulcehre et al., 2016) or copy mechanism (Gu et al., 2016) is utilized by these methods to control whether to generate an ordinal word or an item at each time step. Such integration strategies cannot always incorporate the recommended items into generated replies precisely and appropriately. Besides, current approaches do not consider the generalization ability of the model. Hence, only the items mentioned in the training corpus have a chance of being recommended in the conversation. In this paper, we propose to learn Neural Templates for Recommender Dialogue system, i.e., NTRD. NTRD is a neural approach that firstly generates a response “template” w"
2021.emnlp-main.617,P16-1014,0,0.158412,"rn at Microsoft STCA. † Corresponding author: djiang@microsoft.com. mender module aims at retrieving a subset of items 7821 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7821–7833 c November 7–11, 2021. 2021 Association for Computational Linguistics that meet the user’s interest from the item pool by conversation history, while the dialogue module generates free-form natural responses to proactively seek user preference, chat with users, and provide the recommendation. To incorporate the recommended items into the responses, a switching network (Gulcehre et al., 2016) or copy mechanism (Gu et al., 2016) is utilized by these methods to control whether to generate an ordinal word or an item at each time step. Such integration strategies cannot always incorporate the recommended items into generated replies precisely and appropriately. Besides, current approaches do not consider the generalization ability of the model. Hence, only the items mentioned in the training corpus have a chance of being recommended in the conversation. In this paper, we propose to learn Neural Templates for Recommender Dialogue system, i.e., NTRD. NTRD is a neural approach that first"
2021.emnlp-main.617,2020.emnlp-main.654,0,0.0311599,"estions/answers from a template pool and fill the pre-defined slots with optimal attributes. Although this kind of systems are popular in the industry due to the easy implementation, they are still lack of the flexibility and the interactiveness, which leads to the undesirable user experience in practice. Open-ended CRS. Recently, researchers begin to explore the more free-style item recommendation in the response generation, i.e., open-ended CRS (Li et al., 2018; Chen et al., 2019; Liao et al., 2019; Kang et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Chen et al., 2020; Liu et al., 2020; Hayati et al., 2020; Zhou et al., 2020b; Zhang et al., 2021). Generally, this kind of systems consist of two major components, namely a recommender component to recommend items and a dialogue component to generate natural responses. Li et al. (2018) make the first attempt on this direction. They release a benchmark dataset R E D IAL that collects human conversations about movie recommendation between paired crowd-workers with different roles (i.e., Seeker and Recommender). Further studies (Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Sarkar et al., 2020; Lu et al., 2021) leverage multiple external kno"
2021.emnlp-main.617,D18-1361,1,0.819776,"he hypothesis space to search the optimal items according to the collected user preferences. The various asking strategies have been extensively explored, such as memory network based approach (Zhang et al., 2018b), entropy-ranking based approach (Wu et al., 2018), The contributions of this work are summarized generalized binary search based approaches (Zou as follows: (1) We present a novel framework and Kanoulas, 2019; Zou et al., 2020), reinforcecalled NTRD for recommender dialogue system, ment learning based approaches (Sun and Zhang, which decouples the response generation from the 2018; Hu et al., 2018; Chen et al., 2018; Lei et al., item recommendation via a two-stage strategy; (2) 2020a; Deng et al., 2021; Li et al., 2021), adverOur NTRD first generates a response template that sarial learning based approach (Ren et al., 2020) contains a mix of contextual words and slot loca- and graph based approaches (Xu et al., 2020; Lei tions explicitly associated with target items, and et al., 2020b; Ren et al., 2021; Xu et al., 2021). then fills in the slots with precise items by an item Most of these works (Christakopoulou et al., 2018; 7822 Another unique advantage of our NTRD lies in its zero-sho"
2021.emnlp-main.617,P18-1133,0,0.0185009,"xist in training corpus. 2 Related Work In this section, we first introduce the related work on task-oriented dialogue system. Then we review the existing literature on Conversational Recommender Systems (CRS), which can be roughly divided into two categories, i.e., attribute-centric CRS and open-ended CRS. Task-oriented Dialogue System. From the methodology perspective, there are two lines of the research on the task-oriented dialogue system, i.e., modular approaches (Young et al., 2013) and end-to-end approaches (Serban et al., 2016; Wen et al., 2017; Bordes et al., 2017; Zhao et al., 2017; Lei et al., 2018). Recent works like GLMP (Wu et al., 2019) and dynamic fusion network (Qin et al., 2020) make attempt to dynamically incorporate the external knowledge bases into the end-to-end framework. Wu et al. (2019) introduce a globalto-local memory pointer network to RNN-based encoder-decoder framework to incorporate external knowledge in dialogue generation. By contrast, our approach gets rid of pointer network paradigm and proposes a two-stage framework, which is modeled by the transformer-based architecture. Attribute-centric CRS. The attribute-centric CRS conducts the recommendations by asking clar"
2021.emnlp-main.617,N16-1014,0,0.0200807,"set collects high-quality dialogues for recommendations on movies through crowd-sourcing workers on Amazon Mechanical Turk (AMT). It contains 10,006 conversations consisting of 182,150 utterances related to 6,924 movies, which is split into the training, validation, and test set in an 80-10-10 proportion. 5.2 Evaluation Metrics Both automatic metrics and human evaluation are employed to evaluate the performance of our method. For dialogue generation, automatic metrics include: (1) Fluency: perplexity (PPL) measures the confidence of the generated responses. (2) Diversity: Distinct-n (Dist-n) (Li et al., 2016) are defined as the number of distinct n-grams divided by the total amount of words. Specifically, we use Dist-2/3/4 at the sentence level to evaluate the diversity of generated responses. For recommendation task, existing works (Li et al., 2018; Chen et al., 2019; Zhou et al., 2020a) individually evaluate the performance on recommendation using Recall@k. However, the goal of open-ended CRS is to smoothly chat with users and naturally incorporate proper recommendation items into the responses. In other words, it is important for the system to generate informative replies containing the accurat"
2021.emnlp-main.617,2020.acl-main.98,0,0.0227893,"2021) retrieve questions/answers from a template pool and fill the pre-defined slots with optimal attributes. Although this kind of systems are popular in the industry due to the easy implementation, they are still lack of the flexibility and the interactiveness, which leads to the undesirable user experience in practice. Open-ended CRS. Recently, researchers begin to explore the more free-style item recommendation in the response generation, i.e., open-ended CRS (Li et al., 2018; Chen et al., 2019; Liao et al., 2019; Kang et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Chen et al., 2020; Liu et al., 2020; Hayati et al., 2020; Zhou et al., 2020b; Zhang et al., 2021). Generally, this kind of systems consist of two major components, namely a recommender component to recommend items and a dialogue component to generate natural responses. Li et al. (2018) make the first attempt on this direction. They release a benchmark dataset R E D IAL that collects human conversations about movie recommendation between paired crowd-workers with different roles (i.e., Seeker and Recommender). Further studies (Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Sarkar et al., 2020; Lu et al., 2021) leverage"
2021.emnlp-main.617,2021.findings-acl.99,0,0.0590501,"Missing"
2021.emnlp-main.617,2020.acl-main.565,0,0.0213042,"work on task-oriented dialogue system. Then we review the existing literature on Conversational Recommender Systems (CRS), which can be roughly divided into two categories, i.e., attribute-centric CRS and open-ended CRS. Task-oriented Dialogue System. From the methodology perspective, there are two lines of the research on the task-oriented dialogue system, i.e., modular approaches (Young et al., 2013) and end-to-end approaches (Serban et al., 2016; Wen et al., 2017; Bordes et al., 2017; Zhao et al., 2017; Lei et al., 2018). Recent works like GLMP (Wu et al., 2019) and dynamic fusion network (Qin et al., 2020) make attempt to dynamically incorporate the external knowledge bases into the end-to-end framework. Wu et al. (2019) introduce a globalto-local memory pointer network to RNN-based encoder-decoder framework to incorporate external knowledge in dialogue generation. By contrast, our approach gets rid of pointer network paradigm and proposes a two-stage framework, which is modeled by the transformer-based architecture. Attribute-centric CRS. The attribute-centric CRS conducts the recommendations by asking clarification questions about the user preferences on a constrained set of item attributes."
2021.emnlp-main.617,2021.eacl-main.24,0,0.0797766,"Missing"
2021.emnlp-main.617,2020.coling-main.369,0,0.0179113,"al., 2020; Chen et al., 2020; Liu et al., 2020; Hayati et al., 2020; Zhou et al., 2020b; Zhang et al., 2021). Generally, this kind of systems consist of two major components, namely a recommender component to recommend items and a dialogue component to generate natural responses. Li et al. (2018) make the first attempt on this direction. They release a benchmark dataset R E D IAL that collects human conversations about movie recommendation between paired crowd-workers with different roles (i.e., Seeker and Recommender). Further studies (Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Sarkar et al., 2020; Lu et al., 2021) leverage multiple external knowledge bases to enhance the performance of recommendation. Liu et al. (2020) propose a multi-goal driven conversation generation framework (MGCG) to proactively and naturally lead a conversation from a nonrecommendation dialogue to a recommendationoriented one. Recently, Zhou et al. (2021) release an open-source CRS toolkit, i.e., CRSLab, to facilitate the research on this direction. However, the Pointer Network (Gulcehre et al., 2016) or Copy Mechanism (Gu et al., 2016) used in these approaches cannot be always accurately incorporated the recom"
2021.emnlp-main.617,E17-1042,0,0.0611717,"Missing"
2021.emnlp-main.617,2020.coling-main.463,0,0.0347821,"inary search based approaches (Zou as follows: (1) We present a novel framework and Kanoulas, 2019; Zou et al., 2020), reinforcecalled NTRD for recommender dialogue system, ment learning based approaches (Sun and Zhang, which decouples the response generation from the 2018; Hu et al., 2018; Chen et al., 2018; Lei et al., item recommendation via a two-stage strategy; (2) 2020a; Deng et al., 2021; Li et al., 2021), adverOur NTRD first generates a response template that sarial learning based approach (Ren et al., 2020) contains a mix of contextual words and slot loca- and graph based approaches (Xu et al., 2020; Lei tions explicitly associated with target items, and et al., 2020b; Ren et al., 2021; Xu et al., 2021). then fills in the slots with precise items by an item Most of these works (Christakopoulou et al., 2018; 7822 Another unique advantage of our NTRD lies in its zero-shot capability that can adapt with a regularly updated recommender system. Once a slotted response template is generated by the template generator, different recommender systems could be plugged into the item selector easily to fill in the slots with proper items. Thus, NTRD can produce the diverse natural responses with the"
2021.emnlp-main.617,P18-1205,0,0.179566,"ge in dialogue generation. By contrast, our approach gets rid of pointer network paradigm and proposes a two-stage framework, which is modeled by the transformer-based architecture. Attribute-centric CRS. The attribute-centric CRS conducts the recommendations by asking clarification questions about the user preferences on a constrained set of item attributes. This kind of systems gradually narrow down the hypothesis space to search the optimal items according to the collected user preferences. The various asking strategies have been extensively explored, such as memory network based approach (Zhang et al., 2018b), entropy-ranking based approach (Wu et al., 2018), The contributions of this work are summarized generalized binary search based approaches (Zou as follows: (1) We present a novel framework and Kanoulas, 2019; Zou et al., 2020), reinforcecalled NTRD for recommender dialogue system, ment learning based approaches (Sun and Zhang, which decouples the response generation from the 2018; Hu et al., 2018; Chen et al., 2018; Lei et al., item recommendation via a two-stage strategy; (2) 2020a; Deng et al., 2021; Li et al., 2021), adverOur NTRD first generates a response template that sarial learning"
2021.emnlp-main.617,2020.acl-demos.30,0,0.0327065,"en greatly advanced and brought into a new Though recent end-to-end neural models have frontier over the past few years. Nowadays, we shown the promising progress on Conversaare witnessing the booming of virtual assistants tional Recommender System (CRS), two key with conversational user interface like Microsoft challenges still remain. First, the recomCortana, Apple Siri, Amazon Alexa and Google mended items cannot be always incorporated Assistant. The recent large-scale dialogue modinto the generated replies precisely and appropriately. Second, only the items mentioned els such as DialoGPT (Zhang et al., 2020), Meena in the training corpus have a chance to be (Adiwardana et al., 2020) and Blender (Roller et al., recommended in the conversation. To tackle 2021), demonstrate the impressive performance these challenges, we introduce a novel framein practice. Besides, the social bots such as Xiwork called NTRD for recommender dialogue aoIce (Shum et al., 2018) and PersonaChat (Zhang system that decouples the dialogue generaet al., 2018a) also exhibit the great potential on the tion from the item recommendation. NTRD emotional companion to humans. has two key components, i.e., response template generato"
2021.emnlp-main.617,W17-5505,0,0.0221537,"items that do not exist in training corpus. 2 Related Work In this section, we first introduce the related work on task-oriented dialogue system. Then we review the existing literature on Conversational Recommender Systems (CRS), which can be roughly divided into two categories, i.e., attribute-centric CRS and open-ended CRS. Task-oriented Dialogue System. From the methodology perspective, there are two lines of the research on the task-oriented dialogue system, i.e., modular approaches (Young et al., 2013) and end-to-end approaches (Serban et al., 2016; Wen et al., 2017; Bordes et al., 2017; Zhao et al., 2017; Lei et al., 2018). Recent works like GLMP (Wu et al., 2019) and dynamic fusion network (Qin et al., 2020) make attempt to dynamically incorporate the external knowledge bases into the end-to-end framework. Wu et al. (2019) introduce a globalto-local memory pointer network to RNN-based encoder-decoder framework to incorporate external knowledge in dialogue generation. By contrast, our approach gets rid of pointer network paradigm and proposes a two-stage framework, which is modeled by the transformer-based architecture. Attribute-centric CRS. The attribute-centric CRS conducts the recommendat"
2021.emnlp-main.617,2021.acl-demo.22,0,0.0169551,"They release a benchmark dataset R E D IAL that collects human conversations about movie recommendation between paired crowd-workers with different roles (i.e., Seeker and Recommender). Further studies (Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020; Sarkar et al., 2020; Lu et al., 2021) leverage multiple external knowledge bases to enhance the performance of recommendation. Liu et al. (2020) propose a multi-goal driven conversation generation framework (MGCG) to proactively and naturally lead a conversation from a nonrecommendation dialogue to a recommendationoriented one. Recently, Zhou et al. (2021) release an open-source CRS toolkit, i.e., CRSLab, to facilitate the research on this direction. However, the Pointer Network (Gulcehre et al., 2016) or Copy Mechanism (Gu et al., 2016) used in these approaches cannot be always accurately incorporated the recommended items into the generated replies. Moreover, only the items mentioned in training corpus have a chance of being recommended in conversations by existing approaches. 3 Preliminary Formally, a dialogue consisting of t-turn conversation utterances is denoted as D = {st }N t=1 . Let m denotes an item from the total item set M, and w de"
2021.emnlp-main.617,2020.coling-main.365,0,0.111152,"in the mood for comedies, would likely get a failed recommendation. 1 Introduction In recent years, there is an emerging trend towards building the recommender dialogue sysBuilding an intelligent dialogue system that can tem, i.e., Conversational Recommendation System freely converse with human, and fulfill complex (CRS), which aims to recommend precise items tasks like movie recommendation, travel planning to users through natural conversations. Existing and etc, has been one of longest standing goals of natural language processing (NLP) and artifi- works (Li et al., 2018; Chen et al., 2019; Zhou et al., 2020a; Ma et al., 2020) on this line usually cial intelligence (AI). Thanks to the breakthrough consist of two major components, namely a recom∗ Equal contribution. Work performed when Zujie Liang mender module and a dialogue module. The recomwas an intern at Microsoft STCA. † Corresponding author: djiang@microsoft.com. mender module aims at retrieving a subset of items 7821 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7821–7833 c November 7–11, 2021. 2021 Association for Computational Linguistics that meet the user’s interest from the item pool by"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.36,2020.emnlp-main.751,0,0.0340526,"the average number of words in source inputs. R-L: ROUGE-L. B-4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword co"
2021.findings-acl.36,W05-0909,0,0.337371,"urkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4,"
2021.findings-acl.36,2020.acl-main.9,0,0.366674,"Missing"
2021.findings-acl.36,L18-1269,0,0.017035,"the pretrained model. For these tasks, the non-pretrained model tends to generate universal responses (Li et al., 2016) or outputs. Moreover, there is still a huge gap between the bigram diversity of pretrained models and real samples (golden) on the task of XSUM, MSQG, and PersonaChat. Obviously, there exists great room for future improvement of the pretrained models in terms of output diversity. 4 Related Works Benchmarks Recently, the development of general natural language understanding (NLU) evaluation benchmarks has helped drive the progress of pretraining and transfer learning in NLP. Conneau and Kiela (2018) propose a toolkit, SentEval, for evaluating the quality of universal sentence representations. DecaNLP (McCann et al., 2018) casts ten diversified NLP tasks as a general question-answering format for evaluation. Wang et al. (2019b) propose a widely-used multi-task benchmark, GLUE, for NLU in the English language. There are nine NLU tasks in GLUE, including two single-sentence tasks, three similarity and paraphrase tasks, and four natural language inference tasks. After that, SuperGLUE (Wang et al., 2019a) is proposed as a harder counterpart of GLUE. Besides sentence- and sentence-pair classif"
2021.findings-acl.36,N19-1423,0,0.169557,"eration Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and Prophet1 Net . 1 Introduction Pretrained language models, such as BERT (Devlin et al., 2019) and other advanced pretrained models (Raffel et al., 2020; Yang et al., 2019; Liu et al., 2019; Alberti et al., 2019; Brown et al., 2020; Clark et al., 2020) have made great progress in a host of Natural Language Understanding (NLU) tasks. Meanwhile, the development of general evaluation benchmarks has also helped drive the progress of these models. These benchmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at"
2021.findings-acl.36,P17-1123,0,0.0119354,". Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MS"
2021.findings-acl.36,W18-2706,0,0.0130688,"he same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The performance gap between the pretrained model and non-pretrained model is obvious. The diff"
2021.findings-acl.36,2020.lrec-1.302,0,0.085243,"Missing"
2021.findings-acl.36,2020.acl-main.703,0,0.270812,"l language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for eva"
2021.findings-acl.36,N16-1014,0,0.162483,"mation as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), we seek to give an overall system performance over all GLGE tasks by aggregating the scores of all tasks. We follow GLUE to adopt a simple approach that weighs each task equally. For the tasks with multiple metrics, we firstly average those metrics to get a task score. Besides, because the values of the original Distinct1 (D-1) and Distinct-2 (D-2) (Li et al., 2016) scores which are used as the metrics for dialogue task are usually quite small (less than 0.01), we re-"
2021.findings-acl.36,W04-1013,0,0.0599605,"4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processin"
2021.findings-acl.36,2021.ccl-1.108,0,0.0321684,"Missing"
2021.findings-acl.36,D18-1206,0,0.0116702,"summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news article, and the target output is a single-sentence summary. MSNews MicroSoft News headline generation (MSNews) is a new News headline generation dataset we collected for GLGE. We random select 151K online news articles from 2012-01-01 to 2020-09-01 from a real-world news search engine. Each article contains a professionally written single-s"
2021.findings-acl.36,J04-4002,0,0.0753216,"embedding/hidden size and 4096 feed-forward filter size. The pretraining of BARTlarge uses the same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The"
2021.findings-acl.36,P02-1040,0,0.109921,"on generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target outpu"
2021.findings-acl.36,2020.findings-emnlp.217,1,0.68917,"on benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for evaluating the generalization cap"
2021.findings-acl.36,D16-1264,0,0.287036,"cle, and the target output is a news headline. 2.2.2 Answer-aware Question Generation The question generation task is another typical NLG task, which aims to generate a question based on a given text passage or document. Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie,"
2021.findings-acl.36,Q19-1016,0,0.0115463,"there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4, and METEOR are used as the metrics. 2.2.3 Conversational Question Answering Conversational question answering is a classic and popular generative question answering task. Compared with the extractive question answering, such as SQuAD (Rajpurkar et al., 2016), conversational question answering requires the model to answer the question based on a running conversation history and the given passage. CoQA (Reddy et al., 2019) dataset contains 127K questions with answers, obtained from 8K conversations about text passages from seven diverse domains. After the pre-processing, there are 116,630 ⟨conversation history, passage, question, answer⟩ data 4-tuples, where the source input is a sequence of conversation history along with a given question and a given passage, and the target output is a freeform answer text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversati"
2021.findings-acl.36,2020.tacl-1.18,0,0.177338,"for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new m"
2021.findings-acl.36,D15-1044,0,0.0308312,"contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news"
2021.findings-acl.36,P17-1099,0,0.0378114,"ble 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the"
2021.findings-acl.36,2020.acl-main.704,0,0.0397801,"Missing"
2021.findings-acl.36,2020.aacl-main.85,0,0.247778,"chmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset are publicly available at https://github.com/microsoft/glge. language understanding evaluation benchmarks for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated"
2021.findings-acl.36,P18-1205,0,0.0264513,"text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversations are diverse and might lack of specification, and thus it is hard to use the single ground-truth for automatic evaluation. We select the personalizing dialogue task, which is a challenging multi-turn conversation task. In addition to the conversation history, this task gives the profile information as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) an"
2021.findings-acl.36,2020.acl-demos.30,0,0.075506,"Missing"
2021.findings-acl.36,D18-1424,0,0.0119763,"nswer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K"
2021.findings-acl.403,P19-1470,0,0.139103,".2). Lastly, as in Figure 3, 4587 Figure 2: Our discriminative knowledge model (left), and its script adapter (right) for multi-choice narrative cloze (MCNC). Dash-dot blue rounded rectangles denote parameters optimized towards the objective of the knowledge model, whereas solid blue rounded rectangles denote script adapter’s parameters that will be optimized towards the objective of MCNC. we present a representation learning framework to solve the MCNC task (§3.3). 3.1 Discriminative Knowledge Model To avoid challenging event grounding and satisfy coverage necessity, neural knowledge models (Bosselut et al., 2019b; Hwang et al., 2020) are proposed to memorize eventuality facts from a KG to its parameters during training. They are built upon a pre-trained generative Transformer (e.g., GPT (Radford et al., 2018)) and fine-tuned on triple facts from an eventuality KG via generative objectives of event-based link prediction. However, such generative knowledge models are not perfectly compatible when capturing event-pair relation facts since they focus more on inferring tail events given a head event and an inferential relation. This is consistent with the goal of link prediction for KG completion. Consequ"
2021.findings-acl.403,D15-1075,0,0.0437561,"ncate˜ e into a Transformer encoder, followed nated text w by a pooling layer, i.e., ˜ e ; θ(km) ) ∈ Rd×N , H e = Transformer(w (3) v = Pool(H e ) ∈ Rd , ∀e ∈ {e(h) , e(t) }, (4) where v denotes the resulting event representation, Transformer(·; θ) stands for pre-trained bidirectional Transformer encoder (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)) to produce deep contextualized embeddings, and Pool(·) denotes using the embedding of [CLS] as sequence-level representation by following prior works. Given v of both head and tail events, we apply an interactive concatenation (Bowman et al., 2015; Reimers and Gurevych, 2019) between them to model their inferential relationship, i.e., r := Inter-Concat(h, t) = [h; h × t; h − t; t], where h = v (h) and t = v (t) . (5) R4d Here, r ∈ represents inferential relation between head and tail events, [·; ·] denotes vector concatenation, and “×” denotes element-wise product. Lastly, the relation representation, r, is learned by passing it into a neural classifier to predict the oracle relation in the original triple. In order to enable this knowledge model to represent a null or non-associated relation between events, we define an extra relation"
2021.findings-acl.403,P08-1090,0,0.428579,"didate. We evaluate the proposed approach on a popular multi-choice narrative cloze task for script reasoning and achieve new state-ofthe-art accuracy, compared with baselines either incorporating external KG or not. 1 Figure 1: Comparison of “retrieval and integration” paradigm (green dot line) with ours (blue dash-dot line). Although there is no semantic overlap between the precedent event in the script and the events in the KG, which leads to failed retrieval, our approach still provides supportive evidence by exploiting similar inferential relation patterns. Introduction Script reasoning (Chambers and Jurafsky, 2008; Li et al., 2018; Lv et al., 2020b) aims at determining the subsequent event or plausible ending for an event chain in a script. For example, a tourism script consist of [“Emily took a plane”, “Emily arrived at Oahu”, “Emily went to Waimea Bay”], and the subsequent event is more likely to be “Emily surfed” than “Emily skied”. Script reasoning has attracted more interest in the natural language processing (NLP) community since it plays essential ∗ † Work is done during internship at Microsoft. Corresponding author. roles in many real-world applications like storytelling (Swanson and Gordon, 20"
2021.findings-acl.403,N19-1423,0,0.00829127,"sing all relations and enlarging beam-search size (Bosselut et al., 2019a). And the generated triple must be re-encoded into latent space for the integration (Lv et al., 2020b), not to mention generative models not always reliable. Therefore, we present a discriminative objective based on relation classification for knowledge model learning to directly capture such inferential information in latent space. Formally, given a triple (e(h) , r, e(t) ) ∈ G, we separately pass head event e(h) and tail event e(h) , into a text encoder to generate event-level contextualized representations. Following Devlin et al. (2019), we first concatenate the natural language text we of each event e with special tokens: ˜ e = ([CLS],we ,[SEP]), ∀e ∈ {e(h) ,e(t) }, (2) w where the special tokens could vary with different pre-trained models. Then, we feed the concate˜ e into a Transformer encoder, followed nated text w by a pooling layer, i.e., ˜ e ; θ(km) ) ∈ Rd×N , H e = Transformer(w (3) v = Pool(H e ) ∈ Rd , ∀e ∈ {e(h) , e(t) }, (4) where v denotes the resulting event representation, Transformer(·; θ) stands for pre-trained bidirectional Transformer encoder (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019"
2021.findings-acl.403,D19-1495,0,0.0183888,"Bosselut et al., 2019b), leading to problematic grounding from an event to the nodes in the KG. To circumvent the coverage problem, Bosselut et al. (2019b) and Hwang et al. (2020) propose to learn a generative knowledge model on existing triples from an eventuality KG, where the triples can be regarded as a seed of knowledge. It ondemand generates subsequent events with a prompt of the observed event and an inferential relation, thus avoiding event grounding and satisfying coverage necessity for a broad spectrum of NLP tasks (Shwartz et al., 2020; Majumder et al., 2020; Paul and Frank, 2020; Ding et al., 2019; Ma et al., 2019). However, such generative knowledge models are not perfectly compatible when capturing inferential relations between events because they focus more on inferring tail events rather than the relations. In contrast, our method avoids operating merely on the triples that have lexical or semantic overlap with the targeted script, while directly learn the inferential relation patterns on the whole KG. The learned knowledge model can simply capture the relation between events in a script in latent space, benefiting various event-centric reasoning tasks. 6 Conclusion In this work, w"
2021.findings-acl.403,E12-1034,0,0.0262487,"lthough “Emily went surfing” is a rational subsequent event, the distance between the two events is too long so it is difficult for a reasoning model to capture such relations. 5 Related Work A script (Schank and Abelson, 2013) refers to a kind of structured representation for prototypical sequences of events. Chambers and Jurafsky (2008) formulate a script learning (narrative learning) task and propose statistical models to capture event cooccurrence for subsequent event prediction. Afterwards, the approaches for script reasoning can be categorized into two genres. i.e., event pair modeling (Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016) and event chain modeling (Pichotta and Mooney, 2016; Wang et al., 2017; Lv et al., 2019). But, they still lag far behind humans as the well-labeled training set is usually of small size. In addition, script reasoning is more challenging than traditional NLP tasks and requires models to reason over unobserved events. With recent developments of large-scale eventuality knowledge graphs (KG) (e.g., ASER (Zhang et al., 2020) and ATOMIC (Sap et al., 2019)), an effective remedy is to adopt “retrieval and integration” schema and integrate"
2021.findings-acl.403,D19-1282,0,0.0180954,"llenging than traditional NLP tasks and requires models to reason over unobserved events. With recent developments of large-scale eventuality knowledge graphs (KG) (e.g., ASER (Zhang et al., 2020) and ATOMIC (Sap et al., 2019)), an effective remedy is to adopt “retrieval and integration” schema and integrate the inferential facts retrieved from the KG for script reasoning (Lv et al., 2020b). This paradigm is proven effective in both entity-centric and concept-centric tasks, such as relation extraction (Zhang et al., 2019), named entity recognition (Liu et al., 2020) and commonsense reasoning (Lin et al., 2019; Lv et al., 2020a), etc. However, this paradigm is not that compatible with event-centric script reasoning since script reasoning focuses more on the inferential relation between consecutive events in a script rather than the triple facts with exact event matching. What is worse, these eventuality KGs consisting of free-form event usually encounter low knowledge coverage or incompleteness problem (Zhang et al., 2020; Bosselut et al., 2019b), leading to problematic grounding from an event to the nodes in the KG. To circumvent the coverage problem, Bosselut et al. (2019b) and Hwang et al. (2020"
2021.findings-acl.403,2021.ccl-1.108,0,0.0223412,"Missing"
2021.findings-acl.403,2020.coling-main.27,0,0.21816,"popular multi-choice narrative cloze task for script reasoning and achieve new state-ofthe-art accuracy, compared with baselines either incorporating external KG or not. 1 Figure 1: Comparison of “retrieval and integration” paradigm (green dot line) with ours (blue dash-dot line). Although there is no semantic overlap between the precedent event in the script and the events in the KG, which leads to failed retrieval, our approach still provides supportive evidence by exploiting similar inferential relation patterns. Introduction Script reasoning (Chambers and Jurafsky, 2008; Li et al., 2018; Lv et al., 2020b) aims at determining the subsequent event or plausible ending for an event chain in a script. For example, a tourism script consist of [“Emily took a plane”, “Emily arrived at Oahu”, “Emily went to Waimea Bay”], and the subsequent event is more likely to be “Emily surfed” than “Emily skied”. Script reasoning has attracted more interest in the natural language processing (NLP) community since it plays essential ∗ † Work is done during internship at Microsoft. Corresponding author. roles in many real-world applications like storytelling (Swanson and Gordon, 2008). Understanding and inferring t"
2021.findings-acl.403,2020.emnlp-main.739,0,0.0818012,"Missing"
2021.findings-acl.403,2020.findings-emnlp.267,0,0.0148783,"m (Zhang et al., 2020; Bosselut et al., 2019b), leading to problematic grounding from an event to the nodes in the KG. To circumvent the coverage problem, Bosselut et al. (2019b) and Hwang et al. (2020) propose to learn a generative knowledge model on existing triples from an eventuality KG, where the triples can be regarded as a seed of knowledge. It ondemand generates subsequent events with a prompt of the observed event and an inferential relation, thus avoiding event grounding and satisfying coverage necessity for a broad spectrum of NLP tasks (Shwartz et al., 2020; Majumder et al., 2020; Paul and Frank, 2020; Ding et al., 2019; Ma et al., 2019). However, such generative knowledge models are not perfectly compatible when capturing inferential relations between events because they focus more on inferring tail events rather than the relations. In contrast, our method avoids operating merely on the triples that have lexical or semantic overlap with the targeted script, while directly learn the inferential relation patterns on the whole KG. The learned knowledge model can simply capture the relation between events in a script in latent space, benefiting various event-centric reasoning tasks. 6 Conclus"
2021.findings-acl.403,E14-1024,0,0.0234035,"surfing” is a rational subsequent event, the distance between the two events is too long so it is difficult for a reasoning model to capture such relations. 5 Related Work A script (Schank and Abelson, 2013) refers to a kind of structured representation for prototypical sequences of events. Chambers and Jurafsky (2008) formulate a script learning (narrative learning) task and propose statistical models to capture event cooccurrence for subsequent event prediction. Afterwards, the approaches for script reasoning can be categorized into two genres. i.e., event pair modeling (Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016) and event chain modeling (Pichotta and Mooney, 2016; Wang et al., 2017; Lv et al., 2019). But, they still lag far behind humans as the well-labeled training set is usually of small size. In addition, script reasoning is more challenging than traditional NLP tasks and requires models to reason over unobserved events. With recent developments of large-scale eventuality knowledge graphs (KG) (e.g., ASER (Zhang et al., 2020) and ATOMIC (Sap et al., 2019)), an effective remedy is to adopt “retrieval and integration” schema and integrate the inferential facts retr"
2021.findings-acl.403,D19-1410,0,0.0145182,"former encoder, followed nated text w by a pooling layer, i.e., ˜ e ; θ(km) ) ∈ Rd×N , H e = Transformer(w (3) v = Pool(H e ) ∈ Rd , ∀e ∈ {e(h) , e(t) }, (4) where v denotes the resulting event representation, Transformer(·; θ) stands for pre-trained bidirectional Transformer encoder (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)) to produce deep contextualized embeddings, and Pool(·) denotes using the embedding of [CLS] as sequence-level representation by following prior works. Given v of both head and tail events, we apply an interactive concatenation (Bowman et al., 2015; Reimers and Gurevych, 2019) between them to model their inferential relationship, i.e., r := Inter-Concat(h, t) = [h; h × t; h − t; t], where h = v (h) and t = v (t) . (5) R4d Here, r ∈ represents inferential relation between head and tail events, [·; ·] denotes vector concatenation, and “×” denotes element-wise product. Lastly, the relation representation, r, is learned by passing it into a neural classifier to predict the oracle relation in the original triple. In order to enable this knowledge model to represent a null or non-associated relation between events, we define an extra relation category, named dummy relati"
2021.findings-acl.403,2020.emnlp-main.373,0,0.0231504,"Missing"
2021.findings-acl.403,D17-1006,0,0.0164406,"reasoning model to capture such relations. 5 Related Work A script (Schank and Abelson, 2013) refers to a kind of structured representation for prototypical sequences of events. Chambers and Jurafsky (2008) formulate a script learning (narrative learning) task and propose statistical models to capture event cooccurrence for subsequent event prediction. Afterwards, the approaches for script reasoning can be categorized into two genres. i.e., event pair modeling (Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016) and event chain modeling (Pichotta and Mooney, 2016; Wang et al., 2017; Lv et al., 2019). But, they still lag far behind humans as the well-labeled training set is usually of small size. In addition, script reasoning is more challenging than traditional NLP tasks and requires models to reason over unobserved events. With recent developments of large-scale eventuality knowledge graphs (KG) (e.g., ASER (Zhang et al., 2020) and ATOMIC (Sap et al., 2019)), an effective remedy is to adopt “retrieval and integration” schema and integrate the inferential facts retrieved from the KG for script reasoning (Lv et al., 2020b). This paradigm is proven effective in both entit"
2021.findings-acl.403,P19-1139,0,0.0595558,"is impractical to learn rich relations for large scale commercial applications. Therefore, it is necessary to leverage external knowledge that implies relations between events. Recently, Lv et al. (2020b) propose to leverage a large-scale eventuality knowledge graph (KG), ASER (Zhang et al., 2020), for script reasoning via adopting the “retrieval and integration” paradigm. Given an event chain, this paradigm first retrieves relevant triple facts from the eventuality KG and then integrates them into a script reasoning model. Although such a paradigm is proven effective in entity-centric tasks (Zhang et al., 2019; Liu et al., 2020), it is not competent in event-centric script 4586 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4586–4596 August 1–6, 2021. ©2021 Association for Computational Linguistics reasoning. The reason is that, the retrieval is based on lexical or semantic matching between an event from a script and each event node in the KG. For example, in Figure 1, to determine whether the precedent event “The new toy is not attractive to kids” will result in a subsequent event “It is rejected”, this paradigm will try to retrieve graph triples with the event n"
2021.findings-emnlp.183,P14-1005,0,0.0693568,"Missing"
2021.findings-emnlp.183,D14-1179,0,0.0245997,"Missing"
2021.findings-emnlp.183,2020.acl-main.700,0,0.0108969,"tion (Zeng sionary, it has repeated target user “A”, which leads to a higher re-entry rate than the other two ex- et al., 2019; Backstrom et al., 2013; Budak and Agrawal, 2013) aims to forecast whether the users pansionary threads. Therefore, we can conclude will return to a discussion they once entered that both Spread Pattern and Repeated Target User signals help predict re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase ge"
2021.findings-emnlp.183,D15-1259,1,0.817089,"imental Setup (10) Datasets. For experiments, we construct two new datasets from Twitter and Reddit. The raw Twitter and Reddit data is released by Zeng et al. (2018, By combining the intention of the SP and RT 2019) and both in English. For both Twitter and tasks, we further design Turn Authorship PredicReddit, we form the conversations with postings tion (henceforth TA) task. The TA task aims to and replies (all the comments and replies also predict whether the turn’s author is the target user viewed as a single turn) following the practice in and we label &quot;yes&quot; with 1 and &quot;no&quot; with 0. This Li et al. (2015) and Zeng et al. (2018). task benefits the main task by signaling both the In our main experiment, different from Zeng conversation spread pattern and repeated user patet al. (2019), we do not focus on predicting first retern. Specifically, this is a turn-level authorship entries (i.e. only giving the context until the target prediction and can help learn meaningful turn repuser’s first participation), we generalize the setting resentations, which are essential for conversation into re-entry prediction regardless of the number modeling. of user’s past participation. In this way, our model Form"
2021.findings-emnlp.183,D19-1682,0,0.0276346,"China 4 Microsoft Corporation, Beijing, China 1,2 {lzwang,kfwong}@se.cuhk.edu.hk 3 zeng.xingshan@huawei.com 4 {huahu,djiang}@microsoft.com Abstract target user) will come back to a conversation they once participated in. Nevertheless, the state-of-theIn recent years, world business in online disart work (Zeng et al., 2019) mostly focuses on rich cussions and opinion sharing on social media information in users’ previous chatting history and is booming. Re-entry prediction task is thus ignores the thread pattern information (Backstrom proposed to help people keep track of the diset al., 2013; Tan et al., 2019). To this end, we study cussions which they wish to continue. Neverin re-entry prediction by exploiting the conversatheless, existing works only focus on exploiting chatting history and context information, tion thread pattern to signal whether a user would and ignore the potential useful learning sigcome back since the degree of repeated engagenals underlying conversation data, such as conment of users can indicate their temporary interests versation thread patterns and repeated engagein the ongoing conversation. ment of target users, which help better unSelf-supervised learning aims to train"
2021.findings-emnlp.183,W02-0109,0,0.179999,"that “AB”, “ABA” and “ABC” are the most frequent patterns. And re-entry rate for focused conversations (i.e. only two users participate, such as “AB” and “ABAB”) is generally higher than expansionary conversations, since prior contributions in one conversation may result in continued participation. Such a phenomenon verifies our motivation to design self-supervised tasks. Preprocessing. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014) to the Twitter dataset. As for the Reddit dataset, we first tokenized the words with the open-source natural language toolkit (NLTK) (Loper and Bird, 2002). We then removed all the non-alphabetic tokens and replaced links with the generic tag “URL”. For both datasets, a vocabulary was maintained with all the remaining tokens, including emoticons and punctuation marks. Parameter Setting. For the parameters in the main model, we first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), whose Twitter version is used for the Twitter dataset and Common Crawl version is applied to Reddit2 . For our BiGRU layers, we set the size of hidden states for each direction to 200. We employ Adam optimizer (Kingma and B"
2021.findings-emnlp.183,P19-1214,0,0.0148786,"(Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) and next word prediction (Bengio et al., 2003; Wang and Gupta, 2015) are widely explored in language modeling. Masked language model (Devlin et al., 2018) is also in the line of word-level tasks. In sentence-level tasks, Wang et al. (2019a) exploits Mask, Replace and Switch for extractive summarization. Wu et al. (2019) propose Inconsistent Order Detection for dialogue learning. Xie et al. (2020) exploit Drop, Replace, and TOV (Temporal Order Verification) for story cloze test. Xu et al. (2020) also design several self-supervised tasks to improve the performance of response selection. Most of the previous self-supervised tasks (both in word-level and sentence-level) focus on the general domain while our work is based on taskorientated supervised methods and achieves better performance. 3 Re-entry Prediction Framework This sect"
2021.findings-emnlp.183,Q15-1029,0,0.0113179,"s a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan in training and testing. For this reason, we pro- et al., 2019; Erhan et al., 2010; Hinton et al., 2006). pose our compact main model, which consists of It has been applied to many tasks, such as text three parts, turn encoder, conversation encoder and classification (Yu and Jiang, 2016), neural machine prediction layer. In addition, the chatting history translation (Ruiter et al., 2019), multi-turn response information of the target"
2021.findings-emnlp.183,D14-1162,0,0.0851786,"-axis: re-entry rate for each user pattern. We also present the distribution of thread patterns with their re-entry rate for Reddit in Figure 3. It can be seen that “AB”, “ABA” and “ABC” are the most frequent patterns. And re-entry rate for focused conversations (i.e. only two users participate, such as “AB” and “ABAB”) is generally higher than expansionary conversations, since prior contributions in one conversation may result in continued participation. Such a phenomenon verifies our motivation to design self-supervised tasks. Preprocessing. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014) to the Twitter dataset. As for the Reddit dataset, we first tokenized the words with the open-source natural language toolkit (NLTK) (Loper and Bird, 2002). We then removed all the non-alphabetic tokens and replaced links with the generic tag “URL”. For both datasets, a vocabulary was maintained with all the remaining tokens, including emoticons and punctuation marks. Parameter Setting. For the parameters in the main model, we first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), whose Twitter version is used for the Twitter dataset and Common Cr"
2021.findings-emnlp.183,N18-1202,0,0.0233286,"re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan"
2021.findings-emnlp.183,P19-1178,0,0.0526187,"Missing"
2021.findings-emnlp.183,P19-1240,0,0.0160236,"(Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) and next word prediction (Bengio et al., 2003; Wang and Gupta, 2015) are widely explored in language modeling. Masked language model (Devlin et al., 2018) is also in the line of word-level tasks. In sentence-level tasks, Wang et al. (2019a) exploits Mask, Replace and Switch for extractive summarization. Wu et al. (2019) propose Inconsistent Order Detection for dialogue learning. Xie et al. (2020) exploit Drop, Replace, and TOV (Temporal Order Verification) for story cloze test. Xu et al. (2020) also design several self-supervised tasks to improve the performance of response selection. Most of the previous self-supervised tasks (both in word-level and sentence-level) focus on the general domain while our work is based on taskorientated supervised methods and achieves better performance. 3 Re-entry Prediction Framework This sect"
2021.findings-emnlp.183,P19-1375,0,0.044043,"Missing"
2021.findings-emnlp.183,D16-1023,0,0.0287707,"d Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan in training and testing. For this reason, we pro- et al., 2019; Erhan et al., 2010; Hinton et al., 2006). pose our compact main model, which consists of It has been applied to many tasks, such as text three parts, turn encoder, conversation encoder and classification (Yu and Jiang, 2016), neural machine prediction layer. In addition, the chatting history translation (Ruiter et al., 2019), multi-turn response information of the target user is also applied to our selection (Xu et al., 2020), summarization (Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) a"
2021.findings-emnlp.183,N18-1035,1,0.900347,"conclude will return to a discussion they once entered that both Spread Pattern and Repeated Target User signals help predict re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-"
2021.findings-emnlp.183,P19-1270,1,0.374219,"for Online Conversations via Self-Supervised Learning Lingzhi Wang1,2 , Xingshan Zeng3 , Huang Hu4 , Kam-Fai Wong1,2 , Daxin Jiang4 1 The Chinese University of Hong Kong, Hong Kong, China 2 MoE Key Laboratory of High Confidence Software Technologies, China 3 Huawei Noah’s Ark Lab, Hong Kong, China 4 Microsoft Corporation, Beijing, China 1,2 {lzwang,kfwong}@se.cuhk.edu.hk 3 zeng.xingshan@huawei.com 4 {huahu,djiang}@microsoft.com Abstract target user) will come back to a conversation they once participated in. Nevertheless, the state-of-theIn recent years, world business in online disart work (Zeng et al., 2019) mostly focuses on rich cussions and opinion sharing on social media information in users’ previous chatting history and is booming. Re-entry prediction task is thus ignores the thread pattern information (Backstrom proposed to help people keep track of the diset al., 2013; Tan et al., 2019). To this end, we study cussions which they wish to continue. Neverin re-entry prediction by exploiting the conversatheless, existing works only focus on exploiting chatting history and context information, tion thread pattern to signal whether a user would and ignore the potential useful learning sigcome b"
2021.findings-emnlp.23,S15-2045,0,0.0289652,"Missing"
2021.findings-emnlp.23,S14-2010,0,0.060457,"Missing"
2021.findings-emnlp.23,S16-1081,0,0.0638689,"Missing"
2021.findings-emnlp.23,S12-1051,0,0.0224819,"trix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We average the Spearman’s coefficients among the seven datasets as the final correlation score. White"
2021.findings-emnlp.23,S17-2001,0,0.0115647,"e covariance matrix Cov(E) = (E − m)T (E − m) ∈ Rd×d and U is the corresponding orthogonal matrix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We ave"
2021.findings-emnlp.23,D14-1162,0,0.103087,"Missing"
2021.findings-emnlp.23,D19-1410,0,0.402486,"We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have three main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top and bottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we"
2021.findings-emnlp.23,D19-1059,0,0.0503764,"Missing"
2021.findings-emnlp.23,2020.emnlp-main.124,0,0.136199,"ith less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we investigate PLMs-based unsupervised sentence embeddings from three aspects. First, a standard way of obtaining sentence embedding is to pick the vector of [CLS] token. We explore whether using the hidden vectors of other tokens is beneficial. Second, some works suggest producing sentence embedding from the last layer or the combination of the last t"
2021.findings-emnlp.93,2020.acl-main.728,0,0.0470139,"scriminative settings by a significant margin. 1 2 3 5 H0: a group of women walk down a road H1: what age are the women ? they are teenagers H2: are they all the same race ? most of them look to be Q3: any other people ? 6 4 7 A3-1: only trees A3-2: i see 1 person in the distance A3-3: no, i see one trunk A3-4: no, i see one bench A3-5: no other people A3-6: cannot see A3-7: no i can not see Figure 1: Comparison between different responses when focusing on different visual objects. We see that when the model focuses on wrong visual objects it makes mistakes. (Only the response A3-2 is right.) Agarwal et al., 2020; Chen et al., 2021b; Qi et al., 2020). Specifically, visual dialog, which aims to hold a meaningful conversation (Chen et al., 2021c, 2020b) with a human about a given image, is a challenging task that requires models to locate related visual objects in an image and answer the current question based on the history and the located visual objects. In order to answer the question correctly, we need to accurately locate the question-related visual objects. Most existing methods utilize kinds of attention mechanism (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Guo 1 Int"
2021.findings-emnlp.93,2021.findings-acl.20,1,0.870539,"by a significant margin. 1 2 3 5 H0: a group of women walk down a road H1: what age are the women ? they are teenagers H2: are they all the same race ? most of them look to be Q3: any other people ? 6 4 7 A3-1: only trees A3-2: i see 1 person in the distance A3-3: no, i see one trunk A3-4: no, i see one bench A3-5: no other people A3-6: cannot see A3-7: no i can not see Figure 1: Comparison between different responses when focusing on different visual objects. We see that when the model focuses on wrong visual objects it makes mistakes. (Only the response A3-2 is right.) Agarwal et al., 2020; Chen et al., 2021b; Qi et al., 2020). Specifically, visual dialog, which aims to hold a meaningful conversation (Chen et al., 2021c, 2020b) with a human about a given image, is a challenging task that requires models to locate related visual objects in an image and answer the current question based on the history and the located visual objects. In order to answer the question correctly, we need to accurately locate the question-related visual objects. Most existing methods utilize kinds of attention mechanism (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Guo 1 Introduction et al., 2"
2021.findings-emnlp.93,2021.findings-acl.38,1,0.791815,"by a significant margin. 1 2 3 5 H0: a group of women walk down a road H1: what age are the women ? they are teenagers H2: are they all the same race ? most of them look to be Q3: any other people ? 6 4 7 A3-1: only trees A3-2: i see 1 person in the distance A3-3: no, i see one trunk A3-4: no, i see one bench A3-5: no other people A3-6: cannot see A3-7: no i can not see Figure 1: Comparison between different responses when focusing on different visual objects. We see that when the model focuses on wrong visual objects it makes mistakes. (Only the response A3-2 is right.) Agarwal et al., 2020; Chen et al., 2021b; Qi et al., 2020). Specifically, visual dialog, which aims to hold a meaningful conversation (Chen et al., 2021c, 2020b) with a human about a given image, is a challenging task that requires models to locate related visual objects in an image and answer the current question based on the history and the located visual objects. In order to answer the question correctly, we need to accurately locate the question-related visual objects. Most existing methods utilize kinds of attention mechanism (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Guo 1 Introduction et al., 2"
2021.findings-emnlp.93,2021.findings-acl.105,1,0.866453,"by a significant margin. 1 2 3 5 H0: a group of women walk down a road H1: what age are the women ? they are teenagers H2: are they all the same race ? most of them look to be Q3: any other people ? 6 4 7 A3-1: only trees A3-2: i see 1 person in the distance A3-3: no, i see one trunk A3-4: no, i see one bench A3-5: no other people A3-6: cannot see A3-7: no i can not see Figure 1: Comparison between different responses when focusing on different visual objects. We see that when the model focuses on wrong visual objects it makes mistakes. (Only the response A3-2 is right.) Agarwal et al., 2020; Chen et al., 2021b; Qi et al., 2020). Specifically, visual dialog, which aims to hold a meaningful conversation (Chen et al., 2021c, 2020b) with a human about a given image, is a challenging task that requires models to locate related visual objects in an image and answer the current question based on the history and the located visual objects. In order to answer the question correctly, we need to accurately locate the question-related visual objects. Most existing methods utilize kinds of attention mechanism (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Guo 1 Introduction et al., 2"
2021.findings-emnlp.93,P19-1648,0,0.240771,"ight.) Agarwal et al., 2020; Chen et al., 2021b; Qi et al., 2020). Specifically, visual dialog, which aims to hold a meaningful conversation (Chen et al., 2021c, 2020b) with a human about a given image, is a challenging task that requires models to locate related visual objects in an image and answer the current question based on the history and the located visual objects. In order to answer the question correctly, we need to accurately locate the question-related visual objects. Most existing methods utilize kinds of attention mechanism (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Guo 1 Introduction et al., 2019b) to capture the target visual objects. With the development of deep learning, various ReDAN (Gan et al., 2019) and DMAM (Chen vision-language tasks have been introduced and et al., 2020a) use multi-step reasoning based on attracted widespread attention, such as image cap- dual attention to iteratively update related visual obtioning (Xu et al., 2015; Anderson et al., 2016, jects. DAN (Guo et al., 2019b), MCAN (Agarwal 2018; Cornia et al., 2020; Ghanimifard and Dob- et al., 2020) and LTMI (Nguyen et al., 2020) utilize nik, 2019), visual question answering (Ren"
2021.findings-emnlp.93,W19-8668,0,0.0626108,"Missing"
2021.findings-emnlp.93,2020.emnlp-main.275,1,0.811622,"Missing"
2021.findings-emnlp.93,2020.acl-main.642,0,0.0431042,"en introduced and et al., 2020a) use multi-step reasoning based on attracted widespread attention, such as image cap- dual attention to iteratively update related visual obtioning (Xu et al., 2015; Anderson et al., 2016, jects. DAN (Guo et al., 2019b), MCAN (Agarwal 2018; Cornia et al., 2020; Ghanimifard and Dob- et al., 2020) and LTMI (Nguyen et al., 2020) utilize nik, 2019), visual question answering (Ren et al., multi-head attention mechanisms to manage multi2015a; Gao et al., 2015; Lu et al., 2016; Anderson modal intersection and obtain weight distributions. et al., 2018; Li et al., 2019; Huang et al., 2020) and Moreover, there are some approaches (Zheng et al., visual dialog (Das et al., 2017; Chen et al., 2021a; 2019; Schwartz et al., 2019; Jiang et al., 2020b; ∗ Corresponding author. Guo et al., 2020; Jiang et al., 2020a) using graph1081 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1081–1091 November 7–11, 2021. ©2021 Association for Computational Linguistics based structures to capture related visual objects. FGA (Schwartz et al., 2019) realizes a factor graph attention mechanism, which constructs the graph over all the multi-modal features and estimates their"
2021.findings-emnlp.93,2020.emnlp-main.269,0,0.342543,"RCNN (Ren et al., 2015b). We select µ object proposals for each image, where each object proposal is represented by a 2048-dimension 1084 L = LG + λLKL (G, g), (9) L = LD + λLKL (G, g), (10) L = LG + LD + λLKL (G, g), (11) Model MRR ↑ VisDial v0.9 (val) R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓ NDCG ↑ MRR ↑ VisDial v1.0 (val) R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓ MN (Das et al., 2017) HCIAE (Lu et al., 2017) CorefNMN (Kottur et al., 2018) CoAtt (Wu et al., 2018) RvA (Niu et al., 2019) DVAN (Guo et al., 2019b) Primary (Guo et al., 2019a) ReDAN (Gan et al., 2019) DMRM (Chen et al., 2020a) DAM (Jiang et al., 2020c) VDBERT (Wang et al., 2020) KBGN (Jiang et al., 2020a) 52.59 53.86 53.50 54.11 55.43 55.94 55.96 55.95 - 42.29 44.06 43.66 44.32 45.37 46.58 46.20 46.83 - 62.85 63.55 63.54 63.82 65.27 65.50 66.02 65.43 - 68.88 69.24 69.93 69.75 72.97 71.25 72.43 72.05 - 17.06 16.01 15.69 16.47 10.71 14.79 13.15 13.18 - 51.86 59.70 59.24 60.47 60.93 60.42 47.99 49.07 49.64 49.01 50.02 50.16 50.51 50.05 38.18 39.72 40.09 38.54 40.27 40.15 40.53 40.40 57.54 58.23 59.37 59.82 59.93 60.02 60.84 60.11 64.32 64.73 65.92 66.94 66.78 67.21 67.94 66.82 18.60 18.43 17.86 16.60 17.40 15.19 16.65 17.54 LTMI (Nguyen et al., 2020)† LTMI-LG (Ours) LT"
2021.naacl-industry.18,2020.lrec-1.467,0,0.01609,"hion by fine-tuning the model in one region, and then continue training in another. The actual sequence of how this is conducted is important. We observed that keeping English at the last stage provides the best performance. This is likely because English data (which frequently contains bilingual data through code-switching) covers a large proportion in pre-training corpora, thus serving as an anchor in subsequent training stage to maintain the universal properties of the model. 3.2 Multi-task Learning A total of 79M translation pairs from WikiMatrix (Schwenk et al., 2019) and MultiParaCrawl (Aulamo et al., 2020) data including the languages considered in production are extracted as training data. In addition, we conduct an ablation study on auxiliary task selection by comparing with Masked Language Model (MLM) (Devlin et al., 2018) trained on 370M samples from Wikipedia. The multi-task training alternates between SR and auxiliary tasks according to a set proportion of mini-batches in an epoch. The proportion controls the trade-offs between the tasks, to achieve the desired levels of performance in the system. 3.3 Data Augmentation Native supervised data (m-r pairs) is currently not available for low-"
2021.naacl-industry.18,2021.naacl-main.280,1,0.815328,"Missing"
2021.naacl-industry.18,P19-4007,0,0.137716,"of responses to bias the predictions towards more common ones. Translated responses inherit the penalty score from the corresponding English responses. Using this score in equation 2 we first select top N1 responses, and down-select to top N2 after deduplication using lexical clustering, before presenting to users. Score = φ(mi ) · φK (rk )) + αLMK (rk ) 3 (2) Universal SR Model The universal SR model consists of parallel encoder architecture trained using symmetric loss function 139 similar to the core SR model. We initialize the m-r encoders with InfoXLM (Chi et al., 2020), an XLM-Roberta (Conneau et al., 2019) equivalent multi-lingual model as shown in as Figure 1(a) which creates language-agnostic text representation across 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint train"
2021.naacl-industry.18,D18-1269,0,0.0229146,"φ(mi ) · φK (rk )) + αLMK (rk ) 3 (2) Universal SR Model The universal SR model consists of parallel encoder architecture trained using symmetric loss function 139 similar to the core SR model. We initialize the m-r encoders with InfoXLM (Chi et al., 2020), an XLM-Roberta (Conneau et al., 2019) equivalent multi-lingual model as shown in as Figure 1(a) which creates language-agnostic text representation across 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint training of universal encoders has led to enormous progress on standard benchmarks and industrial applications such as (Ranasinghe and Zampieri, 2020; Gencoglu, 2020). However, privacy policies restrict the data movement across geographic clusters. This prevents the joint training at a single compute cluster. As a result, we trai"
2021.naacl-industry.18,N19-2006,1,0.939083,"quickly respond with a short, generic, To reduce the cost of model management, we and relevant response, without users having to type propose to build a single universal SR model, cain the reply. SR is an increasingly popular feature pable of serving multiple languages and markets. in many commercial applications such as Gmail, To overcome data constraints, we propose to use Outlook, Skype, Facebook Messenger, Microsoft augmentation with machine-translated (MT) data Teams, and Uber (Kannan et al., 2016; Henderson for languages without supervised data. To overet al., 2017a; Shang et al., 2015; Deb et al., 2019; come privacy constraints, we propose a continual Yue Weng, 2019). While the initial versions of learning framework, where the model is trained sethis feature mostly targeted English users, making quentially across regions. To alleviate catastrophic it available in multiple languages and markets is forgetting (French, 1999; McCloskey and Cohen, important not only from the perspective of prod- 1989) in the continual learning process, we reinuct expansion but also from a linguistic inclusivity force the universal properties via multi-task learnpoint of view. ing approach with public task-agnost"
2021.naacl-industry.18,P19-1536,0,0.0543399,"Missing"
2021.naacl-industry.18,P15-1152,0,0.0506031,"Missing"
2021.naacl-industry.18,2020.cl-1.2,0,0.0307627,"Missing"
2021.naacl-industry.18,2021.ccl-1.108,0,0.0388259,"Missing"
2021.naacl-industry.18,W19-4302,0,0.0202868,"e jointly train the (Lample and Conneau, 2019) in continual learning model in EUR for FR, DE, and IT. Next, we move to preserve the universal properties of the model. the model to NAM and continue train with EN, ES, 140 and PT along with auxiliary task. Finally, in LRL, we train the model on machine translated m-r pairs along with original EN data in 2 different ways: (1) jointly train with auxiliary task, or (2) infuse the model with low-resource language adapters. In all stages, we freeze the embedding layer of the encoder during fine-tuning. According to previous studies (Lee et al., 2019; Peters et al., 2019), freezing partial layers can maintain the model quality while reducing training time during fine-tuning. We observed that freezing embedding layer provides a good balance between micro-batch size per GPU (low if no layers are frozen) and learning capacity of the model (low if many layers are frozen). 3.5 Universal Model Graph for Serving For deployment, we create a composite graph with pre-computed response vectors of all languages embedded into the main model. A separate language identifier switches the prediction vectors to the predicted language of the input at run-time. Besides, several a"
2021.naacl-industry.18,2020.emnlp-main.470,0,0.0164283,"ss 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint training of universal encoders has led to enormous progress on standard benchmarks and industrial applications such as (Ranasinghe and Zampieri, 2020; Gencoglu, 2020). However, privacy policies restrict the data movement across geographic clusters. This prevents the joint training at a single compute cluster. As a result, we train the model sequentially in a continual learning fashion by fine-tuning the model in one region, and then continue training in another. The actual sequence of how this is conducted is important. We observed that keeping English at the last stage provides the best performance. This is likely because English data (which frequently contains bilingual data through code-switching) covers a large proportion in pre-traini"
2021.naacl-main.465,P18-1073,0,0.0869171,"f the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5822–5834 June 6–11, 2021. ©2021 Association for Computational Linguistics more languages, we assume that neither translators nor parallel corpora are available in this work. In this paper, to adapt the translation approaches in our zero-resource scenario, we naturally propose to replace the full-supervised machine translator with unsupervised bilingual lexicon induction (BLI) for word-level translation. Specifically, as in prior works (Lample et al., 2018b; Artetxe et al., 2018), a BLI model is first trained on non-parallel bilingual corpora. Then, via bilingual word alignments in BLI, we map the training questions in source language into those in target languages to obtain augmented multilingual training data. Consequently, even simply learning a KGQA model on the augmented data can circumvent language inconsistency between training and inference and thus bridge the performance gap in zero-shot crosslingual transfer. To explain why BLI is competent, it is observed that KGQA mainly involves phraselevel semantics (Berant et al., 2013). Compared to other tasks dependin"
2021.naacl-main.465,P19-1494,0,0.254485,"stion language, leading to superior performance on multilingual KGQA. Experiments conducted on two multilingual KGQA datasets with 11 zero-resource languages verify the effectiveness of our approach. 2 KGQA Task Definition We give a background of monolingual KGQA, followed by multilingual KGQA and its data format. is then transformed into a SPARQL query, and the answer is derived by executing the SPARQL query on G. An example is shown in Figure 1: the question in the bottom, intermediate logical form in the upper right and the corresponding SPARQL query in the top. Following Maheshwari et al. (2019), we take a restricted subset of λ-calculus – query graph, as the intermediate logical form. Typically, a query graph consists of four types of nodes: grounded entity(s) (in rounded rectangle), existential variable(s) “?y” (in circle), a lambda variable “?x” (in shaded circle), and an aggregation function (in diamond). Considering entity-linking is a standalone system and there are many tools, we assume grounded entities in a question are given. This avoids uncertainty caused by entity-linking, and facilitates us to focus on the query graph construction process. Multilingual KGQA. We focus on"
2021.naacl-main.465,D13-1160,0,0.287426,"ior works (Lample et al., 2018b; Artetxe et al., 2018), a BLI model is first trained on non-parallel bilingual corpora. Then, via bilingual word alignments in BLI, we map the training questions in source language into those in target languages to obtain augmented multilingual training data. Consequently, even simply learning a KGQA model on the augmented data can circumvent language inconsistency between training and inference and thus bridge the performance gap in zero-shot crosslingual transfer. To explain why BLI is competent, it is observed that KGQA mainly involves phraselevel semantics (Berant et al., 2013). Compared to other tasks depending on sentence-level contextualization, KGQA is insensitive to long-term dependency but benefits from the language consistency. Moreover, we propose an adversarial strategy to mitigate the syntax-disorder caused by BLI. Specifically, we present a discriminator on top of the encoder, which is trained to distinguish whether the input is a grammatical question in source language or a BLI-translated one in target language. Meanwhile, jointly with KGQA goal, the encoder is fine-tuned to fool the discriminator so that the questions’ representations are both language-"
2021.naacl-main.465,D14-1067,0,0.0328554,"t Case Study Related Work We take several examples of inferential chain rank- There are mainly two categories of approaches ing to show how our approach works. We use to handle monolingual question answering over t-SNE (Maaten and Hinton, 2008) to map the knowledge graph (KGQA) task. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in l"
2021.naacl-main.465,N19-1299,0,0.204064,"stion language, leading to superior performance on multilingual KGQA. Experiments conducted on two multilingual KGQA datasets with 11 zero-resource languages verify the effectiveness of our approach. 2 KGQA Task Definition We give a background of monolingual KGQA, followed by multilingual KGQA and its data format. is then transformed into a SPARQL query, and the answer is derived by executing the SPARQL query on G. An example is shown in Figure 1: the question in the bottom, intermediate logical form in the upper right and the corresponding SPARQL query in the top. Following Maheshwari et al. (2019), we take a restricted subset of λ-calculus – query graph, as the intermediate logical form. Typically, a query graph consists of four types of nodes: grounded entity(s) (in rounded rectangle), existential variable(s) “?y” (in circle), a lambda variable “?x” (in shaded circle), and an aggregation function (in diamond). Considering entity-linking is a standalone system and there are many tools, we assume grounded entities in a question are given. This avoids uncertainty caused by entity-linking, and facilitates us to focus on the query graph construction process. Multilingual KGQA. We focus on"
2021.naacl-main.465,2021.naacl-main.280,0,0.0773629,"Missing"
2021.naacl-main.465,2020.acl-main.747,0,0.550508,"ence on multilingual questions with resource language, while need to answer multhe only access to training data and associated KG tilingual questions without any labeled data in in a high-resource language (e.g., English). target languages. A straightforward approach Providing the success of pre-trained monolinis resorting to pre-trained multilingual modgual encoders (Peters et al., 2018; Liu et al., 2019), els (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA some works (e.g., mBERT (Devlin et al., 2019) and performance between source and target lanXLM-R (Conneau et al., 2020)) pre-train a Transguages. In this paper, we exploit unsupervised former encoder (Vaswani et al., 2017) on largebilingual lexicon induction (BLI) to map trainscale non-parallel multilingual corpora in a selfing questions in source language into those supervised manner. Then given an NLP task, a in target language as augmented training data, general paradigm for zero-shot cross-lingual transwhich circumvents language inconsistency befer is to fine-tune a pre-trained multilingual encoder tween training and inference. Furthermore, we propose an adversarial learning strategy to alon the data in a"
2021.naacl-main.465,D19-1169,0,0.0154748,"rks (Conneau et al., 2020). 2007) and Freebase (Bollacker et al., 2008), quesTo bridge the gap, translation approaches are tion answering over knowledge graph (KGQA) proven effective on multilingual benchmarks (Hu has become a crucial natural language processing et al., 2020; Liang et al., 2020). As a way of data (NLP) task to answer factoid questions. It has been augmentation, they perform source-to-target transintegrated into real-world applications like search lation to obtain multilingual training data. Further engines and personal assistants, so it attracts more with advanced techniques (Cui et al., 2019; Fang attention from both academia and industry (Liang et al., 2020), they achieve state-of-the-art effecet al., 2017; Hu et al., 2018; Shen et al., 2019). tiveness. But these approaches rely heavily on a Recently, a rising demand of KGQA systems is well-performing translator. The translator is not to answer the multilingual questions, motivating us always available especially for a minority language ∗ since its training requires a large volume of parallel Work is done during internship at Microsoft. † Corresponding authors. bilingual corpus. Therefore, to be applicable to 5822 Proceedings of"
2021.naacl-main.465,N19-1423,0,0.665454,"developed to is, we can only access training data in a highperform inference on multilingual questions with resource language, while need to answer multhe only access to training data and associated KG tilingual questions without any labeled data in in a high-resource language (e.g., English). target languages. A straightforward approach Providing the success of pre-trained monolinis resorting to pre-trained multilingual modgual encoders (Peters et al., 2018; Liu et al., 2019), els (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA some works (e.g., mBERT (Devlin et al., 2019) and performance between source and target lanXLM-R (Conneau et al., 2020)) pre-train a Transguages. In this paper, we exploit unsupervised former encoder (Vaswani et al., 2017) on largebilingual lexicon induction (BLI) to map trainscale non-parallel multilingual corpora in a selfing questions in source language into those supervised manner. Then given an NLP task, a in target language as augmented training data, general paradigm for zero-shot cross-lingual transwhich circumvents language inconsistency befer is to fine-tune a pre-trained multilingual encoder tween training and inference. Furth"
2021.naacl-main.465,P16-1004,0,0.0210446,"to a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to the zero-shot transfer scenario. So we adopt"
2021.naacl-main.465,P18-1068,0,0.0126621,"a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to the zero-shot transfer scenario. So we adopt the pipeline by Maheshwari et al. (2019) f"
2021.naacl-main.465,P15-1026,0,0.0229185,"k We take several examples of inferential chain rank- There are mainly two categories of approaches ing to show how our approach works. We use to handle monolingual question answering over t-SNE (Maaten and Hinton, 2008) to map the knowledge graph (KGQA) task. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second ca"
2021.naacl-main.465,E14-1049,0,0.0459483,"ly considers languageagnostic representations but also aims at making the model insensitive to syntax-disorder and thus competent in zero-resource scenario. Given task-specific data in a source language, cross-lingual models are trained to perform inference in target languages in a low- or zero-resource scenario. Typically, cross-lingual models are proposed in two paradigms. 1) Universal encodingbased paradigm represents multilingual natural language text into language-agnostic embeddings the same semantic space. Early works focus on aligning multilingual word embedding (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xu et al., 2018), while recent efforts are mainly made on large-scale pre-trained multilingual encoder, such as mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), Unicoder (Huang et al., 2019a), XLM- 6 Conclusion R (Conneau et al., 2020), InfoXLM (Chi et al., 2020), and ALM (Yang et al., 2020). They can We propose a novel approach for zero-shot crossperform zero-shot cross-lingual transfer by train- lingual transfer in multilingual KGQA, which auging in the source language while directly inference ments training data by bilingual lexicon induction, in target language. 2) translatio"
2021.naacl-main.465,P17-1021,0,0.0195586,"rank- There are mainly two categories of approaches ing to show how our approach works. We use to handle monolingual question answering over t-SNE (Maaten and Hinton, 2008) to map the knowledge graph (KGQA) task. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution"
2021.naacl-main.465,D18-1234,0,0.0119825,"g over knowledge graph (KGQA) proven effective on multilingual benchmarks (Hu has become a crucial natural language processing et al., 2020; Liang et al., 2020). As a way of data (NLP) task to answer factoid questions. It has been augmentation, they perform source-to-target transintegrated into real-world applications like search lation to obtain multilingual training data. Further engines and personal assistants, so it attracts more with advanced techniques (Cui et al., 2019; Fang attention from both academia and industry (Liang et al., 2020), they achieve state-of-the-art effecet al., 2017; Hu et al., 2018; Shen et al., 2019). tiveness. But these approaches rely heavily on a Recently, a rising demand of KGQA systems is well-performing translator. The translator is not to answer the multilingual questions, motivating us always available especially for a minority language ∗ since its training requires a large volume of parallel Work is done during internship at Microsoft. † Corresponding authors. bilingual corpus. Therefore, to be applicable to 5822 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages"
2021.naacl-main.465,D19-1252,1,0.917197,"stion language, leading to superior performance on multilingual KGQA. Experiments conducted on two multilingual KGQA datasets with 11 zero-resource languages verify the effectiveness of our approach. 2 KGQA Task Definition We give a background of monolingual KGQA, followed by multilingual KGQA and its data format. is then transformed into a SPARQL query, and the answer is derived by executing the SPARQL query on G. An example is shown in Figure 1: the question in the bottom, intermediate logical form in the upper right and the corresponding SPARQL query in the top. Following Maheshwari et al. (2019), we take a restricted subset of λ-calculus – query graph, as the intermediate logical form. Typically, a query graph consists of four types of nodes: grounded entity(s) (in rounded rectangle), existential variable(s) “?y” (in circle), a lambda variable “?x” (in shaded circle), and an aggregation function (in diamond). Considering entity-linking is a standalone system and there are many tools, we assume grounded entities in a question are given. This avoids uncertainty caused by entity-linking, and facilitates us to focus on the query graph construction process. Multilingual KGQA. We focus on"
2021.naacl-main.465,P19-1399,0,0.298266,"stion language, leading to superior performance on multilingual KGQA. Experiments conducted on two multilingual KGQA datasets with 11 zero-resource languages verify the effectiveness of our approach. 2 KGQA Task Definition We give a background of monolingual KGQA, followed by multilingual KGQA and its data format. is then transformed into a SPARQL query, and the answer is derived by executing the SPARQL query on G. An example is shown in Figure 1: the question in the bottom, intermediate logical form in the upper right and the corresponding SPARQL query in the top. Following Maheshwari et al. (2019), we take a restricted subset of λ-calculus – query graph, as the intermediate logical form. Typically, a query graph consists of four types of nodes: grounded entity(s) (in rounded rectangle), existential variable(s) “?y” (in circle), a lambda variable “?x” (in shaded circle), and an aggregation function (in diamond). Considering entity-linking is a standalone system and there are many tools, we assume grounded entities in a question are given. This avoids uncertainty caused by entity-linking, and facilitates us to focus on the query graph construction process. Multilingual KGQA. We focus on"
2021.naacl-main.465,N19-1383,0,0.119541,"on several cross-lingual benchmarks. In contrast, we consider a zero-resource scenario where translators are unavailable and we thus resort to unsupervised BLI in light of KGQA’s characteristics. As a branch of universal encoding at word level, bilingual lexicon induction (BLI) (a.k.a crosslingual word embedding – CLWE) is learned to align bilingual word embeddings in the same space, where the embeddings are pre-trained on monolingual corpora and the alignment is trained in either a (semi-)supervised or unsupervised manner (Smith et al., 2017; Lample et al., 2018b; Artetxe et al., 2018, 2019; Huang et al., 2019b; Patra et al., 2019; Karan et al., 2020; Zhao et al., 2020; Ren et al., 2020). To alleviate “hubness” problem (Dinu and Baroni, 2015) in BLI, alternatives of the distance measurement are proposed to substitute nearest neighbor (NN) during the alignment, such as inverted-softmax (Smith et al., 2017) and CSLS (Lample et al., 2018b). In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual vari"
2021.naacl-main.465,N16-2016,0,0.0181816,"xamples of inferential chain rank- There are mainly two categories of approaches ing to show how our approach works. We use to handle monolingual question answering over t-SNE (Maaten and Hinton, 2008) to map the knowledge graph (KGQA) task. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to ge"
2021.naacl-main.465,P16-1002,0,0.0240691,"on embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to th"
2021.naacl-main.465,2020.acl-main.618,0,0.0202061,"ontrast, we consider a zero-resource scenario where translators are unavailable and we thus resort to unsupervised BLI in light of KGQA’s characteristics. As a branch of universal encoding at word level, bilingual lexicon induction (BLI) (a.k.a crosslingual word embedding – CLWE) is learned to align bilingual word embeddings in the same space, where the embeddings are pre-trained on monolingual corpora and the alignment is trained in either a (semi-)supervised or unsupervised manner (Smith et al., 2017; Lample et al., 2018b; Artetxe et al., 2018, 2019; Huang et al., 2019b; Patra et al., 2019; Karan et al., 2020; Zhao et al., 2020; Ren et al., 2020). To alleviate “hubness” problem (Dinu and Baroni, 2015) in BLI, alternatives of the distance measurement are proposed to substitute nearest neighbor (NN) during the alignment, such as inverted-softmax (Smith et al., 2017) and CSLS (Lample et al., 2018b). In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual variant of copy mechanism in summarization (Z"
2021.naacl-main.465,D17-1302,0,0.0148607,"In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual variant of copy mechanism in summarization (Zhu et al., 2020). Moreover, adversarial training is usually integrated into cross-lingual models for languageagnostic representation learning, such as unsupervised BLI (Lample et al., 2018b; Zhang et al., 2017), unsupervised translation (Lample et al., 2018a), cross-Lingual sequence labeling (Kim et al., 2017; Huang et al., 2019c) and cross-Lingual classification (Dong et al., 2020). In contrast, our adversarial strategy not only considers languageagnostic representations but also aims at making the model insensitive to syntax-disorder and thus competent in zero-resource scenario. Given task-specific data in a source language, cross-lingual models are trained to perform inference in target languages in a low- or zero-resource scenario. Typically, cross-lingual models are proposed in two paradigms. 1) Universal encodingbased paradigm represents multilingual natural language text into language-agnos"
2021.naacl-main.465,P17-1003,0,0.0165974,"ed approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to the zero-shot transfer scenario. So we adopt the pipeline by Mah"
2021.naacl-main.465,2021.ccl-1.108,0,0.0610538,"Missing"
2021.naacl-main.465,P19-1018,0,0.264913,"stion language, leading to superior performance on multilingual KGQA. Experiments conducted on two multilingual KGQA datasets with 11 zero-resource languages verify the effectiveness of our approach. 2 KGQA Task Definition We give a background of monolingual KGQA, followed by multilingual KGQA and its data format. is then transformed into a SPARQL query, and the answer is derived by executing the SPARQL query on G. An example is shown in Figure 1: the question in the bottom, intermediate logical form in the upper right and the corresponding SPARQL query in the top. Following Maheshwari et al. (2019), we take a restricted subset of λ-calculus – query graph, as the intermediate logical form. Typically, a query graph consists of four types of nodes: grounded entity(s) (in rounded rectangle), existential variable(s) “?y” (in circle), a lambda variable “?x” (in shaded circle), and an aggregation function (in diamond). Considering entity-linking is a standalone system and there are many tools, we assume grounded entities in a question are given. This avoids uncertainty caused by entity-linking, and facilitates us to focus on the query graph construction process. Multilingual KGQA. We focus on"
2021.naacl-main.465,N18-1202,0,0.0156265,"t a zero-shot cross-lingual multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That transfer setting – a KGQA model is developed to is, we can only access training data in a highperform inference on multilingual questions with resource language, while need to answer multhe only access to training data and associated KG tilingual questions without any labeled data in in a high-resource language (e.g., English). target languages. A straightforward approach Providing the success of pre-trained monolinis resorting to pre-trained multilingual modgual encoders (Peters et al., 2018; Liu et al., 2019), els (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA some works (e.g., mBERT (Devlin et al., 2019) and performance between source and target lanXLM-R (Conneau et al., 2020)) pre-train a Transguages. In this paper, we exploit unsupervised former encoder (Vaswani et al., 2017) on largebilingual lexicon induction (BLI) to map trainscale non-parallel multilingual corpora in a selfing questions in source language into those supervised manner. Then given an NLP task, a in target language as augmented training data, general paradigm for zero-"
2021.naacl-main.465,2020.acl-main.318,0,0.0551303,"Missing"
2021.naacl-main.465,D19-1248,1,0.927146,"graph (KGQA) proven effective on multilingual benchmarks (Hu has become a crucial natural language processing et al., 2020; Liang et al., 2020). As a way of data (NLP) task to answer factoid questions. It has been augmentation, they perform source-to-target transintegrated into real-world applications like search lation to obtain multilingual training data. Further engines and personal assistants, so it attracts more with advanced techniques (Cui et al., 2019; Fang attention from both academia and industry (Liang et al., 2020), they achieve state-of-the-art effecet al., 2017; Hu et al., 2018; Shen et al., 2019). tiveness. But these approaches rely heavily on a Recently, a rising demand of KGQA systems is well-performing translator. The translator is not to answer the multilingual questions, motivating us always available especially for a minority language ∗ since its training requires a large volume of parallel Work is done during internship at Microsoft. † Corresponding authors. bilingual corpus. Therefore, to be applicable to 5822 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5822–5834 June 6–11"
2021.naacl-main.465,W16-1403,0,0.0181295,"g logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to the zero-shot transfer scenario. So we adopt the pipeline by Maheshwari et al. (2019) for monolingual scenario as our base model but update the encoders with the Transformer (Vaswani et al., 2017) to strengthen their expressive power and facilitate recent pre-trained multilingual initializations. cross-lingual performance gap in the first paradigm, which leads to state-of-the-art results on several cross-lingual benchmarks. In contrast, we consider a zero-resource scenario where translator"
2021.naacl-main.465,P16-1127,0,0.0175864,"stion-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while are inapplicable to the zero-shot transfe"
2021.naacl-main.465,P17-1179,0,0.0283503,") during the alignment, such as inverted-softmax (Smith et al., 2017) and CSLS (Lample et al., 2018b). In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual variant of copy mechanism in summarization (Zhu et al., 2020). Moreover, adversarial training is usually integrated into cross-lingual models for languageagnostic representation learning, such as unsupervised BLI (Lample et al., 2018b; Zhang et al., 2017), unsupervised translation (Lample et al., 2018a), cross-Lingual sequence labeling (Kim et al., 2017; Huang et al., 2019c) and cross-Lingual classification (Dong et al., 2020). In contrast, our adversarial strategy not only considers languageagnostic representations but also aims at making the model insensitive to syntax-disorder and thus competent in zero-resource scenario. Given task-specific data in a source language, cross-lingual models are trained to perform inference in target languages in a low- or zero-resource scenario. Typically, cross-lingual models are proposed in two paradigms. 1"
2021.naacl-main.465,2020.acl-main.274,0,0.0368059,"a zero-resource scenario where translators are unavailable and we thus resort to unsupervised BLI in light of KGQA’s characteristics. As a branch of universal encoding at word level, bilingual lexicon induction (BLI) (a.k.a crosslingual word embedding – CLWE) is learned to align bilingual word embeddings in the same space, where the embeddings are pre-trained on monolingual corpora and the alignment is trained in either a (semi-)supervised or unsupervised manner (Smith et al., 2017; Lample et al., 2018b; Artetxe et al., 2018, 2019; Huang et al., 2019b; Patra et al., 2019; Karan et al., 2020; Zhao et al., 2020; Ren et al., 2020). To alleviate “hubness” problem (Dinu and Baroni, 2015) in BLI, alternatives of the distance measurement are proposed to substitute nearest neighbor (NN) during the alignment, such as inverted-softmax (Smith et al., 2017) and CSLS (Lample et al., 2018b). In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual variant of copy mechanism in summarization (Zhu et al., 2020). M"
2021.naacl-main.465,2020.acl-main.121,0,0.0156668,"0; Zhao et al., 2020; Ren et al., 2020). To alleviate “hubness” problem (Dinu and Baroni, 2015) in BLI, alternatives of the distance measurement are proposed to substitute nearest neighbor (NN) during the alignment, such as inverted-softmax (Smith et al., 2017) and CSLS (Lample et al., 2018b). In addition to building bilingual dictionary via word-level translation, a well-trained BLI model can serve as a weak baseline of sentence-level translation (Lample et al., 2018a), a seed model for unsupervised translation (Lample et al., 2018a) or a bilingual variant of copy mechanism in summarization (Zhu et al., 2020). Moreover, adversarial training is usually integrated into cross-lingual models for languageagnostic representation learning, such as unsupervised BLI (Lample et al., 2018b; Zhang et al., 2017), unsupervised translation (Lample et al., 2018a), cross-Lingual sequence labeling (Kim et al., 2017; Huang et al., 2019c) and cross-Lingual classification (Dong et al., 2020). In contrast, our adversarial strategy not only considers languageagnostic representations but also aims at making the model insensitive to syntax-disorder and thus competent in zero-resource scenario. Given task-specific data in"
2021.naacl-main.465,P16-1220,0,0.0230118,"nferential chain rank- There are mainly two categories of approaches ing to show how our approach works. We use to handle monolingual question answering over t-SNE (Maaten and Hinton, 2008) to map the knowledge graph (KGQA) task. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query grap"
2021.naacl-main.465,D18-1268,0,0.020597,"ostic representations but also aims at making the model insensitive to syntax-disorder and thus competent in zero-resource scenario. Given task-specific data in a source language, cross-lingual models are trained to perform inference in target languages in a low- or zero-resource scenario. Typically, cross-lingual models are proposed in two paradigms. 1) Universal encodingbased paradigm represents multilingual natural language text into language-agnostic embeddings the same semantic space. Early works focus on aligning multilingual word embedding (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xu et al., 2018), while recent efforts are mainly made on large-scale pre-trained multilingual encoder, such as mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), Unicoder (Huang et al., 2019a), XLM- 6 Conclusion R (Conneau et al., 2020), InfoXLM (Chi et al., 2020), and ALM (Yang et al., 2020). They can We propose a novel approach for zero-shot crossperform zero-shot cross-lingual transfer by train- lingual transfer in multilingual KGQA, which auging in the source language while directly inference ments training data by bilingual lexicon induction, in target language. 2) translation-based paradigm a"
2021.naacl-main.465,P15-1128,0,0.036153,"ask. (1) Information embedding of a question-chain pair into a two- retrieval-based approaches align a question with dimensional data point. A question in a specific its answer candidates in the same semantic space, 5829 where the candidates usually stem from KG neighbors of the topic entity detected in the questions (Bordes et al., 2014b,a; Dong et al., 2015; Jain, 2016; Xu et al., 2016; Hao et al., 2017; Chen et al., 2019). (2) Semantic parsing-based approaches first translate a question into the corresponding logical form, e.g., program (Guo et al., 2018; Shen et al., 2019) or query graph (Yih et al., 2015; Jia and Liang, 2016; Xiao et al., 2016; Dong and Lapata, 2016; Liang et al., 2017; Dong and Lapata, 2018; Maheshwari et al., 2019), and then execute the logical form over KG to derive the final answer. Note a logical form is usually composed of a series of grammars or operators pre-defined by experts. This paper is in line with the second category to generate query graph for KG execution. To the best of our knowledge, there are only few works targeting multilingual KGQA (Hakimov et al., 2017; Veyseh, 2016), which rely on extensive multilingual training data with hand-crafted features while a"
D19-1248,P13-2009,0,0.0266995,"cal form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and"
D19-1248,Q13-1005,0,0.0265942,"s the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example,"
D19-1248,P17-1005,0,0.0132889,"2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. D"
D19-1248,P16-1004,0,0.197334,"n intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., sema"
D19-1248,P18-1068,0,0.079013,"seen the development of AIdriven personal assistants (e.g., Siri, Alexa, Cortana, and Google Now) that often need to answer factorial questions. Meanwhile, large-scale knowledge base (KB) like DBPedia (Auer et al., 2007) or Freebase (Bollacker et al., 2008) has been built to store world’s facts in a structure database, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entit"
D19-1248,P16-1002,0,0.121012,"distribution over V to score candidates2 . Then, a FFN(·) or a pointer network (Vinyals et al., 2015) is utilized to predict instantiation for entry semantic category (i.e., e, p, tp or u num in V(vec) ) if it is necessary. (n) pj = softmax(sTj W (n) H:,1:n−1 ), (5) where H:,1:n−1 is contextual embedding of tokens in the question except [CTX], W (e) and W (n) are weights of pointer-network for (e) (n) entity and number, pj , pj ∈ Rn−1 are the resulting distributions over positions of input question, and n is the length of the question. The pointer network is also used for semantic parsing in (Jia and Liang, 2016), where the pointer aims at copying out-of-vocabulary words from a question over small-scale KB. Different from that, the pointer used here aims at locating the targeted entity and number in a question, which has two advantages. First, it handles the coreference problem by considering the context of entity mentions in the question. Second, it solves the problem caused by huge entity vocabulary, which reduces the size of decoding vocabulary from several million (i.e., the number of entities in KB) to several dozen (i.e., the length of the question). 3.2.3 Entity Detection and Linking • For pred"
D19-1248,D17-1160,0,0.0176575,"s, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based"
D19-1248,D11-1140,0,0.031991,"n example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng"
D19-1248,P16-1057,0,0.0379427,"Missing"
D19-1248,P16-1138,0,0.0291545,"while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis a"
D19-1248,D16-1147,0,0.0443322,"Missing"
D19-1248,P17-1105,0,0.0137173,"7; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a quest"
D19-1248,Q14-1030,0,0.103666,"Missing"
D19-1248,P07-1121,0,0.0543536,"rold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (R"
D19-1248,1983.tc-1.13,0,0.382158,"Missing"
D19-1248,P16-1127,0,0.19754,"e, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the questi"
D19-1248,P16-1220,0,0.030023,"to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedings of the 2019 Conference on Empirical M"
D19-1248,P15-1128,0,0.0730713,"d then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedin"
D19-1248,P17-1041,0,0.014099,"., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. Dong and Lapata (2018) d"
D19-1248,D07-1071,0,0.0686871,"uestion “Which sexes do King Harold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syn"
D19-1248,P09-1110,0,0.0855188,"Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishna"
D19-1252,W18-6317,0,0.0174688,"sentences as a sequence and input it to Unicoder. The representation of the first token in the final layer will be used for the paraphrase classification task. This procedure is illustrated in Figure 1.b. We created the cross-lingual paraphrase classification dataset from machine translation dataset. Each bilingual sentence pair (X, Y ) servers as a positive sample. For negative samples, the most straight forward method is to replace Y to a ran2487 dom sampled sentence from target language. But this will make the classification task too easy. So we introduce the hard negative samples followed Guo et al. (2018). First, we train a light-weight paraphrase model with random negative samples. Then we use this model to select sentence with high similarity score to X but doesn’t equal to Y as hard negative samples. We choose DAN (Iyyer et al., 2015) as the light model. We create positive and negative samples in 1:1. Cross-lingual Masked Language Model Previous successful pre-training language model (Devlin et al., 2018; Radford et al., 2018) is conducted on document-level corpus rather than sentence-level corpus. The language model perplexity on document also is much lower than sentence (Peters et al., 20"
D19-1252,P15-1162,0,0.0938242,"Missing"
D19-1252,P07-2045,0,0.0085558,"inese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,0"
D19-1252,W08-0336,0,0.0235743,"Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,000. Pre-training deta"
D19-1252,D18-1269,0,0.21585,"corpus in which each sentence and its translation is well aligned. We construct cross-lingual document by replacing the sentences with even index to its translation as illustrated in Figure 1.c. We truncate the cross-lingual document by 256 sequence length and feed it to Unicoder for masked language modeling. Multi-language Fine-tuning A typical setting of cross-lingual language understanding is only one language has training data, but the test is conducted on other languages. We denote the language has training data as source language, and other languages as target languages. A scalable way (Conneau et al., 2018) to address this problem is through Cross-lingual TEST, in which a pre-trained encoder is trained on data in source language and directly evaluated on data in target languages. There are two other machine translation methods that make training and test belong to the same language. TRANSLATE-TRAIN translates the source language training data to a target language and fine-tunes on this pseudo training data. TRANSLATE-TEST fine-tunes on source language training data, but translates the target language test data to source language and test on it. Inspired by multi-task learning (Liu et al., 2018,"
D19-1252,eisele-chen-2010-multiun,0,0.0200481,"ion, we describe the data processing and training details. Then we compare the Unicoder with the current state of the art approaches on two tasks: XNLI and XQA. 2488 Data Processing Our model is pre-trained on 15 languages, including English(en), French(fr), Spanish(es), German(de), Greek(el), Bulgarian(bg), Russian(ru), Turkish(tr), Arabic(ar), Vietnamese(vi), Thai(th), Chinese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M"
D19-1252,N18-1202,0,0.232254,"rman) is obtained. In short, our contributions are 4-fold. First, 3 new cross-lingual pre-training tasks are proposed, which can help to learn a better languageindependent encoder. Second, a cross-lingual question answering (XQA) dataset is built, which can be used as a new cross-lingual benchmark dataset. Third, we verify that by fine-tuning multiple languages together, significant improvements can be obtained. Fourth, on the XNLI dataset, new state-of-the-art results are achieved. Related work Monolingual Pre-training Recently, pretraining an encoder by language model (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2018) and machine translation (McCann et al., 2017) have shown significant improvement on various natural language understanding (NLU) tasks, like tasks in GLUE (Wang et al., 2018). The application scheme is to fine-tune the pre-trained encoder on single sentence classification task or sequential labeling task. If the tasks have multiple inputs, just concatenate them into one sentence. This approach enables one model to be generalized to different language understanding tasks. Our approach also is contextual pre-training so it could been applied to various NLU tasks. Cross-lin"
D19-1252,P16-1162,0,0.0211415,"Keeping the same setting, XLM proposed a new task TLM, which uses a concatenation of the parallel sentences into one sample for masked language modeling. Besides these two tasks, we proposed three new cross-lingual pre-training tasks for building a better language-independent encoder. Approach This section will describe details of Unicoder, including tasks used in the pre-training procedure and its fine-tuning strategy. Model Structure Unicoder follows the network structure of XLM (Lample and Conneau, 2019). A shared vocabulary is constructed by running the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016) on corpus of all languages. We also down sample the rich-resource languages corpus, to prevent words of target languages from being split too much at the character level. Pre-training Tasks in Unicoder Both masked language model and translation language model are used in Unicoder by default, as they have shown strong performance in XLM. Motivated by Liu et al. (2019), which shows that pre-trained models can be further improved by involving more tasks in pre-training, we introduce three new cross-lingual tasks in Unicoder. All training data for these three tasks are acquired from the existing"
D19-1252,W18-5446,0,0.0675618,"Missing"
D19-1252,D19-1077,0,0.089503,"Missing"
D19-1252,N15-1104,0,0.121749,"Missing"
D19-3028,W09-1119,0,0.0970197,"that results on the development sets are reported, since GLUE does not distribute labels for the test sets. Model Zoo (Existing Model JSON files) various tasks, with productivity greatly improved. Model Architecture Building 4.1 Model Building Blocks We evaluated NeuronBlocks on CoNLL2003 (Sang and Meulder, 2003) English NER dataset, following most works on the same task. This dataset includes four types of named entities, namely, PERSON, LOCATION, ORGANIZATION, and MISC. We adopted BIOES tagging scheme instead of IOB, as many previous works indicated meaningful improvement with BIOES scheme (Ratinov and Roth, 2009). Table 2 shows the results on CoNLL-2003 Englist testb dataset, with 12 different combinations of network layers/blocks, such as word/character embedding, CNN/LSTM and CRF. The results suggest that the flexible combination of layers/blocks in NeuJSON model Windows/ Linux CPU/GPU PAI Model Training JSON model + Model weights Windows/ Linux CPU/GPU Sequence Labeling Model Inference Figure 3: The workflow of NeuronBlocks. NLP tasks on public data sets including CoNLL2003 (Sang and Meulder, 2003), GLUE benchmark (Wang et al., 2019), and WikiQA corpus (Yang et al., 2015). The experimental results"
D19-3028,Q16-1026,0,0.0612009,"Missing"
D19-3028,N19-1423,0,0.0354506,"ows the results, where AUC is used as the evaluation criteria and Queries per Second (QPS) is used to measure inference speed. By leveraging knowledge distillation, the student model by NeuronBlocks managed to get 24.8 times inference speedup with only small performance regression compared with BERTbase 6 fine-tuned classifier. • Multi-task learning (MTL). In MTL, multiple related tasks are jointly trained so that knowledge learned in one task can benefit other tasks. • Pre-training and fine-tuning. Deep pre-training models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019) are new directions in NLP. • Sequence generation task. Sequence generation is widely used in NLP fields such as machine translation (Bahdanau et al., 2015), text summarization (See et al., 2017), and dialogue systems (Wen et al., 2015). • AutoML (Elsken et al., 2019). NeuronBlocks facilitates users to build models on top of Model Zoo and Block Zoo. With the integration of AutoML, the toolkit can further support automatic model architecture design. 4.4 WikiQA The WikiQA corpus (Yang et al., 2015) is a publicly available dataset for open-domain question answering. This dataset contains 3,047 qu"
D19-3028,W03-0419,0,0.329955,"Missing"
D19-3028,P17-1099,0,0.0152646,"managed to get 24.8 times inference speedup with only small performance regression compared with BERTbase 6 fine-tuned classifier. • Multi-task learning (MTL). In MTL, multiple related tasks are jointly trained so that knowledge learned in one task can benefit other tasks. • Pre-training and fine-tuning. Deep pre-training models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019) are new directions in NLP. • Sequence generation task. Sequence generation is widely used in NLP fields such as machine translation (Bahdanau et al., 2015), text summarization (See et al., 2017), and dialogue systems (Wen et al., 2015). • AutoML (Elsken et al., 2019). NeuronBlocks facilitates users to build models on top of Model Zoo and Block Zoo. With the integration of AutoML, the toolkit can further support automatic model architecture design. 4.4 WikiQA The WikiQA corpus (Yang et al., 2015) is a publicly available dataset for open-domain question answering. This dataset contains 3,047 questions from Bing query logs, each associated with some candidate answer sentences from Wikipedia. We conducted experiments on WikiQA dataset using CNN, BiLSTM, and Attention based models. The re"
D19-3028,W18-2501,0,0.0455206,"Missing"
D19-3028,P17-4012,0,0.0977379,"Missing"
D19-3028,N16-1030,0,0.143926,"Missing"
D19-3028,D15-1199,0,0.0262958,"dup with only small performance regression compared with BERTbase 6 fine-tuned classifier. • Multi-task learning (MTL). In MTL, multiple related tasks are jointly trained so that knowledge learned in one task can benefit other tasks. • Pre-training and fine-tuning. Deep pre-training models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019) are new directions in NLP. • Sequence generation task. Sequence generation is widely used in NLP fields such as machine translation (Bahdanau et al., 2015), text summarization (See et al., 2017), and dialogue systems (Wen et al., 2015). • AutoML (Elsken et al., 2019). NeuronBlocks facilitates users to build models on top of Model Zoo and Block Zoo. With the integration of AutoML, the toolkit can further support automatic model architecture design. 4.4 WikiQA The WikiQA corpus (Yang et al., 2015) is a publicly available dataset for open-domain question answering. This dataset contains 3,047 questions from Bing query logs, each associated with some candidate answer sentences from Wikipedia. We conducted experiments on WikiQA dataset using CNN, BiLSTM, and Attention based models. The results are shown in Table 5. The models bu"
D19-3028,P16-1101,0,0.0879124,"Missing"
D19-3028,C18-1327,0,0.0240099,"Missing"
D19-3028,P17-1161,0,0.0363391,"Missing"
D19-3028,D15-1237,0,0.0264057,"Missing"
D19-3028,N18-1202,0,0.0368494,"ether the query belongs to a specific domain. Table 4 shows the results, where AUC is used as the evaluation criteria and Queries per Second (QPS) is used to measure inference speed. By leveraging knowledge distillation, the student model by NeuronBlocks managed to get 24.8 times inference speedup with only small performance regression compared with BERTbase 6 fine-tuned classifier. • Multi-task learning (MTL). In MTL, multiple related tasks are jointly trained so that knowledge learned in one task can benefit other tasks. • Pre-training and fine-tuning. Deep pre-training models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019) are new directions in NLP. • Sequence generation task. Sequence generation is widely used in NLP fields such as machine translation (Bahdanau et al., 2015), text summarization (See et al., 2017), and dialogue systems (Wen et al., 2015). • AutoML (Elsken et al., 2019). NeuronBlocks facilitates users to build models on top of Model Zoo and Block Zoo. With the integration of AutoML, the toolkit can further support automatic model architecture design. 4.4 WikiQA The WikiQA corpus (Yang et al., 2015) is a publicly available dataset for open-d"
D19-3028,N03-1031,0,\N,Missing
N16-1176,P14-1066,0,0.0329996,"Missing"
N16-1176,H90-1021,0,0.103151,"Missing"
N16-1176,P15-1162,0,0.00515092,"Missing"
N16-1176,P15-1001,0,0.0274272,"Missing"
N16-1176,P14-1062,0,0.556533,"guage processing (Socher, 2012; Collobert et al., 2011; Gao et al., 2014). They have achieved cuttingedge performance in various tasks such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), machine translation (Bahdanau et al., 2014; Cho et al., 2014; Jean et al., 2015), slot filling (Yao et al., 2014a; Shi et al., 2015a) and syntactic parsing (Wang et al., 2015; Collobert et al., 2011). For query classifications, recurrent neural networks (RNNs) and convolutional neural networks 1502 (CNNs) have emerged as top performing architectures (Zhang and Wallace, 2016; Kim, 2014; Kalchbrenner et al., 2014; Ravuri and Stolcke, 2015a). Due to its superior ability to memorize long distance dependencies, LSTMs have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sentence. The LSTM hidden vector from the ending word is directly used as sentence feature representation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of LSTM hidden vectors from it"
N16-1176,D14-1181,0,0.126772,"natural language processing (Socher, 2012; Collobert et al., 2011; Gao et al., 2014). They have achieved cuttingedge performance in various tasks such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), machine translation (Bahdanau et al., 2014; Cho et al., 2014; Jean et al., 2015), slot filling (Yao et al., 2014a; Shi et al., 2015a) and syntactic parsing (Wang et al., 2015; Collobert et al., 2011). For query classifications, recurrent neural networks (RNNs) and convolutional neural networks 1502 (CNNs) have emerged as top performing architectures (Zhang and Wallace, 2016; Kim, 2014; Kalchbrenner et al., 2014; Ravuri and Stolcke, 2015a). Due to its superior ability to memorize long distance dependencies, LSTMs have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sentence. The LSTM hidden vector from the ending word is directly used as sentence feature representation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of"
N16-1176,D15-1180,0,0.0773967,"Missing"
N16-1176,C02-1150,0,0.0182318,"vel information system) dataset (Hemphill et al., 1990; Yao et al., 2014b) is used. This dataset is mainly about the air travel domain with 26 different intents such as “flight”, “grounds ervice” and “city”. There are 893 utterances for testing (ATIS-III, Nov93 and Dec94), and 4978 utterances for training (rest of ATIS-III and ATIS-II). There are 899 unique running words and 22 intents in the training data. The question type classification task is to classify a question into a specific type, which is a very important step in question answering system. In TREC (Text Retrieval Conference) data (Li and Roth, 2002), all the questions are divided into 6 categories, including “human”, “entity”, “location”, “description”, “abbreviation” and “numeric”. The dataset in total has 5952 questions, 5452 of them for training, the rest for testing. The vocabulary size of TREC dataset is 9592. Following previous work (Iyyer et al., 2015; Tai et al., 2015; Lei et al., 2015), we used word vectors pre-trained on large unannotated corpora to achieve better generalization capability. In this paper, we used a publicly available 300 dimensional GloVe word vectors that are trained using Common Crawl with 840B tokens and 2.2"
N16-1176,D11-1014,0,0.0424528,"long distance dependencies, LSTMs have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sentence. The LSTM hidden vector from the ending word is directly used as sentence feature representation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of LSTM hidden vectors from its words (Tang et al., 2015). Inspired from recursive neural networks (Socher et al., 2011a), LSTM is further combined with a tree structure to model sentence representation (Tai et al., 2015). CNN s have been originally developed for image processing (Lecun et al., 1998). They are firstly applied by Collobert et al. (2008; 2011) for natural language processing tasks using max-over-time pooling method to aggregate convolution layer vectors. CNN s have also been applied to spoken language understanding (Shi et al., 2015b), information retrieval (Shen et al., 2014) and semantic parsing (Yih et al., 2015). Kalchbrenner et al. (2014) proposed to extend CNNs max-over-time pooling to k-m"
N16-1176,D12-1110,0,0.0264981,"Missing"
N16-1176,D13-1170,0,0.0449763,"sional vector where d is the number of LSTM DLSTM layers, h is the output size of each unit. A classification layer gives the prediction output. where αt,i is the learning rate for weight i at epoch t. ∑tj=1 g j,i sums all the historical gradients of weight i. A small positive ε is applied to make the AdaGrad robust. ε is usually set to 1e − 5. 4 4.1 Experiments Datasets We evaluate the proposed query classification models on sentence sentiment classification, question type categorization and query intent detection tasks. For sentence sentiment classification, the Stanford Sentiment Treebank (Socher et al., 2013) is used. In this dataset, 11855 English sentences are annotated at both sentence level and phrases level with fine-grained labels (very positive, positive, neutral, negative and very negative). We use the provided data split, which has 8544 sentences for training, 1101 sentences for developing and 2210 sentences for testing. This dataset also provides a binary classification variant that ignores the neutral sentences. The binary classification task in this dataset has 6920 sentences for training, 872 sentences for developing and 1821 sentences for testing. There are in total 17835 unique runn"
N16-1176,P15-1150,0,0.465262,"014; Cho et al., 2014; Jean et al., 2015), slot filling (Yao et al., 2014a; Shi et al., 2015a) and syntactic parsing (Wang et al., 2015; Collobert et al., 2011). For query classifications, recurrent neural networks (RNNs) and convolutional neural networks 1502 (CNNs) have emerged as top performing architectures (Zhang and Wallace, 2016; Kim, 2014; Kalchbrenner et al., 2014; Ravuri and Stolcke, 2015a). Due to its superior ability to memorize long distance dependencies, LSTMs have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sentence. The LSTM hidden vector from the ending word is directly used as sentence feature representation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of LSTM hidden vectors from its words (Tang et al., 2015). Inspired from recursive neural networks (Socher et al., 2011a), LSTM is further combined with a tree structure to model sentence representation (Tai et al., 2015). CNN s have been originally developed for image pr"
N16-1176,D15-1167,0,0.077227,"(Bahdanau et al., 2014; Cho et al., 2014; Jean et al., 2015), slot filling (Yao et al., 2014a; Shi et al., 2015a) and syntactic parsing (Wang et al., 2015; Collobert et al., 2011). For query classifications, recurrent neural networks (RNNs) and convolutional neural networks 1502 (CNNs) have emerged as top performing architectures (Zhang and Wallace, 2016; Kim, 2014; Kalchbrenner et al., 2014; Ravuri and Stolcke, 2015a). Due to its superior ability to memorize long distance dependencies, LSTMs have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sentence. The LSTM hidden vector from the ending word is directly used as sentence feature representation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of LSTM hidden vectors from its words (Tang et al., 2015). Inspired from recursive neural networks (Socher et al., 2011a), LSTM is further combined with a tree structure to model sentence representation (Tai et al., 2015). CNN s have been originally dev"
N16-1176,P15-1128,0,0.0207289,"s from its words (Tang et al., 2015). Inspired from recursive neural networks (Socher et al., 2011a), LSTM is further combined with a tree structure to model sentence representation (Tai et al., 2015). CNN s have been originally developed for image processing (Lecun et al., 1998). They are firstly applied by Collobert et al. (2008; 2011) for natural language processing tasks using max-over-time pooling method to aggregate convolution layer vectors. CNN s have also been applied to spoken language understanding (Shi et al., 2015b), information retrieval (Shen et al., 2014) and semantic parsing (Yih et al., 2015). Kalchbrenner et al. (2014) proposed to extend CNNs max-over-time pooling to k-max pooling for sentence modeling. Remarkable query classification performance on different benchmark datasets have been achieved by integrating CNNs with different feature mapping channels and pre-trained word vectors (Zhang and Wallace, 2015; Kim, 2014). Recently, Mou et al. (2015) proposed to model sentences by tree structured CNNs. CNN s and LSTM s are complementary in their modeling capabilities; CNNs are good at capturing local invariant regularities and LSTMs are good at modeling temporal features. The combi"
P12-2037,P05-1074,0,0.0600309,"Missing"
P12-2037,P01-1008,0,0.0138383,"o Seattle”) S2 = {Seattle}:(“how far is it from Boston to X1 ” ,“distance from Boston to X1 ”) S3 = {Boston, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction"
P12-2037,D07-1086,0,0.0340618,"et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have been developed for query segmentation (Bergsma and Wang, 2007; Tan and Peng, 2008) and query substitution (Jones et al., 2006; Wang and Zhai, 2008). Yet, most previous research focused on keyword queries without considering 5w1h questions. 3 Mining Question Reformulation Patterns for Web Search Our framework consists of three major components, which is illustrated in Fig. 1. 188 Generating Reformulation Patterns From the search log, we extract all successive query pairs issued by the same user within a certain time period where the first query is a 5w1h question. In such query pair, the second query is considered as a question reformulation. Our method"
P12-2037,P08-1077,0,0.0143149,"on, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). L"
P12-2037,N06-1003,0,0.016163,"lations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been s"
P12-2037,D08-1021,0,0.0149006,"om X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has bee"
P12-2037,N06-1058,0,0.0229005,"Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; J"
P12-2037,N03-1024,0,0.0107922,"“how far is it from Boston to X1 ” ,“distance from Boston to X1 ”) S3 = {Boston, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Dur"
P12-2037,I05-1011,0,0.0229,"Missing"
P12-2037,P08-1003,0,0.0544106,"Missing"
P12-2037,P02-1006,0,0.0468385,"Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have be"
P12-2037,P08-1089,0,0.0156806,"ce from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to autom"
P12-2037,C10-1148,0,0.0537288,"; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have been developed for query segmentation (Bergsma and Wang, 2007; Tan and Peng, 2008) and query substitution (Jones et al., 2006; Wang and Zhai, 2008). Yet, most previous research focused on keyword queries without considering 5w1h questions. 3 Mining Question Reformulation Patterns for Web Search Our framework cons"
P19-1131,D17-1209,0,0.025975,"learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model based on GCN to perform joint type inference for entity relation extraction task. Compar"
P19-1131,P11-1056,0,0.147557,"republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mech"
P19-1131,P18-2014,0,0.141109,"tifies a PHYS relation between “[units]PER ” and “[captial]GPE ”, while the “NN” does not find this relation even the entities are correct. However, both models do not identify the relation ART between “[units]PER ” and “[weapons]WEA ”. We think advanced improvement methods which use more powerful graph neural network might be helpful in this situation. 4.2 Golden Entity Results on ACE05 In order to compare with relation classification methods, we evaluate our models with golden entities on ACE05 corpus in Table 4. We use the same data split to compare with their model (Miwa and Bansal, 2016; Christopoulou et al., 2018). We do not tune hyperparameters extensively. For example, we use the same setting in both end-to-end and golden entity rather than tune parameters on each of them. The baseline systems are (Miwa and Bansal, 2016) and (Christopoulou et al., 2018). In general, our “NN” is competitive, comparing to the dependency tree-based state-of-the-art model (Miwa and Bansal, 2016). It shows that our CNN-based neural networks are able to extract more powerful features to help relation extraction task. After adding GCN, our GCN-based models achieve the better performance. This indicates that the proposed mod"
P19-1131,P82-1020,0,0.824741,"Missing"
P19-1131,P16-1087,0,0.0251959,"tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great"
P19-1131,P17-1085,0,0.448054,"e first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By inspecting the performance of"
P19-1131,P14-1038,0,0.0836562,"el as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 20"
P19-1131,P16-1200,0,0.055208,"AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint"
P19-1131,D17-1159,0,0.0261108,"ation between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model ba"
P19-1131,P16-1105,0,0.140999,"two stages: entities are first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By in"
P19-1131,D09-1013,0,0.0833432,"♠ PER:♥♣♠ ORG:♥♣♠ [republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN usi"
P19-1131,D14-1162,0,0.0870553,"s work in Table 1. In general, our “GCN” achieves the best entity performance 84.2 percent comparing with existing joint models. For relation performance, our “GCN” significantly outperforms all joint models except for (Sun et al., 2018) which uses more complex joint decoder. Comparing with our basic neural network “NN”, our “GCN” has large improvement both on entities and relations. Those observations demonstrate the effectiveness of our “GCN” for capturing information on multiple entity types and relation types from a sentence. 5 Our word embeddings is initialized with 100dimensional glove (Pennington et al., 2014) word embeddings. The dimensionality of the hidden units and node embedding are set to 128. For all CNN in our network, the kernel sizes are 2 and 3, and the output channels are 25. 1365 Model P Entity R F P Relation R F L&J (2014) Zhang (2017) Sun (2018) 85.2 83.9 76.9 83.2 80.8 83.5 83.6 65.4 64.9 39.8 55.1 49.5 57.5 59.6 M&B (2016) K&C (2017) NN GCN 82.9 84.0 85.7 86.1 83.9 81.3 82.1 82.4 83.4 82.6 83.9 84.2 57.2 55.5 65.6 68.1 54.0 51.8 50.7 52.3 55.6 53.6 57.2 59.1 Table 1: Results on the ACE05 test data. Li and Ji (2014) Zhang et al. (2017) and Sun et al. (2018) are joint decoding algori"
P19-1131,P17-1113,0,0.190846,"es hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battagli"
P19-1131,D18-1249,1,0.863882,"oncise joint model to handle the joint type inference problem based on graph convolutional network (GCN). 1362 In this work, we decompose the joint entity relation extraction task into two parts, namely, entity span detection and entity relation type deduction. We first treat entity span detection as a sequence labelling task (Section 3.1), and then construct an entity-relation bipartite graph (Section 3.2) to perform joint type inference on entity nodes and relation nodes (Section 3.3). All submodels share parameters and are trained jointly. Different from existing joint learning algorithms (Sun et al., 2018; Zhang et al., 2017; Katiyar and Cardie, 2017; Miwa and Bansal, 2016), we propose a concise joint model to perform joint type inference on entities and relations based on GCNs. It considers interactions among multiple entity types and relation types simultaneously in a sentence. 3.1 Entity Span Detection To extract entity spans from a sentence (Figure 2), we adopt the BILOU sequence tagging scheme: B, I, L and O denote the begin, inside, last and outside of a target span, U denotes a single word span. For example, for a person (PER) entity “Patrick McDowell”, we assign B to “Patrick” and L to"
P19-1131,P13-1161,0,0.0695515,"el RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been re"
P19-1131,D17-1182,0,0.112899,"Missing"
P19-1131,D18-1244,0,0.123934,"information from their neighborhood over the bipartite graph. It helps us to concisely capture information among entities and relations. For example, in Figure 1, to predict the PER (“Toefting”), our joint model can pool the information of PER-SOC, PHYS, PER (“teammates”) and GPE (captital). To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary relation classification, which makes the proposed adjacency matrix more explanatory. To summarize, the main contributions of this work are 1 bipartite graph in a more efficient and interpretable way. • We show that the proposed joint model on ACE05 achieves best entity performance, and is competitive with the state-of-the-art in relation performance. 2 Background of GCN In this section, we briefly describe graph convolutional networks (GCNs). Given a graph with n nodes, the goal of GCNs is to learn structureaware node representat"
