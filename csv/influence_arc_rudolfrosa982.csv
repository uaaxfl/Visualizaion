2016.eamt-2.1,W15-3009,1,0.880814,"Missing"
2016.eamt-2.1,W15-5711,1,0.827881,"Missing"
2016.eamt-2.1,W15-3006,1,0.877207,"Missing"
2016.eamt-2.1,P15-4020,0,0.0200302,"Missing"
2016.eamt-2.1,P13-4014,0,0.0192755,"Missing"
2016.eamt-2.1,W15-2505,0,0.0245204,"Missing"
2016.eamt-2.1,P15-3002,0,0.0279562,"Missing"
2020.findings-emnlp.150,2020.acl-main.493,0,0.026025,"e neutrality. Zero-shot learning abilities were examined by Pires et al. (2019) on NER and part-of-speech (POS) tagging, showing that the success strongly depends on how typologically similar the languages are. Similarly, Wu and Dredze (2019) trained good multilingual models but struggled to achieve good results in the zero-shot setup for POS tagging, NER, and XLNI. R¨onnqvist et al. (2019) draw similar conclusions for language-generation tasks. Wang et al. (2019) succeeded in zero-shot dependency parsing but required supervised projection trained on word-aligned parallel data. The results of Chi et al. (2020) on dependency parsing suggest that methods like structural probing (Hewitt and Manning, 2019) might be more suitable for zero-shot transfer. Pires et al. (2019) also assessed mBERT on cross-lingual sentence retrieval between three language pairs. They observed that if they subtract the average difference between the embeddings from the target language representation, the retrieval accuracy significantly increases. We systematically study this idea in the later sections. XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), two recently introduced benchmarks for multilingual representation"
2020.findings-emnlp.150,2020.acl-main.747,0,0.173152,"Missing"
2020.findings-emnlp.150,D18-1269,0,0.0412762,"ero-shot transfer. Pires et al. (2019) also assessed mBERT on cross-lingual sentence retrieval between three language pairs. They observed that if they subtract the average difference between the embeddings from the target language representation, the retrieval accuracy significantly increases. We systematically study this idea in the later sections. XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), two recently introduced benchmarks for multilingual representation evaluation, assess representations on a broader range of zero-shot transfer tasks that include natural language inference (Conneau et al., 2018) and question answering (Artetxe et al., 2019; Lewis et al., 2019). Their results show a clearly superior performance of XLMR compared to mBERT. Many works clearly show that downstream task models can extract relevant features from the multilingual representations (Wu and Dredze, 2019; Kudugunta et al., 2019; Kondratyuk and Straka, 2019a). However, they do not directly show language-neutrality, i.e., to what extent similar phenomena are represented similarly across languages. Thus, it is impossible to say whether the representations are language-agnostic or contain some implicit language ident"
2020.findings-emnlp.150,N19-1423,0,0.0426482,"ddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately languageneutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. P"
2020.findings-emnlp.150,N13-1073,0,0.127813,"t the projection by minimizing the element-wise mean squared error between the representation of an English sentence and a linear projection of the representation of its translation. Word Alignment. WA is the task of matching words which are translations of each other in parallel sentences. WA is a key component of statistical machine translation systems (Koehn, 2009). While sentence retrieval could be done with keyword spotting, computing bilingual WA requires resolving detailed correspondence on the word level. Unsupervised statistical methods trained on parallel corpora (Och and Ney, 2003; Dyer et al., 2013) still pose a strong baseline for the task. In a work parallel to ours, Sabet et al. (2020) present a more complex alternative way of leveraging contextual representations for word alignment that outperforms the statistical methods. For a pair of parallel sentences, we find the WA as a minimum weighted edge cover of a bipartite graph. We create an edge for each potential alignment link, weight it by the cosine distance of the token representations, and find the WA as a minimum weighted edge cover of the resulting bipartite graph. Unlike statistical methods, this does not require parallel data"
2020.findings-emnlp.150,D18-1002,0,0.0660149,"Missing"
2020.findings-emnlp.150,W19-6721,0,0.0363306,"Missing"
2020.findings-emnlp.150,L18-1550,0,0.0632994,"Missing"
2020.findings-emnlp.150,N19-1419,0,0.0240241,"and part-of-speech (POS) tagging, showing that the success strongly depends on how typologically similar the languages are. Similarly, Wu and Dredze (2019) trained good multilingual models but struggled to achieve good results in the zero-shot setup for POS tagging, NER, and XLNI. R¨onnqvist et al. (2019) draw similar conclusions for language-generation tasks. Wang et al. (2019) succeeded in zero-shot dependency parsing but required supervised projection trained on word-aligned parallel data. The results of Chi et al. (2020) on dependency parsing suggest that methods like structural probing (Hewitt and Manning, 2019) might be more suitable for zero-shot transfer. Pires et al. (2019) also assessed mBERT on cross-lingual sentence retrieval between three language pairs. They observed that if they subtract the average difference between the embeddings from the target language representation, the retrieval accuracy significantly increases. We systematically study this idea in the later sections. XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), two recently introduced benchmarks for multilingual representation evaluation, assess representations on a broader range of zero-shot transfer tasks that include"
2020.findings-emnlp.150,W11-4615,0,0.0209665,"iction. 6 centroids and training the lng-free version of the model. For parallel sentence retrieval, we use a multiparallel corpus of test data from the WMT14 evaluation campaign (Bojar et al., 2014) with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. To compute the linear projection (for the special linear projection experimental condition), we used the WMT14 development data (500– 3000 sentences per language pair). We use manually annotated WA datasets to evaluate word alignment between English on one side and Czech (2.5k sent.; Mareˇcek, 2016)2 , Swedish (192 sent.; Holmqvist and Ahrenberg, 2011)3 , German (508 sent.)4 , French (447 sent.; Och and Ney, 2000)5 and Romanian (248 sent.; Mihalcea and Pedersen, 2003)6 on the other side. We compare the results with FastAlign (Dyer et al., 2013) and ¨ Efmaral (Ostling and Tiedemann, 2016) models, which were provided with 1M additional parallel sentences from ParaCrawl (Espl`a et al., 2019)7 . For MT QE, we use English-German training and test data provided for the WMT19 QE Shared Task (Fonseca et al., 2019, Task 1), consisting of Experimental Setup 2 http://hdl.handle.net/11234/1-1804 http://hdl.handle.net/11372/LRT-1517 4 https://www-i6.inf"
2020.findings-emnlp.150,D18-1330,0,0.0325126,"Missing"
2020.findings-emnlp.150,P09-5002,0,0.00950591,"ons of all sentences on the parallel side of the corpus and select the sentence with the smallest distance. Besides the plain and centered representations, we evaluate explicit projection of the representations into the “English space.” We fit the projection by minimizing the element-wise mean squared error between the representation of an English sentence and a linear projection of the representation of its translation. Word Alignment. WA is the task of matching words which are translations of each other in parallel sentences. WA is a key component of statistical machine translation systems (Koehn, 2009). While sentence retrieval could be done with keyword spotting, computing bilingual WA requires resolving detailed correspondence on the word level. Unsupervised statistical methods trained on parallel corpora (Och and Ney, 2003; Dyer et al., 2013) still pose a strong baseline for the task. In a work parallel to ours, Sabet et al. (2020) present a more complex alternative way of leveraging contextual representations for word alignment that outperforms the statistical methods. For a pair of parallel sentences, we find the WA as a minimum weighted edge cover of a bipartite graph. We create an ed"
2020.findings-emnlp.150,D19-1279,0,0.357373,"l by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphologic"
2020.findings-emnlp.150,D18-2012,0,0.0165127,"tropy of its output distribution with respect to the output of the teacher mBERT model while keeping the MLM objective in the multi-task learning setup. As the model is forced to use smaller space to obtain the representation, it might leverage the similarities between languages and reach better language neutrality. XLM-RoBERTa. Conneau et al. (2019) claim that the original mBERT is under-trained and train a similar model on a larger dataset that consists of two terabytes of plain text extracted from CommonCrawl (Wenzek et al., 2019). Unlike mBERT, XLM-R uses a SentencePiece-based vocabulary (Kudo and Richardson, 2018) of 250k tokens. The rest of the architecture remains the same as in the case of mBERT. We train the model using the MLM objective only, without the sentence adjacency prediction. 6 centroids and training the lng-free version of the model. For parallel sentence retrieval, we use a multiparallel corpus of test data from the WMT14 evaluation campaign (Bojar et al., 2014) with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. To compute the linear projection (for the special linear projection experimental condition), we used the WMT14 development data (500– 3000 sentences per"
2020.findings-emnlp.150,D19-1167,0,0.110506,", by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Th"
2020.findings-emnlp.150,P12-3005,0,0.0977273,"Missing"
2020.findings-emnlp.150,W03-0301,0,0.0287526,"lel corpus of test data from the WMT14 evaluation campaign (Bojar et al., 2014) with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. To compute the linear projection (for the special linear projection experimental condition), we used the WMT14 development data (500– 3000 sentences per language pair). We use manually annotated WA datasets to evaluate word alignment between English on one side and Czech (2.5k sent.; Mareˇcek, 2016)2 , Swedish (192 sent.; Holmqvist and Ahrenberg, 2011)3 , German (508 sent.)4 , French (447 sent.; Och and Ney, 2000)5 and Romanian (248 sent.; Mihalcea and Pedersen, 2003)6 on the other side. We compare the results with FastAlign (Dyer et al., 2013) and ¨ Efmaral (Ostling and Tiedemann, 2016) models, which were provided with 1M additional parallel sentences from ParaCrawl (Espl`a et al., 2019)7 . For MT QE, we use English-German training and test data provided for the WMT19 QE Shared Task (Fonseca et al., 2019, Task 1), consisting of Experimental Setup 2 http://hdl.handle.net/11234/1-1804 http://hdl.handle.net/11372/LRT-1517 4 https://www-i6.informatik.rwth-aachen.de/ goldAlignment 5 http://web.eecs.umich.edu/∼mihalcea/wpt/data/ English-French.test.tar.gz 6 htt"
2020.findings-emnlp.150,P00-1056,0,0.418501,"allel sentence retrieval, we use a multiparallel corpus of test data from the WMT14 evaluation campaign (Bojar et al., 2014) with 3,000 sentences in Czech, English, French, German, Hindi, and Russian. To compute the linear projection (for the special linear projection experimental condition), we used the WMT14 development data (500– 3000 sentences per language pair). We use manually annotated WA datasets to evaluate word alignment between English on one side and Czech (2.5k sent.; Mareˇcek, 2016)2 , Swedish (192 sent.; Holmqvist and Ahrenberg, 2011)3 , German (508 sent.)4 , French (447 sent.; Och and Ney, 2000)5 and Romanian (248 sent.; Mihalcea and Pedersen, 2003)6 on the other side. We compare the results with FastAlign (Dyer et al., 2013) and ¨ Efmaral (Ostling and Tiedemann, 2016) models, which were provided with 1M additional parallel sentences from ParaCrawl (Espl`a et al., 2019)7 . For MT QE, we use English-German training and test data provided for the WMT19 QE Shared Task (Fonseca et al., 2019, Task 1), consisting of Experimental Setup 2 http://hdl.handle.net/11234/1-1804 http://hdl.handle.net/11372/LRT-1517 4 https://www-i6.informatik.rwth-aachen.de/ goldAlignment 5 http://web.eecs.umich.e"
2020.findings-emnlp.150,J03-1002,0,0.0123249,"glish space.” We fit the projection by minimizing the element-wise mean squared error between the representation of an English sentence and a linear projection of the representation of its translation. Word Alignment. WA is the task of matching words which are translations of each other in parallel sentences. WA is a key component of statistical machine translation systems (Koehn, 2009). While sentence retrieval could be done with keyword spotting, computing bilingual WA requires resolving detailed correspondence on the word level. Unsupervised statistical methods trained on parallel corpora (Och and Ney, 2003; Dyer et al., 2013) still pose a strong baseline for the task. In a work parallel to ours, Sabet et al. (2020) present a more complex alternative way of leveraging contextual representations for word alignment that outperforms the statistical methods. For a pair of parallel sentences, we find the WA as a minimum weighted edge cover of a bipartite graph. We create an edge for each potential alignment link, weight it by the cosine distance of the token representations, and find the WA as a minimum weighted edge cover of the resulting bipartite graph. Unlike statistical methods, this does not re"
2020.findings-emnlp.150,P19-1493,0,0.442784,"presentation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence"
2020.findings-emnlp.150,D19-1077,0,0.364128,"h language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with prom"
2020.findings-emnlp.150,W19-6204,0,0.0884765,"Missing"
2020.findings-emnlp.150,D07-1043,0,0.102303,"es all phenomena in a language-neutral way, it should be difficult to determine what language the sentence is written in. Unlike our other tasks, language ID requires fitting a classifier. We train a linear classifier on top of a sentence representation. Language Similarity. Previous work (Pires et al., 2019; Wang et al., 2019) shows that models can be transferred better between more similar languages, suggesting that similar languages tend to get similar representations. We quantify this observation by V-measure between language families and hierarchical clustering of the language centroids (Rosenberg and Hirschberg, 2007). We cluster the language centroids by their cosine distance using the Nearest Point Algorithm and stop the clustering with a number of clusters equal to the number of language families in the data. Parallel Sentence Retrieval. For each sentence in a multi-parallel corpus, we compute the cosine distance of its representation with representations of all sentences on the parallel side of the corpus and select the sentence with the smallest distance. Besides the plain and centered representations, we evaluate explicit projection of the representations into the “English space.” We fit the projecti"
2020.findings-emnlp.150,2020.findings-emnlp.147,0,0.0264938,"Missing"
2020.findings-emnlp.150,D19-1575,0,0.27855,"o simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach stateof-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data. 1 Introduction Multilingual BERT (mBERT; Devlin et al. 2019) gained popularity as a contextual representation for many multilingual tasks, e.g., dependency parsing (Kondratyuk and Straka, 2019a; Wang et al., 2019), cross-lingual natural language inference (XNLI) or named-entity recognition (NER) (Pires et al., 2019; Wu and Dredze, 2019; Kudugunta et al., 2019). Recently, a new pre-trained model, XLM-RoBERTa (XLM-R; Conneau et al. 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a significantly lower computational cost. Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic task"
2020.findings-emnlp.150,W14-3302,0,\N,Missing
2020.findings-emnlp.150,W19-5401,0,\N,Missing
2020.findings-emnlp.245,I17-1001,0,0.0230053,"Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a small amount of data is sufficient for that purpose (§6.1). 2 Models and Data We analyze the uncased base BERT model for English, which we will refer to as enBERT, and the uncased multilingual BERT model, mBERT, for English, German, French, Czech, Finnish, Indonesian,"
2020.findings-emnlp.245,2020.acl-main.493,0,0.0295344,"et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a small amount of data is sufficient for that purpose (§6.1). 2 Models and Data We analyze the uncased base BERT model for English, which we will refer to as enBERT, and the uncased multilingual BERT model, mBERT, for English, German, French, Czech, Finnish, Indonesian, Turkish, Korean, and Japanese 1 . The code 1"
2020.findings-emnlp.245,P19-1493,0,0.143053,", Korean, Japanese) are significantly lower than for SVO languages (English, French, Czech, Finnish, Indonesian) in both Dependency Accuracy (14.7 pp) and the UAS (10.5 pp). Our methods outperform the baselines in the latter group by 17.2 pp to 25.4 pp for Dependency Accuracy and from 6.1 pp to 15.5 pp for UAS. The influence of Adjective and Noun order is less apparent. On average, the NA languages results are higher than for the AN languages by 2.4 pp in Dependency Accuracy and 2.7 pp in UAS. 2715 7.1 Cross-lingual intersections Representation of mBERT is language independent to some extent (Pires et al., 2019; Libovickỳ et al., 2019). Thus, a natural question is whether the same mBERT heads encode the same syntactic relations for different languages. In particular, subject relations tend to be encoded by similar heads in different languages, which rarely belong to an ensemble for other dependency labels. Again Japanese is an exception here, possibly due to different ObjectVerb order. For adjective modifiers, the French ensemble has two heads in common with the German and one with other considered languages, although the preferred order of adjective and noun is different. 2 4 3 4 1 4 1 2 1 1 2 2 1"
2020.findings-emnlp.245,W17-0412,0,0.033847,"Missing"
2020.findings-emnlp.245,W18-5431,0,0.307782,"s well across languages. 1 Introduction and Related Work In recent years, systems based on Transformer architecture achieved state-of-the-art results in language modeling (Devlin et al., 2019) and machine translation (Vaswani et al., 2017). Additionally, the contextual embeddings obtained from the intermediate representation of the model brought improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al.,"
2020.findings-emnlp.245,K17-3009,0,0.0439431,"ish, German, French, Czech, Finnish, Indonesian, Turkish, Korean, and Japanese 1 . The code 1 Pretrained models are available at https://github. com/google-research/bert 2710 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2710–2722 c November 16 - 20, 2020. 2020 Association for Computational Linguistics shared by Clark et al. (2019) 2 substantially helped us in extracting attention weights from BERT. To find syntactic heads, we use: 1000 EuroParl multi parallel sentences (Koehn, 2004) for five European languages, automatically annotated with UDPipe UD 2.0 models (Straka and Straková, 2017); Google Universal Dependency Treebanks (GSD) for Indonesian, Korean, and Japanese (McDonald et al., 2013); the UD Turkish Treebank (IMST-UD) (Sulubacak et al., 2016). We use another PUD treebanks from the CoNLL 2017 Shared Task for evaluation of mBERT in all languages (Nivre et al., 2017)3 . 3 Adapting UD to BERT Since the explicit dependency structure is not used in BERT training, syntactic dependencies captured in latent layers are expected to diverge from annotation guidelines. After initial experiments, we have observed that some of the differences are systematic (see Table 1). UD Modifie"
2020.findings-emnlp.245,W19-4808,0,0.0721454,"tation of the model brought improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few pa"
2020.findings-emnlp.245,P19-1580,0,0.163251,"improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a sma"
2020.findings-emnlp.245,L16-1262,0,\N,Missing
2020.findings-emnlp.245,N19-1419,0,\N,Missing
2020.findings-emnlp.245,P19-1526,0,\N,Missing
2020.findings-emnlp.245,W19-4828,0,\N,Missing
2020.lantern-1.1,W16-1904,0,0.0284708,"ound by showing that there is a relation between regressions and the syntactic structure of sentences. They tested if the path of regressions from a word to an earlier word coincide – at least partially – with the edges of dependency relations between these words by using dependency parsing features to predict eye-regressions during training. One of their important findings indicates that eye regressions are involved predominantly in dependency parsing at the local level (vast majority being shorter than three words with a predominance of one position backwards), rather than at long distance. Cheri et al. (2016) utilize the eye-movements of several annotators resolving coreference to improve automatic coreference resolution. Mathias et al. (2018) rate the quality of a piece of text by using eye-tracking data and Mishra et al. (2017) try to quantify the effort needed in reading a piece of text by measuring the complexity of the scanpath of various readers. A few studies also found that eye-tracking data can be used to improve sentiment analysis as well as sarcasm detection (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c). 3 Parser Descriptions In this section, we describe the two par"
2020.lantern-1.1,N19-1001,0,0.264437,"rived from eyetracking data were found to be better than the automatic metrics in use (Klerke et al., 2015). The idea that eye-tracking data could be used to evaluate machine translation output seems quite reasonable as bad translations would result in longer fixation durations and more regressions by the reader which can be picked up from the data. Klerke et al. (2016) in their work use gaze data in a multi-task learning setup to improve sentence compression. Søgaard (2016) and Hollenstein et al. (2019) couple fMRI data along with eye-tracking data to evaluate the quality of word embeddings. Hollenstein and Zhang (2019) leveraged eye-tracking data for improving named entity recognition. An important feature of their study was to leverage type-aggregated gaze features to eliminate the need for recording eye-tracking data at test time and also make the features useful for cross-domain settings. A couple of studies have also experimented with some sort of parsing of text using eye-tracking data. Strzyz et al. (2019b) leverage gaze data by learning eye-movement features as an auxiliary task in a multi-task learning setup where both dependency parsing and gaze prediction are addressed as sequence labelling. Their"
2020.lantern-1.1,K19-1050,0,0.0135646,"s of children that have some reading disability. Metrics for evaluating the quality of machine translation output derived from eyetracking data were found to be better than the automatic metrics in use (Klerke et al., 2015). The idea that eye-tracking data could be used to evaluate machine translation output seems quite reasonable as bad translations would result in longer fixation durations and more regressions by the reader which can be picked up from the data. Klerke et al. (2016) in their work use gaze data in a multi-task learning setup to improve sentence compression. Søgaard (2016) and Hollenstein et al. (2019) couple fMRI data along with eye-tracking data to evaluate the quality of word embeddings. Hollenstein and Zhang (2019) leveraged eye-tracking data for improving named entity recognition. An important feature of their study was to leverage type-aggregated gaze features to eliminate the need for recording eye-tracking data at test time and also make the features useful for cross-domain settings. A couple of studies have also experimented with some sort of parsing of text using eye-tracking data. Strzyz et al. (2019b) leverage gaze data by learning eye-movement features as an auxiliary task in a"
2020.lantern-1.1,Q16-1023,0,0.194238,"scanpath of various readers. A few studies also found that eye-tracking data can be used to improve sentiment analysis as well as sarcasm detection (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c). 3 Parser Descriptions In this section, we describe the two parsers we use in our work. Our primary parser is a modification of the system created by Strzyz et al. (2019b) who in turn adapted the NCRF++ system (Yang and Zhang, 2018) which is an open source neural sequence labelling toolkit. Our secondary parser is a more traditional graph-based parser known as the BIST parser from Kiperwasser and Goldberg (2016). One of the main differences between our parsers and the parser by Strzyz et al. (2019b) is that we assume the availability of gaze features both at training and test time whereas Strzyz et al. (2019b) only use the gaze 3 features during training. Another difference is that we incorporate gaze features as the input to the parser whereas Strzyz et al. (2019b) predict the gaze features as an auxiliary task in a multi-task learning setup. The parsers are evaluated with respect to the Labelled Attachment Score (LAS) and the Unlabelled Attachment Score (UAS). The LAS is concerned with the number o"
2020.lantern-1.1,W15-2402,0,0.0285449,"res to predict the grammatical function of a word. The study done by Barrett et al. (2018) leveraged eyetracking information to find out human attention and used it to regularize the attention function used in an RNN and found improvements for a range of NLP tasks like sentiment analysis, abusive language detection, etc. Bingel et al. (2018) use eye-tracking information to predict reading errors of children that have some reading disability. Metrics for evaluating the quality of machine translation output derived from eyetracking data were found to be better than the automatic metrics in use (Klerke et al., 2015). The idea that eye-tracking data could be used to evaluate machine translation output seems quite reasonable as bad translations would result in longer fixation durations and more regressions by the reader which can be picked up from the data. Klerke et al. (2016) in their work use gaze data in a multi-task learning setup to improve sentence compression. Søgaard (2016) and Hollenstein et al. (2019) couple fMRI data along with eye-tracking data to evaluate the quality of word embeddings. Hollenstein and Zhang (2019) leveraged eye-tracking data for improving named entity recognition. An importa"
2020.lantern-1.1,N16-1179,0,0.0201777,"like sentiment analysis, abusive language detection, etc. Bingel et al. (2018) use eye-tracking information to predict reading errors of children that have some reading disability. Metrics for evaluating the quality of machine translation output derived from eyetracking data were found to be better than the automatic metrics in use (Klerke et al., 2015). The idea that eye-tracking data could be used to evaluate machine translation output seems quite reasonable as bad translations would result in longer fixation durations and more regressions by the reader which can be picked up from the data. Klerke et al. (2016) in their work use gaze data in a multi-task learning setup to improve sentence compression. Søgaard (2016) and Hollenstein et al. (2019) couple fMRI data along with eye-tracking data to evaluate the quality of word embeddings. Hollenstein and Zhang (2019) leveraged eye-tracking data for improving named entity recognition. An important feature of their study was to leverage type-aggregated gaze features to eliminate the need for recording eye-tracking data at test time and also make the features useful for cross-domain settings. A couple of studies have also experimented with some sort of pars"
2020.lantern-1.1,W19-2909,0,0.0465624,"Missing"
2020.lantern-1.1,P18-1219,0,0.022893,"ssions from a word to an earlier word coincide – at least partially – with the edges of dependency relations between these words by using dependency parsing features to predict eye-regressions during training. One of their important findings indicates that eye regressions are involved predominantly in dependency parsing at the local level (vast majority being shorter than three words with a predominance of one position backwards), rather than at long distance. Cheri et al. (2016) utilize the eye-movements of several annotators resolving coreference to improve automatic coreference resolution. Mathias et al. (2018) rate the quality of a piece of text by using eye-tracking data and Mishra et al. (2017) try to quantify the effort needed in reading a piece of text by measuring the complexity of the scanpath of various readers. A few studies also found that eye-tracking data can be used to improve sentiment analysis as well as sarcasm detection (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c). 3 Parser Descriptions In this section, we describe the two parsers we use in our work. Our primary parser is a modification of the system created by Strzyz et al. (2019b) who in turn adapted the NCRF"
2020.lantern-1.1,P16-1104,0,0.168583,"ch require annotated data on a large scale to learn good representations and provide meaningful results. Annotating such large quantities of data is not an easy task usually requiring a lot of manual labour which is time consuming and can be expensive not to mention resolving the inter-annotator agreement. However, every single day, millions of people read their daily newspapers, books, magazines, articles on the internet, etc. in a multitude of languages. In the past couple of years some studies have shown that using behavioral data for NLP tasks involving syntax and semantics can be useful (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c; Barrett and Søgaard, 2015a; Barrett and Søgaard, 2015b). This leads us to believe that if there was a way to tap into cognitive data generated by unconscious human parsing of text, it could be leveraged in some manner to support NLP tools thereby reducing our dependency on annotated textual data. Another reason for choosing to work with eye-tracking features is that it is highly likely that eyetracking technology will be available on a much larger scale in the near future and hence can be leveraged easily for NLP tasks. This is evidenced by the av"
2020.lantern-1.1,K16-1016,0,0.155481,"ch require annotated data on a large scale to learn good representations and provide meaningful results. Annotating such large quantities of data is not an easy task usually requiring a lot of manual labour which is time consuming and can be expensive not to mention resolving the inter-annotator agreement. However, every single day, millions of people read their daily newspapers, books, magazines, articles on the internet, etc. in a multitude of languages. In the past couple of years some studies have shown that using behavioral data for NLP tasks involving syntax and semantics can be useful (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c; Barrett and Søgaard, 2015a; Barrett and Søgaard, 2015b). This leads us to believe that if there was a way to tap into cognitive data generated by unconscious human parsing of text, it could be leveraged in some manner to support NLP tools thereby reducing our dependency on annotated textual data. Another reason for choosing to work with eye-tracking features is that it is highly likely that eyetracking technology will be available on a much larger scale in the near future and hence can be leveraged easily for NLP tasks. This is evidenced by the av"
2020.lantern-1.1,L16-1262,0,0.0205574,"Missing"
2020.lantern-1.1,petrov-etal-2012-universal,0,0.051769,"ic information related to verbs allowing the parser to parse the verbs much better. As verbs are heads of syntactic clauses, it may also be that gaze features help the parser distinguish the main clause (headed by a verb with the ’root’ label) from 6 Detailed results are available in the appendix. 9 subordinate clauses. The improvement for CONJ is also high meaning gaze features somehow help the parser in understanding co-ordination structures. An alternate hypothesis to explain the improvements in delexicalized parsing could also be that since our data makes use of Google universal POS tags (Petrov et al., 2012), which is more coarse than the UD tagset, the gaze features may simply be distinguishing the POS categories into more fine grained ones like distinguishing between full verbs and auxiliary verbs or co-ordinating and sub-ordinating conjunctions which then in turn helps the parser. It might be the case that were the UD POS tagset had been used in the data, the improvements in the delexicalized parser might have been smaller. 6 Conclusion In this paper, we explored the benefits of using eye-tracking features for dependency parsing. We performed a set of experiments wherein we tried different par"
2020.lantern-1.1,W16-2521,0,0.0197827,"edict reading errors of children that have some reading disability. Metrics for evaluating the quality of machine translation output derived from eyetracking data were found to be better than the automatic metrics in use (Klerke et al., 2015). The idea that eye-tracking data could be used to evaluate machine translation output seems quite reasonable as bad translations would result in longer fixation durations and more regressions by the reader which can be picked up from the data. Klerke et al. (2016) in their work use gaze data in a multi-task learning setup to improve sentence compression. Søgaard (2016) and Hollenstein et al. (2019) couple fMRI data along with eye-tracking data to evaluate the quality of word embeddings. Hollenstein and Zhang (2019) leveraged eye-tracking data for improving named entity recognition. An important feature of their study was to leverage type-aggregated gaze features to eliminate the need for recording eye-tracking data at test time and also make the features useful for cross-domain settings. A couple of studies have also experimented with some sort of parsing of text using eye-tracking data. Strzyz et al. (2019b) leverage gaze data by learning eye-movement feat"
2020.lantern-1.1,P19-1531,0,0.0423313,"Missing"
2020.lantern-1.1,D19-1160,0,0.0379425,"Missing"
2020.lantern-1.1,N19-1077,0,0.0373587,"Missing"
2020.lantern-1.1,P18-4013,0,0.0162493,"uality of a piece of text by using eye-tracking data and Mishra et al. (2017) try to quantify the effort needed in reading a piece of text by measuring the complexity of the scanpath of various readers. A few studies also found that eye-tracking data can be used to improve sentiment analysis as well as sarcasm detection (Mishra et al., 2016a; Mishra et al., 2016b; Mishra et al., 2016c). 3 Parser Descriptions In this section, we describe the two parsers we use in our work. Our primary parser is a modification of the system created by Strzyz et al. (2019b) who in turn adapted the NCRF++ system (Yang and Zhang, 2018) which is an open source neural sequence labelling toolkit. Our secondary parser is a more traditional graph-based parser known as the BIST parser from Kiperwasser and Goldberg (2016). One of the main differences between our parsers and the parser by Strzyz et al. (2019b) is that we assume the availability of gaze features both at training and test time whereas Strzyz et al. (2019b) only use the gaze 3 features during training. Another difference is that we incorporate gaze features as the input to the parser whereas Strzyz et al. (2019b) predict the gaze features as an auxiliary task in a mul"
2020.lantern-1.1,I08-3008,0,0.0573757,".84 81.90 76.31 74.89 **68.69 Parser setup Table 3: Evaluation of best parser setups on test set with UAS and LAS scores (best results in bold; *, ** indicates significant improvement over the baseline; * p &lt; 0.05, ** p &lt; 0.01 McNemar’s test). i.e. word features, character features (in case of sequence labelling parser), POS tags and the gaze features.5 The baseline system is a model where no gaze features were used. • Experiment 2: The next setup that we try out is delexicalized parsing of the data. Delexicalized parsing has been found quite useful in low resource and cross-lingual settings (Zeman and Resnik, 2008; Aufrant et al., 2016). A part of the motivation behind choosing this setup was also that in case of successful delexicalized parsing, it would be interesting to explore if gaze and syntax co-relations can be transferred across a pair of languages, helping to create a cross-lingual dependency parser. That being said, our interest in this setup is merely to explore if eye-tracking features are useful in any form of parsing. In this case we omit the word and character level features and only use the POS tag embeddings and the gaze features. The baseline system is a model where no gaze features"
2020.sigtyp-1.4,2020.sigtyp-1.1,0,0.0711656,"Missing"
2020.sigtyp-1.4,P07-1009,0,0.0613293,"Missing"
2020.sigtyp-1.4,C16-1123,0,0.0570439,"Missing"
barancikova-etal-2014-improving,W12-3146,1,\N,Missing
barancikova-etal-2014-improving,W99-0604,0,\N,Missing
barancikova-etal-2014-improving,W12-3102,0,\N,Missing
barancikova-etal-2014-improving,P02-1040,0,\N,Missing
barancikova-etal-2014-improving,N06-1058,0,\N,Missing
barancikova-etal-2014-improving,W07-0734,0,\N,Missing
barancikova-etal-2014-improving,P05-1074,0,\N,Missing
barancikova-etal-2014-improving,P10-2016,0,\N,Missing
barancikova-etal-2014-improving,bojar-etal-2012-joy,1,\N,Missing
barancikova-etal-2014-improving,W13-2202,0,\N,Missing
barancikova-etal-2014-improving,P00-1056,0,\N,Missing
K18-2019,W17-0401,0,0.0905027,"Missing"
K18-2019,N13-1073,0,0.0190644,"s there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a prominent word form and is thus likely to be translated correctly even in this way. 6 https://unimorph.github.io/ 7 We have not evaluated the influence on delexicalized 2 http://www.opensubtitles.org/ http://opus.nlpl.eu/"
K18-2019,L16-1147,0,0.0188728,"icon, we change its tag (unless it is AUX), lemma, and morphological features according to the lexicon. Each feature that was mapped from UniMorph style to UD style is added to the features obtained by the tagger. In case it was there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a pro"
K18-2019,P15-2040,1,0.890588,"Missing"
K18-2019,W17-7615,1,0.859487,"Missing"
K18-2019,W17-1226,1,0.848863,"ost of the target languages; the specific setups used for each of the target languages are described in later sections. 2.1 6. now UDPipe can be trained in a standard way on the resulting pseudo-target treebank and applied to target texts Treebank translation using parallel data 2.2 Tiedemann (2014) introduced the approach of automatically translating the word forms in a source treebank into the target language, and then training a pseudo-target parser (and/or a tagger) on the resulting pseudo-target treebank. This approach was further investigated by Rosa ˇ et al. (2017), Rosa and Zabokrtsk´ y (2017) and Rosa (2018a), finding that the sophistication of the Machine Translation (MT) system plays a rather minor role in cross-lingual parsing, while there is a significant benefit in using word-based translation – this forces the translations to be more literal, and enables a trivial approach to annotation transfer. In this work, we use probably the simplest possible approach, based on extracting a dictionary from word-aligned data, and translating each source word into the target word most frequently aligned to it, ignoring any context or other information. While we had found that using state-"
K18-2019,N06-2033,0,0.118422,"s-lingual parsing of target languages without any training data, McDonald et al. (2013) showed that combining syntactic information from multiple source languages can lead to a more accurate parsing than when using only one source language. Moreover, this idea can be easily extended to target languages with small training data, combining the target language resources with larger resources for other close languages (Zhang and Barzilay, 2015). To combine the multilingual resources, we use the weighted parse tree combination method of ˇ Rosa and Zabokrtsk´ y (2015), which is based on the work of Sagae and Lavie (2006). It consists of training separate parsers on the source language treebanks (and also the target language treebank if it is available), applying them independently to the input sentence, and then combining the resulting dependency trees into a directed graph, with each edge weighted by a sum of weights of the parsers which produced this edge. The final parse tree is then obtained by applying the directed maximum spanning tree algorithm of Chu and Liu (1965) and Edmonds (1967) to the weighted graph. To make the source parser applicable to the target language sentences, we either use a translati"
K18-2019,L16-1680,0,0.0680977,"Missing"
K18-2019,tiedemann-2012-parallel,0,0.0605145,"ing to the lexicon. Each feature that was mapped from UniMorph style to UD style is added to the features obtained by the tagger. In case it was there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a prominent word form and is thus likely to be translated correctly even in thi"
K18-2019,C14-1175,0,0.0492526,"019 and each lemma5 by its translation from the translation table (keep the word untranslated if it does not appear in the translation table) cross-lingual techniques, as an enrichment of the baseline approach to achieve better performance. In this section, we introduce several approaches that we apply to many or most of the target languages; the specific setups used for each of the target languages are described in later sections. 2.1 6. now UDPipe can be trained in a standard way on the resulting pseudo-target treebank and applied to target texts Treebank translation using parallel data 2.2 Tiedemann (2014) introduced the approach of automatically translating the word forms in a source treebank into the target language, and then training a pseudo-target parser (and/or a tagger) on the resulting pseudo-target treebank. This approach was further investigated by Rosa ˇ et al. (2017), Rosa and Zabokrtsk´ y (2017) and Rosa (2018a), finding that the sophistication of the Machine Translation (MT) system plays a rather minor role in cross-lingual parsing, while there is a significant benefit in using word-based translation – this forces the translations to be more literal, and enables a trivial approach"
K18-2019,K18-2001,0,0.108655,"Missing"
K18-2019,D15-1213,0,0.146359,"r, as opposed to dependency trees, there are no strict structural constraints, which means that instead of the spanning tree algorithm, we can use a simple weighted voting. For cross-lingual parsing of target languages without any training data, McDonald et al. (2013) showed that combining syntactic information from multiple source languages can lead to a more accurate parsing than when using only one source language. Moreover, this idea can be easily extended to target languages with small training data, combining the target language resources with larger resources for other close languages (Zhang and Barzilay, 2015). To combine the multilingual resources, we use the weighted parse tree combination method of ˇ Rosa and Zabokrtsk´ y (2015), which is based on the work of Sagae and Lavie (2006). It consists of training separate parsers on the source language treebanks (and also the target language treebank if it is available), applying them independently to the input sentence, and then combining the resulting dependency trees into a directed graph, with each edge weighted by a sum of weights of the parsers which produced this edge. The final parse tree is then obtained by applying the directed maximum spanni"
P13-3025,W12-3132,1,0.855405,"Missing"
P13-3025,2011.mtsummit-papers.35,0,0.451304,"Missing"
P13-3025,W12-3130,0,0.3097,"B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng 1.0 (Bojar et al., 2012b), a large EnglishCzech parallel corpus. Identically to the paper, we split the training data into 10 parts, trained 10 systems (each on nine tenths of the data) and used them to translate the remaining part. The second step was then trained on the concatenation of these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are report"
P13-3025,P07-2045,0,0.00639385,"successful research in statistical post-editing (SPE) of SMT (see Section 2). In our paper, we describe a statistical approach to correcting one particular type of English-toCzech SMT errors – errors in the verb-noun valency. The term valency stands for the way in which verbs and their arguments are used together, usually together with prepositions and morphological cases, and is described in Section 4. Several examples of the valency of the English verb ‘to go’ and the corresponding Czech verb ‘j´ıt’ are shown in Table 1. We conducted our experiments using a state-ofthe-art SMT system Moses (Koehn et al., 2007). An example of Moses making a valency error is translating the sentence ‘The government spends on the middle schools.’, adapted from our development data set. As shown in Table 2, Moses translates the sentence incorrectly, making an error in the valency of the ‘utr´acet – sˇkola’ (‘spend – school’) pair. The missing preposition changes the meaning dramatically, as the verb ‘utr´acet’ is pol172 Proceedings of the ACL Student Research Workshop, pages 172–179, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Direction en→cs cs→en ysemous and can mean ‘to spend ("
P13-3025,bojar-etal-2012-joy,1,0.896011,"Missing"
P13-3025,W04-3250,0,0.11784,"these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1 Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, possibly because our training data is too large for this method to bring any benefit. We therefore proceed with a more comple"
P13-3025,P05-1012,0,0.164008,"Missing"
P13-3025,J03-1002,0,0.00593089,"Missing"
P13-3025,W10-1703,0,0.0747674,"Missing"
P13-3025,W07-0704,0,0.368871,"to automatically determine the structure of each sentence, and to detect and correct valency errors using a simple statistical valency model. We describe our approach in detail in Section 5. We evaluate and discuss our experiments in Section 6. We then conclude the paper and propose areas to be researched in future in Section 7. 2 Baseline 10.85±0.47 17.20±0.53 SPE 10.70±0.44 17.11±0.52 Context SPE 10.73±0.49 17.18±0.54 Table 3: Results of SPE approach of B´echara et al. (2011) evaluated on English-Czech SMT. words, eventually reaching an improvement of 2.29 BLEU points. However, similarly to Oflazer and El-Kahlout (2007), the training data used are very small, and it is not clear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. Related Work The first reported results of au"
P13-3025,W11-2103,0,0.0479084,"Missing"
P13-3025,P02-1040,0,0.0879185,"nd reported that the SMT system alone performed worse than the post-edited rule-based system. They then tried to post-edit the bilingual SMT system with another monolingual instance of the same SMT system, but concluded that no improvement in quality was observed. The first known positive results in SPE of SMT are reported by Oflazer and El-Kahlout (2007) on English to Turkish machine translation. The authors followed a similar approach to Simard et al. (2007), training an SMT system to postedit its own output. They use two iterations of post-editing to get an improvement of 0.47 BLEU points (Papineni et al., 2002). The authors used a rather small training set and do not discuss the scalability of their approach. To the best of our knowledge, the best results reported so far for SPE of SMT are by B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source 3 Evaluation o"
P13-3025,W12-3102,0,0.0733791,"Missing"
P13-3025,W12-4205,1,0.902315,"Missing"
P13-3025,W12-3146,1,0.908164,"Missing"
P13-3025,N07-1064,0,0.205241,"ear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. Related Work The first reported results of automatic post-editing of machine translation outputs are (Simard et al., 2007) where the authors successfully performed statistical post-editing (SPE) of rule-based machine translation outputs. To perform the postediting, they used a phrase-based SMT system in a monolingual setting, trained on the outputs of the rule-based system as the source and the humanprovided reference translations as the target, to achieve massive translation quality improvements. The authors also compared the performance of the post-edited rule-based system to directly using the SMT system in a bilingual setting, and reported that the SMT system alone performed worse than the post-edited rule-ba"
P13-3025,W07-1709,0,0.0764048,"Missing"
P15-2040,P15-2044,0,0.0650444,"Missing"
P15-2040,P05-1012,0,0.0425171,"spanning tree over the graph, using the algorithm of Chu and Liu (1965) and Edmonds (1967). 4 KLcpos 3 Language Similarity Delexicalized Parser Transfer We introduce KLcpos 3 , a language similarity measure based on distributions of coarse POS tags in source and target POS-tagged corpora. This is motivated by the fact that POS tags constitute a key feature for delexicalized parsing. The distributions are estimated as frequencies of UPOS trigrams3 in the treebank training sections: Throughout this work, we use MSTperl (Rosa, 2015b), an implementation of the unlabelled single-best MSTParser of McDonald et al. (2005b), with first-order features and nonprojective parsing, trained using 3 iterations of MIRA (Crammer and Singer, 2003).1 Our delexicalized feature set is based on the set of McDonald et al. (2005a) with lexical features removed. It consists of combinations of signed edge length (distance of head and parent, bucketed for values above 4 and for values above 10) with POS tag of the head, dependent, their neighbours, and all nodes between them.2 We use the Universal POS Tagset (UPOS) of Petrov et al. (2012). f (cpos i−1 , cpos i , cpos i+1 ) = count(cpos i−1 , cpos i , cpos i+1 ) =P ; (1) ∀cpos a,"
P15-2040,H05-1066,0,0.147077,"Missing"
P15-2040,D11-1006,0,0.699345,"language similarity estimation, and the test sections for evaluation.6 5.1 Avg KL−4 (tgt, src) cpos 3 Table 1: Weighted multi-source transfer using various similarity measures. Evaluation using average UAS on the development set. 4.1 KLcpos 3 for Source Selection 5 Measure 5.2 Other datasets Additionally, we also report preliminary results on the Prague style conversion of HamleDT, which loosely follows the style of the Prague Dependency Treebank of B¨ohmov´a et al. (2003), and on the subset of CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nilsson et al., 2007) that was used by McDonald et al. (2011).8 Tuning To avoid overfitting the exact definition of KLcpos 3 and KL−4 to the 30 treebanks, we used only 12 cpos 3 6 4 The KL divergence is non-symmetric; DKL (P ||Q) expresses the amount of information lost when a distribution Q is used to approximate the true distribution P . Thus, in our setting, we use DKL (tgt||src), as we try to minimize the loss of using a src parser as an approximation of a tgt parser. 5 A high value of the exponent strongly promotes the most similar source language, giving minimal power to the other languages, which is good if there is a very similar source language"
P15-2040,W06-2920,0,0.0983583,"ts. We use the treebank training sections for parser training and language similarity estimation, and the test sections for evaluation.6 5.1 Avg KL−4 (tgt, src) cpos 3 Table 1: Weighted multi-source transfer using various similarity measures. Evaluation using average UAS on the development set. 4.1 KLcpos 3 for Source Selection 5 Measure 5.2 Other datasets Additionally, we also report preliminary results on the Prague style conversion of HamleDT, which loosely follows the style of the Prague Dependency Treebank of B¨ohmov´a et al. (2003), and on the subset of CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nilsson et al., 2007) that was used by McDonald et al. (2011).8 Tuning To avoid overfitting the exact definition of KLcpos 3 and KL−4 to the 30 treebanks, we used only 12 cpos 3 6 4 The KL divergence is non-symmetric; DKL (P ||Q) expresses the amount of information lost when a distribution Q is used to approximate the true distribution P . Thus, in our setting, we use DKL (tgt||src), as we try to minimize the loss of using a src parser as an approximation of a tgt parser. 5 A high value of the exponent strongly promotes the most similar source language, giving minimal power to the other lang"
P15-2040,P12-1066,0,0.515752,"st source treebank. An alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2013) to estimate source-target language similarity for delexicalized transfer, focusing on genealogy distance and word-order features. Søgaard and Wulff (2012) also introduced weighting into the treebank concatenation approach, using a POS ngram model trained on a target-language corpus Introduction The approach of delexicalized dependency parser transfer is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably part-of-speech (POS) tags, and to apply"
P15-2040,P11-1061,0,0.077807,"Missing"
P15-2040,petrov-etal-2012-universal,0,0.116175,"Missing"
P15-2040,P11-1157,0,0.0820505,"Missing"
P15-2040,rosa-etal-2014-hamledt,1,0.346083,"Missing"
P15-2040,zeman-2008-reusable,0,0.164507,"Missing"
P15-2040,W15-2131,1,0.650057,"plied by its weight. 4. Find the final dependency parse tree as the maximum spanning tree over the graph, using the algorithm of Chu and Liu (1965) and Edmonds (1967). 4 KLcpos 3 Language Similarity Delexicalized Parser Transfer We introduce KLcpos 3 , a language similarity measure based on distributions of coarse POS tags in source and target POS-tagged corpora. This is motivated by the fact that POS tags constitute a key feature for delexicalized parsing. The distributions are estimated as frequencies of UPOS trigrams3 in the treebank training sections: Throughout this work, we use MSTperl (Rosa, 2015b), an implementation of the unlabelled single-best MSTParser of McDonald et al. (2005b), with first-order features and nonprojective parsing, trained using 3 iterations of MIRA (Crammer and Singer, 2003).1 Our delexicalized feature set is based on the set of McDonald et al. (2005a) with lexical features removed. It consists of combinations of signed edge length (distance of head and parent, bucketed for values above 4 and for values above 10) with POS tag of the head, dependent, their neighbours, and all nodes between them.2 We use the Universal POS Tagset (UPOS) of Petrov et al. (2012). f (c"
P15-2040,N06-2033,0,0.490553,"ng treebank annotation styles into a common style, which later developed into the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delexicalized transfer in a setting with multiple source treebanks available, finding that the problem of selecting the best source treebank without access to a target language treebank for evaluation is non-trivial. They combined all source treebanks by concatenating them but noted that this yields worse results than using only the best source treebank. An alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2"
P15-2040,C12-2115,0,0.472057,"n alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2013) to estimate source-target language similarity for delexicalized transfer, focusing on genealogy distance and word-order features. Søgaard and Wulff (2012) also introduced weighting into the treebank concatenation approach, using a POS ngram model trained on a target-language corpus Introduction The approach of delexicalized dependency parser transfer is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably part-of-speech (POS) tags, and to apply that parser to POS-tagged"
P15-2040,Q13-1001,0,0.0109865,"Missing"
P15-2040,N13-1126,0,0.496894,"Missing"
P15-2040,I08-3008,0,0.700849,"Missing"
P15-2040,de-marneffe-etal-2014-universal,0,\N,Missing
P15-2040,N10-1091,0,\N,Missing
P15-2040,D07-1096,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2006-generating,0,\N,Missing
rosa-etal-2014-hamledt,zeman-2008-reusable,1,\N,Missing
rosa-etal-2014-hamledt,J93-2004,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2014-universal,0,\N,Missing
rosa-etal-2014-hamledt,C00-2143,0,\N,Missing
rosa-etal-2014-hamledt,W08-1301,0,\N,Missing
rosa-etal-2014-hamledt,W13-3721,0,\N,Missing
rosa-etal-2014-hamledt,D11-1006,0,\N,Missing
rosa-etal-2014-hamledt,P13-1051,1,\N,Missing
rosa-etal-2014-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
rosa-etal-2014-hamledt,berovic-etal-2012-croatian,0,\N,Missing
rosa-etal-2014-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
rosa-etal-2014-hamledt,W03-2405,0,\N,Missing
rosa-etal-2014-hamledt,P13-2017,0,\N,Missing
rosa-etal-2014-hamledt,taule-etal-2008-ancora,0,\N,Missing
rosa-etal-2014-hamledt,W10-1819,0,\N,Missing
rosa-etal-2014-hamledt,afonso-etal-2002-floresta,0,\N,Missing
W11-2152,W10-1705,1,0.760025,"decimal and thousand separators in numbers. While there are language-specific conventions, they are not always followed and the normalization can in such cases confuse the order of magnitude by 3. 427 the output based on non-normalized test sets as our primary English-to-Czech submission. We invested much less effort into the submission called CU - BOJAR for Czech-to-English. The only interesting feature there is the use of alternative decoding paths to translate either from the Czech form or from the Czech lemma equipped with meaningbearing morphological properties, e.g. the number of nouns. Bojar and Kos (2010) used the same setup with simple lemmas in the fallback decoding path. The enriched lemmas perform marginally better. 2.3 Two-step translation Our two-step translation is essentially the same setup as detailed by Bojar and Kos (2010): (1) the English source is translated to simplified Czech, and (2) the simplified Czech is monotonically translated to fully inflected Czech. Both steps are simple phrase-based models. Instead of word forms, the simplified Czech uses lemmas enriched by a subset of morphological features selected manually to encode only properties overt both in English and Czech su"
W11-2152,H05-1066,0,0.0673478,"Missing"
W11-2152,J03-1002,0,0.00353942,"ical example of a correction is the agreement between the subject and the predicate: they should share the morphological number and gender. If they do not, we simply change the number and gender of the predicate in agreement with the subject.4 An example of such a changed predicate is in Figure 1. Apart from the dependency tree of the target sentence, we can also use the dependency tree of the source sentence. Source sentences are grammatically correct and the accuracy of the tagger and the parser is accordingly higher there. Words in the source and target sentences are aligned using GIZA++5 (Och and Ney, 2003) but verbose outputs of the original MT systems would be possibly a better option. The rules for fixing grammatical agreement between words can thus consider also the dependency relations and morphological caregories of their English counterparts in the input sentence. 4 In this case, we suppose that the number of the subject has a much higher chance to be correct. 5 GIZA++ was run on lemmatized texts in both directions and intersection symmetrization was used. 428 . AuxK came Pred people pl Sb Some pl Atr later Adv . AuxK přišel lidé Někteří Pred sg, m Sb pl Atr později Adv přišli Pred pl Fig"
W11-2152,W07-1709,0,0.109407,"Missing"
W11-2152,N07-1064,0,\N,Missing
W12-3146,hajic-etal-2012-announcing,0,0.0154183,"Missing"
W12-3146,W11-2103,0,0.036468,"ced the rule set used by the original DEPFIX system and measured the performance of the individual rules. We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems. 1 Introduction The today’s outputs of Machine Translation (MT) often contain serious grammatical errors. This is particularly apparent in statistical MT systems (SMT), which do not employ structural linguistic rules. These systems have been dominating the area in the recent years (Callison-Burch et al., 2011). Such errors make the translated text less fluent and may even lead to unintelligibility or misleading statements. The problem is more evident in languages with rich morphology, such as Czech, where morphological agreement is of a relatively high importance for the interpretation of syntactic relations. The DEPFIX system (Mareˇcek et al., 2011) attempts to correct some of the frequent SMT sys∗ This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH120"
W12-3146,2009.mtsummit-commercial.6,0,0.0396722,"Missing"
W12-3146,P07-2045,0,0.00498126,"atures computed over its aligned source word, if there is one. To address the differences between the gold standard training data and SMT outputs, we “worsen” the treebank used to train the parser, i.e. introduce errors similar to those found in target sentences: The trees retain their correct structure, only the word forms are modified to resemble SMT output. We have computed a “part-of-speech tag error model” on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al., 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al., 2007) and estimating the Maximum Likelihood probabilities of errors for each part-ofspeech tag. We then applied this error model to the Czech PCEDT 2.0 sentences and used the resulting “worsened” treebank to train the parser. 4 Rules 2012 uses 20 hand-written rules, addressing various frequent errors in MT output. Each rule takes an analyzed target sentence as its input, often together with its analyzed source senDEPFIX 2 http://ufal.mff.cuni.cz/treex 363 tence, and attempts to correct any errors found – usually by changing morphosyntactic categories of a word (such as number, gender, case, person"
W12-3146,N09-2055,0,0.298447,"Missing"
W12-3146,H05-1066,0,0.00799007,"Missing"
W12-3146,J03-1002,0,0.0030145,"ors induced by this type of MT systems, it can be applied to virtually any MT system (Mareˇcek et al., 2011). 362 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 362–368, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics TectoMT/Treex NLP framework (Popel and ˇ Zabokrtsk´ y, 2010),2 using the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005) trained on the CoNLL 2007 Shared Task English data (Nivre et al., 2007) to analyze the source sentences. The source and target sentences are aligned using GIZA++ (Och and Ney, 2003). 3 Parsing The DEPFIX 2011 system used the MST parser (McDonald et al., 2005) with an improved feature set ˇ for Czech (Nov´ak and Zabokrtsk´ y, 2007) trained on the Prague Dependency Treebank (PDT) 2.0 (Hajiˇc and others, 2006) to analyze the target sentences. DEPFIX 2012 uses a reimplementation of the MST parser capable of utilizing parallel features from the source side in the parsing of the target sentence. The source text is usually grammatical and therefore is likely to be analyzed more reliably. The source structure obtained in this way can then provide hints for the target parser. We"
W12-3146,P02-1040,0,0.104954,"Missing"
W12-3146,W07-1709,0,0.0676744,"Missing"
W12-3146,W11-2152,1,\N,Missing
W12-3146,N07-1064,0,\N,Missing
W12-3146,D07-1096,0,\N,Missing
W12-4205,hajic-etal-2012-announcing,0,0.0852801,"Missing"
W12-4205,W06-2920,0,0.0367148,"targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in"
W12-4205,N10-1015,0,0.0755019,"011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,W10-1703,0,0.0245956,"parser training data, so that the training sentences resemble SMT output. We evaluate the modified parser on DEP FIX , a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality. 1 Introduction The machine translation (MT) quality is on a steady rise, with mostly statistical systems (SMT) dominating the area (Callison-Burch et al., 2010; CallisonBurch et al., 2011). Most MT systems do not employ structural linguistic knowledge and even the stateof-the-art MT solutions are unable to avoid making serious grammatical errors in the output, which often leads to unintelligibility or to a risk of misinterpretations of the text by a reader. ∗ This research has been supported by the EU Seventh Framework Programme under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310 and GA201/09/H057. This problem is particularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the"
W12-4205,W11-2103,0,0.0450256,"Missing"
W12-4205,P10-1003,0,0.0232916,"ek et al., 2011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,P99-1065,0,0.316623,"Missing"
W12-4205,P08-2056,0,0.0358,"Missing"
W12-4205,D09-1127,0,0.0558467,"Missing"
W12-4205,P07-2045,0,0.0114404,"g one, creating the respective alignment link from word A (in the reference) to word B (in the SMT output) and deleting all scores of links from A or to B, so that one-to-one alignments are enforced. The process is terminated when no links with a score higher than a given threshold are available; some words may thus remain unaligned. The score is computed as a linear combination of the following four features: • word form (or lemma if available) similarity based on Jaro-Winkler distance (Winkler, 1990), 1. We translated the English side of PCEDT5 to Czech using SMT (we chose the Moses system (Koehn et al., 2007) for our experiments) and tagged the resulting translations using the Morˇce tagger (Spoustov´a et al., 2007). • fine-grained morphological tag similarity, • similarity of the relative position in the sentence, 2. We aligned the Czech side of PCEDT, now serving as a reference translation, to the SMT output using our Monolingual Greedy Aligner (see Section 4.2). 3. Collecting the counts of individual errors, we estimated the Maximum Likelihood probabilities of changing a correct fine-grained morphological tag (of a word from the reference) into a possibly incorrect fine-grained morphological ta"
W12-4205,J93-2004,0,0.0469362,"er. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR"
W12-4205,H05-1066,0,0.30865,"Missing"
W12-4205,W06-2932,0,0.0233358,"entence first and include features computed over the parsed source sentence in the set of features used for parsing SMT output. We first align the source and SMT output sentences on the word level and then use alignment-wise local features – i.e. for each SMT output word, we add features computed over its aligned source word, if applicable (cf. Section 3.4 for a listing). 3.2 Parsers Used We have reimplemented the MST parser (McDonald et al., 2005) in order to provide for a simple insertion of the parallel features into the models. We also used the original implementation of the MST parser by McDonald et al. (2006) for comparison in our experiments. To distinguish the two variants used, we denote the original MST parser as M C D parser,2 and the new reimplementation as RUR parser. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the"
W12-4205,D11-1006,0,0.0305903,"010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and"
W12-4205,J03-1002,0,0.0141147,"IRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR parser follows those described by McDonald et al. (2005). For parsing, we use the features described below. The individual features are computed for both the parent node and the child node of an edge and conjoined in various ways. The coarse morphological tag and lemma are provided by the Morˇce tagger (Spoustov´a"
W12-4205,P02-1040,0,0.0832465,"Missing"
W12-4205,W07-1709,0,0.0974333,"Missing"
W12-4205,stymne-ahrenberg-2010-using,0,0.0223864,"icularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the relations between individual words using morphological agreement instead of word order, together with the word order itself being relatively free, choosing the correct inflection becomes crucial. Since the output of phrase-based SMT shows frequent inflection errors (even in adjacent words) due to each word belonging to a different phrase, a possible way to address the grammaticality problem is a combination of statistical and structural approach, such as SMT output post-editing (Stymne and Ahrenberg, 2010; Mareˇcek et al., 2011). In this paper, we focus on improving SMT output parsing quality, as rule-based post-editing systems rely heavily on the quality of SMT output analysis. Parsers trained on gold standard parse trees often fail to produce the expected result when applied to SMT output with grammatical errors. This is partly caused by the fact that when parsing highly inflected free word-order languages the parsers have to rely on morphological agreement, which, as stated above, is often erroneous in SMT output. Training a parser specifically by creating a manually annotated treebank of M"
W12-4205,J97-3002,0,0.138024,"ation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in a joint scenario. We concentrate here on using dependency parsers because of tools and training data availability for the examined language pair. Regarding treebank adaptation for parser robustness, Foster et al. (2008) introduce various kinds of artificial errors into the training data to make the final parser less sensitive to grammar errors. However, their approach concentrates on mistakes made by humans (such as misspellings, word repetition or omission etc.) and the error models used are handcrafted. Our work focuse"
W12-4205,P10-1062,0,0.0211229,"ncy scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT output sentences and, ultimately, improve rule-based SMT post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDo"
W12-4205,I08-3008,0,0.0945642,"post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences"
W12-4205,P09-1007,0,0.0247054,"“RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual"
W12-4205,W11-2152,1,\N,Missing
W12-4205,D11-1007,0,\N,Missing
W12-4205,W09-1201,0,\N,Missing
W12-4205,D07-1096,0,\N,Missing
W12-4205,W10-1705,0,\N,Missing
W13-2208,W12-3139,0,0.0143246,"rain a 4gram language model using KenLM (Heafield et al., 2013). Unfortunately, we did not manage to use a model of higher order. The model file (even in the binarized trie format with probability quantization) was so large that we ran out of memory in decoding.5 We also tried pruning these larger models but we did not have enough RAM. To cater for a longer-range coherence, we trained a 7-gram language model only on the News Crawl corpus (concatenation of all years). In this case, we used SRILM (Stolcke, 2002) and pruned n-grams so that (training set) model perplexity 2.1.5 Bigger Tuning Sets Koehn and Haddow (2012) report benefits from tuning on a larger set of sentences. We experimented with a down-scaled MT system to compare a couple of options for our tuning set: the default 3003 sentences of newstest2011, the default and three more Czech references that were created by translating from German, the default and two more references that were created by postediting a variant of our last year’s Moses system and also a larger single-reference set consisting of several newstest years. The preliminary results were highly inconclusive: negligibly higher BLEU scores obtained lower manual scores. Unable to pic"
W13-2208,W07-0702,0,0.0218649,"h and in Czech, such as: – a subject in English is marked by being a left modifier of the predicate, while in Czech a subject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation t"
W13-2208,2005.mtsummit-papers.11,0,0.005624,"hus less prone to errors in local morphological agreement. Table 1 summarizes the final (case-sensitive!) BLEU scores for four setups. The standard approach is to train SMT lowercase and apply a recaser, e.g. the Moses one, on the output. Another option (denoted “lc→form”) is to lowercase only the source side of the parallel data. This more or less makes the translation model responsible for identifying names and the language model for identifying beginnings of sentences. 2.1.3 Large Parallel Data The main source of our parallel data was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of paralle"
W13-2208,W11-2138,1,0.834334,"e nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation to Czech by CU - BOJAR. The analysis includes tagging, word-alignment, and dependency parsing both to shallow-syntax (“analytical”) and deep-sy"
W13-2208,W11-2101,1,0.892202,"Missing"
W13-2208,P00-1056,0,0.1175,"mes and the language model for identifying beginnings of sentences. 2.1.3 Large Parallel Data The main source of our parallel data was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in B"
W13-2208,W12-3130,1,0.916037,"gure 1. Each of the intermediate stages of processing has been submitted as a separate primary system for the WMT manual evalution, allowing for a more thorough analysis. Instead of an off-the-shelf system combination technique, we use TectoMT output as synthetic training data for Moses as described in Section 2.1 and finally we process its output using rule-based corrections of Depfix (Section 2.2). All steps directly use the source sentence. 2.1 Moses Setup for CU - BOJAR We ran a couple of probes with reduced training data around the setup of Moses that proved successful in previous years (Bojar et al., 2012a). 2.1.1 Pre-processing We use a stable pre-processing pipeline that includes normalization of quotation marks,1 tokenization, tagging and lemmatization with tools 1 We do not simply convert them to unpaired ASCII quotes but rather balance them and use other heuristics to convert most cases to the typographically correct form. 92 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92–98, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Case BLEU recaser 9.05 lc→form 9.13 utc 9.70 stc 9.81 Corpus CzEng 1.0 Europarl Common Crawl Table 1"
W13-2208,W12-3146,1,0.783173,"Missing"
W13-2208,N12-1047,0,0.026653,"person, if applicable • transfer of meaning in cases where the same meaning is expressed by different grammatical means in English and in Czech, such as: – a subject in English is marked by being a left modifier of the predicate, while in Czech a subject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System"
W13-2208,P13-3025,1,0.887247,"Missing"
W13-2208,P11-2031,0,0.0255261,"for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in BLEU using the procedure by Clark et al. (2011) and GIZA++ alignments were indeed The final two approaches attempt at “truecasing” the data, i.e. the ideal lowercasing of everything except names. Our simple unsupervised truecaser (“utc”) uses a model trained on monolingual data (1 million sentences in this case, same as the parallel training data used in this experiment) to identify the most frequent “casing shape” of each token type when it appears within a sentence and then converts its occurrences at the beginnings of sentences to this shape. Our supervised truecaser (“stc”) casts the case of the lemma on the form, because our lemmatize"
W13-2208,W08-0328,0,0.0307767,"bject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation to Czech by CU - BOJAR. The analysis includes tagging, word-alignment, and dependency parsing both to shallow-syntax"
W13-2208,spoustova-spousta-2012-high,0,0.0280766,"Missing"
W13-2208,E12-1068,0,0.0139164,"matic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. 1 2 System Description Input TectoMT Moses Depﬁx Introduction cu-tectomt cu-bojar cu-depﬁx = Chimera Figure 1: C HIMERA: three systems combined. Targeting Czech in statistical machine translation (SMT) is notoriously difficult due to the large number of possible word forms and complex agreement rules. Previous attempts to resolve these issues include specific probabilistic models (Subotin, 2011) or leaving the morphological generation to a separate processing step (Fraser et al., 2012; Mareˇcek et al., 2011). TectoMT (CU - TECTOMT, Galuˇscˇ a´ kov´a et al. (2013)) is a hybrid (rule-based and statistical) MT system that closely follows the analysis-transfersynthesis pipeline. As such, it suffers from many issues but generating word forms in proper agreements with their neighbourhood as well as the translation of some diverging syntactic structures are handled well. Overall, TectoMT sometimes even ties with a highly tuned Moses configuration in manual evaluations, see Bojar et al. (2011). Finally, Rosa et al. (2012) describes Depfix, a rule-based system for post-processing ("
W13-2208,P11-1024,0,0.0163434,"vel of representation, factored phrase-based translation using Moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. 1 2 System Description Input TectoMT Moses Depﬁx Introduction cu-tectomt cu-bojar cu-depﬁx = Chimera Figure 1: C HIMERA: three systems combined. Targeting Czech in statistical machine translation (SMT) is notoriously difficult due to the large number of possible word forms and complex agreement rules. Previous attempts to resolve these issues include specific probabilistic models (Subotin, 2011) or leaving the morphological generation to a separate processing step (Fraser et al., 2012; Mareˇcek et al., 2011). TectoMT (CU - TECTOMT, Galuˇscˇ a´ kov´a et al. (2013)) is a hybrid (rule-based and statistical) MT system that closely follows the analysis-transfersynthesis pipeline. As such, it suffers from many issues but generating word forms in proper agreements with their neighbourhood as well as the translation of some diverging syntactic structures are handled well. Overall, TectoMT sometimes even ties with a highly tuned Moses configuration in manual evaluations, see Bojar et al. (201"
W13-2208,W13-2216,1,0.598857,"Missing"
W13-2208,W08-0509,0,0.0189865,"ta was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in BLEU using the procedure by Clark et al. (2011) and GIZA++ alignments were indeed The final two approaches attempt at “truecasing”"
W13-2208,P13-2121,0,0.0341719,"Missing"
W13-2208,W11-2152,1,\N,Missing
W13-2208,bojar-etal-2012-joy,1,\N,Missing
W14-3322,W11-2138,1,0.918214,"Missing"
W14-3322,W13-2208,1,0.864737,"DEPFIX, Depfix post-processing is added; and CU - FUNKY also employs documentspecific language models. Introduction 2 TectoMT (§2.4) Factored Moses (§2.1) Adapted LM (§2.2) Document-specific LMs (§2.3) Depfix (§2.5) Y NK FI X -D -FU CU EP R -BO JA CU CU -TE CT OM T In this paper, we describe translation systems submitted by Charles University (CU or CUNI) to the Translation task of the Ninth Workshop on Statistical Machine Translation (WMT) 2014. In §2, we present our English→Czech systems, CU - TECTOMT, CU - BOJAR , CU - DEPFIX and CU FUNKY . The systems are very similar to our submissions (Bojar et al., 2013) from last year, the main novelty being our experiments with domainspecific and document-specific language models. In §3, we describe our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the fol"
W14-3322,P13-2121,0,0.043073,"Missing"
W14-3322,2005.mtsummit-papers.11,0,0.0931675,"our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the following) is similar to last year – we trained a factored Moses model on the concatenation of CzEng (Bojar et al., 2012) and Europarl (Koehn, 2005), see Table 2. We use two factors: tag, which is the part-of-speech tag, and stc, which is “supervised truecasing”, i.e. the surface form with letter case set according to the lemma; see (Bojar et al., 2013). Our factored Moses system translates from English stc to Czech stc |tag in one translation step. Our basic language models are identical to last year’s submission. We added an adapted language English→Czech Our submissions for English→Czech build upon last year’s successful C HIMERA system (Bojar et al., 2013). We combine several different approaches: • factored phrase-based Moses model ("
W14-3322,W10-1730,1,0.935053,"Missing"
W14-3322,P00-1056,0,0.191162,"on of vocabulary reported in the paper is to roughly one half. In our case, the vocabulary is reduced much more, so we opted for a more conservative back-off, namely “nosuf2”. Baseline System The baseline system was eventually our bestperforming one. Its design is completely straightforward – it uses one phrase table trained on all parallel data (we translate from “supervisedtruecased” English into Hindi forms) and one 5gram language model trained on all monolingual data. We used KenLM (Heafield et al., 2013) for estimating the model as the data was rather large (see Table 6). We used GIZA++ (Och and Ney, 2000) as our word alignment tool. We experimented with several coarser representations to make the final alignment more reliable. Table 7 shows the results. The factor “stem4” refers to simply taking the first four characters of each word. For lemmas, we used the outputs of the tools mentioned above. However, lemmas as output by the Hindi tagger were not much coarser than surface forms – the ratio between the number of types is merely 1.11 – so we also tried “stemming” the lemmas (lemma4). Of these variants, stem4-stem4 alignment worked best and we used it for the rest of our experiments. 3.2 BLEU"
W14-3322,P02-1040,0,0.0888957,"we do not use any stopwords or keyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→"
W14-3322,2006.amta-papers.25,0,0.0448398,"eyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→Hindi translation. We used Hin"
W14-3322,W07-1709,0,0.0300845,"Missing"
W14-3322,P09-2037,1,0.836354,"er General General General News stc stc tag stc 4 7 10 6 2.4 Sents Tokens ARPA.gz Trie [M] [M] [GB] [GB] 201.31 3430.92 28.2 11.8 24.91 444.84 13.1 8.1 14.83 205.17 7.2 3.0 0.25 4.73 0.2 – Table 4: Czech LMs used in CU - BOJAR. The last small model is described in §2.2. 1 Document-Specific Language Models TectoMT2 was one of the three key components in last year’s C HIMERA. It is a linguisticallymotivated tree-to-tree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models ˇ (Zabokrtsk´ y and Popel, 2009). It is trained on the WMT-provided data: CzEng 1.0 (parallel data) and News Crawl (2007–2012 Czech monolingual sets). We maintain the same approach to combining TectoMT with Moses as last year – we translate WMT test sets from years 2007–2014 and use them as additional synthetic parallel training data – a corpus consisting of the test set source side (English) and TectoMT output (synthetic Czech). We then use the standard extraction pipeline to create 2 http://lucene.apache.org 196 TectoMT Deep-Syntactic MT System http://ufal.mff.cuni.cz/tectomt/ an additional phrase table from this corpus. T"
W14-3322,W12-3148,1,\N,Missing
W14-3322,bojar-etal-2014-hindencorp,1,\N,Missing
W14-3326,D11-1033,0,0.426651,"ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general dom"
W14-3326,2011.iwslt-evaluation.18,0,0.0458693,"ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et"
W14-3326,bojar-etal-2012-joy,1,0.843764,"Missing"
W14-3326,N13-1073,0,0.0271109,"s are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables"
W14-3326,C04-1114,0,0.358032,", 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provi"
W14-3326,E12-3006,0,0.0294557,"Missing"
W14-3326,2005.eamt-1.19,0,0.105464,"nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the stan"
W14-3326,2011.iwslt-papers.5,0,0.0956886,"aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general domain” here is used to denote data Statistical"
W14-3326,P10-2041,0,0.508413,") or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general"
W14-3326,W08-0320,0,0.122804,"paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M"
W14-3326,W07-0733,0,0.0769909,"work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo"
W14-3326,P03-1021,0,0.0285359,"Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably"
W14-3326,P07-2045,0,0.00503854,"ined or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expecte"
W14-3326,2011.mtsummit-plenaries.5,0,0.0420922,"xts of nonmedical patents in the PatTR collection. Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12"
W14-3326,2005.mtsummit-papers.11,0,0.0172802,"and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distribution of the domain-specificity s"
W14-3326,C10-2124,0,0.432112,"arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l"
W14-3326,W02-1405,0,0.131062,"ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co"
W14-3326,E12-1055,0,0.0150313,"n con unc unc concat interpol concat interpol cs→en 30.87±4.70 32.46±5.05 34.88±5.04 33.82±5.16 de→en 33.21±5.03 33.74±4.97 31.24±5.59 34.19±5.27 en→cs 23.25±4.85 21.56±4.80 22.61±4.91 23.93±5.16 en→de 17.72±4.75 16.90±4.39 19.13±5.66 15.87±11.31 en→fr 28.64±3.77 29.34±3.73 33.08±3.80 31.19±3.73 fr→en 35.56±4.94 35.28±5.26 36.73±4.88 40.25±5.14 Table 4: BLEU scores of query translations. each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. The two language models for sentence scoring are trained with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language"
W14-3326,P13-1135,0,0.0121052,"rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distri"
W14-3326,wu-wang-2004-improving-domain,0,0.0358057,"approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et"
W14-3326,W12-3151,1,0.842853,"difference) in the FR–EN parallel data and FR monolingual data is illustrated in Figures 1 and 2, respectively.6 The scores (Y axis) are presented for each sentence in increasing order from left to right (X axis). The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert ""quoted"" to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 6 For the medical domain, constrained"
W14-3326,W12-3102,0,\N,Missing
W14-3326,eck-etal-2004-language,0,\N,Missing
W15-2104,P09-1035,0,0.0599014,"Missing"
W15-2104,P10-2016,0,0.0522402,"Missing"
W15-2104,N06-1058,0,0.0396202,"ojar (2011)). These metrics shows better correlation with human judgment, but their wide usage is limited by being complex and language-dependent. As a result, there is a trade-off between linguistic-rich strategy for better performance and applicability of simple string level matching. Our approach makes use of linguistic tools for creating new reference sentences. The advantage of this method is that we can choose among many traditional metrics for evaluation on our new references while eliminating some shortcomings of these metrics. Targeted paraphrasing for MT evaluation was introduced by Kauchak and Barzilay (2006). Their algorithm creates new reference sentences by one-word substitution based on WordNet (Miller, 1995) synonymy and contextual evaluation. This solution is not readily applicable to the Czech language – a Czech word has typically many forms and the correct form depends heavily on its context, e.g., morphological cases of nouns depend on verb valency frames. Changing a single word may result in an ungrammatical sentence. Therefore, we do not attempt to change a single word in a reference sentence but we focus on creating one single correct reference sentence. In Baranˇc´ıkov´a and Tamchyna"
W15-2104,W11-2101,0,0.0688494,"Missing"
W15-2104,P07-2045,0,0.0055991,"ler, 1995) synonymy and contextual evaluation. This solution is not readily applicable to the Czech language – a Czech word has typically many forms and the correct form depends heavily on its context, e.g., morphological cases of nouns depend on verb valency frames. Changing a single word may result in an ungrammatical sentence. Therefore, we do not attempt to change a single word in a reference sentence but we focus on creating one single correct reference sentence. In Baranˇc´ıkov´a and Tamchyna (2014), we experimented with targeted paraphrasing using the freely available SMT system Moses (Koehn et al., 2007). We adapted Moses for targeted monolingual phrase-based translation. However, results of this method was inconclusive. It was mainly due to a high amount of noise in the translation tables and unbalanced targeting feature. As a result, we rather chose to employ rulebased translation system. This approach has many 3 Treex Treex implements a stratificational approach to language, adopted from the Functional Generative Description theory (Sgall, 1967) and its later extension by the Prague Dependency Treebank (Bejˇcek et al., 2013). It represents sentences at four layers: • w-layer: word layer; n"
W15-2104,W05-0900,0,0.11905,"change only parts of a sentence and thus create more conservative paraˇ phrases. We utilize Treex (Popel and Zabokrtsk´ y, 2010), highly modular NLP software system developed for machine translation system TectoMT ˇ (Zabokrtsk´ y et al., 2008) that translates on a deep syntactic layer. We performed our experiment on the Czech language, however, we plan to extend it to more languages, including English and Spanish. Treex is open-source and is available on GitHub,1 including the two blocks that we contributed. In the rest of the paper, we describe the implementation of our approach. ment (e.g. Liu and Gildea (2005), Owczarzak et al. (2007), Amig´o et al. (2009), Pad´o et al. (2009), Mach´acˇ ek and Bojar (2011)). These metrics shows better correlation with human judgment, but their wide usage is limited by being complex and language-dependent. As a result, there is a trade-off between linguistic-rich strategy for better performance and applicability of simple string level matching. Our approach makes use of linguistic tools for creating new reference sentences. The advantage of this method is that we can choose among many traditional metrics for evaluation on our new references while eliminating some sh"
W15-2104,W11-2108,0,0.0177504,"Popel and Zabokrtsk´ y, 2010), highly modular NLP software system developed for machine translation system TectoMT ˇ (Zabokrtsk´ y et al., 2008) that translates on a deep syntactic layer. We performed our experiment on the Czech language, however, we plan to extend it to more languages, including English and Spanish. Treex is open-source and is available on GitHub,1 including the two blocks that we contributed. In the rest of the paper, we describe the implementation of our approach. ment (e.g. Liu and Gildea (2005), Owczarzak et al. (2007), Amig´o et al. (2009), Pad´o et al. (2009), Mach´acˇ ek and Bojar (2011)). These metrics shows better correlation with human judgment, but their wide usage is limited by being complex and language-dependent. As a result, there is a trade-off between linguistic-rich strategy for better performance and applicability of simple string level matching. Our approach makes use of linguistic tools for creating new reference sentences. The advantage of this method is that we can choose among many traditional metrics for evaluation on our new references while eliminating some shortcomings of these metrics. Targeted paraphrasing for MT evaluation was introduced by Kauchak and"
W15-2104,W13-2202,0,0.0134967,"ment of MT systems. Wellperforming automatic MT evaluation metrics are essential precisely for these tasks. The pioneer metrics correlating well with human judgment were BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). They are computed from an n-gram overlap between the translated sentence (hypothesis) and one or more corresponding reference sentences, i.e., translations made by a human translator. Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist (Mach´acˇ ek and Bojar (2013), Bojar et al. (2014)). Furthermore, the standard practice is using only one reference sentence and BLEU then tends to perform badly. There are many translations of a single sentence and even a perfectly correct translation might get a low score as BLEU disregards 2 Related Work Second generation metrics Meteor (Denkowski and Lavie, 2014), TERp (Snover et al., 2009) and ParaEval (Zhou et al., 2006) still largely focus on an n-gram overlap while including other linguistically motivated resources. They utilize paraphrase support in form of their own paraphrase tables (i.e. collection of synonymo"
W15-2104,H05-1066,0,0.0450882,"Missing"
W15-2104,W10-1751,0,0.0783817,"Missing"
W15-2104,W14-3348,0,0.0225371,"responding reference sentences, i.e., translations made by a human translator. Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist (Mach´acˇ ek and Bojar (2013), Bojar et al. (2014)). Furthermore, the standard practice is using only one reference sentence and BLEU then tends to perform badly. There are many translations of a single sentence and even a perfectly correct translation might get a low score as BLEU disregards 2 Related Work Second generation metrics Meteor (Denkowski and Lavie, 2014), TERp (Snover et al., 2009) and ParaEval (Zhou et al., 2006) still largely focus on an n-gram overlap while including other linguistically motivated resources. They utilize paraphrase support in form of their own paraphrase tables (i.e. collection of synonymous expressions) and show higher correlation with human judgment than BLEU. Meteor supports several languages including Czech. However, its Czech paraphrase tables are so noisy (i.e. they contain pairs of nonparaphrastic expressions) that they actually harm the performance of the metric, as it can reward mistranslated and even untranslated"
W15-2104,W07-0714,0,0.0707683,"Missing"
W15-2104,P02-1040,0,0.0949972,"uted using these paraphrased reference sentences show higher correlation with human judgment than scores computed on the original reference sentences. 1 Introduction Since the very first appearance of machine translation (MT) systems, a necessity for their objective evaluation and comparison has emerged. The traditional human evaluation is slow and unreproducible; thus, it cannot be used for tasks like tuning and development of MT systems. Wellperforming automatic MT evaluation metrics are essential precisely for these tasks. The pioneer metrics correlating well with human judgment were BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). They are computed from an n-gram overlap between the translated sentence (hypothesis) and one or more corresponding reference sentences, i.e., translations made by a human translator. Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist (Mach´acˇ ek and Bojar (2013), Bojar et al. (2014)). Furthermore, the standard practice is using only one reference sentence and BLEU then tends to perform badly. There are many translations of a single sente"
W15-2104,P14-5003,0,0.0304493,"Missing"
W15-2104,W08-0325,0,0.0242082,"ting paying by mobile phone Banks are testing paying by mobile phone telefonu phone Figure 1: Example from WMT12 - Even though the hypothesis is grammatically correct and the meaning of both sentences is the same, it doesn’t contribute to the BLEU score. There is only one unigram overlapping. advantages, e.g. there is no need for creating a targeting feature and we can change only parts of a sentence and thus create more conservative paraˇ phrases. We utilize Treex (Popel and Zabokrtsk´ y, 2010), highly modular NLP software system developed for machine translation system TectoMT ˇ (Zabokrtsk´ y et al., 2008) that translates on a deep syntactic layer. We performed our experiment on the Czech language, however, we plan to extend it to more languages, including English and Spanish. Treex is open-source and is available on GitHub,1 including the two blocks that we contributed. In the rest of the paper, we describe the implementation of our approach. ment (e.g. Liu and Gildea (2005), Owczarzak et al. (2007), Amig´o et al. (2009), Pad´o et al. (2009), Mach´acˇ ek and Bojar (2011)). These metrics shows better correlation with human judgment, but their wide usage is limited by being complex and language-"
W15-2104,W06-1610,0,0.0332281,"anslator. Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist (Mach´acˇ ek and Bojar (2013), Bojar et al. (2014)). Furthermore, the standard practice is using only one reference sentence and BLEU then tends to perform badly. There are many translations of a single sentence and even a perfectly correct translation might get a low score as BLEU disregards 2 Related Work Second generation metrics Meteor (Denkowski and Lavie, 2014), TERp (Snover et al., 2009) and ParaEval (Zhou et al., 2006) still largely focus on an n-gram overlap while including other linguistically motivated resources. They utilize paraphrase support in form of their own paraphrase tables (i.e. collection of synonymous expressions) and show higher correlation with human judgment than BLEU. Meteor supports several languages including Czech. However, its Czech paraphrase tables are so noisy (i.e. they contain pairs of nonparaphrastic expressions) that they actually harm the performance of the metric, as it can reward mistranslated and even untranslated words (Baranˇc´ıkov´a, 2014). String matching is hardly disc"
W15-2104,W12-3102,0,\N,Missing
W15-2104,W14-3302,0,\N,Missing
W15-2104,W13-2201,0,\N,Missing
W15-2131,W08-1301,0,0.187739,"Missing"
W15-2131,de-marneffe-etal-2014-universal,0,0.109252,"Missing"
W15-2131,P13-3005,0,0.20494,"While Stanford style trees may be more useful for further processing in NLP applications, it has been argued that Prague style trees are easier to obtain by using statistical parsers. Among other differences, adpositions provide important cues to the parser for adpositional group attachment, which is one of the most notorious parsing problems. This information becomes harder to access when the adpositions are annotated as leafs. The issue of dependency representation learnability has been studied by several authors, generally reaching similar conclusions (Schwartz et al., 2012; Søgaard, 2013; Ivanova et al., 2013). The approach suggested by de Marneffe et al. (2014) is to use a different annotation style for parsing, with Prague style adposition annotation, among other, and to convert the dependency trees to full Stanford style only after parsing for subsequent applications. Still, while the aforementioned observations seem to hold in the general case, in multilingual parsing scenarios, the higher cross-lingual similarity of Stanford style dependency trees may be of benefit. From all of the differences between Prague and Stanford, the adposition attachment seems to be the most interesting, as adpositio"
W15-2131,P05-1012,0,0.557707,"f the target language. In multi-source delexicalized parser transfer, multiple source treebanks are used for training. McDonald et al. (2011) used simple treebank concatenation, thus obtaining one multilingual source treebank, and trained a multilingual delexicalized parser. In our work, we extend the method of Sagae and Lavie (2006), originally suggested for (monolingual) parser combination. In this approach, several independent parsers are applied to the same input sentence, and the parse trees they produce are combined into one resulting tree. The combination is performed using the idea of McDonald et al. (2005a), who formulated the problem of finding a parse tree as a problem of finding the maximum spanning tree (MST) of a weighted directed graph of potential parse tree edges. In the tree combination method, the weight of each edge is defined as the number of parsers which include that edge in their output (it can thus also be regarded as a parser voting approach). To find the MST, one can use e.g. the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which was used by McDonald et al. (2005b) for non-projective parsing, and which we use in our work. The tree combination method can be ea"
W15-2131,H05-1066,0,0.532277,"Missing"
W15-2131,W13-3733,0,0.0216928,"ional groups.3 While Stanford style trees may be more useful for further processing in NLP applications, it has been argued that Prague style trees are easier to obtain by using statistical parsers. Among other differences, adpositions provide important cues to the parser for adpositional group attachment, which is one of the most notorious parsing problems. This information becomes harder to access when the adpositions are annotated as leafs. The issue of dependency representation learnability has been studied by several authors, generally reaching similar conclusions (Schwartz et al., 2012; Søgaard, 2013; Ivanova et al., 2013). The approach suggested by de Marneffe et al. (2014) is to use a different annotation style for parsing, with Prague style adposition annotation, among other, and to convert the dependency trees to full Stanford style only after parsing for subsequent applications. Still, while the aforementioned observations seem to hold in the general case, in multilingual parsing scenarios, the higher cross-lingual similarity of Stanford style dependency trees may be of benefit. From all of the differences between Prague and Stanford, the adposition attachment seems to be the most in"
W15-2131,N13-1126,0,0.0575287,"Missing"
W15-2131,P12-1066,0,0.0785791,"Missing"
W15-2131,I08-3008,0,0.0157938,"in multilingual parsing scenarios, the higher cross-lingual similarity of Stanford style dependency trees may be of benefit. From all of the differences between Prague and Stanford, the adposition attachment seems to be the most interesting, as adpositions are usually very frequent and diverse in languages, as well as very important in parsing. Therefore, in this work, we evaluate the influence of adposition annotation style in cross-lingual multi-source delexicalized parser transfer. 1.2 Delexicalized parser transfer In the approach of single-source delexicalized dependency parser transfer (Zeman and Resnik, 2008), we train a parser on a treebank for a resource-rich source language, using non-lexical features, most notably part-of-speech (POS) tags, but not using word forms or lemmas. Then, we apply that parser to a POS-tagged corpus of an under-resourced target language, to obtain a dependency parse tree. Delexicalized transfer typically yields worse results than a fully supervised lexicalized parser, trained on a treebank for the target language. However, for a vast majority of languages, there are no manually devised treebanks, in which case it may be useful to obtain at least a lower-quality parse"
W15-2131,petrov-etal-2012-universal,0,0.169892,"Missing"
W15-2131,zeman-etal-2012-hamledt,0,0.0493422,"Missing"
W15-2131,P15-2040,1,0.686717,"Missing"
W15-2131,zeman-2008-reusable,0,0.127178,"Missing"
W15-2131,rosa-etal-2014-hamledt,1,0.914262,"Missing"
W15-2131,N06-2033,0,0.058679,"s no easy way of evaluating such experiments. Rather, we follow the usual way of using target languages for which there is a treebank available and thus the experiments can be easily evaluated, but we do not use the target treebank for training, thus simulating the under-resourcedness of the target language. In multi-source delexicalized parser transfer, multiple source treebanks are used for training. McDonald et al. (2011) used simple treebank concatenation, thus obtaining one multilingual source treebank, and trained a multilingual delexicalized parser. In our work, we extend the method of Sagae and Lavie (2006), originally suggested for (monolingual) parser combination. In this approach, several independent parsers are applied to the same input sentence, and the parse trees they produce are combined into one resulting tree. The combination is performed using the idea of McDonald et al. (2005a), who formulated the problem of finding a parse tree as a problem of finding the maximum spanning tree (MST) of a weighted directed graph of potential parse tree edges. In the tree combination method, the weight of each edge is defined as the number of parsers which include that edge in their output (it can thu"
W15-2131,C12-1147,0,0.369989,"Missing"
W15-2131,C12-2115,0,0.121293,"Missing"
W15-2131,D11-1006,0,\N,Missing
W15-2131,P13-2017,0,\N,Missing
W15-2209,D11-1006,0,0.0844784,"ce language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tre"
W15-2209,P15-2044,0,0.0820359,"Missing"
W15-2209,P12-1066,0,0.234958,"e use the Universal POS Tagset (UPOS) of Petrov et al. (2012). The parser configuration files containing the full feature set, together with the scripts we used for our experiments, are available in (Rosa, 2015a). the method to a crosslingual setting by combining delex parsers for different languages, weighted by src-tgt language similarity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser train"
W15-2209,petrov-etal-2012-universal,0,0.188168,"Missing"
W15-2209,P15-2040,1,0.601957,"Missing"
W15-2209,D11-1005,0,0.0130867,"arity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser training). We use a simplified version of normalization by standard deviation. First, we compute the uncorrected sample standard deviation of the weights of the features in the model as s 1 X sM = (wf − w) ¯ 2, (2) |M | Method In this section, we present our suggested approach of combining information from multiple src treebanks for parsin"
W15-2209,rosa-etal-2014-hamledt,1,0.867626,"Missing"
W15-2209,P11-1061,0,0.0578228,"to the tgt language, and is defined as the negative fourth power of the KL divergence (Kullback and Leibler, 1951) of coarse POS tag trigram distributions in tgt and src corpora: KL−4 (tgt, src) = cpos 3  We carry out all experiments using HamleDT 2.0 (Rosa et al., 2014), a collection of 30 treebanks converted into Universal Stanford Dependencies (de Marneffe et al., 2014). We use goldstandard UPOS tags in all experiments; while this is not fully realistic in the setting of underresourced languages, there exist high-performance semi-supervised taggers that could be used instead of gold tags (Das and Petrov, 2011; Agi´c et al., 2015), which we plan to evaluate in future. We use the treebank training sections for parser training and KL−4 computation, and the test sections cpos 3 for evaluation. We used 12 of the treebanks as a development set to select the model normalization method to avoid overfitting it to the dataset.4 −4  X ftgt (cpos 3 )  3   f (cpos ) · log tgt  fsrc (cpos 3 )  , (6) ∀cpos 3 ∈tgt where cpos 3 is a UPOS trigram, and f (cpos 3 ) is its relative frequency in a src or tgt corpus.3 4 Baseline Methods In this section, we describe the two baseline resource combination methods ag"
W15-2209,N06-2033,0,0.109557,"th multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tree combination method was introduced by Sagae and Lavie (2006) for a supervised monolingual setting, optionally weighting each src parser with a weight based on its accuˇ racy. In (Rosa and Zabokrtsk´ y, 2015), we ported Introduction The task of delexicalized dependency parser transfer (or delex transfer for short) is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably partof-speech (POS) tags, and to apply that parser to POS-tagged sentences of a target language (tgt) to obtain dependency parse trees. Delex transfer yields worse results than a supervised lexicalized parser trained on the tgt languag"
W15-2209,de-marneffe-etal-2014-universal,0,0.0580442,"Missing"
W15-2209,C12-2115,0,0.213776,"and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tree combination method was introduced by Sagae and Lavie (2006) for a supervised monolingual setting, optionally weighting each src parser with a weight based on its accuˇ racy. In (Rosa and Zabokrtsk´ y, 2015), we ported Introduction The task of delexicalized dependency parser transfer (or delex"
W15-2209,N13-1126,0,0.318788,"Missing"
W15-2209,I08-3008,0,0.117669,"of separate src parsers, only one parser is run. We introduce interpolation of trained MSTParser models as a resource combination method for multi-source delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff"
W15-2209,N10-1004,0,0.0238571,"sers for different languages, weighted by src-tgt language similarity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser training). We use a simplified version of normalization by standard deviation. First, we compute the uncorrected sample standard deviation of the weights of the features in the model as s 1 X sM = (wf − w) ¯ 2, (2) |M | Method In this section, we present our suggested approach of"
W15-2209,zeman-etal-2012-hamledt,1,0.899437,"Missing"
W15-2209,P05-1012,0,0.632893,"(Section 3.2). 3. Interpolate the parser models (Section 3.3). 4. Parse the tgt text with a delex parser using the interpolated model. 3.1 Model Normalization where w ¯ is the average feature weight, and |M |is the number of feature weights in model M ; only features that were assigned a weight by the training algorithm are taken into account. We then divide each feature weight by the standard deviation:1 wf ∀f ∈ M : wf := . (3) sM Delexicalized MSTParser Throughout this work, we use MSTperl (Rosa, 2015b), an unlabelled first-order non-projective single-best implementation of the MSTParser of McDonald et al. (2005b), trained using 3 iterations of MIRA (Crammer and Singer, 2003). The MSTParser model uses a set of binary features F that are assigned weights wf by training on a treebank. When parsing a sentence, the parser constructs a complete weighted directed graph over the tokens of the input sentence, and assigns each edge e a score se which is the sum of weights of features that are active for that edge: X se = f (e) · wf . (1) The choice of normalization by standard deviation is based on its high and stable performance on our development set, and Occam’s razor.2 3.3 Model Interpolation The interpol"
W15-2209,zeman-2008-reusable,0,0.0143199,"rce delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weigh"
W15-2209,H05-1066,0,0.190315,"Missing"
W15-3009,bojar-etal-2012-joy,1,0.894425,"Missing"
W15-3009,P13-3023,1,0.902684,"Missing"
W15-3009,P15-1044,1,0.88208,"Missing"
W15-3009,W12-3132,1,0.897194,"Missing"
W15-3009,hajic-etal-2012-announcing,0,0.0376956,"Missing"
W15-3009,P14-5003,0,0.120191,"Missing"
W15-3009,W10-1730,1,0.934479,"Missing"
W15-3009,W15-4103,0,0.0311546,"uages indicated that it is sufficient in most cases. The output sentence is then obtained by just combining all the nodes in the resulting surface dependency tree. 5 WMT 2015 Translation Task Results TectoMT reached a BLEU score of 13.9 for the English-to-Czech direction in the WMT 2015 Translation Task. This ranks it among the last systems, which is consistent with results from previous years. However, English-to-Czech TectoMT has also been used in the Chimera system combination, which ranks first in both automatic and human evaluation results. TectoMT plays a very important role in Chimera (Tamchyna and Bojar, 2015). TectoMT’s Czech-to-English translation reached a BLEU score of 12.8, and finished last 4. Subject-predicate agreement in number and person is enforced – predicates have their number and person filled based on their subject(s). 5. Auxiliary words are added. These are based on the contents of formemes (prepositions, subordinating conjunction, infinitive particles, possessive markers) and t-lemmas (phrasal verb particles). 10 Alternatively, an n-gram language model could be used to select the word forms. Flect uses just a short context of neighboring lemmas, but it generalizes also to unseen wo"
W15-3009,H05-1066,0,0.0520577,"Missing"
W15-3009,P09-2037,1,0.891858,"as, formemes, and grammatemes are translated using separate models. The t-lemma and formeme translation models are an interpolation of maximum entropy discriminative models (MaxEnt) of Mareˇcek et al. (2010) and simple conditional probability models. The MaxEnt models are in fact an ensemble of models, one for each individual source t-lemma/formeme. The combined translation models provide several translation options for each node along with their estimated probability (see Section 1). The best options are then selected using a Hidden Markov Tree Model (HMTM) with a target-language tree model (Žabokrtský and Popel, 2009), which roughly corresponds to the target-language n-gram model in phrase-based MT. Grammateme transfer is rule-based; in most cases, grammatemes remain the same as in the source language. Adding New Language Pairs Using different languages in an MT system with deep transfer is mainly hindered by differences in the analysis and synthesis of the individual languages. To overcome these problems, we decided to use existing multilingual annotation standards (see Section 3.1) and to simplify and automate translation model training (see Section 3.2). In addition, we introduce an easier way of combin"
W15-3009,W13-3307,1,0.84883,"and Rudolf Rosa∗ ∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics {odusek,mnovak,popel,rosa}@ufal.mff.cuni.cz ‡ University of Lisbon, Faculty of Sciences, Department of Informatics luis.gomes@di.fc.ul.pt Abstract system in the shared task. The performance of the current version leaves a lot of room for improvement, but proves the potential of TectoMT for different language pairs. The original TectoMT system for EnglishCzech translation has seen just small changes, e.g., adding specialized translation models for selected pronouns (Novák et al., 2013a; Novák et al., 2013b) and fine-tuning of a handful of rules. Therefore, its performance is virtually identical to that of the last year’s version. This paper is structured as follows: in Section 2, we introduce the TectoMT basic architecture. In Section 3, we describe the improvements to TectoMT that were added for an easier support of new language pairs. Section 4 then details the Czech-to-English TectoMT system submitted to WMT15. We discuss TectoMT’s performance in the task and examine the most severe error sources in Section 5. Section 6 then concludes the paper. The TectoMT tree-to-tree"
W15-3009,W08-0325,0,0.622363,"Missing"
W15-3009,I13-1142,1,0.831364,"Missing"
W15-3009,zeman-etal-2012-hamledt,1,0.902954,"Missing"
W15-3009,P02-1040,0,0.0941085,"ining New Language Pairs • HM-P – harmonic mean of probabilities, Other improvements to support adding new language pairs quickly are rather technical. We automated the translation model training in a set of makefiles. To train a new translation pair, one only needs to implement analysis and synthesis pipelines for both languages and edit a configuration file. Debugging and testing of the new analysis and synthesis pipelines is supported by monolingual “roundtrip” experiments: a development data set is first analyzed up to t-layer, then synthesized back to word forms. BLEU score measurements (Papineni et al., 2002) and a direct comparison of the results are then used to improve performance before the translation models are trained and other transfer blocks are implemented.3 3.3 • GM-Log-P – geometric mean of logarithmic probabilities,7 • HM-Log-P – harmonic mean of logarithmic probabilities.8 We compared the functions against a baseline of just using the first option given by each of the models (regardless of compatibility). We used corpora of 1,000 sentences from the IT domain collected in the QTLeap project to evaluate all variants in English-to-Czech, English-toSpanish, and English-to-Portuguese tran"
W15-3009,zeman-2008-reusable,0,0.205436,"o-tree machine translation (MT) system (Žabokrtský et al., 2008) has been competing in WMT translation tasks since 2008 and has seen a number of improvements. Until now, the only supported translation direction was English to Czech. This year, as a part of the QTLeap project,1 we have enhanced TectoMT and its underlying natural language processing (NLP) framework, Treex (Popel and Žabokrtský, 2010), to support more language pairs. We simplified the training pipeline to be able to retrain the translation models faster, and we use abstracted language-independent rules with the help of Interset (Zeman, 2008) where possible. Together with our partners on the QTLeap project, we have implemented translation systems for other language pairs (English to and from Dutch, Spanish, Basque, and Portuguese) which are not part of WMT shared Translation Task this year. However, we were also able to submit the results of a newly built Czech-English translation 1 2 The TectoMT Translation System TectoMT (Žabokrtský et al., 2008) is a tree-totree MT system system consisting of an analysistransfer-synthesis pipeline, with transfer on the level of deep syntax. It is based on the Prague Tectogrammatics theory (Sgal"
W15-5711,D11-1033,0,0.0265192,"models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we"
W15-5711,J96-1002,0,0.23014,"of rules which collapse auxiliaries and assign all the required attributes to each t-node. 2.2 Transfer In the transfer phase, an initial target t-tree is obtained as a copy of the source t-tree. Target t-lemmas and formemes of the t-nodes are suggested by a set of TMs, and the other attributes are transferred by a set of rules. For both t-lemmas and formemes, we use two separate TMs: • MaxEnt TM – a discriminative model whose prediction is based on features extracted from the source tree. The discriminative TM (Mareˇcek et al., 2010) is in fact an ensemble of maximum entropy (MaxEnt) models (Berger et al., 1996), each trained for one specific source t-lemma/formeme. However, as the number of types observed in the parallel treebank may be too large, infrequent source t-lemmas/formemes are not covered by this type of TM. • Static TM – this is only a dictionary of possible translations with relative frequencies (no contextual features are taken into account). This model is available for most source t-lemmas/formemes seen in training data.3 1 http://ufal.mff.cuni.cz/treex and https://github.com/ufal/treex The modules used for the analysis in the individual languages vary, but all of them follow the same"
W15-5711,2011.iwslt-evaluation.18,0,0.0393918,"erforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusio"
W15-5711,bojar-etal-2012-joy,1,0.910006,"Missing"
W15-5711,W04-3237,0,0.0447917,"he linear interpolation constant is tuned on a development set. • A LL – concatenation of training data. • W EIGHT – as A LL, but the out-of-domain training examples are downweighted so the in-domain examples (which are typically much fewer) have bigger effect on the resulting model. The weight is chosen by cross-validation. • P RED – the prediction of the out-of-domain model is used as an additional feature for training the final model on the in-domain data. • P RIOR – out-of-domain weights are used as a prior (via the regularization term) when training the final model on the in-domain data (Chelba and Acero, 2004). 4 http://metashare.metanet4u.eu/go2/qtleapcorpus http://www.statmt.org/wmt13/translation-task.html 6 See cuni_train/Makefile in https://github.com/ufal/qtleap. 5 93 • E ASYA DAPT (called AUGMENT in the original paper, sometimes referred to as the “Frustratingly Easy Domain Adaptation”) – create three variants of each feature: general, in-specific and outspecific; train on concatenation of in- and out-of-domain data, where on in-domain data, the general and in-specific features are active and on the out-of-domain data, the general and out-specific features are active. Daumé III (2009) showed"
W15-5711,W10-2608,0,0.0563296,"Missing"
W15-5711,W14-3326,1,0.79882,"Missing"
W15-5711,P13-3023,1,0.892993,"Missing"
W15-5711,W12-3132,1,0.901012,"Missing"
W15-5711,W15-3009,1,0.84035,"two possible ways of combining the lists: 1. Just using the first item of both lists (the simplest way, but its performance may not be ideal since incompatible combinations are sometimes produced). 2. Using a Hidden Markov Tree Model (Žabokrtský and Popel, 2009), where a Viterbi search is used to find the best t-lemma/formeme combinations globally over the whole tree. In the current TectoMT version, HMTM is only used in EN→CS translation. HMTM for the remaining languages will be added in the near future. 2.3 Synthesis The synthesis is a pipeline of rule-based modules (Žabokrtský et al., 2008; Dušek et al., 2015) that gradually change the translated t-tree into an a-tree (surface dependency tree), adding auxiliary words and punctuation and resolving morphological attributes. Some basic word-order rules are also applied. The individual a-tree nodes/words are then inflected using a morphological dictionary (Straková et al., 2014) or a statistical tool trained on an annotated corpus (Dušek and Jurˇcíˇcek, 2013). The resulting tree is then simply linearized into the output sentence. 3 Domain Adaptation by Model Interpolation The general approach of domain adaptation by model interpolation is rather simple"
W15-5711,eck-etal-2004-language,0,0.0138674,"ments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, a"
W15-5711,2005.eamt-1.19,0,0.0123169,"neral-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset."
W15-5711,W07-0733,0,0.0220004,"NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented o"
W15-5711,2005.mtsummit-papers.11,0,0.105998,"nswers) as the in-domain training data. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on develo"
W15-5711,W02-1405,0,0.0494824,"c features are active. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) o"
W15-5711,2007.mtsummit-papers.42,0,0.0276696,"a. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on development data to set TM pruning thresholds and i"
W15-5711,2011.iwslt-papers.5,0,0.0195592,"s-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we will obtain additional in-domain data, whi"
W15-5711,W10-1730,1,0.924267,"Missing"
W15-5711,H05-1066,0,0.33499,"Missing"
W15-5711,P10-2041,0,0.034953,"bine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up"
W15-5711,W08-0320,0,0.0279926,"ctive. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combi"
W15-5711,C10-2124,0,0.0226748,"III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et"
W15-5711,W07-1709,0,0.0772611,"Missing"
W15-5711,P14-5003,0,0.0926819,"Missing"
W15-5711,P09-2037,1,0.952471,"ne translation system with a tree-to-tree transfer on the deep syntax layer, first introduced by Žabokrtský et al. (2008). It is based on the Prague “tectogrammatics” theory of Sgall et al. (1986). The system uses two layers of structural description with dependency trees: surface syntax (a-layer, a-trees) and deep syntax (t-layer, t-trees). The analysis phase is two-step and proceeds from plain text over a-layer to t-layer (see Section 2.1). The transfer phase of the system is based on maximum entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Markov Tree Models (Žabokrtský and Popel, 2009) (see Section 2.2). The subsequent generation phase consists of rule-based components that gradually change the deep target language representation into a shallow one, which is then converted to text (see Section 2.3). 2.1 Analysis The analysis phase consists of a pipeline of standard NLP tools that perform the analysis to the a-layer, followed by a rule-based conversion to t-layer. In the analysis pipeline, the input is first segmented into sentences and tokenized using rule-based modules from the Treex toolkit1 (Popel and Žabokrtský, 2010). A statistical part-of-speech tagger and dependency"
W15-5711,W08-0325,0,0.606583,"Missing"
W16-2334,W16-2332,1,0.730263,"f performance of the forced non-translations from the evaluation section: we simply always apply it in the TectoMT system, but never in the Moses system.9 4 Moses 4.2 TectoMT TectoMT is a hybrid MT system, combining statistical and rule-based Treex blocks to perform translation with transfer on the layer of tectogrammatical (deep) syntax. MT Systems We use two systems, Moses (Koehn et al., 2007) ˇ and TectoMT (Zabokrtsk´ y et al., 2008), as well as their combination Chimera (Bojar et al., 2013); see also a more detailed description of the Moses and TectoMT systems within the QTLeap project by Gaudio et al. (2016) in these proceedings. 10 W2A::ResegmentSentences W2A::TokenizeMoses 12 W2A::TokenizeMorphoDiTa for EN→CS 13 W2W::NormalizeEnglishSentence 14 W2A::EscapeMoses 15 W2A::TruecaseMoses 16 A2A::ProjectCase 17 A2W::CapitalizeSentStart 11 9 This holds even for the Chimera combination, i.e. this method is applied in its TectoMT component but not in the Moses component. 452 System Annotations (not adapted) XXX XML (not adapted) XXX XML →ES 22.23 23.61 24.22 26.01 26.89 27.40 →NL 23.40 24.89 25.41 21.82 23.52 23.26 →PT 14.01 15.47 15.58 13.11 14.19 14.21 We use TectoMT’s translation model interpolation"
W16-2334,P07-2045,1,0.0394348,"rained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focu"
W16-2334,P03-1021,0,0.0324255,"ems domain-adapted: they are trained and tuned on the Batch1 and Batch2 parts of the in-domain training data, as described below. Thus, even without the domain adaptation through in-domain lexicons (which were also provided by the task organizers), the systems constitute strong baselines within the IT domain. Still, the lexicons were not used to train nor tune the systems. 3.5 4.1 Forced non-translations Moses is a standard phrase-based statistical machine translation system. We train Moses on general-domain training data and tune it on the Batch2 part from in-domain training data using MERT (Och, 2003). We perform domain adaptation of Moses using either XXX placeholders or XML annotations. EN→CS uses factored translation (with part-of-speech tags as additional target-side factors), which is not compatible with the XML annotations, and thus only XXX placeholders are used for EN→CS. We apply some rather standard pre- and postprocessing steps (implemented as Treex blocks). Preprocessing: • segmentation into sentences10 • tokenization11,12 • normalization of quotes, dashes and contracted forms (for EN→CS)13 • entity escaping14 • truecasing (for EN→NL)15 /lowercasing Postprocessing: • projection"
W16-2334,P02-1040,0,0.100304,"of the in-domain training data. Unlike Moses, TectoMT does not support automatic tuning of parameters; however, some parameters were tuned manually using Batch2 from in-domain training data. We only experiment with domain adaptation of TectoMT via Treex wild attributes (§ 3.4). Table 1: BLEU evaluation of two forced translation styles for Moses: XXX placeholders and XML markup. For comparison, the non-adapted system is also included. 4.3 5 Moses Chimera Chimera We use the WMT16 IT task test set (i.e. Batch3 from the QTLeap corpus19 ) to evaluate our experiments using (case-insensitive) BLEU (Papineni et al., 2002). First, in Table 1, we compare the two annotation styles we can use for Moses. In general, the XML annotations perform better, in half of the cases leading to a result better by about +0.5 BLEU than that of the XXX placeholders while performing worse only once. Although the documentation in the Moses manual is not very detailed in this respect, we believe that the XML annotations are more palatable to the language model, which can then make meaningful decisions at the boundaries of the force-translated entities, while the XXX placeholders simply constitute out-ofvocabulary items for the langu"
W16-2334,W12-3146,1,0.930914,"Missing"
W16-2334,W15-5711,1,0.848136,"Missing"
W16-2334,W08-0325,0,0.736805,"g. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focuses on high-quality translation for the"
W16-2334,P07-1033,0,\N,Missing
W16-2334,W13-2208,1,\N,Missing
W16-6401,W10-1705,1,0.929434,"oses + Moses post-editing, simple Moses + Moses post-editing, TwoStep Google Translate + TectoMT post-editing Moses + TectoMT post-editing § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setup"
W16-6401,W15-3006,1,0.779365,"nslation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been"
W16-6401,W12-3130,1,0.860797,"s, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd Deep Machine Translation Workshop (DMTW 2016), pages 1–10, Lisbon, Portugal, 21 October 2016. Figure 1: TectoMT 2.1.1 Factored Moses In the more recent experiments that we report, the Moses system used is actually the Factored Moses of Bojar et al. (2012). It translates the source English text into a factored representation of Czech, where each word is represented by a tuple of a word form and a corresponding part-of-speech (PoS) tag. This enables Moses to use an additional language model which operates on PoS tags instead of word forms. This helps overcome data sparsity issues of the word-based language model and thus leads to a higher output quality, especially to its better grammaticality. Factored Moses is trained on parallel corpora pre-analyzed by Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically"
W16-6401,W13-2208,1,0.933672,"to perform forced translation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at"
W16-6401,W15-3009,1,0.853769,"y Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically motivated NLP framework. It consists of a large number of smaller components performing a specific NLP-task (blocks), both Treex-specific as well as Treex-wrapped external tools, which can be flexibly combined into processing pipelines. Sentences are represented by surface and deep syntactic dependency trees, richly annotated with numerous linguistic attributes, similarly to the Prague Dependency Treebank (Hajiˇc, 1998). 2.2.1 TectoMT The main application of Treex is TectoMT3 (Žabokrtský et al., 2008; Dušek et al., 2015), a linguistically motivated hybrid machine translation system. Its pipieline consists of three main steps: analysis of each source sentence up to t-layer (a deep syntactic representation of the sentence in a labelled dependency t-tree), transfer of the source t-tree to the target t-tree (i.e., the translation per se), and generation of the target sentence from the target t-tree (see Figure 1). The transfer is performed by copying the t-tree structure and grammatemes4 (attributes describing grammatical meaning) from source, and predicting target lemmas and formemes5 (deep morphosyntactic attri"
W16-6401,W12-3132,1,0.90614,"Missing"
W16-6401,W13-2216,1,0.89877,"Missing"
W16-6401,P07-2045,1,0.0127177,"rtcomings cancel out. In our paper, we review a set of such attempts, performed with Moses, a prominent representative of the PB-SMT systems, and Treex, a linguistically motivated NLP framework, featuring, among other, a full-fledged deep syntactic MT system, TectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to othe"
W16-6401,W09-0424,0,0.0237791,"u-plain Moses output downloaded from http://matrix.statmt.org/systems/show/2807, test set downloaded from http://matrix.statmt.org/test_sets/list. 10 Figure 8: Moses with TectoMT pre- and post-processing 6 Figure 9: Two-headed Chimera Zeman (2010) used several pre-processing steps to make the source English text more similar to Czech, such as removing articles, marking subjects by artificial suffixes (“/Sb”), and reordering auxiliary verbs to neighbor their main verbs. Of course, the SMT system was also trained on texts preprocessed in that way; in these experiments, the Joshua PB-SMT system (Li et al., 2009) was used instead of Moses. This approach may seem too aggressive, prone to making the input noisier as well as being potentially lossy. However, the author showed that with careful selection and tuning of the pre-processing steps, a significant improvement of translation quality can be achieved; moreover, this was also confirmed on English-to-Hindi translation. Rosa et al. (2016) successfully apply Treex pre-processing and post-processing to Moses, but this time with the main objective being an adaptation of Moses trained on general-domain data to a specific domain (namely the domain of Infor"
W16-6401,J03-1002,0,0.0116228,"ectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd De"
W16-6401,W07-0704,0,0.0827717,"Missing"
W16-6401,W12-3146,1,0.888935,"Missing"
W16-6401,W16-2334,1,0.910577,"ng § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setups have been properly described and evaluated in a peer-reviewed publication, others, especially some of the unsuccessful ones, were ne"
W16-6401,N07-1064,0,0.0327481,"Majliš (2009) Zeman (2010) Table 2: Base systems. Figure 3: PhraseFix translating one t-node with two or more t-nodes or deleting some t-nodes.8 It also uses MERT tuning and it should scale with more training data. In the experiments with two factors (Popel, 2013), two language models were used: one for lemmas and one for formemes. Unfortunately, the TectoMoses experiment brought negative results, presumably due to additional noise introduced by the added transformations. 3.2 PhraseFix: TectoMT with Moses Post-editing The PhraseFix system of Galušˇcáková et al. (2013) is based on the work of Simard et al. (2007), who introduced the idea of automatically post-editing a first-stage MT system by a second-stage MT system, trained to “translate” the output of the first-stage system into a reference translation. This has been shown to be particularly beneficial for conceptually different MT systems. In PhraseFix, the source English side of the CzEng parallel corpus of Bojar and Žabokrtský (2009) is translated by TectoMT into Czech, and Moses is then trained in a monolingual setting to translate the TectoMT-Czech into reference-Czech, i.e., the target side of CzEng (see Figure 3). Evaluation shows that this"
W16-6401,W07-1709,0,0.0726314,"Missing"
W16-6401,W16-2325,1,0.836879,"parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been shown to have a positive effect, e.g., in choosing the correct inflection of a word when the language model encounters an unknown context, or in generating a translation for a word that constitutes an out-of-vocabulary item for Moses (as TectoMT can abstract from word forms to lemmas and beyond, which Moses cannot). 3.8 Chimera: Moses with Additional TectoMT Phrase-table and Depfix Post-editing The Three-headed Chimera, or simply Chimera (Bojar et al., 2013b; Tamchyna et al., 2016), is a combination of TectoMT and Moses, as in Section 3.7, complemented by a final post-editing step performed by Depfix, as in Section 3.5 (see Figure 10). It has been repeatedly confirmed as the best system by both automatic and manual evaluations, not only among the ones reported in this paper, but also in general, 12 http://www.statmt.org/moses/?n=Advanced.Hybrid 7 Figure 10: Three-headed Chimera being the winner of the WMT English-to-Czech translation task in the years 2013, 2014 and 2015 (Bojar et al., 2013a; Bojar et al., 2014; Bojar et al., 2015). 4 Conclusion We reviewed a range of e"
W16-6401,W08-0325,0,0.0651825,"Missing"
W17-1226,C16-1012,0,0.0121703,"ctly, as there are various spelling and morpho211 proach is documented by T¨ackstr¨om et al. (2013). logical differences even between very close languages. Using such shared features allows a parser that was trained on a source treebank to be used directly on target texts; i.e. the source-target “transfer” of the parser is trivial, compared to a sourcetarget transfer of the treebank as described in §2.1. The common abstraction features used by the parser can be linguistically motivated, or induced by mathematical methods such as clustering and vector space representation: 2.3 Other variations Aufrant et al. (2016) combines both main strategies described above by adapting the word order in source sentences to be more similar to that of the target language, e.g. by swapping the order of an attribute and its nominal head; the information about these configurations was extracted from the WALS World Atlas of Language Structures (Dryer and Haspelmath, 2013). Such processing of source language trees fits to the first family of approaches, as it resembles a (very limited) MT preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a"
W17-1226,J92-4003,0,0.101154,"gs: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question"
W17-1226,W06-2920,0,0.108524,"ions whether a tree (and what kind of tree) is a reasonable representation for a sentence structure, and whether all languages do really share their structural properties to such an extent that a single type of representation is viable for all of them. Though such issues deserve intensive attention, and perhaps even more so now when UD have gained such a fascinating momentum, we take the two assumptions simply for granted. Neither do we present the genesis of the current UD collection, preceded by HamleDT treebank collection by Zeman et al. (2014), going back to the CoNLL 2006 and 2007 tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), and to earlier POS standardization efforts. In this overview, we limit ourselves to the scope outlined by the VarDial shared task, whose goal is to develop a parser for a (virtually) underresourced language closely related to a resourcerich language.2 We believe that most of the published approaches could be classified into two broad families which we call tree-transfer-based methods and common-abstraction-based methods. The former project individual dependency trees across the language boundary prior to training a target parser. The latter methods transfer a parser mode"
W17-1226,J03-1002,0,0.00596816,"translation. To reduce the OOV rate, two backoff layers are also stored, the first disregarding the morpho feats, and the second also disregarding the UPOS. An option that we leave for future research is to use the alignment scores provided by the MGA when constructing the translation table. For simplicity, we create only one joint translation table for translating DS into NO. Word-alignment Since the source and target languages in our task are very close to each other, we decided to use the heuristic Monolingual Greedy Aligner (MGA) of Rosa et al. (2012),9 rather than e.g. the usual Giza++ (Och and Ney, 2003) – most standard word aligners ignore word similarity, which we believe to be useful and important in our setting. MGA utilizes the word, lemma, and tag similarity based on Jaro-Winkler distance (Winkler, 1990), and the similarity of relative positions in the sentences, to devise a score for each potential alignment link as a linear combination of these, weighted by pre-set weights. The iterative alignment process then greedily chooses the currently highest scoring pair of words to align in each step; each word can only be aligned once. The process stops when one of the sides is fully aligned,"
W17-1226,P99-1065,0,0.341674,"Missing"
W17-1226,D15-1039,0,0.0140857,"inks. In addition, such alignment typically has a higher amount of one-to-one word alignments, which facilitates tree projection; in case of extremely close languages, as in this paper, the MT system can be constrained to produce only 1:1 translations. There are two additional advantages of the treetransfer-based approach: • the feature set used by the target language parser is independent of the features that are applicable to the source language, • we can easily use only sentence pairs (or tree fragments) with a reasonably high correspondence between source and target structures, as done by Rasooli and Collins (2015). Tree-transfer-based approaches In the tree-transfer-based approaches, a synthetic pseudo-target treebank is created by some sort of projection of individual source trees into the target language. Then a standard monolingual parser can be trained using the pseudo-target treebank in a more or less standard way. As it is quite unlikely that a manually annotated source treebank 2.2 2 Crosslingual transfer is not used only in truly underresourced scenarios, but also in situations in which it is hoped that features explicitly manifested in one language (such as morphological agreement) could boost"
W17-1226,K15-1012,0,0.0123173,"n since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question arises what is the best granularity of shared tags. The more simplified tags, the more languageuniversal information is captured, but the more information is lost at the same time. Moreover, even if two languages share a particular morphological category, e.g. pronoun reflexivity, it is hard to predict whether adding this distinction into the shared tagset"
W17-1226,P15-2040,1,0.922144,"Missing"
W17-1226,W12-4205,1,0.895855,"Missing"
W17-1226,D11-1006,0,0.0460896,"preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a few underresourced languages, choosing the best source language should be ideally automatized too. One could rely on language phylogenetic trees or on linguistic information available e.g. in WALS, or on more mechanized measures, such as KullbackLeibler divergence of POS trigram distributions ˇ (Rosa and Zabokrtsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harm"
W17-1226,L16-1680,0,0.150363,"Missing"
W17-1226,N12-1052,0,0.242999,"Missing"
W17-1226,N13-1126,0,0.0549572,"Missing"
W17-1226,C14-1175,0,0.407656,"et parser. The latter methods transfer a parser model trained directly on the source treebank, but limited only to abstract features shared by both languages. 2.1 with high-quality human-made target translations and high-quality alignment exists, one or more of the necessary components must be approximated. And even if all these data components existed, the task of dependency tree projection would inevitably lead to collisions that have to be resolved heuristically, especially in the case of many-to-one or many-to-many alignments, as investigated e.g. by Hwa et al. (2005) and more recently by Tiedemann (2014) or Ramasamy et al. (2014). This family embraces the following approaches: • using a parallel corpus and projecting the trees through word-alignment links, with authentic texts in both languages but an automatically parsed source side, • using a machine-translated parallel corpus, with only one side containing authentic texts and the other being created by MT; both translation directions have pros and cons: – source-to-target MT allows for using a gold treebank on the source side, – target-to-source MT allows the parser to learn to work with real texts in the target language, for which, in add"
W17-1226,W17-1216,0,0.0957939,"Missing"
W17-1226,N01-1026,0,0.132429,"er is trained on monolingually predicted tags, as explained in §4.1. We have found source-xtag to work well for heterogeneous source data, such as the DS mixture. Conversely, target-xtag proved useful for SK, where the source treebank is much larger than the target data used to train the target tagger. A tagger trained on the large source treebank provides much better tags, which in turn boosts the parsing accuracy, despite the noise from MT and xtag. Note that if no target tagger is available, we must either use target-xtag, or we may project a tagger across the parallel data in the style of Yarowsky and Ngai (2001) and use the resulting tagger in our baseline or source-xtag scenarios.12 We also experimented with cross-tagging of only the UPOS or only the morpho feats, with different setups being useful for different languages. Although the UDPipe tagger can also be trained to perform lemmatization, we have not found any way to obtain and utilize lemmas that would improve the cross-lingual parsing.13 12 Our approach still needs a target tagger to perform the word alignment, but we believe that for very close languages, the word forms alone might be sufficient to obtain a goodenough alignment; or, a diffe"
W17-1226,W17-1201,0,0.0972651,"Missing"
W17-1226,I08-3008,1,0.743945,"tsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual pa"
W17-1226,zeman-2008-reusable,1,0.792983,"eights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obviou"
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W17-4769,D16-1134,1,0.825917,"rst one works only on Czech and uses many semantic features based of rich Czech tectogrammatical annotation (B¨ohmov´a et al., 2003). The second one uses much fewer features, however, it is language universal and needs only a dependency parsing model available. 2.1 AutoDA Using Czech Tectogrammatics This metric automatically parses the Czech translation candidate and the reference translation and uses various semantic features to compute the final score. 1. AutoDA: A linear regression model using semantic features trained on WMT Direct Assessment scores (Bojar et al., 2016) or HUMEseg scores (Birch et al., 2016). 2.1.1 Word Alignment AutoDA relies on automatic alignment between the translation candidate and the reference trans2. TreeAggreg: N-gram based metric computed over aligned syntactic structures instead of 604 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 604–611 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Method AutoDA TreeAggreg NMTScorer Resource Type Monolingual/Bilingual* Monolingual Bilingual Trainable Yes No Yes Metric Type Segment-level Linear Regression Tree Segment-level ChrF** Segment-le"
W17-4769,L16-1262,0,0.102773,"Missing"
W17-4769,C00-2163,0,0.158333,"sible. TreeAggreg can use any string-level metric for score computation instead of ChrF (**). Dataset WMT16 DAseg WMT15 DAseg WMT16 HUMEseg Source TR/FI/CS/RO/RU/DE EN DE/RU/FI/CS EN EN Target EN RU EN RU CS/DE/PL/RO # Sentences 560 500 ∼350 Table 2: Overview of the available data for training AutoDA. • functor: the semantic value of the syntactic dependency relation. Functors express the functions of individual modifications in the sentence, e.g. ACT (Actor), PAT (Patient), ADDR (Addressee), LOC (Location), MANN (Manner), lation. The easiest way of obtaining word alignments is to run GIZA++ (Och and Ney, 2000) on the set of sentence pairs. GIZA++ was designed to align documents in two languages and it can obviously also align documents in a single language, although it does not benefit in any way from the fact that many words are identical in the aligned sentences. GIZA++ works well if the input corpus is sufficiently large, to allow for extraction of reliable word co-occurrence statistics. While the test sets alone are too small, we have a corpus of paraphrases for Czech (Bojar et al., 2013). We thus run GIZA++ on all possible paraphrase combinations together with the reference-translation pairs w"
W17-4769,W16-2301,1,0.86698,"Missing"
W17-4769,P02-1040,0,0.114911,"ranslation “Jako kofeinov´y n´apoj, alkohol v tˇele zabraˇnuje vstˇreb´av´an´ı kalcia z potravy.” metric aligned-tnode-tlemma-exact-match aligned-tnode-formeme-match aligned-tnode-functor-match aligned-tnode-sempos-match lexrf-form-exact-match lexrf-lemma-exact-match BLEU on forms BLEU on lemmas chrF3 AutoDA (87 features) AutoDA (selected 23 features) 2.1.4 Linear Regression Training We collect 83 various features based on matching tectogrammatical attributes computed on all nodes or a subsets defined by particular semantic part-of-speech tags. To this set of features, we add two BLEU scores (Papineni et al., 2002) computed on forms and on lemmas and two chrF3 scores (Popovic, 2015) computed on trigrams and sixgrams, so we have 87 features in total. We train a linear regression model to obtain a weighted mix of features that fits best the WMT16 HUMEseg scores. Since the amount of annotated data available is low, we use the jackknife strategy: en-cs 0.449 0.429 0.391 0.416 0.372 0.436 0.361 0.395 0.540 0.625 0.659 Table 3: Selected Czech deep-syntactic features and their correlation against WMT16 HUMEseg dataset. Comparison with BLEU, chrF3, and our trainable AutoDA (using chrF3 as well). • We split the"
W17-4769,W15-3049,0,0.139982,"Missing"
W17-4769,W12-4205,1,0.901962,"Missing"
W17-4769,L16-1680,0,0.0491449,"Missing"
W17-7615,P15-2044,0,0.0655623,"Missing"
W17-7615,W17-0401,0,0.0776366,"Missing"
W17-7615,P15-1165,0,0.0334271,"Missing"
W17-7615,L16-1680,0,0.1026,"Missing"
W17-7615,N12-1052,0,0.0258171,"words: cross-lingual parsing, cross-lingual tagging, Universal Dependencies 2 Cross-lingual parsing Cross-lingual parsing is the task of performing syntactic analysis of a target language with no treebank available for that language by using annotated data for a different source language and a method for transferring the knowledge about syntactic structures from that source language into the target language. It has already been studied for over a decade, starting with the works of Hwa et al. (2005) and Zeman and Resnik (2008), and then continued by many others, such as McDonald et al. (2011), Täckström et al. (2012), Georgi et al. (2013), Agi´c et al. (2015), Søgaard et al. (2015), and Duong et al. (2015). A thorough overview, analysis and comparison of existing methods can be found in (Tiedemann et al., 2016). The authors also include a detailed analysis of the performance of the systems based on various factors, such as part-of-speech (POS) labelling accuracy or size of training data. Another work dealing with error analysis of cross-lingual parsing systems is that of Ramasamy et al. (2014). The system evaluated in this paper is a new version of the aforementioned SFNW (Rosa et al., 2017), improved and"
W18-5444,P18-2003,0,0.0767232,"Missing"
W18-5444,Q16-1037,0,0.170228,"Missing"
W18-5444,J93-2004,0,0.0617662,"syntactic theories and annotations. We would also like to discuss results across various languages and natural language processing (NLP) tasks. In this abstract, we present our preliminary results, analyzing the encoder in English-to-German NMT within the NeuralMonkey toolkit (Helcl and Libovick´y, 2017). We introduce aggregation of self-attention through layers to get a distribution over the input tokens for each encoder position and layer (Section 2). We then propose algorithms for constructing two types of syntactic trees (Sections 3 and 4), apply them to 42 sentences sampled from PennTB (Marcus et al., 1993), and compare the resulting structures to established syntax annotation styles, such as that of PennTB, UD (Nivre et al., 2016), or PDT (B¨ohmov´a et al., 2003). ._ apart_ ped_ rip markets_ stock_ and_ es_ futur the_ between_ link_ the_ ,_ a_ Introduction as_ 1 result_ David Mareˇcek and Rudolf Rosa Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics Charles University, Prague, Czechia {marecek,rosa}@ufal.mff.cuni.cz as_ a_ result_ ,_ the_ link_ between_ the_ futur es_ and_ stock_ markets_ rip ped_ apart_ ._ Figure 1: Aggregated encoder’s self-attentions after the 6t"
W18-5444,L16-1262,0,0.0440475,"Missing"
W18-5444,D16-1159,0,0.21182,"Missing"
W19-4827,N19-1112,0,0.0157325,"of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions, but rather looking for structures explicitly presen"
W19-4827,P14-5010,0,0.00462413,"requently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrasestructure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall. 1 Introduction The classical approach to Natural Language Processing used to be complex pipelines, e.g. (Popel ˇ and Zabokrtsk´ y, 2010; Manning et al., 2014; Forcada et al., 2011), consisting of multiple steps of linguistically motivated analyses, such as partof-speech tagging or syntactic parsing, using explicit intermediate representations (e.g. dependency trees) to abstract over the underlying texts. In recent years, this has changed with the introduction of deep neural end-to-end models, which take raw text as input and produce the desired output directly. Any intermediate representations of the text may emerge during the training of the neural network, and are hidden to us. We focus on the encoder part of the Transformer architecture (Vaswan"
W19-4827,J93-2004,0,0.0660621,"Missing"
W19-4827,P18-2003,0,0.0178098,"es or anaphora links. In this work, we analyze the syntactic properties of the self-attention heads both qualitatively 2 Related Work Initial analyses of syntax captured by neural networks focused on RNNs. Shi et al. (2016) examine how much syntax is learned by RNN encoder by freezing its weights and using a decoder to predict syntactic trees. Adi et al. (2016) examine sentence vector representations by training auxiliary classifiers to take sentence encodings and predict attributes like word order. Linzen et al. (2016) assess the ability of LSTMs to learn syntax by predicting verbal numbers. Blevins et al. (2018) measure the amount of syntax in RNNs by predicting part-of-speech tags and constituent labels. In the last year, related studies appeared also for the Transformer architecture. Tang et al. (2018) show the Transformer networks perform better 263 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 263–275 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics to the states on the previous layer (input states), passed through a feed-forward layer. This may allow the encoder to do more advanced multi-step processin"
W19-4827,W18-5444,1,0.902996,"Missing"
W19-4827,N18-1202,0,0.0231646,"ource subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions,"
W19-4827,W18-1816,0,0.0585938,"Missing"
W19-4827,W18-5431,0,0.177451,"cluding syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions, but rather looking for structures explicitly present in the network representations. In a study closely related to ours, Raganato and Tiedemann (2018) also observe syntax-like patterns in Transformer encoder self-attentions, and try to extract syntactic trees without using annotated data (except for taking the root node from the gold annotation). However, they construct dependency trees, while we observe phrase-like rather than dependency-like structures. Moreover, their findings are somewhat inconclusive, as the accuracy of the resulting trees is close to the baseline, while our results are clearly positive. A similar approach was already suggested (but not evaluated) in (Mareˇcek and Rosa, 2018). 3 3.1 Encoder Self-Attention Visualization"
W19-4827,N19-1419,0,0.0479625,"he source subword embeddings forward, bypassing the self-attention mechanism, and get averaged with the outputs of the self-attention. This ensures that the output state at each position retains a significant amount of the corresponding source subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by"
W19-4827,P16-1162,0,0.0245687,"ce-target language pairs (en-fr, en-de, fr-en, fr-de, de-en, de-fr).4 From the Europarl corpus, we take first 1,000 sentences as development data, last 1,000 sentences as evaluation data, and the remaining 486,272 sentences for training. Table 1 lists the BLEU scores of the systems. All inspections and evaluations, both manual and automatic, have been performed on the evaluation data. The data are tokenized by the Stanford Tokenizer5 to make the tokens consistent with the constituency trees with which we will compare our results. We then build a shared dictionary of 100,000 BPE subword units (Sennrich et al., 2016) on the concatenated training data of all three languages, append an EOS symbol to each sentence, and train the translation model. 4 4.1 Diagonals Especially at the first encoder layer, there often appear various simple diagonal heads. Typically, each output state attends to the input state at the same position. This may serve to pass the subword information to the higher layers. In some cases, most of the output states attend to the corresponding input states, but some of them attend elsewhere. The role of such partial diagonal may be looking for a specific phenomenon that only occurs for som"
W19-4827,2005.mtsummit-papers.11,0,0.0300562,"ads concentrate nearly all of the attention at each output state onto just one input state. In the following subsections, we list all of the distinctive patterns that we have identified.6 An important thing to note is that typically, a head behaves consistently across all sentences, i.e., for a given head on a given layer of a given trained Transformer encoder, we typically see the same attention patterns across all sentences. Table 1: BLEU scores measured on the test data. French (fr), and German (de). We selected those particular languages because they are available in the Europarl corpus1 (Koehn, 2005) comprising large high-quality multiparallel data, and because constituency syntax parse trees can be obtained for them by the Stanford parser (Klein and Manning, 2003) out-of-the-box.2 As we want to explore a state-of-the-art setup, we use the Transformer model (Vaswani et al., 2017) as reimplemented by Helcl et al. (2018) in the Neural Monkey framework3 in standard setting: 6 encoder and decoder layers, 16 attention heads, embedding size of 512, hidden-layers’ size of 4096, dropout 0.9, and batch size 30. We train the translator for all 6 source-target language pairs (en-fr, en-de, fr-en, fr"
W19-4827,D16-1159,0,0.0332131,"ut directly. Any intermediate representations of the text may emerge during the training of the neural network, and are hidden to us. We focus on the encoder part of the Transformer architecture (Vaswani et al., 2017), applied to neural machine translation (NMT), as visualizations presented by the authors suggest that its attention heads capture various phenomena such as syntax, semantic roles or anaphora links. In this work, we analyze the syntactic properties of the self-attention heads both qualitatively 2 Related Work Initial analyses of syntax captured by neural networks focused on RNNs. Shi et al. (2016) examine how much syntax is learned by RNN encoder by freezing its weights and using a decoder to predict syntactic trees. Adi et al. (2016) examine sentence vector representations by training auxiliary classifiers to take sentence encodings and predict attributes like word order. Linzen et al. (2016) assess the ability of LSTMs to learn syntax by predicting verbal numbers. Blevins et al. (2018) measure the amount of syntax in RNNs by predicting part-of-speech tags and constituent labels. In the last year, related studies appeared also for the Transformer architecture. Tang et al. (2018) show"
W19-4827,D18-1458,0,0.0579193,"Missing"
W19-4827,W18-5448,0,0.0141338,"osition on the higher layers. Another notable feature of the Transformer encoder is the use of residual connections, which transport the source subword embeddings forward, bypassing the self-attention mechanism, and get averaged with the outputs of the self-attention. This ensures that the output state at each position retains a significant amount of the corresponding source subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing model"
W19-4827,Q16-1037,0,\N,Missing
W19-4827,Q19-1004,0,\N,Missing
W19-4827,N19-1423,0,\N,Missing
W19-8508,P19-1310,0,0.0195764,"er of languages. The vast majority of the world’s languages are under-resourced, lacking such datasets or tools, which gravely limits any research on such languages. The ability to perform the inflection-derivation distinction automatically, assuming only the availability of a plain text corpus of the language, would thus be of great value. Admittedly, for many languages, no plain text corpus of a considerable size is available; in such cases, we are out of luck. Nevertheless, medium-size plain text corpora exist for hundreds of languages – Wikipedia1 covers 300 languages (Rosa, 2018), JW300 (Agić and Vulić, 2019) features texts 1https://www.wikipedia.org/ 61 Proceedings of the 2nd Int. Workshop on Resources and Tools for Derivational Morphology (DeriMo 2019), pages 61–70, Prague, Czechia, 19-20 September 2019. from Watchtower2 for 300 languages (around 100k sentences each), and the text of the whole or a part of the Bible is available for as many as 1,400 languages (Mayer and Cysouw, 2014). Still, in this work, our goal is not (yet) practical, i.e. devising a tool applicable to under-resourced languages, but rather exploratory, investigating the mere feasibility of such an approach. Therefore, we only"
W19-8508,J01-2001,0,0.19746,"s. We utilize their work to provide categories of derivational operations, which are not yet annotated in DeriNet and have to be estimated heuristically. While the methods used by us and previously mentioned authors are rather simple, we are unaware of any other substantial research in this direction. There is research on unsupervised morphology induction, represented by the well-known Morfessor system of Creutz and Lagus (2007), the interesting ParaMor system (Monson et al., 2008) which attempts to find inflectional paradigms, as well as the earlier minimum description length-based system of Goldsmith (2001). While some ideas behind these systems are related to our interests and may potentially be useful to us, their goal is to perform morphological segmentation, which is a related but different task. Another related area is stemming (Lovins, 1968; Porter, 2001), which can be thought of as simple lemmatization. However, stemmers tend to be too coarse, often assigning the same stem to both inflections and derivations. Moreover, they are typically rule-based and thus language-specific, which is not in line with our goals. 3 Approach Our central hypothesis is that word forms that are inflections of"
W19-8508,L18-1550,0,0.156192,"ls. 3 Approach Our central hypothesis is that word forms that are inflections of the same lemma tend to be more similar than inflections of different lemmas. To measure the similarity of word forms, we investigate two somewhat orthogonal simple approaches. Our first method is to use string edit distances, which measure how much the word forms differ on the character level. In our work, we use the Jaro-Winkler (JW) edit distance (Winkler, 1990) and the Levenshtein edit distance (Levenshtein, 1966). As the second method, we propose to measure similarity of word embeddings (Mikolov et al., 2013; Grave et al., 2018). It has been shown that cosine similarity of word embeddings tends to capture various kinds of word similarities, including morphological, syntactic, and semantic similarities, and can be thought of as a proxy to meaning similarity. We then apply the methods to sets of corpus-attested words belonging to one derivational family, i.e. a set of words that are, according to a database of word formation relations, all derived from a common root, together with their inflections extracted from a lemmatized corpus. Some words in the set are thus inflections of a common lemma, while others are inflect"
W19-8508,mayer-cysouw-2014-creating,0,0.0262775,"n text corpus of a considerable size is available; in such cases, we are out of luck. Nevertheless, medium-size plain text corpora exist for hundreds of languages – Wikipedia1 covers 300 languages (Rosa, 2018), JW300 (Agić and Vulić, 2019) features texts 1https://www.wikipedia.org/ 61 Proceedings of the 2nd Int. Workshop on Resources and Tools for Derivational Morphology (DeriMo 2019), pages 61–70, Prague, Czechia, 19-20 September 2019. from Watchtower2 for 300 languages (around 100k sentences each), and the text of the whole or a part of the Bible is available for as many as 1,400 languages (Mayer and Cysouw, 2014). Still, in this work, our goal is not (yet) practical, i.e. devising a tool applicable to under-resourced languages, but rather exploratory, investigating the mere feasibility of such an approach. Therefore, we only use a single resource-rich language for the investigation, so that we can reliably analyze the performance of our approach, for which we need annotated datasets. Moreover, as an outlook to future work, we are also interested in empirically exploring the boundary between derivation and inflection, which is notoriously vague. We hope that empirical computational methods could provid"
W19-8508,W19-4818,0,0.0897627,"s, and both works make the usual choice of using word embeddings as a proxy to word meanings. As the criterion that we test is simpler, our method is also simpler: we directly measure the difference of word embeddings to estimate the distance of meanings. To estimate the regularity of meaning change, Bonami and Paperno (2018) take a further step of estimating an embedding vector shift corresponding to a particular morphological operation, and observe that the vector shift tends to be more regular for inflectional operations than for derivational operations. A partially related work is that of Musil et al. (2019), showing that there is some regularity in the vector shift corresponding to individual derivational operations. However, the authors do not contrast this with inflectional operations. We utilize their work to provide categories of derivational operations, which are not yet annotated in DeriNet and have to be estimated heuristically. While the methods used by us and previously mentioned authors are rather simple, we are unaware of any other substantial research in this direction. There is research on unsupervised morphology induction, represented by the well-known Morfessor system of Creutz an"
W19-8508,sevcikova-zabokrtsky-2014-word,1,0.818083,"s are naturally hard to scale to bigger data and/or more languages. In our study, we take the existence of a crisp inflection-derivation boundary as an assumption, and we try to get close to the boundary in a fully unsupervised way, using only unlabelled corpus data. For evaluation purposes, we accept the boundary as technically defined in existing morphological NLP resources for Czech. More specifically, we use MorfFlex CZ (Hajič and Hlaváčová, 2016) to bind inflected word forms with their lemmas (more exactly, we use only corpus-attested word forms), and the word-formation database DeriNet (Ševčíková and Žabokrtský, 2014), in which relations between derivationally related lexemes are represented in the form of rooted trees (one tree per a derivational family). To the best of our knowledge, the only work to investigate a similar question is the recent research of Bonami and Paperno (2018). Similar to us, the authors are interested in a way to turn the human-centered criteria of distinguishing inflection from derivation into something empirically testable. The authors investigated the semantic regularity criterion (“inflection is semantically more regular than derivation”), 2https://www.jw.org/ 3The debate seems"
W19-8508,L16-1208,1,0.838022,"the pairwise method, with the objective that inflections should fall into common clusters and non-inflections should fall into different clusters. We define Winf l as in (4), and Wclust as the set of all pairs of word forms that fell into the same cluster: Wclust = {w1, w2|clust(w1 ) = clust(w2 )} (7) We then compute the precision, recall, and F1 score of inflection pairs clustered together: Pclust = 5.2 |Winf l ∩ Wclust | |Winf l ∩ Wclust | 2 · Pclust · Rclust ; Rclust = ; Fclust = |Wclust | |Winf l | Pclust + Rclust (8) Experiment setting We extract derivational families from DeriNet v1.7 (Žabokrtský et al., 2016),11 a database of Czech word formation relations. As the database only contains word lemmas, we enrich the extracted lemma sets with inflections of the lemmas found in the Czech National Corpus, subcorpus SYN v4 (Křen et al., 2016), a large corpus of Czech lemmatized automatically using morphological analyzer MorfFlex CZ (Hajič and Hlaváčová, 2016).12 We lowercase all the word forms. 10https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering 11http://ufal.mff.cuni.cz/derinet 12MorfFlex CZ (Hajič and Hlaváčová, 2016) offers complete inflectional paradigms, which leads to"
