2020.msr-1.7,Surface Realization Using Pretrained Language Models,2020,-1,-1,3,1,16532,farhood farahnak,Proceedings of the Third Workshop on Multilingual Surface Realisation,0,"In the context of Natural Language Generation, surface realization is the task of generating the linear form of a text following a given grammar. Surface realization models usually consist of a cascade of complex sub-modules, either rule-based or neural network-based, each responsible for a specific sub-task. In this work, we show that a single encoder-decoder language model can be used in an end-to-end fashion for all sub-tasks of surface realization. The model is designed based on the BART language model that receives a linear representation of unordered and non-inflected tokens in a sentence along with their corresponding Universal Dependency information and produces the linear sequence of inflected tokens along with the missing words. The model was evaluated on the shallow and deep tracks of the 2020 Surface Realization Shared Task (SR{'}20) using both human and automatic evaluation. The results indicate that despite its simplicity, our model achieves competitive results among all participants in the shared task."
2020.lrec-1.134,On the Creation of a Corpus for Coherence Evaluation of Discursive Units,2020,-1,-1,3,1,16883,elham mohammadi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we report on our experiments towards the creation of a corpus for coherence evaluation. Most corpora for textual coherence evaluation are composed of randomly shuffled sentences that focus on sentence ordering, regardless of whether the sentences were originally related by a discourse relation. To the best of our knowledge, no publicly available corpus has been designed specifically for the evaluation of coherence of known discursive units. In this paper, we focus on coherence modeling at the intra-discursive level and describe our approach to build a corpus of incoherent pairs of sentences. We experimented with a variety of corruption strategies to create synthetic incoherent pairs of discourse arguments from coherent ones. Using discourse argument pairs from the Penn Discourse Tree Bank, we generate incoherent discourse argument pairs, by swapping either their discourse connective or a discourse argument. To evaluate how incoherent the generated corpora are, we use a convolutional neural network to try to distinguish the original pairs from the corrupted ones. Results of the classifier as well as a manual inspection of the corpora show that generating such corpora is still a challenge as the generated instances are clearly not {``}incoherent enough{''}, indicating that more effort should be spent on developing more robust ways of generating incoherent corpora."
2020.lrec-1.615,Cooking Up a Neural-based Model for Recipe Classification,2020,-1,-1,6,1,16883,elham mohammadi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we propose a neural-based model to address the first task of the DEFT 2013 shared task, with the main challenge of a highly imbalanced dataset, using state-of-the-art embedding approaches and deep architectures. We report on our experiments on the use of linguistic features, extracted by Charton et. al. (2014), in different neural models utilizing pretrained embeddings. Our results show that all of the models that use linguistic features outperform their counterpart models that only use pretrained embeddings. The best performing model uses pretrained CamemBERT embeddings as input and CNN as the hidden layer, and uses additional linguistic features. Adding the linguistic features to this model improves its performance by 4.5{\%} and 11.4{\%} in terms of micro and macro F1 scores, respectively, leading to state-of-the-art results and an improved classification of the rare classes."
2020.jeptalnrecital-taln.7,Du bon usage d{'}ingr{\\'e}dients linguistiques sp{\\'e}ciaux pour classer des recettes exceptionnelles (Using Special Linguistic Ingredients to Classify Exceptional Recipes ),2020,-1,-1,4,1,16883,elham mohammadi,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Nous pr{\'e}sentons un mod{\`e}le d{'}apprentissage automatique qui combine mod{\`e}les neuronaux et linguistiques pour traiter les t{\^a}ches de classification dans lesquelles la distribution des {\'e}tiquettes des instances est d{\'e}s{\'e}quilibr{\'e}e. Les performances de ce mod{\`e}le sont mesur{\'e}es {\`a} l{'}aide d{'}exp{\'e}riences men{\'e}es sur les t{\^a}ches de classification de recettes de cuisine de la campagne DEFT 2013 (Grouin et al., 2013). Nous montrons que les plongements lexicaux (word embeddings) associ{\'e}s {\`a} des m{\'e}thodes d{'}apprentissage profond obtiennent de meilleures performances que tous les algorithmes d{\'e}ploy{\'e}s lors de la campagne DEFT. Nous montrons aussi que ces m{\^e}mes classifieurs avec plongements lexicaux peuvent gagner en performance lorsqu{'}un mod{\`e}le linguistique est ajout{\'e} au mod{\`e}le neuronal. Nous observons que l{'}ajout d{'}un mod{\`e}le linguistique au mod{\`e}le neuronal am{\'e}liore les performances de classification sur les classes rares."
2020.coling-main.58,{TIMBERT}: Toponym Identifier For The Medical Domain Based on {BERT},2020,-1,-1,2,0,21105,mohammadreza davari,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we propose an approach to automate the process of place name detection in the medical domain to enable epidemiologists to better study and model the spread of viruses. We created a family of Toponym Identification Models based on BERT (TIMBERT), in order to learn in an end-to-end fashion the mapping from an input sentence to the associated sentence labeled with toponyms. When evaluated with the SemEval 2019 task 12 test set (Weissenbacher et al., 2019), our best TIMBERT model achieves an F1 score of 90.85{\%}, a significant improvement compared to the state-of-the-art of 89.13{\%} (Wang et al., 2019)."
W19-3004,{CL}a{C} at {CLP}sych 2019: Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts,2019,-1,-1,3,1,16883,elham mohammadi,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,0,"This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an ensemble method which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 tasks (tasks A and C)."
S19-2023,{CL}a{C} Lab at {S}em{E}val-2019 Task 3: Contextual Emotion Detection Using a Combination of Neural Networks and {SVM},2019,0,1,3,1,16883,elham mohammadi,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes our system at SemEval 2019, Task 3 (EmoContext), which focused on the contextual detection of emotions in a dataset of 3-round dialogues. For our final system, we used a neural network with pretrained ELMo word embeddings and POS tags as input, GRUs as hidden units, an attention mechanism to capture representations of the dialogues, and an SVM classifier which used the learned network representations to perform the task of multi-class classification.This system yielded a micro-averaged F1 score of 0.7072 for the three emotion classes, improving the baseline by approximately 12{\%}."
R19-1091,Neural Feature Extraction for Contextual Emotion Detection,2019,0,0,3,1,16883,elham mohammadi,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"This paper describes a new approach for the task of contextual emotion detection. The approach is based on a neural feature extractor, composed of a recurrent neural network with an attention mechanism, followed by a classifier, that can be neural or SVM-based. We evaluated the model with the dataset of the task 3 of SemEval 2019 (EmoContext), which includes short 3-turn conversations, tagged with 4 emotion classes. The best performing setup was achieved using ELMo word embeddings and POS tags as input, bidirectional GRU as hidden units, and an SVM as the final classifier. This configuration reached 69.93{\%} in terms of micro-average F1 score on the main 3 emotion classes, a score that outperformed the baseline system by 11.25{\%}."
D19-6308,The Concordia {NLG} Surface Realizer at {SRST} 2019,2019,0,0,3,1,16532,farhood farahnak,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"This paper presents the model we developed for the shallow track of the 2019 NLG Surface Realization Shared Task. The model reconstructs sentences whose word order and word inflections were removed. We divided the problem into two sub-problems: reordering and inflecting. For the purpose of reordering, we used a pointer network integrated with a transformer model as its encoder-decoder modules. In order to generate the inflected forms of tokens, a Feed Forward Neural Network was employed."
L18-1306,Attention for Implicit Discourse Relation Recognition,2018,0,0,2,1,29213,andre cianflone,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.jeptalnrecital-deft.3,{CL}a{C} @ {DEFT} 2018: Sentiment analysis of tweets on transport from {{\\^I}}le-de-{F}rance,2018,-1,-1,3,0,30969,simon jacques,"Actes de la Conf{\\'e}rence TALN. Volume 2 - D{\\'e}monstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT",0,"CLaC @ DEFT 2018: Analysis of tweets on transport on the {\^I}le-de-France This paper describes the system deployed by the CLaC lab at Concordia University in Montreal for the DEFT 2018 shared task. The competition consisted in four different tasks; however, due to lack of time, we only participated in the first two. We participated with a system based on conventional supervised learning methods: a support vector machine classifier and an artificial neural network. For task 1, our best approach achieved an F-measure of 87.61{\%}; while at task 2, we achieve 51.03{\%}, situating our system below the average of the other participants."
W17-5501,Automatic Mapping of {F}rench Discourse Connectives to {PDTB} Discourse Relations,2017,13,0,2,1,31481,majid laali,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"In this paper, we present an approach to exploit phrase tables generated by statistical machine translation in order to map French discourse connectives to discourse relations. Using this approach, we created DisCoRel, a lexicon of French discourse connectives and their PDTB relations. When evaluated against LEXCONN, DisCoRel achieves a recall of 0.81 and an Average Precision of 0.68 for the Concession and Condition relations."
davoodi-kosseim-2017-automatic,Automatic Identification of {A}lt{L}exes using Monolingual Parallel Corpora,2017,13,0,2,1,27692,elnaz davoodi,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"The automatic identification of discourse relations is still a challenging task in natural language processing. Discourse connectives, such as since or but, are the most informative cues to identify explicit relations; however discourse parsers typically use a closed inventory of such connectives. As a result, discourse relations signalled by markers outside these inventories (i.e. AltLexes) are not detected as effectively. In this paper, we propose a novel method to leverage parallel corpora in text simplification and lexical resources to automatically identify alternative lexicalizations that signal discourse relation. When applied to the Simple Wikipedia and Newsela corpora along with WordNet and the PPDB, the method allowed the automatic discovery of 91 AltLexes."
hooda-kosseim-2017-argument,Argument Labeling of Explicit Discourse Relations using {LSTM} Neural Networks,2017,15,2,2,0,32447,sohail hooda,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Argument labeling of explicit discourse relations is a challenging task. The state of the art systems achieve slightly above 55{\%} F-measure but require hand-crafted features. In this paper, we propose a Long Short Term Memory (LSTM) based model for argument labeling. We experimented with multiple configurations of our model. Using the PDTB dataset, our best model achieved an F1 measure of 23.05{\%} without any feature engineering. This is significantly higher than the 20.52{\%} achieved by the state of the art RNN approach, but significantly lower than the feature based state of the art systems. On the other hand, because our approach learns only from the raw dataset, it is more widely applicable to multiple textual genres and languages."
laali-kosseim-2017-improving,Improving Discourse Relation Projection to Build Discourse Annotated Corpora,2017,20,0,2,1,31481,majid laali,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"The naive approach to annotation projection is not effective to project discourse annotations from one language to another because implicit relations are often changed to explicit ones and vice-versa in the translation. In this paper, we propose a novel approach based on the intersection between statistical word-alignment models to identify unsupported discourse annotations. This approach identified 65{\%} of the unsupported annotations in the English-French parallel sentences from Europarl. By filtering out these unsupported annotations, we induced the first PDTB-style discourse annotated corpus for French from Europarl. We then used this corpus to train a classifier to identify the discourse-usage of French discourse connectives and show a 15{\%} improvement of F1-score compared to the classifier trained on the non-filtered annotations."
W16-4831,N-gram and Neural Language Models for Discriminating Similar Languages,2016,0,3,2,1,29213,andre cianflone,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"This paper describes our submission to the 2016 Discriminating Similar Languages (DSL) Shared Task. We participated in the closed Sub-task 1 with two separate machine learning techniques. The first approach is a character based Convolution Neural Network with an LSTM layer (CLSTM), which achieved an accuracy of 78.45{\%} with minimal tuning. The second approach is a character-based n-gram model of size 7. It achieved an accuracy of 88.45{\%} which is close to the accuracy of 89.38{\%} achieved by the best submission."
W16-3620,On the Contribution of Discourse Structure on Text Complexity Assessment,2016,0,2,2,1,27692,elnaz davoodi,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
S16-1151,{CL}a{C} at {S}em{E}val-2016 Task 11: Exploring linguistic and psycho-linguistic Features for Complex Word Identification,2016,0,3,2,1,27692,elnaz davoodi,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
K16-2013,The {CL}a{C} Discourse Parser at {C}o{NLL}-2016,2016,18,0,3,1,31481,majid laali,Proceedings of the {C}o{NLL}-16 shared task,0,"This paper describes our submission (CLaC) to the CoNLL-2016 shared task on shallow discourse parsing. We used two complementary approaches for the task. A standard machine learning approach for the parsing of explicit relations, and a deep learning approach for non-explicit relations. Overall, our parser achieves an F1score of 0.2106 on the identification of discourse relations (0.3110 for explicit relations and 0.1219 for non-explicit relations) on the blind CoNLL-2016 test set."
K15-2008,The {CL}a{C} Discourse Parser at {C}o{NLL}-2015,2015,8,7,3,1,31481,majid laali,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,"This paper describes our submission (kosseim15) to the CoNLL-2015 shared task on shallow discourse parsing. We used the UIMA framework to develop our parser and used ClearTK to add machine learning functionality to the UIMA framework. Overall, our parser achieves a result of 17.3 F1 on the identification of discourse relations on the blind CoNLL-2015 test set, ranking in sixth place."
C14-1058,Inducing Discourse Connectives from Parallel Texts,2014,33,11,2,1,31481,majid laali,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Discourse connectives (e.g. however, because) are terms that explicitly express discourse relations in a coherent text. While a list of discourse connectives is useful for both theoretical and empirical research on discourse relations, few languages currently possess such a resource. In this article, we propose a new method that exploits parallel corpora and collocation extraction techniques to automatically induce discourse connectives. Our approach is based on identifying candidates and ranking them using Log-Likelihood Ratio. Then, it relies on several filters to filter the list of candidates, namely: Word-Alignment, POS patterns, and Syntax. Our experiment to induce French discourse connectives from an English-French parallel text shows that Syntactic filter achieves a much higher MAP value (0.39) than the other filters, when compared with LEXCONN resource."
S13-2019,{C}la{C}: Semantic Relatedness of Words and Phrases,2013,13,2,2,0,41200,reda siblini,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"The measurement of phrasal semantic relatedness is an important metric for many natural language processing applications. In this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distributional similarity model, and a hybrid between the two. Our hybrid approach achieved an Fmeasure of 77.4% on the task of evaluating the semantic similarity of words and compositional phrases."
R13-1080,Using a Weighted Semantic Network for Lexical Semantic Relatedness,2013,32,6,2,0,41200,reda siblini,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"The measurement of semantic relatedness between two words is an important metric for many natural language processing applications. In this paper, we present a novel approach for measuring semantic relatedness that is based on a weighted semantic network. This approach explores the use of a lexicon, semantic relation types as weights, and word definitions as a basis to calculate semantic relatedness. Our results show that our approach outperforms many lexicon-based methods to semantic relatedness, especially on the TOEFL synonym test, achieving an accuracy of 91.25%."
I13-1197,Measuring the Effect of Discourse Relations on Blog Summarization,2013,18,2,2,1,41731,shamima mithun,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"The work presented in this paper attempts to evaluate and quantify the use of discourse relations in the context of blog summarization and compare their use to more traditional and factual texts. Specifically, we measured the usefulness of 6 discourse relations - namely comparison, contingency, illustration, attribution, topic-opinion, and attributive for the task of text summarization from blogs. We have evaluated the effect of each relation using the TAC 2008 opinion summarization dataset and compared them with the results with the DUC 2007 dataset. The results show that in both textual genres, contingency, comparison, and illustration relations provide a significant improvement on summarization content; while attribution, topic-opinion, and attributive relations do not provide a consistent and significant improvement. These results indicate that, at least for summarization, discourse relations are just as useful for informal and affective texts as for more traditional news articles."
W12-2606,Discrepancy Between Automatic and Manual Evaluation of Summaries,2012,11,3,2,1,41731,shamima mithun,Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,0,"Today, automatic evaluation metrics such as ROUGE have become the de-facto mode of evaluating an automatic summarization system. However, based on the DUC and the TAC evaluation results, (Conroy and Schlesinger, 2008; Dang and Owczarzak, 2008) showed that the performance gap between human-generated summaries and system-generated summaries is clearly visible in manual evaluations but is often not reflected in automated evaluations using ROUGE scores. In this paper, we present our own experiments in comparing the results of manual evaluations versus automatic evaluations using our own text summarizer: BlogSum. We have evaluated BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList). The t-test results showed that there is no significant difference between BlogSum-generated summaries and OList summaries. However, two manual evaluations for content using two different datasets show that BlogSum performed significantly better than OList. A manual evaluation of summary coherence also shows that BlogSum performs significantly better than OList. These results agree with previous work and show the need for a better automated summary evaluation metric rather than the standard ROUGE metric."
R11-1066,Discourse Structures to Reduce Discourse Incoherence in Blog Summarization,2011,17,7,2,1,41731,shamima mithun,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Discourse incoherence is an important and typical problem with multi-document extractive summaries. To address this issue, we have developed a schema-based summarization approach for query-based blog summaries that utilizes discourse structures. In our schema design, we tried to model discourse structures which are typically used by humans in their summary writing in response to a particular question type. In our approach, a sentence instantiates a specific slot of the schema based on its discourse structures. To validate our approach, we have built a system named BlogSum and have evaluated its performance through 4 human participants using a likert scale of 1 to 5. The evaluation results show that our approach has significantly improved summary coherence compared to the summaries with no discourse structuring without compromising on content evaluation."
2010.jeptalnrecital-long.11,A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization,2010,11,3,2,1,41731,shamima mithun,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences."
W09-4301,Summarizing Blog Entries versus News Texts,2009,17,12,2,1,41731,shamima mithun,Proceedings of the Workshop on Events in Emerging Text Types,0,"As more and more people are expressing their opinions on the web in the form of weblogs (or blogs), research on the blogosphere is gaining popularity. As the outcome of this research, different natural language tools such as query-based opinion summarizers have been developed to mine and organize opinions on a particular event or entity in blog entries. However, the variety of blog posts and the informal style and structure of blog entries pose many difficulties for these natural language tools. In this paper, we identify and categorize errors which typically occur in opinion summarization from blog entries and compare blog entry summaries with traditional news text summaries based on these error types to quantify the differences between these two genres of texts for the purpose of summarization. For evaluation, we used summaries from participating systems of the TAC 2008 opinion summarization track and updated summarization track. Our results show that some errors are much more frequent to blog entries (e.g. topic irrelevant information) compared to news texts; while other error types, such as content overlap, seem to be comparable. These findings can be used to prioritize these error types and give clear indications as to where we should put effort to improve blog summarization."
razmara-kosseim-2008-answering,Answering List Questions using Co-occurrence and Clustering,2008,11,9,2,0,41426,majid razmara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Although answering list questions is not a new research area, answering them automatically still remains a challenge. The median F-score of systems that participated in TREC 2007 Question Answering track is still very low (0.085) while 74{\%} of the questions had a median F-score of 0. In this paper, we propose a novel approach to answering list questions. This approach is based on the hypothesis that answer instances of a list question co-occur in the documents and sentences related to the topic of the question. We use a clustering method to group the candidate answers that co-occur more often. To pinpoint the right cluster, we use the target and the question keywords as spies to return the cluster that contains these keywords."
W04-0833,Simple features for statistical Word Sense Disambiguation,2004,5,10,3,0,51640,abolfazl lamjiri,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
W04-0503,The Problem of Precision in Restricted-Domain Question Answering. Some Proposed Methods of Improvement,2004,-1,-1,2,0,51684,hai doannguyen,Proceedings of the Conference on Question Answering in Restricted Domains,0,None
2003.jeptalnrecital-poster.1,Generation of natural responses through syntactic patterns,2003,-1,-1,2,0,53034,glenda anaya,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"The goal of Question-Answering (QA) systems is to find short and factual answers to opendomain questions by searching a large collection of documents. The subject of this research is to formulate complete and natural answer-sentences to questions, given the short answer. The answer-sentences are meant to be self-sufficient; that is, they should contain enough context to be understood without needing the original question. Generating such sentences is important in question-answering as they can be used to enhance existing QA systems to provide answers to the user in a more natural way and to provide a pattern to actually extract the answer from the document collection."
2001.jeptalnrecital-poster.6,Crit{\\`e}res de s{\\'e}lection d{'}une approche pour le suivi automatique du courriel,2001,8,1,1,1,16534,leila kosseim,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article discute de diff{\'e}rentes approches pour faire le suivi automatique du courrier-{\'e}lectronique. Nous pr{\'e}sentons tout d{'}abord les m{\'e}thodes de traitement automatique de la langue (TAL) les plus utilis{\'e}es pour cette t{\^a}che, puis un ensemble de crit{\`e}res influen{\c{c}}ant le choix d{'}une approche. Ces crit{\`e}res ont {\'e}t{\'e} d{\'e}velopp{\'e}s gr{\^a}ce {\`a} une {\'e}tude de cas sur un corpus fourni par Bell Canada Entreprises. Avec notre corpus, il est apparu que si aucune m{\'e}thode n{'}est compl{\`e}tement satisfaisante par elle-m{\^e}me, une approche combin{\'e}e semble beaucoup plus prometteuse."
2001.jeptalnrecital-poster.7,Extraction de noms propres {\\`a} partir de textes vari{\\'e}s: probl{\\'e}matique et enjeux,2001,2,1,1,1,16534,leila kosseim,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article porte sur l{'}identification de noms propres {\`a} partir de textes {\'e}crits. Les strat{\'e}gies {\`a} base de r{\`e}gles d{\'e}velopp{\'e}es pour des textes de type journalistique se r{\'e}v{\`e}lent g{\'e}n{\'e}ralement insuffisantes pour des corpus compos{\'e}s de textes ne r{\'e}pondant pas {\`a} des crit{\`e}res r{\'e}dactionnels stricts. Apr{\`e}s une br{\`e}ve revue des travaux effectu{\'e}s sur des corpus de textes de nature journalistique, nous pr{\'e}sentons la probl{\'e}matique de l{'}analyse de textes vari{\'e}s en nous basant sur deux corpus compos{\'e}s de courriers {\'e}lectroniques et de transcriptions manuelles de conversations t{\'e}l{\'e}phoniques. Une fois les sources d{'}erreurs pr{\'e}sent{\'e}es, nous d{\'e}crivons l{'}approche utilis{\'e}e pour adapter un syst{\`e}me d{'}extraction de noms propres d{\'e}velopp{\'e} pour des textes journalistiques {\`a} l{'}analyse de messages {\'e}lectroniques."
W94-0307,Content and Rhetorical Status Selection in Instructional Texts,1994,13,12,1,1,16534,leila kosseim,Proceedings of the Seventh International Workshop on Natural Language Generation,0,"This paper discusses an approach to planning the content of instructional texts. The research is based on a corpus study of 15 French procedural texts ranging from step-by-step device manuals to general artistic procedures. The approach taken starts from an AI task planner building a task representation, from which semantic carriers are selected. The most appropriate RST relations to communicate these carriers are then chosen according to heuristics developed during the corpus analysis."
