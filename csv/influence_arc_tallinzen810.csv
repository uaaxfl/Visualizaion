2020.acl-main.212,D19-1418,0,0.0828238,"Missing"
2020.acl-main.212,K18-1007,0,0.0253979,"which is a contiguous subsequence of it) and the constituent heuristic (assume that a premise entails all of its constituents). While we focus on counteracting the lexical overlap heuristic, we will also test for generalization to the other heuristics, which can be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superficial properties by generating a diverse set of"
2020.acl-main.212,N18-1202,0,0.0183212,"ut affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations. 1 Introduction In the supervised learning paradigm common in NLP, a large collection of labeled examples of a particular classification task is randomly split into a training set and a test set. The system is trained on this training set, and is then evaluated on the test set. Neural networks—in particular systems pretrained on a word prediction objective, such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019)—excel in this paradigm: with large enough pretraining corpora, these models match or even exceed the accuracy of untrained human annotators on many test sets (Raffel et al., 2019). At the same time, there is mounting evidence that high accuracy on a test set drawn from the (1) The lawyer saw the actor. (2) The actor saw the lawyer. McCoy et al. constructed the HANS challenge set, which includes examples of a range of such constructions, and used it to show that, when BERT is fine-tuned on the MNLI corpus (Williams et al., 2018), the fine-tuned model achieves high"
2020.acl-main.212,P18-1079,0,0.0477108,"be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superficial properties by generating a diverse set of corpus-based examples, which differ from the challenge set both lexically and syntactically. Finally, Kim et al. (2018) used a similar augmentation approach to ours but did not study generalization to types of examples not in the augmentation set. Backgroun"
2020.acl-main.212,P19-1485,0,0.048587,"Missing"
2020.acl-main.212,D19-1221,0,0.0187659,"ily Pitler2 Tal Linzen1 1 Department of Cognitive Science, Johns Hopkins University, Baltimore, MD 2 Google Research, New York, NY {jmin10, tom.mccoy, tal.linzen}@jhu.edu {dipanjand, epitler}@google.com Abstract same distribution as the training set does not indicate that the model has mastered the task. This discrepancy can manifest as a sharp drop in accuracy when the model is applied to a different dataset that illustrates the same task (Talmor and Berant, 2019; Yogatama et al., 2019), or as excessive sensitivity to linguistically irrelevant perturbations of the input (Jia and Liang, 2017; Wallace et al., 2019). One such discrepancy, where strong performance on a standard test set did not correspond to mastery of the task as a human would define it, was documented by McCoy et al. (2019b) for the Natural Language Inference (NLI) task. In this task, the system is given two sentences, and is expected to determine whether one (the premise) entails the other (the hypothesis). Most if not all humans would agree that NLI requires sensitivity to syntactic structure; for example, the following sentences do not entail each other, even though they contain the same words: Pretrained neural models such as BERT,"
2020.acl-main.212,D19-1670,0,0.0626803,"that a premise entails any hypothesis which is a contiguous subsequence of it) and the constituent heuristic (assume that a premise entails all of its constituents). While we focus on counteracting the lexical overlap heuristic, we will also test for generalization to the other heuristics, which can be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superfic"
2020.acl-main.212,N18-1101,0,0.0804875,"d prediction objective, such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019)—excel in this paradigm: with large enough pretraining corpora, these models match or even exceed the accuracy of untrained human annotators on many test sets (Raffel et al., 2019). At the same time, there is mounting evidence that high accuracy on a test set drawn from the (1) The lawyer saw the actor. (2) The actor saw the lawyer. McCoy et al. constructed the HANS challenge set, which includes examples of a range of such constructions, and used it to show that, when BERT is fine-tuned on the MNLI corpus (Williams et al., 2018), the fine-tuned model achieves high accuracy on the test set drawn from that corpus, yet displays little sensitivity to syntax; the model wrongly concluded, for example, that (1) entails (2). We consider two explanations as to why BERT fine-tuned on MNLI fails on HANS. Under the Representational Inadequacy Hypothesis, BERT fails on HANS because its pretrained representations are missing some necessary syntactic information. Under the Missed Connection Hypothesis, BERT extracts the relevant syntactic information from the input (cf. Goldberg 2019; 2339 Proceedings of the 58th Annual Meeting of"
2020.acl-main.212,S19-1027,0,\N,Missing
2020.acl-main.212,N19-1423,0,\N,Missing
2020.acl-main.303,Q16-1026,0,0.0160964,"., 2015; Li et al., 2015). One contribution of the present work is to tease apart the two major types of syntactic structure to see which one imparts more effective syntactic biases. 3 Models 3.1 BiLSTM As our baseline model, we used a simple extension to the LSTM architecture (Hochreiter and Schmidhuber, 1997), the bidirectional LSTM (BiLSTM; Schuster and Paliwal, 1997). This model runs one LSTM from left to right over a sequence, and another from right to left, without appealing to tree structure. Bidirectional LSTMs outperform unidirectional LSTMs on a variety of tasks (Huang et al., 2015; Chiu and Nichols, 2016), including syntaxsensitive tasks (Kiperwasser and Goldberg, 2016). Ravfogel et al. (2019) also employs BiLSTMs for a similar agreement task. 3.2 Tree LSTMs To study the effects of explicitly building tree structure into the model architecture, we used the Constituency LSTM and the Dependency LSTM (Tai et al., 2015), which are types of recursive neural networks (Goller and Kuchler, 1996). The Constituency LSTM operates in accordance with a binary constituency parse, composing together vectors representing a left child and a right child into a vector representing their parent. Models similar to"
2020.acl-main.303,P82-1020,0,0.660668,"Missing"
2020.acl-main.303,Q16-1023,0,0.0319446,"work is to tease apart the two major types of syntactic structure to see which one imparts more effective syntactic biases. 3 Models 3.1 BiLSTM As our baseline model, we used a simple extension to the LSTM architecture (Hochreiter and Schmidhuber, 1997), the bidirectional LSTM (BiLSTM; Schuster and Paliwal, 1997). This model runs one LSTM from left to right over a sequence, and another from right to left, without appealing to tree structure. Bidirectional LSTMs outperform unidirectional LSTMs on a variety of tasks (Huang et al., 2015; Chiu and Nichols, 2016), including syntaxsensitive tasks (Kiperwasser and Goldberg, 2016). Ravfogel et al. (2019) also employs BiLSTMs for a similar agreement task. 3.2 Tree LSTMs To study the effects of explicitly building tree structure into the model architecture, we used the Constituency LSTM and the Dependency LSTM (Tai et al., 2015), which are types of recursive neural networks (Goller and Kuchler, 1996). The Constituency LSTM operates in accordance with a binary constituency parse, composing together vectors representing a left child and a right child into a vector representing their parent. Models similar to the Constituency LSTM have been proposed by Le and Zuidema (2015)"
2020.acl-main.303,W18-5426,0,0.117054,"st for the subject-verb agreement task, constituency structure is more important than dependency structure. Both tree-based model structure and data augmentation appear to be viable approaches for imparting these biases. 2 Related Work Prior work has shown that neural networks without explicit mechanisms for representing syntactic structure can show considerable sensitivity to syntactic dependencies (Goldberg, 2019; Gulordava et al., 2018; Linzen et al., 2016), and that certain aspects of the structure of the sentence can be reconstructed from their internal representations (Lin et al., 2019; Giulianelli et al., 2018; Hewitt and Manning, 2019). Marvin and Linzen (2018) showed that sequential models still have substantial room for improvement in capturing syntax, and other work has shown that models with a greater degree of syntactic structure outperform sequential models on syntax-sensitive tasks (Yogatama et al., 2018; Kuncoro et al., 2018, 2017), including some of the tree-based models used here (Bowman et al., 2015; Li et al., 2015). One contribution of the present work is to tease apart the two major types of syntactic structure to see which one imparts more effective syntactic biases. 3 Models 3.1 Bi"
2020.acl-main.303,D15-1278,0,\N,Missing
2020.acl-main.303,P14-5010,0,\N,Missing
2020.acl-main.303,D14-1162,0,\N,Missing
2020.acl-main.303,P16-1139,0,\N,Missing
2020.acl-main.303,Q16-1037,1,\N,Missing
2020.acl-main.303,E17-1117,0,\N,Missing
2020.acl-main.303,Q17-1012,0,\N,Missing
2020.acl-main.303,P18-1132,0,\N,Missing
2020.acl-main.303,D18-1151,1,\N,Missing
2020.acl-main.303,N19-1419,0,\N,Missing
2020.acl-main.303,2020.acl-main.212,1,\N,Missing
2020.acl-main.465,2020.tacl-1.3,0,0.030537,"et al., 2019; Hudson and Manning, 2019), developing models that learn from them efficiently, and using the Generalization Leaderboard to measure how effective this signal is in aligning the model’s generalization behavior with that of humans. Normative evaluation. Performance metrics should be derived not from samples from the same distribution as the fine-tuning set, but from what we might term normative evaluation: expert-created controlled data sets that capture our intuitions about how an agent should perform the task (Marelli et al., 2014; Marvin and Linzen, 2018; Warstadt et al., 2019; Ettinger, 2020). Such data sets should be designed to be difficult to solve using heuristics that ignore linguistic principles. While experts are more expensive than crowdworkers, the payoff in terms of data set quality is likely to be consider5213 able. In parallel, we should continue to explore approaches such as adversarial filtering that may limit crowdworkers’ ability to resort to shortcuts (Zellers et al., 2018; Nie et al., 2019). Normative evaluation is related to but distinct from adversarial evaluation. Adversarial attacks usually focus on a specific trained model, starting from an example that the"
2020.acl-main.465,N19-1225,0,0.0366866,"changes the system’s response (Jia and Liang, 2017). By contrast, the goal of the normative evaluation paradigm is not to fool a particular system by exploiting its weaknesses, but simply to describe the desirable performance on the task in a unambiguous way. Test-only benchmarks. A central point that bears repeating is that we should not fine-tune our models on the evaluation benchmark. Despite our best efforts, we may never be able to create a benchmark that does not have unintended statistical regularities. Fine-tuning on the benchmark may clue the model into such unintended correlations (Liu et al., 2019a). Any pretrained model will still need to be taught how to perform the transfer task, of course, but this should be done using a separate data set, perhaps one of those that are currently aggregated in GLUE. Either way, the Generalization Leaderboard should favor models that, like humans, are able to perform tasks with minimal instruction (few-shot learning, Yogatama et al. 2019). What about efficiency? The PAID paradigm is agnostic not only to pretraining resources, but also to properties of the model such as the number of parameters, the speed of inference, or the number of GPU hours requi"
2020.acl-main.465,N18-2017,0,0.118908,"Missing"
2020.acl-main.465,2021.ccl-1.108,0,0.101323,"Missing"
2020.acl-main.465,P18-1031,0,0.0704636,"Missing"
2020.acl-main.465,marelli-etal-2014-sick,0,0.0163923,"ld address it experimentally, by collecting multimodal data sets (Suhr et al., 2019; Hudson and Manning, 2019), developing models that learn from them efficiently, and using the Generalization Leaderboard to measure how effective this signal is in aligning the model’s generalization behavior with that of humans. Normative evaluation. Performance metrics should be derived not from samples from the same distribution as the fine-tuning set, but from what we might term normative evaluation: expert-created controlled data sets that capture our intuitions about how an agent should perform the task (Marelli et al., 2014; Marvin and Linzen, 2018; Warstadt et al., 2019; Ettinger, 2020). Such data sets should be designed to be difficult to solve using heuristics that ignore linguistic principles. While experts are more expensive than crowdworkers, the payoff in terms of data set quality is likely to be consider5213 able. In parallel, we should continue to explore approaches such as adversarial filtering that may limit crowdworkers’ ability to resort to shortcuts (Zellers et al., 2018; Nie et al., 2019). Normative evaluation is related to but distinct from adversarial evaluation. Adversarial attacks usually focu"
2020.acl-main.465,D18-1151,1,0.862791,"ntally, by collecting multimodal data sets (Suhr et al., 2019; Hudson and Manning, 2019), developing models that learn from them efficiently, and using the Generalization Leaderboard to measure how effective this signal is in aligning the model’s generalization behavior with that of humans. Normative evaluation. Performance metrics should be derived not from samples from the same distribution as the fine-tuning set, but from what we might term normative evaluation: expert-created controlled data sets that capture our intuitions about how an agent should perform the task (Marelli et al., 2014; Marvin and Linzen, 2018; Warstadt et al., 2019; Ettinger, 2020). Such data sets should be designed to be difficult to solve using heuristics that ignore linguistic principles. While experts are more expensive than crowdworkers, the payoff in terms of data set quality is likely to be consider5213 able. In parallel, we should continue to explore approaches such as adversarial filtering that may limit crowdworkers’ ability to resort to shortcuts (Zellers et al., 2018; Nie et al., 2019). Normative evaluation is related to but distinct from adversarial evaluation. Adversarial attacks usually focus on a specific trained m"
2020.acl-main.465,P19-1334,1,0.92303,"e Association for Computational Linguistics, pages 5210–5217 c July 5 - 10, 2020. 2020 Association for Computational Linguistics edge in a consistent way to structures that are infrequent or non-existent in corpora (Sprouse et al., 2013), and quickly learn to do new things with language (what we sometimes refer to in NLP as “tasks”). As I discuss below, this is not the case for current deep learning systems: when tested on cases sampled from a distribution that differs from the one they were trained on, their behavior is unpredictable and inconsistent with that of humans (Jia and Liang, 2017; McCoy et al., 2019b), and they require extensive instruction on each new task (Yogatama et al., 2019). Humans’ rapid and consistent generalization abilities rely on powerful inductive biases, which likely arise from a combination of innate building blocks and experience with diverse learning problems (Lake et al., 2017). Systems that generalize like humans would be useful not only for NLP, but also for the scientific study of human language acquisition and processing (Keller, 2010; Dupoux, 2018). But, as I will argue in the next two sections, it is unclear whether our dominant evaluation paradigms are getting u"
2020.acl-main.465,2020.acl-main.212,1,0.83592,"’s inductive biases and the statistical properties of the data set. In the case of BERT’s insensitivity to word order in NLI, the model does not seem to have a strong inductive bias one way or another; its sensitivity to word order varies widely depending on the weight initialization of the fine-tuning classifier and the order of the fine-tuning examples (McCoy et al., 2019a), and its syntactic behavior in the inference task can be made to be more consistent with human intuitions if the training set is augmented to include a larger number of examples illustrating the importance of word order (Min et al., 2020). While BERT is capable of learning to use syntax for inference given a sufficiently strong signal, then, it prefers to use other heuristics, if possible. This contrasts with human-like generalization in this task, which would likely start from the assumption that any language understanding task should recruit our 2 Comparisons between human annotators and transformers are arguably unfair: before observing the test set, the models receive hundreds of thousands of examples of the output of the data-generating process. This contrasts with humans annotators, who need to perform the task based on"
2020.acl-main.465,P19-1449,0,0.0216523,"at the detective followed the suspect entails the suspect followed the detective. In short, the models, unable to discern the intentions of the data set’s designers, happily recapitulate any statistical patterns they find in the training data. With a random training/test split, any correlation observed in the training set will hold approximately for the test set, and a system that learned it could achieve high test set accuracy. And indeed, we have models that excel in the PAID paradigm, even exceeding the performance of human annotators on the test portion of the corpus used for fine-tuning (Nangia and Bowman, 2019), but, when tested on controlled examples, make mistakes that a human would rarely make.2 The generalizations that a statistical model extracts from the data are always the result of the interaction between the model’s inductive biases and the statistical properties of the data set. In the case of BERT’s insensitivity to word order in NLI, the model does not seem to have a strong inductive bias one way or another; its sensitivity to word order varies widely depending on the weight initialization of the fine-tuning classifier and the order of the fine-tuning examples (McCoy et al., 2019a), and"
2020.acl-main.465,N18-1202,0,0.102894,"Missing"
2020.acl-main.465,S18-2023,0,0.0607578,"Missing"
2020.acl-main.465,D19-1592,1,0.909923,"Missing"
2020.acl-main.465,P19-1644,0,0.0210001,"pointed out that humans do not learn language from text alone—we also observe the world and interact with it. This, according to this argument, renders the comparison meaningless. While the observation that children learn from diverse sources of information is certainly correct, it is unclear whether any plausible amount of non-linguistic input could offset the difference between 50 million words (humans) and 130 billion words (T5). Instead of taking this observation as a carte blanche to ignore sample efficiency, then, we should address it experimentally, by collecting multimodal data sets (Suhr et al., 2019; Hudson and Manning, 2019), developing models that learn from them efficiently, and using the Generalization Leaderboard to measure how effective this signal is in aligning the model’s generalization behavior with that of humans. Normative evaluation. Performance metrics should be derived not from samples from the same distribution as the fine-tuning set, but from what we might term normative evaluation: expert-created controlled data sets that capture our intuitions about how an agent should perform the task (Marelli et al., 2014; Marvin and Linzen, 2018; Warstadt et al., 2019; Ettinger, 202"
2020.acl-main.465,N19-1334,0,0.0243376,"characters and a space or punctuation mark after each word. 5211 rewarded by PAID, such as massive transformers, only work well when given an amount of data that is orders of magnitude greater than that available to humans. If that is the case, our exploration of the space of possible models could be going in a direction that is orthogonal to the one that might lead us to models that can imitate humans’ sample efficiency (one example of such direction is neural networks with explicit symbolic structure, which are harder to scale up, but perform well on smaller data sets: Kuncoro et al. 2018; Wilcox et al. 2019). 4 Identically Distributed Training Set and Test Set The remaining two letters of the PAID acronym refer to the practice of evaluating success on classification tasks using training and test set generated using the same process. Typically, a single data set is collected and is randomly split into a training portion and test portion. While this may seem reasonable from a machine learning perspective, it has become clear that this form of evaluation obscures possible mismatches between the generalizations that we as humans believe a system performing the task should acquire, and the generalizat"
2020.acl-main.465,N18-1101,0,0.0547833,"ot notice such a correlation, and indeed they do, leading them to respond CONTRADICTION with high probability any time the hypothesis contains a negation word (Gururangan et al., 2018; Poliak et al., 2018). Of course, relying on the presence of the word not is not a generally valid inference strategy; for example, the man is awake entails, rather than contradicts, the man is not sleeping. Numerous generalization issues of this sort have been documented, for NLI and for other tasks. In the syntactic domain, McCoy et al. (2019b) showed that BERT fine-tuned on the crowdsourced MultiNLI data set (Williams et al., 2018) achieves high accuracy on the MultiNLI test set, but shows very little sensitivity to word order when tested on constructed examples that require an analysis of the structure of the sentence; for example, this model is likely to conclude that the detective followed the suspect entails the suspect followed the detective. In short, the models, unable to discern the intentions of the data set’s designers, happily recapitulate any statistical patterns they find in the training data. With a random training/test split, any correlation observed in the training set will hold approximately for the tes"
2020.acl-main.465,D18-1009,0,0.0229182,"normative evaluation: expert-created controlled data sets that capture our intuitions about how an agent should perform the task (Marelli et al., 2014; Marvin and Linzen, 2018; Warstadt et al., 2019; Ettinger, 2020). Such data sets should be designed to be difficult to solve using heuristics that ignore linguistic principles. While experts are more expensive than crowdworkers, the payoff in terms of data set quality is likely to be consider5213 able. In parallel, we should continue to explore approaches such as adversarial filtering that may limit crowdworkers’ ability to resort to shortcuts (Zellers et al., 2018; Nie et al., 2019). Normative evaluation is related to but distinct from adversarial evaluation. Adversarial attacks usually focus on a specific trained model, starting from an example that the model classifies correctly, and perturbing it in ways that, under the normative definition of the task, should not affect the classifier’s decision. For example, adversarial evaluation for a given question answering system may take an existing instance from the data set, and find an irrelevant sentence that, when added to the paragraph that the question is about, changes the system’s response (Jia and"
2020.acl-main.465,P10-2012,0,\N,Missing
2020.acl-main.465,P18-1132,0,\N,Missing
2020.acl-main.465,N19-1423,0,\N,Missing
2020.acl-main.490,D19-1287,0,0.0243916,"rb-phrase coordination: a. The woman laughs and talks/*talk. b. My friends play tennis every week and then get/*gets ice cream. In computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To the extent that the acceptability contrast relies on a single word in a particular location, as in (2), this approach can be extended to bidirectional word prediction systems such as BERT, even though they do not assign a probability to the sentence (Goldberg, 2019). As we describe below, the current version of CLAMS only includes contrasts of"
2020.acl-main.490,C18-1012,0,0.021915,"n computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To the extent that the acceptability contrast relies on a single word in a particular location, as in (2), this approach can be extended to bidirectional word prediction systems such as BERT, even though they do not assign a probability to the sentence (Goldberg, 2019). As we describe below, the current version of CLAMS only includes contrasts of this category. An alternative use of acceptability judgments in NLP involves training an encoder to classify sentences into acceptable and una"
2020.acl-main.490,N19-1423,0,0.625194,"icher morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Neural networks can be trained to predict words from their context with much greater accuracy than the architectures used for this purpose in the past. This has been shown to be the case for both recurrent neural networks (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016) and non-recurrent attention-based models (Devlin et al., 2019; Radford et al., 2019). To gain a better understanding of these models’ successes and failures, in particular in the domain of syntax, proposals have been made for testing the † Work done while at Johns Hopkins University. Now in the University of British Columbia’s Linguistics Department. To correctly predict the form of the verb (underlined), the model needs to determine that the head of the subject of the sentence—an abstract, structurally defined notion—is the word key rather than cabinets or coins. The approach of sampling challenging sentences from a test corpus has its limitations. Exa"
2020.acl-main.490,W18-5453,0,0.0188572,"cceptable sentences; by contrast, the prediction approach we adopt can be used to evaluate any word prediction model without additional training. 2.3 Grammatical Evaluation Beyond English Most of the work on grammatical evaluation of word prediction models has focused on English. However, there are a few exceptions, which we discuss in this section. To our knowledge, all of these studies have used sentences extracted from a corpus rather than a controlled challenge set, as we propose. Gulordava et al. (2018) extracted English, Italian, Hebrew, and Russian evaluation sentences from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Gra"
2020.acl-main.490,N19-1004,0,0.10445,"Missing"
2020.acl-main.490,D17-1263,0,0.0708785,"Missing"
2020.acl-main.490,Q16-1037,1,0.85534,"et al., 2019), we focus our evaluation on the pre-trained English BERT and multilingual BERT. English, French, German and Russian are all IndoEuropean languages, and (Modern) Hebrew syntax exhibits European areal influence (for different perspectives, see Wexler 1990; Zuckermann 2006; Zeldes 2013). 2 https://github.com/aaronmueller/clams (2) Verb-phrase coordination: a. The woman laughs and talks/*talk. b. My friends play tennis every week and then get/*gets ice cream. In computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To t"
2020.acl-main.490,D18-1151,1,0.0767055,"otion—is the word key rather than cabinets or coins. The approach of sampling challenging sentences from a test corpus has its limitations. Examples of relevant constructions may be difficult to find in the corpus, and naturally occurring sentences often contain statistical cues (confounds) that make it possible for the model to predict the correct form of the verb without an adequate syntactic analysis (Gulordava et al., 2018). To address these limitations, a growing number of studies have used constructed materials, which improve experimental control and coverage of syntactic constructions (Marvin and Linzen, 2018; Wilcox et al., 2018; Futrell et al., 2019; Warstadt et al., 2019a). Existing experimentally controlled data sets—in particular, those targeting subject-verb agreement— have largely been restricted to English. As such, we have a limited understanding of the effect of the cross-linguistic variability in neural networks’ syntactic prediction abilities. In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian. By focusing"
2020.acl-main.490,2020.acl-main.309,0,0.0171083,"es without attractors—Simple Agreement and Short VP Coordination—was close to perfect in all languages. This suggests that all monolingual models learned the basic facts of agreement, and were able to apply them to the vocabulary items in our materials. At the other end of the spectrum, performance was only slightly higher than chance in the Across an Object Relative Clause condition for all languages except German, suggesting that LSTMs tend to struggle with center embedding—that is, when a subject-verb dependency is nested within another dependency of the same kind (Marvin and Linzen, 2018; Noji and Takamura, 2020). There was higher variability across languages in the remaining three constructions. The German models had almost perfect accuracy in Long VP Coordination and Across Prepositional Phrase, compared to accuracies ranging between 0.76 and 0.87 for other languages in those constructions. The Hebrew, Russian, and German models showed very high performance on the Across Subject Relative Clause condition: ≥ 0.88 compared to 0.6–0.71 5527 7 https://github.com/yoavg/bert-syntax English French German Hebrew Russian Mono Multi Mono Multi Mono Multi Mono Multi Mono Multi Test Perplexity 57.90 66.13 35.48"
2020.acl-main.490,N19-1356,1,0.896022,"Missing"
2020.acl-main.490,W18-5412,0,0.0565845,"valuation Beyond English Most of the work on grammatical evaluation of word prediction models has focused on English. However, there are a few exceptions, which we discuss in this section. To our knowledge, all of these studies have used sentences extracted from a corpus rather than a controlled challenge set, as we propose. Gulordava et al. (2018) extracted English, Italian, Hebrew, and Russian evaluation sentences from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Grammar Framework To construct our challenge sets, we use a lightweight grammar engineering framework that we term attribute-varying grammars (AVGs). This framework"
2020.acl-main.490,D19-1592,1,0.907687,"Missing"
2020.acl-main.490,E17-2060,0,0.0366035,"nces from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Grammar Framework To construct our challenge sets, we use a lightweight grammar engineering framework that we term attribute-varying grammars (AVGs). This framework provides more flexibility than the hard-coded templates of Marvin and Linzen (2018) while avoiding the unbounded embedding depth of sentences generated from a recursive contextfree grammar (CFG, Chomsky 1956). This is done using templates, which consist of preterminals (which have attributes) and terminals. A vary statement specifies which preterminal attributes are varied to generate ungrammatical sente"
2020.acl-main.490,D18-1503,0,0.0329714,"Missing"
2020.acl-main.490,Q19-1040,0,0.138694,"sampling challenging sentences from a test corpus has its limitations. Examples of relevant constructions may be difficult to find in the corpus, and naturally occurring sentences often contain statistical cues (confounds) that make it possible for the model to predict the correct form of the verb without an adequate syntactic analysis (Gulordava et al., 2018). To address these limitations, a growing number of studies have used constructed materials, which improve experimental control and coverage of syntactic constructions (Marvin and Linzen, 2018; Wilcox et al., 2018; Futrell et al., 2019; Warstadt et al., 2019a). Existing experimentally controlled data sets—in particular, those targeting subject-verb agreement— have largely been restricted to English. As such, we have a limited understanding of the effect of the cross-linguistic variability in neural networks’ syntactic prediction abilities. In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian. By focusing on a single lin5523 Proceedings of the 58th Annual Meeting of the"
2020.acl-main.490,E17-2102,0,\N,Missing
2020.acl-main.490,W16-4117,0,\N,Missing
2020.acl-main.490,P18-1132,0,\N,Missing
2020.blackboxnlp-1.21,W19-4820,0,0.0207919,"alization, while Zhou et al. only report overall accuracy averaged across categories. In addition, we fine-tuned 100 instances of BERT, while Zhou et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works have generally used models without fine-tuning or have used only a small number of fine-tuni"
2020.blackboxnlp-1.21,P19-1283,0,0.0220249,"d aim for depth: We analyze the particular categories within HANS to give a fine-grained investigation of syntactic generalization, while Zhou et al. only report overall accuracy averaged across categories. In addition, we fine-tuned 100 instances of BERT, while Zhou et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning"
2020.blackboxnlp-1.21,W19-4828,0,0.0199875,"apers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works have generally used models without fine-tuning or have used only a small number of fine-tuning runs (usually only one fine-tuning run, or at most ten fine-tuning runs). 3 3.1 Method Task and datasets We used the task of natural language inference (NLI, also known as Recognizing Textual Entailment; Condoravdi et al., 2003; Dagan et al., 2006, 2013), which involves giving a model two sentences, called the premise and the"
2020.blackboxnlp-1.21,W03-0906,0,0.207466,"Missing"
2020.blackboxnlp-1.21,N19-1423,0,0.150688,"cture. This question is important because, in order to tell which types of architectures generalize best, we need to know whether suc217 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217–227 c Online, November 20, 2020. 2020 Association for Computational Linguistics cesses and failures of generalization should be attributed to aspects of the architecture or to random luck in the choice of the model’s initial weights. We investigate this question using the task of natural language inference (NLI). We fine-tuned 100 instances of BERT (Devlin et al., 2019) on the MNLI dataset (Williams et al., 2018).1 These 100 instances differed only in (i) the initial weights of the classifier trained on top of BERT, and (ii) the order in which training examples were presented. All other aspects of training, including the initial weights of BERT, were held constant. We evaluated these 100 instances on both the in-distribution MNLI development set and the out-of-distribution HANS evaluation set (McCoy et al., 2019), which tests syntactic generalization in NLI models. We found that these 100 instances were remarkably consistent in their in-distribution generali"
2020.blackboxnlp-1.21,2020.tacl-1.3,0,0.0254019,"nt of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works have generally used models without fine-tuning or have used only a small number of fine-tuning runs (usually only one fine-tuning run, or at most ten fine-tuning runs). 3 3.1 Method Task and datasets We used the task of natural language inference (NLI, also known as Recognizing Textual Entailment; Condoravdi et al., 2003; Dagan et al."
2020.blackboxnlp-1.21,2021.ccl-1.108,0,0.0924844,"Missing"
2020.blackboxnlp-1.21,N18-1108,1,0.756831,"ather than deeper linguistic knowledge. Therefore, evaluating only on standard test sets cannot reveal whether a model has learned abstract properties of language or if it has only learned shallow heuristics. An alternative evaluation approach addresses this flaw by testing how the model handles particular linguistic phenomena, using datasets designed to be impossible to solve using shallow heuristics. In this line of investigation, which tests out-of-distribution generalization, the results are more mixed. Some works have found successful handling of phenomena such as subject-verb agreement (Gulordava et al., 2018) and filler-gap dependencies (Wilcox et al., 2018). Other works, however, have illuminated surprising failures even on seemingly simple types of examples (Marvin and Linzen, 2018; McCoy et al., 2019). Such results make it clear that there is still much room for improvement in how neural models perform on syntactic structures that are rare in training corpora. In this work, we investigate whether the linguistic generalization behavior of a given neural architecture is consistent across multiple instances of that architecture. This question is important because, in order to tell which types of a"
2020.blackboxnlp-1.21,N19-1419,0,0.0272649,"breadth, whereas we instead aim for depth: We analyze the particular categories within HANS to give a fine-grained investigation of syntactic generalization, while Zhou et al. only report overall accuracy averaged across categories. In addition, we fine-tuned 100 instances of BERT, while Zhou et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a"
2020.blackboxnlp-1.21,P19-1356,0,0.0217052,"he particular categories within HANS to give a fine-grained investigation of syntactic generalization, while Zhou et al. only report overall accuracy averaged across categories. In addition, we fine-tuned 100 instances of BERT, while Zhou et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works h"
2020.blackboxnlp-1.21,S19-1026,1,0.878864,"Missing"
2020.blackboxnlp-1.21,W19-4825,0,0.0987408,"Missing"
2020.blackboxnlp-1.21,P19-1441,0,0.0299832,"hich are novel but which are drawn from the same distribution as the training set. 2. Out-of-distribution generalization: Generalization to examples drawn from a different distribution than the training set. Standard test sets in natural language processing are generated in the same way as the corresponding training set, therefore testing only in-distribution generalization. Current neural architectures perform very well at this type of generalization. For example, on the natural language understanding tasks included in the GLUE benchmark (Wang et al., 2019), several Transformer-based models (Liu et al., 2019b,a; Raffel et al., 2020) have surpassed the human baselines from Nangia and Bowman (2019). However, this strong performance does not necessarily indicate mastery of language. Because of biases in training distributions, it is often possible for a model to achieve strong in-distribution generalization by using shallow heuristics rather than deeper linguistic knowledge. Therefore, evaluating only on standard test sets cannot reveal whether a model has learned abstract properties of language or if it has only learned shallow heuristics. An alternative evaluation approach addresses this flaw by t"
2020.blackboxnlp-1.21,K19-1087,0,0.13277,"Missing"
2020.blackboxnlp-1.21,D18-1151,1,0.782134,"ly learned shallow heuristics. An alternative evaluation approach addresses this flaw by testing how the model handles particular linguistic phenomena, using datasets designed to be impossible to solve using shallow heuristics. In this line of investigation, which tests out-of-distribution generalization, the results are more mixed. Some works have found successful handling of phenomena such as subject-verb agreement (Gulordava et al., 2018) and filler-gap dependencies (Wilcox et al., 2018). Other works, however, have illuminated surprising failures even on seemingly simple types of examples (Marvin and Linzen, 2018; McCoy et al., 2019). Such results make it clear that there is still much room for improvement in how neural models perform on syntactic structures that are rare in training corpora. In this work, we investigate whether the linguistic generalization behavior of a given neural architecture is consistent across multiple instances of that architecture. This question is important because, in order to tell which types of architectures generalize best, we need to know whether suc217 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217–227 c"
2020.blackboxnlp-1.21,P19-1334,1,0.928925,"tics. An alternative evaluation approach addresses this flaw by testing how the model handles particular linguistic phenomena, using datasets designed to be impossible to solve using shallow heuristics. In this line of investigation, which tests out-of-distribution generalization, the results are more mixed. Some works have found successful handling of phenomena such as subject-verb agreement (Gulordava et al., 2018) and filler-gap dependencies (Wilcox et al., 2018). Other works, however, have illuminated surprising failures even on seemingly simple types of examples (Marvin and Linzen, 2018; McCoy et al., 2019). Such results make it clear that there is still much room for improvement in how neural models perform on syntactic structures that are rare in training corpora. In this work, we investigate whether the linguistic generalization behavior of a given neural architecture is consistent across multiple instances of that architecture. This question is important because, in order to tell which types of architectures generalize best, we need to know whether suc217 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217–227 c Online, November 20,"
2020.blackboxnlp-1.21,P19-1449,0,0.0462555,"2. Out-of-distribution generalization: Generalization to examples drawn from a different distribution than the training set. Standard test sets in natural language processing are generated in the same way as the corresponding training set, therefore testing only in-distribution generalization. Current neural architectures perform very well at this type of generalization. For example, on the natural language understanding tasks included in the GLUE benchmark (Wang et al., 2019), several Transformer-based models (Liu et al., 2019b,a; Raffel et al., 2020) have surpassed the human baselines from Nangia and Bowman (2019). However, this strong performance does not necessarily indicate mastery of language. Because of biases in training distributions, it is often possible for a model to achieve strong in-distribution generalization by using shallow heuristics rather than deeper linguistic knowledge. Therefore, evaluating only on standard test sets cannot reveal whether a model has learned abstract properties of language or if it has only learned shallow heuristics. An alternative evaluation approach addresses this flaw by testing how the model handles particular linguistic phenomena, using datasets designed to b"
2020.blackboxnlp-1.21,D17-1035,0,0.0656371,"Missing"
2020.blackboxnlp-1.21,P19-1452,0,0.0338068,"es within HANS to give a fine-grained investigation of syntactic generalization, while Zhou et al. only report overall accuracy averaged across categories. In addition, we fine-tuned 100 instances of BERT, while Zhou et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works have generally used mo"
2020.blackboxnlp-1.21,D19-1286,0,0.0431499,"u et al. only fine-tuned 10 instances. The larger number of instances allows us to investigate the extent of the variability in more detail. 2.3 Linguistic analysis of BERT Many recent papers have sought a deeper understanding of BERT, whether to assess its encoding of sentence structure (Lin et al., 2019; Hewitt and Manning, 2019; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Tenney et al., 2019b); its representational structure more generally (Abnar et al., 2019); its handling of specific linguistic 218 phenomena such as subject-verb agreement (Goldberg, 2019), negative polarity items (Warstadt et al., 2019), function words (Kim et al., 2019), or a variety of psycholinguistic phenomena (Ettinger, 2020); its internal workings (Coenen et al., 2019; Tenney et al., 2019a; Clark et al., 2019); or its inductive biases (Warstadt and Bowman, 2020). The novel contribution of this work is the focus on variability across a large number of fine-tuning runs; previous works have generally used models without fine-tuning or have used only a small number of fine-tuning runs (usually only one fine-tuning run, or at most ten fine-tuning runs). 3 3.1 Method Task and datasets We used the task of natural language inf"
2020.blackboxnlp-1.21,W18-1004,0,0.0543175,"Missing"
2020.blackboxnlp-1.21,W18-5423,0,0.0293057,"aluating only on standard test sets cannot reveal whether a model has learned abstract properties of language or if it has only learned shallow heuristics. An alternative evaluation approach addresses this flaw by testing how the model handles particular linguistic phenomena, using datasets designed to be impossible to solve using shallow heuristics. In this line of investigation, which tests out-of-distribution generalization, the results are more mixed. Some works have found successful handling of phenomena such as subject-verb agreement (Gulordava et al., 2018) and filler-gap dependencies (Wilcox et al., 2018). Other works, however, have illuminated surprising failures even on seemingly simple types of examples (Marvin and Linzen, 2018; McCoy et al., 2019). Such results make it clear that there is still much room for improvement in how neural models perform on syntactic structures that are rare in training corpora. In this work, we investigate whether the linguistic generalization behavior of a given neural architecture is consistent across multiple instances of that architecture. This question is important because, in order to tell which types of architectures generalize best, we need to know whet"
2020.blackboxnlp-1.21,N18-1101,0,0.338676,"in order to tell which types of architectures generalize best, we need to know whether suc217 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217–227 c Online, November 20, 2020. 2020 Association for Computational Linguistics cesses and failures of generalization should be attributed to aspects of the architecture or to random luck in the choice of the model’s initial weights. We investigate this question using the task of natural language inference (NLI). We fine-tuned 100 instances of BERT (Devlin et al., 2019) on the MNLI dataset (Williams et al., 2018).1 These 100 instances differed only in (i) the initial weights of the classifier trained on top of BERT, and (ii) the order in which training examples were presented. All other aspects of training, including the initial weights of BERT, were held constant. We evaluated these 100 instances on both the in-distribution MNLI development set and the out-of-distribution HANS evaluation set (McCoy et al., 2019), which tests syntactic generalization in NLI models. We found that these 100 instances were remarkably consistent in their in-distribution generalization accuracy, with all accuracies on the"
2020.blackboxnlp-1.21,2020.emnlp-main.659,0,0.0499093,"Missing"
2020.blackboxnlp-1.23,W18-5426,0,0.0135051,"fully chosen to require competence in particular linguistic phenomena (Marvin and Linzen, 2018; Wang et al., 2018; Dasgupta et al., 2019; Poliak et al., 2018; Linzen et al., 2016; McCoy et al., 2019b; Warstadt et al., 2020). This technique can illuminate behavioral shortcomings but says little about how the internal representations are struc239 tured, treating the model as a black box. (2019) and Bowman et al. (2016). In the probing approach, an auxiliary classifier is trained to classify the model’s internal representations based on some linguistically-relevant distinction (Adi et al., 2017; Giulianelli et al., 2018; Conneau et al., 2018; Conneau and Kiela, 2018; Belinkov et al., 2017; Blevins et al., 2018; Peters et al., 2018; Tenney et al., 2019). In contrast with the behavioral approach, the probing approach tests whether some particular information is present in the model’s encodings, but it says little about whether this information is actually used by the model. Indeed, in some cases models fail despite having the necessary information to succeed in their representations, showing that the ability of a classifier to extract that information does not mean that the model is using it (Voita and Titov,"
2020.blackboxnlp-1.23,2020.acl-main.177,0,0.0391477,"Missing"
2020.blackboxnlp-1.23,N19-1419,0,0.15244,": natural language is only partially compositional in nature. For example, if ϕ is the function that assigns meanings to English adjectives, it generally obeys the rule that ϕ(in- + x) = not ϕ(x), (e.g., ϕ(inoffensive) = not ϕ(offensive)), yet there are exceptions: ϕ(inflammable) = ϕ(flammable). On these “partially-compositional” tasks, this strategy of compositional analysis has demonstrated considerable, but limited, generalization capabilities. 2.2 Analysis of NNs Many past works in the rich body of literature about analyzing NNs focus on compositional structure (Hupkes et al., 2020, 2018; Hewitt and Manning, 2019; Li et al., 2019) and systematicity (Lake and Baroni, 2018; Goodwin et al., 2020). Two of the most popular analysis techniques are the behavioral and probing approaches. In the behavioral approach, a model is evaluated on a set of examples carefully chosen to require competence in particular linguistic phenomena (Marvin and Linzen, 2018; Wang et al., 2018; Dasgupta et al., 2019; Poliak et al., 2018; Linzen et al., 2016; McCoy et al., 2019b; Warstadt et al., 2020). This technique can illuminate behavioral shortcomings but says little about how the internal representations are struc239 tured, t"
2020.blackboxnlp-1.23,P82-1020,0,0.769017,"Missing"
2020.blackboxnlp-1.23,2020.acl-main.158,0,0.0203404,"Missing"
2020.blackboxnlp-1.23,P19-1356,0,0.0267667,"ng models in Table 2. Further work has found impressive degrees of syntactic structure in Transformer encodings (Hewitt and Manning, 2019), suggesting that there may well be compositional structure for ROLE to pick up on. The main difficulty in applying ROLE to Transformers—and the reason we did not include Transformers in our study—is that the sentence representation used by a Transformer is typically viewed as a variable-sized collection of vectors, whereas ROLE requires single-vector representations; this discrepancy must be overcome if ROLE is to be applied to Transformers. One past work (Jawahar et al., 2019) has applied ROLE’s precursor (the TPDN of McCoy et al. (2019a)) to Transformer representations by choosing the [CLS] token of BERT (Devlin et al., 2019) as the single-vector sentence encoding to decompose. Jawahar et al. found that these encodings were approximated better by human-specified treeposition roles than by other human-specified candidates (e.g., left-to-right and right-to-left roles). By removing the constraint of requiring humandesigned role schemes, ROLE may be able to discover other role schemes that approximate BERT’s encodings even more closely. 7 Conclusion We have introduced"
2020.blackboxnlp-1.23,P03-1054,0,0.0547953,"Missing"
2020.blackboxnlp-1.23,N19-1002,0,0.0109236,"e last word in a subcommand is informative because that is where a cardinality (i.e., twice or thrice) appears if there is one. Thus, by checking what filler is at the end of a subcommand, the model can determine whether there is a cardinality present and, if so, which one. 243 ROLE itself does not provide an interpretation for the symbolic structure it generates, but we have shown that this structure can be successfully interpreted by humans. By contrast, it is very difficult to interpret the continuous neuron values of RNN representations; even the rare successful cases of doing so, such as Lakretz et al. (2019) and Mu and Andreas (2020), only interpret a few isolated units, while we were able to exhaustively explain the entire symbolic structure discovered by ROLE. 5.3 Precision constituent-surgery on internal representations produces desired outputs The substitution-accuracy results above show that if the entire learned representation is replaced by ROLE’s approximation, the output remains correct. But do the individual word embeddings in this TPR have the appropriate causal consequences when processed by the decoder? To address this causal question (Pearl, 2000), we actively intervene on the const"
2020.blackboxnlp-1.23,D19-1438,0,0.0187887,"partially compositional in nature. For example, if ϕ is the function that assigns meanings to English adjectives, it generally obeys the rule that ϕ(in- + x) = not ϕ(x), (e.g., ϕ(inoffensive) = not ϕ(offensive)), yet there are exceptions: ϕ(inflammable) = ϕ(flammable). On these “partially-compositional” tasks, this strategy of compositional analysis has demonstrated considerable, but limited, generalization capabilities. 2.2 Analysis of NNs Many past works in the rich body of literature about analyzing NNs focus on compositional structure (Hupkes et al., 2020, 2018; Hewitt and Manning, 2019; Li et al., 2019) and systematicity (Lake and Baroni, 2018; Goodwin et al., 2020). Two of the most popular analysis techniques are the behavioral and probing approaches. In the behavioral approach, a model is evaluated on a set of examples carefully chosen to require competence in particular linguistic phenomena (Marvin and Linzen, 2018; Wang et al., 2018; Dasgupta et al., 2019; Poliak et al., 2018; Linzen et al., 2016; McCoy et al., 2019b; Warstadt et al., 2020). This technique can illuminate behavioral shortcomings but says little about how the internal representations are struc239 tured, treating the model"
2020.blackboxnlp-1.23,Q16-1037,1,0.813795,"alization capabilities. 2.2 Analysis of NNs Many past works in the rich body of literature about analyzing NNs focus on compositional structure (Hupkes et al., 2020, 2018; Hewitt and Manning, 2019; Li et al., 2019) and systematicity (Lake and Baroni, 2018; Goodwin et al., 2020). Two of the most popular analysis techniques are the behavioral and probing approaches. In the behavioral approach, a model is evaluated on a set of examples carefully chosen to require competence in particular linguistic phenomena (Marvin and Linzen, 2018; Wang et al., 2018; Dasgupta et al., 2019; Poliak et al., 2018; Linzen et al., 2016; McCoy et al., 2019b; Warstadt et al., 2020). This technique can illuminate behavioral shortcomings but says little about how the internal representations are struc239 tured, treating the model as a black box. (2019) and Bowman et al. (2016). In the probing approach, an auxiliary classifier is trained to classify the model’s internal representations based on some linguistically-relevant distinction (Adi et al., 2017; Giulianelli et al., 2018; Conneau et al., 2018; Conneau and Kiela, 2018; Belinkov et al., 2017; Blevins et al., 2018; Peters et al., 2018; Tenney et al., 2019). In contrast with"
2020.blackboxnlp-1.23,D18-1151,1,0.858598,"his strategy of compositional analysis has demonstrated considerable, but limited, generalization capabilities. 2.2 Analysis of NNs Many past works in the rich body of literature about analyzing NNs focus on compositional structure (Hupkes et al., 2020, 2018; Hewitt and Manning, 2019; Li et al., 2019) and systematicity (Lake and Baroni, 2018; Goodwin et al., 2020). Two of the most popular analysis techniques are the behavioral and probing approaches. In the behavioral approach, a model is evaluated on a set of examples carefully chosen to require competence in particular linguistic phenomena (Marvin and Linzen, 2018; Wang et al., 2018; Dasgupta et al., 2019; Poliak et al., 2018; Linzen et al., 2016; McCoy et al., 2019b; Warstadt et al., 2020). This technique can illuminate behavioral shortcomings but says little about how the internal representations are struc239 tured, treating the model as a black box. (2019) and Bowman et al. (2016). In the probing approach, an auxiliary classifier is trained to classify the model’s internal representations based on some linguistically-relevant distinction (Adi et al., 2017; Giulianelli et al., 2018; Conneau et al., 2018; Conneau and Kiela, 2018; Belinkov et al., 2017"
2020.blackboxnlp-1.23,P19-1334,1,0.40807,"interpretable compositional model (§4) Step 1: Assign structural roles to words using a learned role assigner. Step 2: Combine word and role vectors using a closed-form equation with learned parameters. Df: filler embedding dictionary Introduction Key Traditional models of cognition, and language in particular, have relied heavily on symbol structures and symbol manipulation. However, in the current era, deep learning research has shown that Neural Networks (NNs) can display remarkable degrees of generalization on tasks traditionally viewed as depending on symbolic structure (Wu et al., 2016; McCoy et al., 2019a), albeit with some important limits to their generalization (Lake and Baroni, 2018). Given that standard NNs have no obvious mechanisms for representing symbolic structures, parsing inputs into such structures, nor applying compositional symbol-manipulating rules to them, this success raises the question that we address in this paper: How do NNs achieve such strong performance on compositional tasks? Could it be that NNs do learn symbolic representations—covertly embedded as vectors in their state spaces? McCoy et al. (2019a) showed that when trained on highly compositional tasks, Dr: role e"
2020.blackboxnlp-1.23,N13-1090,0,0.0580216,"act the vector embedding of the opposite constituent, add the embedding of the around constituent, and see whether this causes the output to change from the correct output for jump opposite left (TL TL JUMP) to the correct output for jump around left (TL JUMP TL JUMP TL JUMP TL JUMP). The roles in these constituents are determined by the algorithm of Appendix A.5. If changing a word leads other roles in the sequence to change (according to the algorithm), we update the encoding with those new roles as well. Such surgery can be viewed as a more general extension of the analogy approach used by Mikolov et al. (2013) for analysis of word embeddings. An example of applying a sequence of five such constituent surgeries to a sequence is shown in Figure 5 (left). Even long sequences of such replacements produce the expected change in the decoder’s output with high accuracy (Figure 5, 2 We extract syntactic categories from the SCAN grammar (Lake and Baroni, 2018, Supplementary Fig. 6) by saying that two words belong to the same category if every occurrence of one could be grammatically replaced by the other. We do not replace occurrences of and and after since the presence of either of these words causes subst"
2020.emnlp-main.731,W08-2222,0,0.154088,"Missing"
2020.emnlp-main.731,W15-4002,0,0.0369287,"oth Transformers and LSTMs performed poorly on the generalization set, with high variability across runs, while their performance on the in-domain test set was consistently near-perfect. Furthermore, the models found structural generalization much more challenging compared to lexical generalization. Our results suggest that achieving high generalization accuracy on COGS is beyond the capacity of models that we tested, and COGS can therefore motivate the development of new computational models. What architecture would be needed to solve COGS? For structural generalization cases, the results of Bowman et al. (2015); Evans et al. (2018) and McCoy et al. (2019) suggest that treestructured models may provide a better inductive bias. In particular, Bowman et al. (2015) showed that tree-structured neural networks generalized to longer sequences. For lexical generalization cases, the RNN-based model from Gordon et al. (2020) that implements permutation equivariance may help, considering that it was able to solve all primitive generalizations in SCAN. Acknowledgments We thank Sadhwi Srinivas and Kyle Rawlins for discussions about the logical form. We also thank Paul 9095 Smolensky, Benjamin Van Durme, and memb"
2020.emnlp-main.731,D19-1438,0,0.167626,"isplayed the degree of systematicity that humans do. Recently Lake and Baroni (2018) revisited this question using contemporary neural architectures—sequence-to-sequence models with LSTM and GRU units—and came to the same conclusion as Hadley. Lake and Baroni based their study on the SCAN task, a novel task in which word sequences in a synthetic language need to be mapped to navigation command sequences (e.g., jump twice → JUMP JUMP). Crucially, their training/evaluation split required compositional generalization. A number of models have been developed that have improved performance on SCAN (Li et al., 2019; Gordon et al., 2020). However, since the semantic representation used by SCAN only covers a small subset of English grammar, SCAN does not enable testing various systematic linguistic abstractions that humans are known to make (e.g., verb argument structure alternation). Thus, it is unclear whether progress on SCAN would generalize to natural language. To bring the evaluation of compositional generalization a step closer to natural language, COGS includes a wide range of syntactic constructions, and uses semantic representations based on lambda calculus, inspired by the formalisms employed i"
2020.emnlp-main.731,D15-1166,0,0.0129727,"Missing"
2020.emnlp-main.731,D18-1151,1,0.92036,"bring the evaluation of compositional generalization a step closer to natural language, COGS includes a wide range of syntactic constructions, and uses semantic representations based on lambda calculus, inspired by the formalisms employed in formal semantics (Parsons, 1990) and semantic parsing (Palmer et al., 2005; Reddy et al., 2017). Following Dong and Lapata (2016) and Daza and Frank (2018), we cast semantic parsing as a sequence-to-sequence problem. 3 Overview of COGS In a semantic parsing task such as COGS, the goal is to map a sentence to a logical form. Following recent works such as Marvin and Linzen (2018) and Keysers et al. (2020), we generate the dataset using a rule-based approach; this allows us to maintain full control over the distribution of inputs that the learners are exposed to, and to ensure coverage of rare constructions that are not guaranteed to appear in natural corpora. COGS is not inherently grounded but could potentially be linked to a knowledge base or a visual world. The COGS dataset1 is split into a training set and a generalization set. The training set includes systematic gaps that, in the generalization set, must be filled via compositional generalization. Success on the"
2020.emnlp-main.731,D17-1009,0,0.0668725,"Missing"
2020.emnlp-main.731,D19-1592,1,0.881882,"Missing"
2020.scil-1.6,N18-1108,1,0.938101,"erform applied natural language tasks with high accuracy – they can provide a rich source of information about the mechanisms underlying hierarchical structure rule learning. A number of questions need to be asked. How much grammar can language models learn just from a corpus? What are the limitations on the generalizations they can make about hierarchical structures? Recently, several studies have addressed these questions by testing RNNs’ performance on structure-sensitive grammatical tasks. The results of these studies showed that RNNs can learn subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018), fillergap dependencies (Wilcox et al., 2018), hierarchical rules of question formation (McCoy et al., 2018), and the contexts that license negative polarity items (Jumelet and Hupkes, 2018). In this paper, we contribute to this line of research by extending it to issues in Russian syntax. What makes Russian compelling is that it has rich morphology, which allows us to expand the range of tasks that have been used in previous work to explore RNN learning of structural dependencies. In particular, Russian has casemarking alternations involving the genitive case: along w"
2020.scil-1.6,N01-1021,0,0.0410711,"va et al. (2018). The model was trained on a 90-million-word corpus extracted from the Russian Wikipedia and had two layers of 650 hidden LSTM units. Additionally, we trained a 3-gram model on the same corpus to provide a baseline for our experiment. The 3-gram model which backs off to smaller n-grams using linear interpolation. Following previous work (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018), we assessed the model’s performance by examining the probabilities it assigned to grammatical sentences from our dataset, compared to ungrammatical ones. We used surprisal (Hale, 2001): surprisal(wi ) = log P(wi |w1 . . . wi 1) The higher the surprisal, the more unexpected a word is under the model’s probability distribution. Since the sentences in (3) and (4) are minimally different from each other (the only difference being that the verb in (3) is negated), we can directly compare the surprisal the model assigned to the genitive-marked objects in these sentences. Assuming the probability distribution defined by the model reflects the grammar of the genitive of negation construction, we expected that the genitive-marked object would be assigned higher surprisal in (4), whe"
2020.scil-1.6,W18-5424,0,0.0164072,"d. How much grammar can language models learn just from a corpus? What are the limitations on the generalizations they can make about hierarchical structures? Recently, several studies have addressed these questions by testing RNNs’ performance on structure-sensitive grammatical tasks. The results of these studies showed that RNNs can learn subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018), fillergap dependencies (Wilcox et al., 2018), hierarchical rules of question formation (McCoy et al., 2018), and the contexts that license negative polarity items (Jumelet and Hupkes, 2018). In this paper, we contribute to this line of research by extending it to issues in Russian syntax. What makes Russian compelling is that it has rich morphology, which allows us to expand the range of tasks that have been used in previous work to explore RNN learning of structural dependencies. In particular, Russian has casemarking alternations involving the genitive case: along with the accusative case (which is typical cross-linguistically), the genitive can mark direct objects of transitive verbs. However, it is only licensed under negation, and is optional – the accusative case can be us"
2020.scil-1.6,Q16-1023,0,0.0157177,"iness. 1 Introduction Statistical language models are probability distributions over sequences of words, which they learn from large corpora during training. For any given context, these models assign a probability to all of its possible continuations: for a example, given the context “he was eating soup with a. . . ”, language models can predict that the word “spoon” is much more likely to occur next than “shoe”. A class of language models – Recurrent Neural Network (RNN) models – have been particularly successful on various applied language tasks (Mikolov et al., 2010; Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Bahdanau et al., 2014). But what kind of linguistic knowledge do these models capture? Arguably, human language knowledge is comprised of more than word co-occurrence statistics – it encompasses abstract rules and generalizations that concern hierarchical structure. According to the argument from the poverty of the stimulus (Chomsky, 1980), the kind of structural knowledge that underlies huTal Linzen Department of Cognitive Science Johns Hopkins University tal.linzen@jhu.edu man linguistic performance is impossible to derive purely from the input language learners receive, since many structu"
2020.scil-1.6,Q16-1037,1,0.932612,"s innate bias – but perform applied natural language tasks with high accuracy – they can provide a rich source of information about the mechanisms underlying hierarchical structure rule learning. A number of questions need to be asked. How much grammar can language models learn just from a corpus? What are the limitations on the generalizations they can make about hierarchical structures? Recently, several studies have addressed these questions by testing RNNs’ performance on structure-sensitive grammatical tasks. The results of these studies showed that RNNs can learn subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018), fillergap dependencies (Wilcox et al., 2018), hierarchical rules of question formation (McCoy et al., 2018), and the contexts that license negative polarity items (Jumelet and Hupkes, 2018). In this paper, we contribute to this line of research by extending it to issues in Russian syntax. What makes Russian compelling is that it has rich morphology, which allows us to expand the range of tasks that have been used in previous work to explore RNN learning of structural dependencies. In particular, Russian has casemarking alternations involving th"
2020.scil-1.6,D18-1151,1,0.848266,"where it is optional. 4 Methodology To explore whether RNN language models can capture the constraints on genitive-marked direct objects, we studied the performance of the model presented in Gulordava et al. (2018). The model was trained on a 90-million-word corpus extracted from the Russian Wikipedia and had two layers of 650 hidden LSTM units. Additionally, we trained a 3-gram model on the same corpus to provide a baseline for our experiment. The 3-gram model which backs off to smaller n-grams using linear interpolation. Following previous work (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018), we assessed the model’s performance by examining the probabilities it assigned to grammatical sentences from our dataset, compared to ungrammatical ones. We used surprisal (Hale, 2001): surprisal(wi ) = log P(wi |w1 . . . wi 1) The higher the surprisal, the more unexpected a word is under the model’s probability distribution. Since the sentences in (3) and (4) are minimally different from each other (the only difference being that the verb in (3) is negated), we can directly compare the surprisal the model assigned to the genitive-marked objects in these sentences. Assuming the probability d"
2020.scil-1.6,W18-5412,0,0.0130928,"anguage tasks with high accuracy – they can provide a rich source of information about the mechanisms underlying hierarchical structure rule learning. A number of questions need to be asked. How much grammar can language models learn just from a corpus? What are the limitations on the generalizations they can make about hierarchical structures? Recently, several studies have addressed these questions by testing RNNs’ performance on structure-sensitive grammatical tasks. The results of these studies showed that RNNs can learn subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018), fillergap dependencies (Wilcox et al., 2018), hierarchical rules of question formation (McCoy et al., 2018), and the contexts that license negative polarity items (Jumelet and Hupkes, 2018). In this paper, we contribute to this line of research by extending it to issues in Russian syntax. What makes Russian compelling is that it has rich morphology, which allows us to expand the range of tasks that have been used in previous work to explore RNN learning of structural dependencies. In particular, Russian has casemarking alternations involving the genitive case: along with the accusative case"
2020.scil-1.6,W18-5423,0,0.015892,"ide a rich source of information about the mechanisms underlying hierarchical structure rule learning. A number of questions need to be asked. How much grammar can language models learn just from a corpus? What are the limitations on the generalizations they can make about hierarchical structures? Recently, several studies have addressed these questions by testing RNNs’ performance on structure-sensitive grammatical tasks. The results of these studies showed that RNNs can learn subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018), fillergap dependencies (Wilcox et al., 2018), hierarchical rules of question formation (McCoy et al., 2018), and the contexts that license negative polarity items (Jumelet and Hupkes, 2018). In this paper, we contribute to this line of research by extending it to issues in Russian syntax. What makes Russian compelling is that it has rich morphology, which allows us to expand the range of tasks that have been used in previous work to explore RNN learning of structural dependencies. In particular, Russian has casemarking alternations involving the genitive case: along with the accusative case (which is typical cross-linguistically), the g"
2020.tacl-1.9,D14-1179,0,0.0737887,"Missing"
2020.tacl-1.9,P82-1020,0,0.786471,"Missing"
2020.tacl-1.9,Q16-1037,1,0.903779,"Missing"
2020.tacl-1.9,D15-1166,0,0.0377524,"Missing"
2020.tacl-1.9,N19-1356,1,0.851863,"Missing"
2020.tacl-1.9,P19-1439,1,0.891674,"Missing"
2020.tacl-1.9,P18-2117,0,\N,Missing
2021.acl-long.144,2020.acl-main.493,0,0.0145923,"el, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal evidence (Belinkov and Glass, 2019). See Belinkov (2021) for a review of the shortcomings of pro"
2021.acl-long.144,P19-1285,0,0.0713695,"Missing"
2021.acl-long.144,N19-1112,1,0.815376,"inflection given the same context. This approach investigates the output behavior of the model, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal ev"
2021.acl-long.144,N19-1002,0,0.252743,"example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not well understood. Recent work has performed causal analyses of syntactic agreement units in LSTM (Hochreiter and Schmidhuber, 1997)-based LMs (Lakretz et al., 2019; Lu et al., 2020) or causal analyses of LSTM hidden representations’ impact on syntactic agreement (Giulianelli et al., 2018), but the agreement mechanisms of Transformer-based LMs have not been as extensively investigated. Transformerbased LMs’ syntactic generalization abilities are superior to those of LSTMs (Hu et al., 2020), which makes Transformer-based models enticing candidates for further analysis. We apply the behavioral-structural method of causal mediation analysis (Pearl, 2001) to investigate syntactic agreement in Transformers, following the approach used by Vig et al. (2020a) fo"
2021.acl-long.144,D18-1151,1,0.857953,"d that language models rely on similar sets of neurons when given sentences with similar syntactic structure. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Targeted syntactic evaluations have shown that neural language models (LMs) are able to predict the correct token from a set of grammatically minimally different continuations with high accuracy, even in difficult contexts (Linzen et al., 2016; Gulordava et al., 2018), for constructions such as subject-verb agreement (van Schijndel et al., 2019), filler-gap dependencies (Wilcox et al., 2018), and reflexive anaphora (Marvin and Linzen, 2018). As an illustration of the targeted syntactic evaluation paradigm, consider the following example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not well understood. Recent work has performed causal an"
2021.acl-long.144,2020.acl-main.490,1,0.768172,"well with human intuitions of syntactic similarity between structures. 2 2.1 Related Work Targeted Syntactic Evaluation Many recent studies have treated neural LMs and contextualized word prediction models—primarily LSTM LMs (Sundermeyer et al., 2012), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019)—as psycholinguistic subjects to be studied behaviorally (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Some have studied whether models prefer grammatical completions in subjectverb agreement contexts (Marvin and Linzen, 2018; van Schijndel et al., 2019; Goldberg, 2019; Mueller et al., 2020; Lakretz et al., 2021; Futrell et al., 2019), as well as in filler-gap dependencies (Wilcox et al., 2018, 2019). These are based on the approach of Linzen et al. (2016), where a model’s ability to syntactically generalize is measured by its ability to choose the correct inflection in difficult structural contexts instantiated by tokens that the model has not seen together during training. In other words, this approach tests whether the model assigns the correct inflection a higher probability than an incorrect inflection given the same context. This approach investigates the output behavior o"
2021.acl-long.144,2021.naacl-main.290,0,0.031086,"sal mediation analysis to discover and interpret the mechanisms behind syntactic agreement in pre-trained neural language models. Our results reveal the location and importance of various neurons within various models, and provide insights into the inner workings of these LMs. For future work, we suggest intervening on groups of neurons and attention heads to see how these components work together, and extending the analysis to phenomena such as filler-gap dependencies and negative polarity items. Further work should also explore the impact of specific verbs on syntactic agreement mechanisms (Newman et al., 2021). Lastly, we suggest examining examples where the model makes incorrect predictions to determine how models misuse the mechanisms from Section 6.1. Acknowledgements Y.B. was supported in part by the ISRAEL SCIENCE FOUNDATION (grant no. 448/20) and by an Azrieli Foundation Early Career Faculty Fellowship. A.M. was supported by a National Science Foundation Graduate Research Fellowship (grant no. 1746891). 1836 Impact Statement chine Translation and Speech Recognition. Ph.D. thesis, Massachusetts Institute of Technology. In this paper, we apply causal mediation analysis in order to study the sub"
2021.acl-long.144,D19-1592,1,0.931743,"ment in Transformers, following the approach used by Vig et al. (2020a) for interpreting gender bias in pre-trained English LMs. This method allows us to implicate specific model components in the observed behavior of a model. If we view a neural LM as a causal graph proceeding from inputs to outputs, we can view each model component (e.g., a neuron) as a mediator. We measure the contribution of a mediator to the observed output behavior by performing controlled interventions on input sentences and observing how they change the probabilities of continuation pairs. We focus primarily on GPT-2 (Radford et al., 2019), although we also analyze TransformerXL (Dai et al., 1828 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1828–1843 August 1–6, 2021. ©2021 Association for Computational Linguistics 2019) and XLNet (Yang et al., 2019). We find that both GPT-2 and Transformer-XL use two distinct mechanisms to accomplish subjectverb agreement, one of which is active only when the subject and verb are adjacent. Conversely, XLNet uses one unified mechanism across syntactic structures. Even tho"
2021.acl-long.144,2021.emnlp-main.230,0,0.0821816,"Missing"
2021.acl-long.144,P19-1452,0,0.0617523,"the same context. This approach investigates the output behavior of the model, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal evidence (Belinkov and"
2021.acl-long.144,W18-5423,0,0.0872603,"ructure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Targeted syntactic evaluations have shown that neural language models (LMs) are able to predict the correct token from a set of grammatically minimally different continuations with high accuracy, even in difficult contexts (Linzen et al., 2016; Gulordava et al., 2018), for constructions such as subject-verb agreement (van Schijndel et al., 2019), filler-gap dependencies (Wilcox et al., 2018), and reflexive anaphora (Marvin and Linzen, 2018). As an illustration of the targeted syntactic evaluation paradigm, consider the following example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not we"
2021.acl-long.144,N19-1334,0,0.039094,"Missing"
2021.blackboxnlp-1.4,W18-5426,0,0.0281734,"Fodor, 1978), or are all parses tracked in parallel, weighted by probability (Hale, 2001)? After disambiguation, how is the analysis revised (Grodner et al., 2003), and do traces of initial misinterpretations linger (Christianson et al., 2001)? We consider analogous questions, focusing on LMs instead of humans. Several studies have examined the syntactic abilities of LMs, through targeted evaluations on specific syntactic phenomena (e.g., Linzen et al. 2016; Wilcox et al. 2018), or by analysing the degree to which syntactic information can be decoded from their internal representations (e.g., Giulianelli et al. 2018; Hewitt and Manning 2019). These studies show that LMs track syntax to a large extent, even when not explicitly trained to do so. Some previous studies have investigated the behavior of LMs on temporary ambiguities focusing on the garden-path effect (Bever, 1970), where a high cognitive cost at disambiguation is taken to signal a preference for the alternative analysis. On the one hand, LMs’ next-word probabilities can be used to model these effects (Van Schijndel and Linzen, 2018). On the other, one can test whether Temporary Ambiguities This section describes the types of temporary ambiguit"
2021.blackboxnlp-1.4,P81-1022,0,0.47195,"Missing"
2021.blackboxnlp-1.4,D18-1277,0,0.0179568,"(in contrast to results by Linzen et al. (2016) and subsequent research on subject-verb agreement), or in generally overriding a default preference for the incorrect analysis. Using generation proved to be an informative tool to inquire a LM’s uncertainty over an unfolding sentence, and could be used also to inquire more types of ambiguities (e.g., semantic). Yet, there are some challenges to our proposed methodology. First, relying on an automatic classification of sentences can introduce noise: ambiguities can be difficult for NLP systems even when explicitly trained to analyse expressions (Elkahky et al., 2018). Second, we could not automatically detect ungrammatical completions or with an unclear analysis (the parser always returns an output), whereas it may be useful to be identify these cases. While these issues did not prevent us from inferring the main trends in the LMs behavior, all confirmed by qualitative inspections of the data, we look forward to future work that will attempt to overcome the aforementioned limitations. Discussion and Conclusions In this work, we have probed the syntactic uncertainty of LMs by generating sentence completions from the LMs. Our results contribute to research"
2021.blackboxnlp-1.4,N18-1202,0,0.0238813,"to deal with temporary ambiguities, as they process text incrementally. In this work, we probe the degree of syntactic uncertainty that LMs maintain when processing temporary ambiguities, using generation (sampling) from those models as an analysis tool. LMs are trained to output contextual probabilities of words in a next-word prediction task. In spite of this generic objective, these models have been shown to track syntactic information to a remarkable extent (Linzen and Baroni, 2021) and build context-sensitive internal representations, potentially resolving ambiguities in the input (e.g., Peters et al. 2018). The behavior of LMs on temporary syntactic ambiguities was previously investigated through the lens of word surprisal (e.g., Futrell et al. 2018), providing evidence for incremental syntactic processing in LMs. At the same time, the extent that a LM expects each interpretation of an ambiguous input, and therefore its degree of syntactic uncertainty, has not been quantified. In this paper, we generate text from a LM, as a window into the LM’s processing of an unfolding sentence. As (1) shows, the completion of a temporarily ambiguous fragment clarifies the intended interpretation. We can use"
2021.blackboxnlp-1.4,W18-2501,0,0.0197355,"Missing"
2021.conll-1.15,N19-1002,0,0.0282888,"ion in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to our work is Elazar et al. (2021), where the authors applied INLP to “erase” certain distinctions from the representation, and then measured the effect of the intervention on language modeling. We extend INLP to generate flexible counterfactual representations (§3) and use these to instantiate hypotheses about the linguistic factors"
2021.conll-1.15,2020.acl-main.647,1,0.805284,"(4) My cousin that likes the books was interesting. (Subject RC) These differences do not affect the strategy that a system that follows the grammar of English should use to determine the number of the verb: regardless of the internal structure of the RC, a verb outside the RC should agree with the subject of the main clause, whereas a verb inside the RC should agree with the subject of the RC. Thus, a model that does not properly identify the boundaries of the RC will often predict a singular verb where a plural one is required, or vice versa. 2.2 Iterative Null Space Projection (INLP) INLP (Ravfogel et al., 2020) is a method for selectively identifying and removing user-defined concept features from a contextual representation. Let T be a set of words-in-context, and let H be the set d of representations of T , such that h~t ∈ R is the contextualized representation of the word t ∈ T . Let F be a linguistic feature that we hypothesize is encoded in H. Given H and the values ft of the feature F for each word, INLP returns a set of m linear classifiers, each of which predicts F with above-chance accuracy. Each of these classifiers is a vector in Rd , and corresponds to a direction in the representation s"
2021.conll-1.15,2021.eacl-main.295,0,0.0428525,"sis Behavioral tests of neural models, such as the ability of the model to master agreement prediction (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019), have exposed both impressive capabilities and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement pr"
2021.conll-1.15,2020.findings-emnlp.280,0,0.0218661,"ol for testing hypotheses about the function of the linguistic information encoded in the internal representations of neural LMs. Counterfactuals The relation between counterfactual reasoning and causality is extensively discussed in social science and philosophy literature (Woodward, 2005; Miller, 2018, 2019). Attempts have been made to generate counterfactual examples (Maudslay et al., 2019; Zmigrod et al., 2019; Ross et al., 2020; Kaushik et al., 2020; Hvilshøj et al., 2021) and recently to derive counterfactual representations (Feder et al., 2020; Elazar et al., 2021; Jacovi et al., 2021; Shin et al., 2020; Tucker et al., 2021). Contrary to our approach, previ- Acknowledgements ous attempts to generate counterfactual representations were either limited to amnesic operations This work was supported by United States–Israel (i.e., focused on the removal of information and Binational Science Foundation award 2018284, not on modifying the encoded information) or used and has received funding from the European Regradient-based interventions, which are expressive search Council (ERC) under the European Union’s and powerful, but less controllable. Our linear ap- Horizon 2020 research and innovation pro"
2021.conll-1.15,2021.naacl-main.8,0,0.0395278,"ties and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to"
2021.conll-1.15,2020.acl-main.490,1,0.892596,"nformation during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category. 1 (1) The skater the officers love [MASK] happy. Introduction The success of neural language models, both in NLP tasks and as cognitive models, has fueled targeted evaluation of these models’ word prediction accuracy on a range of syntactically complex constructions (Linzen et al., 2016; Gauthier et al., 2020; Warstadt et al., 2020; Mueller et al., 2020; Marvin and Linzen, 2018). What are the internal representations that support such sophisticated syntactic behavior? In this paper, we tackle this question using an intervention-based approach (Woodward, 2005). Our method, AlterRep, is designed to study whether a model uses a particular linguistic feature in a manner which is consistent with the grammar of the language. The method involves two steps: ∗ Equal contribution. To investigate whether a neural model uses RC boundary representations as predicted by the grammar of English, we use AlterRep to generate two counterfactual representations"
2021.conll-1.15,2020.findings-emnlp.125,0,0.0252739,"bing and Causal Analysis Behavioral tests of neural models, such as the ability of the model to master agreement prediction (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019), have exposed both impressive capabilities and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that ca"
2021.conll-1.15,K19-1007,1,0.854173,"Missing"
2021.conll-1.15,2021.naacl-main.323,0,0.0141104,"ese paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to our work is Elazar et"
2021.conll-1.15,2021.findings-acl.76,0,0.0895582,"Missing"
2021.conll-1.15,2020.tacl-1.25,0,0.0408256,"iants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category. 1 (1) The skater the officers love [MASK] happy. Introduction The success of neural language models, both in NLP tasks and as cognitive models, has fueled targeted evaluation of these models’ word prediction accuracy on a range of syntactically complex constructions (Linzen et al., 2016; Gauthier et al., 2020; Warstadt et al., 2020; Mueller et al., 2020; Marvin and Linzen, 2018). What are the internal representations that support such sophisticated syntactic behavior? In this paper, we tackle this question using an intervention-based approach (Woodward, 2005). Our method, AlterRep, is designed to study whether a model uses a particular linguistic feature in a manner which is consistent with the grammar of the language. The method involves two steps: ∗ Equal contribution. To investigate whether a neural model uses RC boundary representations as predicted by the grammar of English, we use AlterRep to generate two counterf"
2021.conll-1.15,P19-1161,0,0.0209413,"about RC boundaries that is encoded in its word representations when inflecting the number of masked verb in a manner consistent with the grammar of English. We conclude that AlterRep is an effective tool for testing hypotheses about the function of the linguistic information encoded in the internal representations of neural LMs. Counterfactuals The relation between counterfactual reasoning and causality is extensively discussed in social science and philosophy literature (Woodward, 2005; Miller, 2018, 2019). Attempts have been made to generate counterfactual examples (Maudslay et al., 2019; Zmigrod et al., 2019; Ross et al., 2020; Kaushik et al., 2020; Hvilshøj et al., 2021) and recently to derive counterfactual representations (Feder et al., 2020; Elazar et al., 2021; Jacovi et al., 2021; Shin et al., 2020; Tucker et al., 2021). Contrary to our approach, previ- Acknowledgements ous attempts to generate counterfactual representations were either limited to amnesic operations This work was supported by United States–Israel (i.e., focused on the removal of information and Binational Science Foundation award 2018284, not on modifying the encoded information) or used and has received funding from the Eu"
2021.emnlp-main.72,2021.scil-1.3,0,0.054954,"Missing"
2021.emnlp-main.72,2020.conll-1.17,0,0.0984211,"Missing"
2021.emnlp-main.72,D18-1151,1,0.771295,"Missing"
2021.emnlp-main.72,N19-1423,0,0.038232,"1988). Most speakers of English will agree, for combinations that never occurred in the training example, that if “gorp” is a singular noun, then, set is consistent with a model that learns abstract regardless of the meaning of “gorp”, the utter- representations of lexical items and patterns, i.e., ance “the gorp adds nothing” is grammatical, but abstract features and rules. Second, BERT’s perfor“the gorp add nothing” is not. mance is influenced by absolute frequency effects, The success of contemporary neural language but probing classifiers show that this influence can models such as BERT (Devlin et al., 2019) on lan- be explained by the model’s inability to learn the guage understanding tasks, as well as in more tar- features of a verb form (singular vs. plural) for ingeted linguistic evaluations (Marvin and Linzen, frequent lexical items, rather than a failure to apply ∗ Work done while visiting Google. the rule when the verb form has been classified. Fi932 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 932–948 c November 7–11, 2021. 2021 Association for Computational Linguistics nally, although BERT generally applies rules with high accuracy, it fai"
2021.emnlp-main.72,W16-2524,0,0.0287378,"ural, we use the following procedure. We randomly split our 60 VOI into two groups of 30 verbs each, which we denote as S and P. In each experiment, we set the frequency of the singular verbs in S to Nvary , while holding the frequency of the plural forms of the verbs in S constant at Nconstant . 7.2 Predicting Agreement Feature Likewise, we set the frequency of the plural verbs To test whether the above predicted pattern holds, in P to Nvary , and hold the frequency of the singular we use two probing classifiers (see Veldhoen et al., form of these verbs constant at Nconstant . We run ex2016; Ettinger et al., 2016, on probing) which we periments with Nvary = {1, 10, 30, 100, 300, 1,000, describe below. 3,000, 10,000}, and do this twice for Nconstant set Subject agreement feature probe. Our first to 100 and 1,000. As our evaluation stimuli are 0 balanced such that both v and v occur as the target probe evaluates whether, given a sentence with the in every template for every VOI, we are able to ana- verb masked, the embedding at the masked posilyze the effect of the v:v 0 frequency ratio—holding tion contains information as to whether a singular 0 the absolute frequency of v fixed—for v:v ranging or plur"
2021.emnlp-main.72,2020.acl-main.177,0,0.0611679,"Missing"
2021.emnlp-main.72,N18-1108,1,0.817912,"ic needs providers ... review/reviews a damaging effect. Table 1: Examples of natural and nonce stimuli. Target verbs and their subjects are bolded. The model takes as input the sentence with the verb masked, and is evaluated based on which verb inflection it scores more highly. achieve performances comparable to the public BERT-Base release on GLUE (Wang et al., 2018) (see Appendix A). 4.2 Evaluation Stimuli We evaluate the model’s SVA ability on two classes of stimuli: (1) natural sentences, which are generally both syntactically and semantically coherent, and (2) nonce sentences (following Gulordava et al. 2018) which are grammatically valid but not necessarily semantically coherent (“colorless green ideas sleep furiously”, Chomsky, 1956). Examples of each are shown in Table 1. Evaluating on natural sentences provides a measurement of how well the model can be expected to perform in realistic settings, but these sentences are not ideal for a targeted SVA evaluation since they often contain additional cues relevant to verb inflection, such as other plural verbs or plural determiners, as in “two [SUBJECT] and their dogs [VERB],” making it difficult to discern whether a model has chosen a particular ver"
2021.scil-1.12,C18-1250,0,0.060646,"Missing"
2021.scil-1.12,D14-1179,0,0.0198388,"Missing"
2021.scil-1.12,2020.tacl-1.9,1,0.937027,"cases that models assigned higher probability to the hierarchicallygenerated, correctly fronted questions. Other work followed more closely the approach taken by Crain and Nakayama and investigated models trained to perform the transformation by producing an output sequence. Most relevant to this work, Frank and Mathis (2007) trained a neural network language model to output a question after observing a declarative sentence, often producing the correct auxiliary but failing to generalize on held-out cases. This paradigm has recently been extended to contemporary sequence-to-sequence networks (McCoy et al., 2020a), the architecture we use in this paper, with similar results: their models only generalized hierarchically when they were architecturally constrained to base its output on hierarchical syntactic structure. Previous computational work has focused on inductive bias arising from the learning architecture, which would correspond to the innate biases often considered in linguistics. In this work, by contrast, we assess whether a structure-sensitivity bias for a particular transformation can arise from learning the target transformation jointly with other syntactic transformations. We hypothesize"
2021.scil-1.12,P15-1166,0,0.0330999,"which the car should be streered, the learner might be taught a series of side tasks, such as predicting the location of the center of the road, the intensity of the road surface, and so on. Caruana shows that when tested for generalization on unseen roadways, the multitask model outperforms the single-task model on the same quantity of within-task data. Multitask learning has been shown to be effective in natural language tasks. For instance, the various similarities underlying all human languages have made multitask learning with multiple languages a useful approach to machine translation. Dong et al. (2015) designed encoderdecoder networks that learn to decode into many languages in parallel, where the shared encoder learns a more useful syntactic representation for translating into the target language, whereas Firat et al. (2016) used separate encoders and decoders for each language but shared the attention mechanism across languages. Johnson et al. (2017) performed multilingual translation using a single encoder and decoder by appending an artificial token in the input which indicates the language to be translated into. In this setting, they were able to perform zero-shot translation, i.e., tr"
2021.scil-1.12,N16-1101,0,0.0157905,"generalization on unseen roadways, the multitask model outperforms the single-task model on the same quantity of within-task data. Multitask learning has been shown to be effective in natural language tasks. For instance, the various similarities underlying all human languages have made multitask learning with multiple languages a useful approach to machine translation. Dong et al. (2015) designed encoderdecoder networks that learn to decode into many languages in parallel, where the shared encoder learns a more useful syntactic representation for translating into the target language, whereas Firat et al. (2016) used separate encoders and decoders for each language but shared the attention mechanism across languages. Johnson et al. (2017) performed multilingual translation using a single encoder and decoder by appending an artificial token in the input which indicates the language to be translated into. In this setting, they were able to perform zero-shot translation, i.e., translating between language pairs for which the model never received parallel data; instead, it relied on a shared representation learned via multitask learning. The present work adopts the architecture used by Johnson et al. (20"
2021.scil-1.12,Q17-1024,0,0.0509414,"Missing"
D15-1134,N04-1039,0,0.0811902,"of σ (we set µ = 0 in all simulations). For σ = 0.05, the model showed little learning after a single exposure set. When σ was set to higher values, MaxEnt rapidly preferred attested to unattested items, failing to reproduce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF - UNATT items diminished after multiple exposure sets (in particular for σ = 0.5). A straightforward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regularization method that encouraged sparsity (Goodman, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman"
D15-1134,N15-1034,0,0.0175474,"= 0 in all simulations). For σ = 0.05, the model showed little learning after a single exposure set. When σ was set to higher values, MaxEnt rapidly preferred attested to unattested items, failing to reproduce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF - UNATT items diminished after multiple exposure sets (in particular for σ = 0.5). A straightforward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regularization method that encouraged sparsity (Goodman, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman et al., 2008). In that"
D18-1151,J99-2004,0,0.144775,"with a constructed dataset, which allows us to examine a much larger range of specific grammatical phenomena than has been possible before. We use templates to automatically create our test sentences, making it possible to generate a large test set while maintaining experimental control over our materials as well as a balanced number of examples of each phenomenon. We test three LMs on the data set we develop: an n-gram baseline, an RNN LM trained on an unannotated corpus, and an RNN LM trained on a multitask objective: language modeling and Combinatory Categorial Grammar (CCG) supertagging (Bangalore and Joshi, 1999). We also conduct a human experiment using the same materials. The n-gram baseline largely performed at chance, suggesting that good performance on the task requires syntactic representations. The RNN LMs performed well on simple cases, but struggled on more complex ones. Multi-task training with a supervised syntactic objective improved the performance of the RNN, but it was still much weaker than humans. This suggests that our data set is challenging, especially when explicit syntactic supervision is not available, and can therefore motivate richer language modeling architectures. 2 Overview"
D18-1151,D11-1037,0,0.209262,"should be higher than the probability of the ungrammatical one. For example, the following minimal pair illustrates the fact that thirdIt would not be possible to compare these two sentences using the prediction task. In the current paper, we use the more general setting and compare the probability of the two complete sentences. 2.2 Data set construction Previous work has used syntactically complex sentences identified from a parsed corpus. This approach has several limitations. If the corpus is automatically parsed, the risk of a parse error increases with the complexity of the construction (Bender et al., 2011). If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of examples of syntactically challenging cases. Moreover, using naturally occurring sentences can introduce 1 Nor is it possible to have a threshold  such that all grammatical sentences have probability higher than  and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017). 2 In practice, the conditions that govern the distribution of NPIs are much more complicated,"
D18-1151,N16-1024,0,0.0374839,"we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”). Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012). While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model. This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017). Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models. 4 Data set composition This section describes all of the types of sentence pairs included in our data set, which include examples of subject-verb agreement (Sections 4.1 and 4.2), reflexive anaphoras (Section 4.3) and negative polarity items (Section 4.4). 1194 4.1 Subject-verb agreement Determining the correct number of the verb is trivial in examples such as (1) above"
D18-1151,K17-1003,1,0.875167,"Missing"
D18-1151,C18-1152,0,0.0953654,"r, using naturally occurring sentences can introduce 1 Nor is it possible to have a threshold  such that all grammatical sentences have probability higher than  and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017). 2 In practice, the conditions that govern the distribution of NPIs are much more complicated, but this first approximation will suffice for the present purposes. For a review, see Giannakidou (2011). 1193 confounds that may complicate the interpretation of the experiments (Ettinger et al., 2018). To circumvent these issues, we use templates to automatically construct a large number of English sentence pairs (∼350,000). Our data set includes three phenomena that linguists consider to be sensitive to hierarchical syntactic structure (Everaert et al., 2015; Xiang et al., 2009): subjectverb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and negative polarity items (Section 4.4). The templates can be described using nonrecursive context-free grammars. We specify the preterminal symbols that make up a syntactic construction and have different term"
D18-1151,N18-1108,1,0.791129,"xt word: common collocations, semantics, pragmatics, syntax, and so on. The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions. We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language. Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one. The value of this approach can be illustrated with a recent study by Tran et al. (2018), where a standard LSTM language model was compared to an attention-only LM without recurrence (Vaswani et al., 2017). Although the attention-only model had somewhat better perplexity on the validation set, when the models were tested specifically on challenging subject-verb agreement dependenc"
D18-1151,P82-1020,0,0.850303,"Missing"
D18-1151,J07-3004,0,0.0164286,"Missing"
D18-1151,P18-1132,0,0.110404,"Missing"
D18-1151,Q16-1037,1,0.744623,"in predicting the next word: common collocations, semantics, pragmatics, syntax, and so on. The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions. We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language. Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one. The value of this approach can be illustrated with a recent study by Tran et al. (2018), where a standard LSTM language model was compared to an attention-only LM without recurrence (Vaswani et al., 2017). Although the attention-only model had somewhat better perplexity on the validation set, when the models were tested specifically on challenging subject"
D18-1151,C10-1094,0,0.101513,"Missing"
D18-1151,P16-1144,0,0.0239541,"structed “colorless green ideas” test cases by substituting random content words into sentences from a corpus. We take a more moderate position and avoid combinations that are very implausible or violate selectional restrictions (e.g., the apple laughs). We do this by having separate templates for animate and inanimate subjects and verbs so that the resulting sentences are always reasonably plausible. 3 Related work Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016). Evaluation sets consisting of chal3 The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval. lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation. Acceptability judgments: Lau et al. (2017) compared th"
D18-1151,P12-1101,0,0.0206301,"rg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality. Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction. Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”). Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012). While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model. This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017). Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models. 4 Data set composition This section describes all of the types of sentence pairs included in our data set,"
D18-1151,P11-2038,0,0.030041,"et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation. Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments. The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017). In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality. Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction. Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”). Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012)"
D18-1151,D09-1085,0,0.0385928,"s). We do this by having separate templates for animate and inanimate subjects and verbs so that the resulting sentences are always reasonably plausible. 3 Related work Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016). Evaluation sets consisting of chal3 The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval. lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation. Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments. The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017). In some early wo"
D18-1151,J01-2004,0,0.108317,"and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality. Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction. Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”). Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012). While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model. This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017). Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models. 4 Data set composition This section describes all of the types of sentence pairs in"
D18-1151,N18-2002,0,0.0608897,"Missing"
D18-1151,E17-2060,0,0.0344856,"ation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016). Evaluation sets consisting of chal3 The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval. lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation. Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments. The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017). In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser t"
D18-1151,P17-1024,0,0.0192966,"plausible. 3 Related work Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016). Evaluation sets consisting of chal3 The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval. lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation. Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments. The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017). In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) l"
D18-1151,D18-1503,0,0.0963217,"Missing"
D18-1499,N18-1108,1,0.899807,"Missing"
D18-1499,C00-1027,0,0.499817,"ords that are consistent with these expectations are identified more quickly (Ehrlich and Rayner, 1981; Smith and Levy, 2013). For the reader’s expectations to be maximally effective, they should not only reflect the reader’s past experience with the language (Hale, 2001; MacDonald and Christiansen, 2002), but should also be adapted to the current context. Optimal adaptation would reflect properties of the text being read, such as genre, topic and writer identity, as well as the general tendency for recently used words and syntactic structures to be reused with higher probability (Bock, 1986; Church, 2000; Dubey et al., 2006). Several studies have suggested that readers do in fact adapt their lexical and syntactic predictions to the current context (Otten and Van Berkum, 2008; Fine et al., 2013; Fine and Jaeger, 2016).1 For example, Fine and Jaeger investigated the processing of “garden path” sentences such as (1): 1 Recently, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013). (2) The experienced soldiers who were warned about the dangers conducted the midnight raid. Fine and Jaeger included a large proportion of reduced relatives in their experiment"
D18-1499,P06-1053,0,0.179689,"consistent with these expectations are identified more quickly (Ehrlich and Rayner, 1981; Smith and Levy, 2013). For the reader’s expectations to be maximally effective, they should not only reflect the reader’s past experience with the language (Hale, 2001; MacDonald and Christiansen, 2002), but should also be adapted to the current context. Optimal adaptation would reflect properties of the text being read, such as genre, topic and writer identity, as well as the general tendency for recently used words and syntactic structures to be reused with higher probability (Bock, 1986; Church, 2000; Dubey et al., 2006). Several studies have suggested that readers do in fact adapt their lexical and syntactic predictions to the current context (Otten and Van Berkum, 2008; Fine et al., 2013; Fine and Jaeger, 2016).1 For example, Fine and Jaeger investigated the processing of “garden path” sentences such as (1): 1 Recently, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013). (2) The experienced soldiers who were warned about the dangers conducted the midnight raid. Fine and Jaeger included a large proportion of reduced relatives in their experiment. As the experiment p"
D18-1499,L18-1012,0,0.0556846,"sed on its cross-entropy loss when predicting that sentence; the new weights are then used to predict the next test sentence.2 Our baseline LM is a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) language model trained on 90 million words of English Wikipedia by Gulordava et al. (2018) (see Supplementary Materials for details). For adaptation, we keep the learning rate of 20 used by Gulordava et al. (the gradient is multiplied by this learning rate during weight updates). We examine the effect of this parameter in Section 5.2. We tested the model on the Natural Stories Corpus (Futrell et al., 2018), which has 10 narratives with self-paced reading times from 181 native English speakers. There are two narrative genres in the corpus: fairy tales (seven texts) and documentary accounts (three texts). 3 Linguistic accuracy We first measured how well the adaptive model predicted upcoming words. We report the model’s perplexity, a quantity which is lower when the LM assigns higher probabilities to the words that in fact occurred. We adapted the model to the first k sentences of each text, then tested it on sentence k + 1, for all k. Adaptation dramatically improved test perplexity compared to t"
D18-1499,P82-1020,0,0.822349,"Missing"
D18-1499,P18-2111,0,0.0626306,"Missing"
D18-1499,Q16-1037,1,0.84377,"the dangers conducted the midnight raid. Fine and Jaeger included a large proportion of reduced relatives in their experiment. As the experiment progressed, the cost of disambiguation in favor of the reduced relative interpretation decreased, suggesting that readers had come to expect a construction that is normally infrequent. Human syntactic expectations have been successfully modeled with syntax-based language models (Hale, 2001; Levy, 2008; Roark et al., 2009). Recently, language models (LMs) based on recurrent neural networks (RNNs) have been shown to make adequate syntactic predictions (Linzen et al., 2016; Gulordava et al., 2018), and to make comparable reading time predictions to syntax-based LMs (van Schijndel and Linzen, 2018). In this paper, we propose a simple way to continuously adapt a neural LM, and test the method’s psycholinguistic plausibility. We show that LM adaptation significantly improves our ability to predict human reading times using the LM. Follow-up experiments with controlled materials show that the LM adapts not only to specific 4704 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4704–4710 c Brussels, Belgium, October 31 - N"
D18-1499,D09-1034,0,0.0876119,"tly, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013). (2) The experienced soldiers who were warned about the dangers conducted the midnight raid. Fine and Jaeger included a large proportion of reduced relatives in their experiment. As the experiment progressed, the cost of disambiguation in favor of the reduced relative interpretation decreased, suggesting that readers had come to expect a construction that is normally infrequent. Human syntactic expectations have been successfully modeled with syntax-based language models (Hale, 2001; Levy, 2008; Roark et al., 2009). Recently, language models (LMs) based on recurrent neural networks (RNNs) have been shown to make adequate syntactic predictions (Linzen et al., 2016; Gulordava et al., 2018), and to make comparable reading time predictions to syntax-based LMs (van Schijndel and Linzen, 2018). In this paper, we propose a simple way to continuously adapt a neural LM, and test the method’s psycholinguistic plausibility. We show that LM adaptation significantly improves our ability to predict human reading times using the LM. Follow-up experiments with controlled materials show that the LM adapts not only to sp"
D18-1499,N18-1101,0,0.0187538,"gher learning rates. 4707 Perplexity 450 400 350 300 250 200 150 100 50 0 (a) initial (b) adaptive (c) post-adaptation Figure 4: Perplexity on the held-out set of G1 (a) before adaptation, (b) after adaptation to G1 , (c) after adapting to G1 then adapting to G2 . 6 Testing for catastrophic forgetting Our analysis of the Natural Stories corpus did not indicate that the model suffered from catastrophic forgetting. Yet the Natural Stories corpus contained only two genres; to address the issue of catastrophic forgetting more systematically, we used the premise sentences from the MultiNLI corpus (Williams et al., 2018) — a total of 2000 sentences for each of 10 genres. For each genre pair G1 and G2 (omitting cases where G1 = G2 ), we first adapted the baseline Wikipedia model to 1000 sentences of G1 using a learning rate of 2 (shown to be optimal in Section 5.2). We then adapted the model to 1000 sentences of G2 . Finally, we froze the model’s weights and tested its perplexity on the 1000 heldout sentences from G1 . The results averaged across all pairs of genres are plotted in Figure 4. Unsurprisingly, the model performed best on G1 immediately after adapting to it (middle bar). Crucially, even after adapt"
D18-1499,N01-1021,0,\N,Missing
D19-1592,D19-1539,0,0.0359552,"Missing"
D19-1592,P19-1285,0,0.1017,"Missing"
D19-1592,N19-1423,0,0.297955,"y to improve our LMs’ syntactic representations (Kuncoro et al., 2018)? This paper takes a first step towards addressing this question. We train 125 RNN LMs with long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997) units, systematically varying the size of the training corpus and the dimensionality of the models’ hidden layer, and track the relationship between these parameters and the performance of the models on agreement dependencies in a range of syntactic constructions (Marvin and Linzen, 2018). We also compare our RNNs’ accuracy to that of GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), Transformer-based LMs trained on very large corpora. We find that model capacity does not consistently improve performance beyond a minimum threshold. Increased corpus size likewise has a moderate and inconsistent effect on accuracy. We estimate that even if training data yielded consistent improvements, an unreasonable amount of data would be required to match human accuracy. We conclude that reliable and data-efficient learning of syntax is likely to require external supervision signals or a stronger inductive bias than that provided by RNNs and Transformers. 1 http://www.incompleteideas.n"
D19-1592,K17-1003,1,0.877727,"Missing"
D19-1592,N18-1108,1,0.874399,"ty et al., 2016) for each corpus size;2 in total, we trained 125 models (5 layer sizes × 5 corpus sizes × 5 corpus subsets).3 We used the WikiText-103 validation set for validation. Vocabulary: To ensure comparability across different models trained on different data, we used the same vocabulary for all the models we trained. The vocabulary consisted of an intersection of the 400k word GloVe vocabulary (Pennington et al., 2014) with the 50k words used by GRNN (see below); the resulting vocabulary had 28,438 words. GRNN: We also report the syntactic performance of a publicly available LSTM LM (Gulordava et al., 2018, henceforth GRNN). This trained model has been the focus of a considerable amount of analysis work in the past year. The model has two layers of 650 units each, and was trained on 80M words. Comparison with Transformers: Finally, we report results from two publicly available LMs based on non-recurrent self-attention (Transformers; Vaswani et al., 2017): GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). Both of these models have been argued to learn powerful syntactic representations (Goldberg, 2019; Wolf, 2019). We compare our results to those reported by Wolf (2019) on a similar cha"
D19-1592,P82-1020,0,0.810493,"Missing"
D19-1592,P19-1337,0,0.0540412,"Missing"
D19-1592,Q16-1037,1,0.818994,"more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures. 1 Introduction Recurrent neural network language models (LMs) can learn to predict upcoming words with remarkably low perplexity (Mikolov et al., 2010; Jozefowicz et al., 2016; Radford et al., 2019). This overall success has motivated targeted paradigms that measure whether the LM’s predictions reflect a correct analysis of sentence structure. One such evaluation strategy compares the probability assigned by the LM to a minimal pair of sentences differing only in grammaticality (Linzen et al., 2016). In the following example, the LM is expected to assign a higher probability to the sentence when the verb agrees in number with the subject (1a) than when it does not (1b): (1) a. The author laughs. b. *The author laugh. RNN LMs have been shown to favor the grammatTal Linzen Johns Hopkins University tal.linzen@jhu.edu ical variant in the vast majority of cases sampled at random from a corpus (Linzen et al., 2016), but their accuracy decreases in the presence of distracting nouns intervening between the head of the subject and the verb, especially when those nouns are in relative clauses (Mar"
D19-1592,D18-1151,1,0.933106,"16). In the following example, the LM is expected to assign a higher probability to the sentence when the verb agrees in number with the subject (1a) than when it does not (1b): (1) a. The author laughs. b. *The author laugh. RNN LMs have been shown to favor the grammatTal Linzen Johns Hopkins University tal.linzen@jhu.edu ical variant in the vast majority of cases sampled at random from a corpus (Linzen et al., 2016), but their accuracy decreases in the presence of distracting nouns intervening between the head of the subject and the verb, especially when those nouns are in relative clauses (Marvin and Linzen, 2018). Can we hope to address these deficits by training larger and larger networks on larger and larger corpora, relying on the “unreasonable effectiveness” of massive datasets (Halevy et al., 2009) and computational power?1 Or would architectural advances be necessary to improve our LMs’ syntactic representations (Kuncoro et al., 2018)? This paper takes a first step towards addressing this question. We train 125 RNN LMs with long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997) units, systematically varying the size of the training corpus and the dimensionality of the models’ hidden lay"
D19-1592,E17-2025,0,0.0286417,"s than that provided by RNNs and Transformers. 1 http://www.incompleteideas.net/ IncIdeas/BitterLesson.html 5831 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5831–5837, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Language models Architecture: All of the models we trained consisted of two LSTM layers. We trained models with 100, 200, 400, 800 or 1600 units in each hidden layer. Input and output weights were tied during training (Press and Wolf, 2017; Inan et al., 2017); consequently, the input embedding had the same dimensionality as the hidden layers. Training data: We trained networks of each size on 2M, 10M, 20M, 40M and 80M words (2M = 2 million). We extracted five disjoint sections of the WikiText-103 corpus (Merity et al., 2016) for each corpus size;2 in total, we trained 125 models (5 layer sizes × 5 corpus sizes × 5 corpus subsets).3 We used the WikiText-103 validation set for validation. Vocabulary: To ensure comparability across different models trained on different data, we used the same vocabulary for all the models we traine"
D19-1592,D17-1035,0,0.0553794,"Missing"
D19-1592,N19-1334,0,0.129506,"Missing"
E17-2020,P16-1160,0,0.0354243,"guage Models Using a Lexical Decision Task Ga¨el Le Godais1,3 Tal Linzen1,2 Emmanuel Dupoux1 1 2 3 LSCP, CNRS, EHESS and ENS, PSL Research University IJN ENSIMAG gael.le-godais@orange.fr,{tal.linzen,emmanuel.dupoux}@gmail.com Abstract study the components of the human mind; it is natural to use these tasks to understand the abilities of artificial neural networks (Dunbar et al., 2015; Linzen et al., 2016). The present work takes up character-level neural network language models. Such models have been surprisingly competitive in applications, even though they do not explicitly represent words (Chung et al., 2016; Kim et al., 2016). Our goal is to shed light on the ability of characterlevel models to implicitly learn a lexicon. We use a task designed to investigate humans lexical processes. This task is based on a simple question: how well can the subject distinguish real words from character strings that do not belong to the language (nonwords)? Since character-level language models define a probability distribution over all character strings, we can perform this task in a particularly straightforward way: given a word and a nonword that are matched on low-level properties such as length and characte"
E17-2020,P82-1020,0,0.794406,"Missing"
E17-2020,Q16-1037,1,0.884757,"Missing"
E17-2020,N13-1090,0,0.0162944,"biquitous in natural language processing systems, but our ability to understand those networks has not kept pace: we typically have little understanding of a typical neural network beyond its accuracy on the task it was trained to do. One potential way to gain insight into the ability of a trained model is to evaluate it on an interpretable auxiliary task that is distinct from the task that the network was trained on: a network that performs a particular auxiliary task successfully is likely to have internal representations that encode the information relevant for that task (Adi et al., 2017; Mikolov et al., 2013). Linguistics and psycholinguistics offer a rich repertoire of tasks that have been used for decades to 2 Lexical decision The lexical decision task is widely used in cognitive psychology to probe human lexical representations (Meyer and Schvaneveldt, 1971; Balota 125 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 125–130, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics recurrent layers with a tanh nonlinearity, each followed by a batch normalization layer (Ioffe and S"
E17-2020,E17-2060,0,0.0608708,"Missing"
K17-1003,P16-2038,1,0.749856,"Missing"
K17-1003,J07-3004,0,0.0182219,"s difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least one third-person present-tense agreement dependency. Following Linzen et al. (2016), we replaced rare words by their part-ofspeech tags, using the Penn Treebank tag set (Marcus et"
K17-1003,E17-2020,1,0.888529,"Missing"
K17-1003,N16-1027,0,0.0447326,"mpionships. Determining that the subject of the verb in boldface is banners rather than the singular nouns 4 with an appropriate tag. In fact, supertagging is almost as difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least on"
K17-1003,Q16-1037,1,0.693621,"the RNN with an incentive to develop more sophisticated representations, we trained it to perform one of two tasks: the first is combinatory categorical grammar (CCG) supertagging (Bangalore and Joshi, 1999), a sequence labeling task likely to require robust syntactic representations; the second task is language modeling. We also investigate the inverse question: can Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multitask training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on"
K17-1003,P15-2041,0,0.0386091,"building are for national or NCAA Championships. Determining that the subject of the verb in boldface is banners rather than the singular nouns 4 with an appropriate tag. In fact, supertagging is almost as difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had"
K17-1003,J93-2004,0,0.0799242,"Missing"
K17-1003,J99-2004,0,\N,Missing
K17-1003,N16-1024,0,\N,Missing
K17-1003,E17-1005,0,\N,Missing
K19-1007,W18-5426,0,0.0335427,"after reading sentences like (1a) (the prime), readers expect sentences like (2a), which shares syntactic structure with the prime, to occur with a greater likelihood than the alternative variant like (2b) which does not (Wells et al., 2009).1 Introduction Neural networks trained on text alone, without explicit syntactic supervision, have been surprisingly successful in tasks that require sensitivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syn"
K19-1007,Q19-1004,0,0.022417,"itivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syntactic representations (Bock, 1986). This paradigm is based on the empirical finding that people tend to reuse (2) a. The lawyer sent the letter to the client. b. The lawyer sent the client the letter. We use the priming paradigm to analyze neural network language models (LMs), systems that define a probability distribution over the nth word of a sentence given its first n − 1 words. Building o"
K19-1007,N18-1108,1,0.845263,"ame way that non-cumulative priming does: when participants (or LMs) are exposed to sentences with structure SX , if there is a greater decrease in surprisal when they are tested on other sentences with SX than when they are tested on other sentences with SY , we can infer that the representations of sentences with SX are more similar to each other than to the Background Syntactic predictions in neural LMs We build on paradigms that use LM probability estimates for words in a given context as a measure of the model’s sensitivity to the syntactic structure of the sentence (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018). If a language model assigns a higher probability to a verb form that agrees in number with the subject (the boy... writes) than a verb form that does not (the boy... write), we can infer that the model encodes information about the agreement features of nouns and verbs (that is, the difference between singular and plural) and has correctly identified the subject that corresponds to this verb. This reasoning has been extended beyond subject-verb agreement to study whether the predictions of neural LMs are sensitive to a range of other syntactic dependencies, includin"
K19-1007,W19-4821,0,0.0224158,"Missing"
K19-1007,N01-1021,0,0.376958,"lly measured the priming effect on the sentence immediately following the prime, more recent studies have demonstrated that the effects of syntactic priming can be cumulative and long-lasting: sentences with a shared structure SX become progressively easier to process when preceded by n sentences with the same structure SX than when preceded by n sentences with a different structure SY (Kaschak et al., 2011; Wells et al., 2009).2 In conjunction with the finding that words that are consistent with a probable syntactic parse are easier to process than words consistent with less probable parses (Hale, 2001; Levy, 2008), the increased ease of processing in cumulative priming studies can be interpreted as evidence that, with increased exposure to a structure, participants begin to expect that structure with a greater probability (Chang et al., 2006). Cumulative priming allows us to study how sentences are related to each other in the human (or LM) representation space in the same way that non-cumulative priming does: when participants (or LMs) are exposed to sentences with structure SX , if there is a greater decrease in surprisal when they are tested on other sentences with SX than when they are"
K19-1007,P18-1198,0,0.058115,"Missing"
K19-1007,W18-5424,0,0.0120856,"If a language model assigns a higher probability to a verb form that agrees in number with the subject (the boy... writes) than a verb form that does not (the boy... write), we can infer that the model encodes information about the agreement features of nouns and verbs (that is, the difference between singular and plural) and has correctly identified the subject that corresponds to this verb. This reasoning has been extended beyond subject-verb agreement to study whether the predictions of neural LMs are sensitive to a range of other syntactic dependencies, including negative polarity items (Jumelet and Hupkes, 2018), filler-gap dependencies (Wilcox et al., 2018) and reflexive pronoun binding (Futrell et al., 2019). 2 67 In studies looking at non-cumulative priming, n = 1. Abstract structure Example Unreduced Object RC Reduced Object RC Unreduced Passive RC Reduced Passive RC Active Subject RC PS/ORC-matched Coordination ASRC-matched Coordination The conspiracy that the employee welcomed divided the beautiful country. The conspiracy the employee welcomed divided the beautiful country. The conspiracy that was welcomed by the employee divided the beautiful country. The conspiracy welcomed by the employee di"
K19-1007,N16-1024,0,0.0351967,"ch other in the representation spaces of models trained on 2 million tokens than in the representation spaces for models trained on 20 million tokens. We infer from this that LMs’ ability to track abstract properties of sentences decreases with an increase in the training corpus size. This suggests that if we want these LMs to track more abstract linguistic properties, training them on more data from the same distribution is unlikely to help (cf. van Schijndel et al. 2019). Future work can explore how to bias these models to track linguistically useful properties through architectural biases (Dyer et al., 2016), training on auxiliary tasks (Enguehard et al., 2017) or data augmentation (Perez and Wang, 2017). 7 Conclusion We proposed a novel technique to analyze how the representations of various syntactic structures are organized in neural language models. As a case study, we applied this technique to gain insight into the representations of sentences with relative clauses in RNN language models and found that the representations of sentences were organized in a linguistically interpretable manner. 8 Acknowledgments We would like to thank Sadhwi Srinivas and the members of the CAP lab at JHU for hel"
K19-1007,K17-1003,1,0.870584,"rained on 2 million tokens than in the representation spaces for models trained on 20 million tokens. We infer from this that LMs’ ability to track abstract properties of sentences decreases with an increase in the training corpus size. This suggests that if we want these LMs to track more abstract linguistic properties, training them on more data from the same distribution is unlikely to help (cf. van Schijndel et al. 2019). Future work can explore how to bias these models to track linguistically useful properties through architectural biases (Dyer et al., 2016), training on auxiliary tasks (Enguehard et al., 2017) or data augmentation (Perez and Wang, 2017). 7 Conclusion We proposed a novel technique to analyze how the representations of various syntactic structures are organized in neural language models. As a case study, we applied this technique to gain insight into the representations of sentences with relative clauses in RNN language models and found that the representations of sentences were organized in a linguistically interpretable manner. 8 Acknowledgments We would like to thank Sadhwi Srinivas and the members of the CAP lab at JHU for helpful discussions and valuable feedback. 74 References"
K19-1007,N19-1004,0,0.0738891,"Missing"
K19-1007,N19-1002,0,0.0242185,"ich does not (Wells et al., 2009).1 Introduction Neural networks trained on text alone, without explicit syntactic supervision, have been surprisingly successful in tasks that require sensitivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syntactic representations (Bock, 1986). This paradigm is based on the empirical finding that people tend to reuse (2) a. The lawyer sent the letter to the client. b. The lawyer sent the client the letter. We"
K19-1007,D19-1592,1,0.865376,"Missing"
K19-1007,D16-1159,0,0.0314846,"eaders expect sentences like (2a), which shares syntactic structure with the prime, to occur with a greater likelihood than the alternative variant like (2b) which does not (Wells et al., 2009).1 Introduction Neural networks trained on text alone, without explicit syntactic supervision, have been surprisingly successful in tasks that require sensitivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syntactic representations (Bock, 1986). This"
K19-1007,Q16-1037,1,0.939673,"actic supervision, have been surprisingly successful in tasks that require sensitivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syntactic representations (Bock, 1986). This paradigm is based on the empirical finding that people tend to reuse (2) a. The lawyer sent the letter to the client. b. The lawyer sent the client the letter. We use the priming paradigm to analyze neural network language models (LMs), systems that define a probability"
K19-1007,W18-1004,0,0.0193188,"ve been surprisingly successful in tasks that require sensitivity to sentence structure. The difficulty of interpreting the learned neural representations that underlie this success has motivated a range of analysis techniques, including diagnostic classifiers (Giulianelli et al., 2018; Conneau et al., 2018; Shi et al., 2016), visualization of individual neuron activations (K´ad´ar et al., 2017; Qian et al., 2016), ablation of individual neurons or sets of neurons (Lakretz et al., 2019) and behavioral tests of generalization to infrequent or held out syntactic structures (Linzen et al., 2016; Weber et al., 2018; McCoy et al., 2018); for reviews, see Belinkov and Glass (2019) and Alishahi et al. (2019). This paper expands the toolkit of neural network analysis techniques by drawing on the syntactic priming paradigm, a central tool in psycholinguistics for analyzing human syntactic representations (Bock, 1986). This paradigm is based on the empirical finding that people tend to reuse (2) a. The lawyer sent the letter to the client. b. The lawyer sent the client the letter. We use the priming paradigm to analyze neural network language models (LMs), systems that define a probability distribution over t"
K19-1007,D18-1151,1,0.937765,"s with a particular type of RC were most similar to other sentences with the same type of RC in the LMs’ representation space. Furthermore, sentences with different types of RCs were more similar to each other than sentences without RCs. We demonstrate that the similarity between sentences was not driven merely by specific words that appeared in the sentence, suggesting that the LMs tracked abstract properties of the sentence. This ability to track abstract properties decreased as the training corpus size increased. Finally, we tested the hypothesis that LMs’ accuracy on agreement prediction (Marvin and Linzen, 2018) would increase with the LMs’ ability to track more abstract properties of the sentence, but did not find evidence for this hypothesis. 2 2.1 Syntactic priming in humans Syntactic priming has been used to study whether the representations of two sentences have shared structure. For example, (1a) (repeated below as (3)) shares the structure VP → V NP PP with (4a) but not (4b). (3) The boy threw the ball to the dog. (4) a. The renowned chef made some wonderful pasta for the guest. b. The renowned chef made the guest some wonderful pasta. If (3) primes (4a) more than it primes (4b), we can infer"
K19-1007,W18-5423,0,0.0662187,"a verb form that agrees in number with the subject (the boy... writes) than a verb form that does not (the boy... write), we can infer that the model encodes information about the agreement features of nouns and verbs (that is, the difference between singular and plural) and has correctly identified the subject that corresponds to this verb. This reasoning has been extended beyond subject-verb agreement to study whether the predictions of neural LMs are sensitive to a range of other syntactic dependencies, including negative polarity items (Jumelet and Hupkes, 2018), filler-gap dependencies (Wilcox et al., 2018) and reflexive pronoun binding (Futrell et al., 2019). 2 67 In studies looking at non-cumulative priming, n = 1. Abstract structure Example Unreduced Object RC Reduced Object RC Unreduced Passive RC Reduced Passive RC Active Subject RC PS/ORC-matched Coordination ASRC-matched Coordination The conspiracy that the employee welcomed divided the beautiful country. The conspiracy the employee welcomed divided the beautiful country. The conspiracy that was welcomed by the employee divided the beautiful country. The conspiracy welcomed by the employee divided the beautiful country. The employee that"
K19-1007,D16-1079,0,0.030544,"Missing"
K19-1007,D18-1499,1,0.852569,"Missing"
K19-1007,J17-4003,0,\N,Missing
N18-1108,P17-1080,0,0.0665188,"d controlled artificial languages, in which complex hierarchical phenomena were often overrepresented compared to natural languages. Our work, which is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used"
N18-1108,2017.lilt-15.3,0,0.141251,"rtantly, their analysis did not rule out the possibility that RNNs might be relying on semantic or collocational/frequency-based information, rather than purely on syntactic structure. In “dogs dogs bark an RNN might in the neighbourhood often bark bark”, get the right agreement by encoding information 1195 Proceedings of NAACL-HLT 2018, pages 1195–1205 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics about what typically barks (dogs, not neighbourhoods), without relying on more abstract structural cues. In a follow-up study to Linzen and colleagues’, Bernardy and Lappin (2017) observed that RNNs are better at long-distance agreement when they construct rich lexical representations of words, which suggests effects of this sort might indeed be at play. We introduce a method to probe the syntactic abilities of RNNs that abstracts away from potential lexical, semantic and frequency-based confounds. Inspired by Chomsky’s (1957) insight that “grammaticalness cannot be identified with meaningfulness” (p. 106), we test long-distance agreement both in standard corpus-extracted examples and in comparable nonce sentences that are grammatical but completely meaningless, e.g.,"
N18-1108,P16-2006,0,0.0233094,"ext-free languages (Elman, 1991). More recently, RNNs have ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task."
N18-1108,K17-1003,1,0.730838,"Missing"
N18-1108,W11-2123,0,0.0268527,"ts for sRNNs can be found in the SM. 8 https://www.mturk.com/ HE RU #constructions #original 8 119 2 41 18 373 21 442 Unigram Original Nonce 54.6 54.1 65.9 42.5 67.8 63.1 60.2 54.0 5-gram KN Original Nonce 63.9 52.8 63.4 43.4 72.1 61.7 73.5 56.8 Perplexity Baselines. We consider three baselines: first, a unigram baseline, which picks the most frequent form in the training corpus out of the two candidate target forms (singular or plural); second, a 5-gram model with Kneser-Ney smoothing (KN, Kneser and Ney, 1995) trained using the IRSTLM package (Federico et al., 2008) and queried using KenLM (Heafield, 2011); and third, a 5-gram LSTM, which only had access to windows of five tokens (Chelba et al., 2017). Compared to KN, the 5-gram LSTM can generalize to unseen ngrams thanks to its embedding layer and recurrent connections. However, it cannot discover longdistance dependency patterns that span more than five words. See SM for details on the hyperparameters of this baseline. EN 5-gram LSTM Original Nonce Perplexity LSTM Original Nonce Perplexity 147.8 168.9 122.0 166.6 81.8 70.2 90.9 91.5 ±3.2 ±5.8 ±1.2 ±0.4 78.0 58.2 77.5 85.7 ±1.3 ±2.1 ±0.8 ±0.7 62.6 71.6 59.9 61.1 ±0.2 ±0.3 ±0.2 ±0.4 92.1 81.0 9"
N18-1108,P82-1020,0,0.869559,"Missing"
N18-1108,Q16-1023,0,0.0450461,"an, 1991). More recently, RNNs have ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued th"
N18-1108,Q16-1037,1,0.557788,"nducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued that the unsupervised language modeling objec"
N18-1108,N13-1090,0,0.106847,"Missing"
N18-1108,E17-2060,0,0.0400373,"is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used nonce sentences made of real words to evaluate the syntactic abilities of models or human subjects. As a proof of concept, Pereira (2000) and,"
N18-1108,D16-1159,0,0.0897948,"languages, in which complex hierarchical phenomena were often overrepresented compared to natural languages. Our work, which is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used nonce sentences mad"
N18-1108,E17-1063,0,0.0154163,"ave ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued that the unsupervised l"
N18-1108,L16-1262,0,\N,Missing
N19-1356,N18-1108,1,0.949853,"separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfogel et al., 2018). Existing cross-linguistic comparisons are difficult to interpret, however. Models were inevitably trained on a different corpus for each language. The constructions tested can differ across languages (Gulordava et al., 2018). Perhaps most importantly, any two natural langua"
N19-1356,N18-2085,0,0.031905,"trained a model to mimic the POS tags one model for each combination of case system order-statistics of the target language, we manuand word order. We jointly predicted the plurality ally modified the parsed corpora; this allows us to of subject and the object. 3539 control for selected parameters, at the expense of reducing generality. Simpler synthetic languages (not based on natural corpora) have been used in a number of recent studies to examine the inductive biases of different neural architectures (Bowman et al., 2015; Lake and Baroni, 2018; McCoy et al., 2018). In another recent study, Cotterell et al. (2018) measured the ability of RNN and n-gram models to perform character-level language modeling in a sample of languages, using a parallel corpus; the main typological property of interest in that study was morphological complexity. Finally, a large number of studies, some mentioned in the introduction, have used syntactic prediction tasks to examine the generalizations acquired by neural models (see also Bernardy and Lappin 2017; Futrell et al. 2018; Lau et al. 2017; Conneau et al. 2018; Ettinger et al. 2018; Jumelet and Hupkes 2018). even when the case system was highly syncretic. Agreement feat"
N19-1356,K17-1003,1,0.895435,"Missing"
N19-1356,C18-1152,0,0.024413,"man et al., 2015; Lake and Baroni, 2018; McCoy et al., 2018). In another recent study, Cotterell et al. (2018) measured the ability of RNN and n-gram models to perform character-level language modeling in a sample of languages, using a parallel corpus; the main typological property of interest in that study was morphological complexity. Finally, a large number of studies, some mentioned in the introduction, have used syntactic prediction tasks to examine the generalizations acquired by neural models (see also Bernardy and Lappin 2017; Futrell et al. 2018; Lau et al. 2017; Conneau et al. 2018; Ettinger et al. 2018; Jumelet and Hupkes 2018). even when the case system was highly syncretic. Agreement feature prediction in some of our synthetic languages is likely to be difficult not only for RNNs but for many other classes of learners, including humans. For example, agreement in a language with very flexible word order and without case marking is impossible to predict in many cases (see §4.2), and indeed such languages are very rare. In future work, a human experiment based on the agreement prediction task can help determine whether the difficulty of our languages is consistent across humans and RNNs. Ack"
N19-1356,W18-5426,0,0.0225802,"te corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 2018; Wilcox et al., 3532 Proceedings of NAACL-HLT 2019, pages 3532–3542 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Original they say the broker took them out for lunch frequently . (they, broker: subjects; say, took: verbs; them: object) Polypersonal agreement they saykon the broker tookkarker them out for lunch frequently . (kon: plural subject; kar: singular subject; ker: plural object) Word order variation"
N19-1356,P18-1027,0,0.0265555,"thheld in training). This 1. Sentences with an object of the opposite pluconstitutes strong evidence for the RNN’s recency rality from the subject (object attractor). bias: our models extracted the generalization that subjects directly precede the verb, even though the 2. Sentences with an object of the same pluraldata were equally compatible with the generalizaity as the subject (non-attractor object).5 tion that the subject is the first core argument in 3. Sentences without an object, but with one the clause. or more nouns of the opposite plurality inThese findings align with the results of Khandelwal et al. (2018), who demonstrated that RNN 5 When the object is a noun-noun compound, it is considlanguage models are more sensitive to perturbaered a non-attractor if its head is not of the opposite plurality of the subject, regardless of the plurality of other elements. tions in recent input words compared with perturThis can only make the task harder compared with the albations to more distant parts of the input. While ternative of considering compound objects such as “screen in their case the model’s recency preference can displays” as attractors for plural subjects. 3537 SOV VOS Object (attractor) Objec"
N19-1356,P18-1132,0,0.0800223,"Missing"
N19-1356,Q16-1037,1,0.896285,"periments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 2018; Wilcox et al., 3532 Proceedings of NAACL-HLT 2019, pages 3532–3542 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Original they say the broker took them out for lunch frequently . (they, broker: subjects; say, took: verbs; them: object) Polypersonal agreement they saykon the broker tookkarker them out for lunch frequently . (kon: plural subject; kar: singular subject; ker: plural ob"
N19-1356,J93-2004,0,0.0661216,"ency edge, and record the plurality of those arguments. Verbs that were the head of a clausal complement without a subject (xcomp dependencies) were excluded. We recorded the plurality of the dependents of the verb regardless of whether the tense and person of the verb condition agreement in English (that is, not only in third-person Synthetic Language Generation We used an expert-annotated corpus, to avoid potential confounds between the typological parameters we manipulated and possible parse errors in an automatically parsed corpus. As our starting point, we took the English Penn Treebank (Marcus et al., 1993), converted to the Universal Dependencies scheme (Nivre et al. 2016) using the Stanford converter (Schuster and Manning, 2016). We then manipulated the tree representations of the sentences in the corpus to generate parametrically modified English corpora, varying in case sys3533 1 https://github.com/Shaul1321/rnn typology Prediction task Subject accuracy Object accuracy Object recall Subject Object Joint 94.7 ± 0.3 95.7 ± 0.23 88.9 ± 0.26 90.0 ± 0.1 81.8 ± 1.4 85.4 ± 2.3 Table 1: Results of the polypersonal agreement experiments. “Joint” refers to multitask prediction of subject and object pl"
N19-1356,D18-1151,1,0.877321,"ach of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reporte"
N19-1356,W18-5423,0,0.0295496,"nguages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfoge"
N19-1356,L16-1262,1,0.891305,"Missing"
N19-1356,W18-5412,1,0.766515,", 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfogel et al., 2018). Existing cross-linguistic comparisons are difficult to interpret, however. Models were inevitably trained on a different corpus for each language. The constructions tested can differ across languages (Gulordava et al., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a parti"
N19-1356,L16-1376,0,0.0707862,"., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a particular natural language (English, in our experiments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 201"
N19-1356,Q16-1035,0,0.0510525,"(Gulordava et al., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a particular natural language (English, in our experiments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 201"
N19-1356,Q17-1011,0,0.0247937,"s are consistent with the observation that languages with explicit case marking tend to allow a more flexible word orders compared with languages such as English that make use of word order to express grammatical function of words. 6 Related Work Setup We evaluated the interaction between difOur approach of constructing synthetic languages ferent case marking schemes and three word orby parametrically modifying parsed corpora for ders: flexible word order and the two orders on natural languages is closely inspired by Wang and which the model achieved the best (OVS) and Eisner (2016) (see also Wang and Eisner 2017). worst (VOS) subject prediction accuracy. We train While they trained a model to mimic the POS tags one model for each combination of case system order-statistics of the target language, we manuand word order. We jointly predicted the plurality ally modified the parsed corpora; this allows us to of subject and the object. 3539 control for selected parameters, at the expense of reducing generality. Simpler synthetic languages (not based on natural corpora) have been used in a number of recent studies to examine the inductive biases of different neural architectures (Bowman et al., 2015; Lake a"
P19-1334,D16-1203,0,0.0429074,"e for the model to learn to generalize to more challenging cases as a human performing the task would. This issue has been documented across domains in artificial intelligence. In computer vision, for example, neural networks trained to recognize objects are misled by contextual heuristics: a network that is able to recognize monkeys in a typical context with high accuracy may nevertheless label a monkey holding a guitar as a human, since in the training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradict"
P19-1334,D15-1075,0,0.753994,"contextual heuristics: a network that is able to recognize monkeys in a typical context with high accuracy may nevertheless label a monkey holding a guitar as a human, since in the training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradiction in standard NLI training sets. The focus of our work is on heuristics that are based on superficial syntactic properties. Consider the following sentence pair, which has the target label entailment: (1) Premise: The judge was paid by the actor. Hypothesis: The act"
P19-1334,P16-1139,0,0.124797,"euristics is that their input representations may make them susceptible to these heuristics. The lexical overlap heuristic disregards the order of the words in the sentence and considers only their identity, so it is likely to be adopted by bag-of-words NLI models (e.g., Parikh et al. 2016). The subsequence heuristic considers linearly adjacent chunks of words, so one might expect it to be adopted by standard RNNs, which process sentences in linear order. Finally, the constituent heuristic appeals to components of the parse tree, so one might expect to see it adopted by tree-based NLI models (Bowman et al., 2016). 3 Dataset Construction For each heuristic, we generated five templates for examples that support the heuristic and five templates for examples that contradict it. Below is one template for the subsequence heuristic; see Appendix B for a full list of templates. (4) The N1 P the N2 V. 9 The N2 V. The lawyer by the actor ran. 9 The actor ran. We generated 1,000 examples from each template, for a total of 10,000 examples per heuristic. Some heuristics are special cases of others, but we made sure that the examples for one heuristic did not also fall under a more narrowly defined heuristic. That"
P19-1334,N19-1423,0,0.254969,"Missing"
P19-1334,C18-1152,0,0.182781,"al overlap cases, the hypothesis was not a subsequence or constituent of the premise; for subsequence cases, the hypothesis was not a constituent of the premise. 3.1 Dataset Controls Plausibility: One advantage of generating examples from templates—instead of, e.g., modifying naturally-occurring examples—is that we can ensure the plausibility of all generated sentences. For example, we do not generate cases such as The student read the book 9 The book read the student, which could ostensibly be solved using a hypothesis-plausibility heuristic. To achieve this, we drew our core vocabulary from Ettinger et al. (2018), where every noun was a plausible subject of every verb or a plausible object of every transitive verb. Some templates required expanding this core vocabulary; in those cases, we manually curated the additions to ensure plausibility. 3430 Selectional criteria: Some of our example types depend on the availability of lexically-specific verb frames. For example, (5) requires awareness of the fact that believed can take a clause (the lawyer saw the officer) as its complement: (5) The doctor believed the lawyer saw the officer. 9 The doctor believed the lawyer. It is arguably unfair to expect a mo"
P19-1334,P17-1152,0,0.131556,"Missing"
P19-1334,N18-1108,1,0.895833,"Missing"
P19-1334,W18-5441,1,0.897302,"Missing"
P19-1334,S18-2023,0,0.205224,"Missing"
P19-1334,S10-1060,0,0.0364375,"er than chance accuracy on those datasets by only looking at the hypothesis. Other recent works address possible ways in which NLI models might use fallible heuristics, focusing on semantic phenomena, such as lexical inferences (Glockner et al., 2018) or quantifiers (Geiger et al., 2018), or biases based on specific words (Sanchez et al., 2018). Our work focuses instead on structural phenomena, following the proof-of-concept work done by Dasgupta et al. (2018). Our focus on using NLI to address how models capture structure follows some older work about using NLI for the evaluation of parsers (Rimell and Clark, 2010; Mehdad et al., 2010). NLI has been used to investigate many other types of linguistic information besides syntactic structure (Poliak et al., 2018a; White et al., 2017). Outside NLI, multiple projects have used classification tasks to understand what linguistic and/or structural information is present in vector encodings of sentences (e.g., Adi et al., 2017; Ettinger et al., 2018; Conneau et al., 2018). We instead choose the behavioral approach of using task performance on critical cases. Unlike the classification approach, this approach is agnostic to model structure; our dataset could be u"
P19-1334,N18-1067,0,0.0457247,"Missing"
P19-1334,N18-1179,0,0.197271,"e training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016). The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradiction in standard NLI training sets. The focus of our work is on heuristics that are based on superficial syntactic properties. Consider the following sentence pair, which has the target label entailment: (1) Premise: The judge was paid by the actor. Hypothesis: The actor paid the judge. An NLI system that labels this example correctly might do so not by reasoning about the meanings of these sentences, but rather by assuming that the premise enta"
P19-1334,W03-0906,0,\N,Missing
P19-1334,P03-1054,0,\N,Missing
P19-1334,N10-1146,0,\N,Missing
P19-1334,D16-1244,0,\N,Missing
P19-1334,Q16-1037,1,\N,Missing
P19-1334,D16-1240,1,\N,Missing
P19-1334,D17-1070,0,\N,Missing
P19-1334,I17-1100,0,\N,Missing
P19-1334,Q18-1019,0,\N,Missing
P19-1334,W18-2501,0,\N,Missing
P19-1334,P18-2103,0,\N,Missing
P19-1334,D18-1501,0,\N,Missing
P19-1334,D18-1151,1,\N,Missing
P19-1334,D18-1007,1,\N,Missing
P19-1334,P19-1449,0,\N,Missing
P19-1334,N18-1101,0,\N,Missing
Q16-1037,W13-2604,0,0.0167308,"complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K´ad´ar et al. (2016) and Li et al. (2016) suggest visualization techniques for word-level RNNs trained to perform tasks that aren’t explicitly syntactic (image captioning and sentiment analysis). Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al. (1996). More recently, the connection between grammaticality judgments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al. (2015). Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011). 9 Discussion and Future Work Neural network architectures are typically evaluated on random samples of naturally occurring sentences, e.g., using perplexity on held-out data in language modeling. Since the majority of natural language sentences are grammatically simple, models can achieve high overall accuracy using flawed heuristic"
Q16-1037,N16-1024,0,0.00901657,"tured. 1 Introduction Recurrent neural networks (RNNs) are highly effective models of sequential data (Elman, 1990). The rapid adoption of RNNs in NLP systems in recent years, in particular of RNNs with gating mechanisms such as long short-term memory (LSTM) units Yoav Goldberg Computer Science Department Bar Ilan University yoav.goldberg@gmail.com (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014), has led to significant gains in language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), parsing (Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Dyer et al., 2016), machine translation (Bahdanau et al., 2015) and other tasks. The effectiveness of RNNs1 is attributed to their ability to capture statistical contingencies that may span an arbitrary number of words. The word France, for example, is more likely to occur somewhere in a sentence that begins with Paris than in a sentence that begins with Penguins. The fact that an arbitrary number of words can intervene between the mutually predictive words implies that they cannot be captured by models with a fixed window such as n-gram models, but can in principle be captured by RNNs, which do not have an arc"
Q16-1037,N16-1082,0,0.0156208,"pecific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic. As in our study, the performance of the network degraded as the complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K´ad´ar et al. (2016) and Li et al. (2016) suggest visualization techniques for word-level RNNs trained to perform tasks that aren’t explicitly syntactic (image captioning and sentiment analysis). Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al. (1996). More recently, the connection between grammaticality judgments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al. (2015). Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences"
Q16-1037,Q16-1023,1,\N,Missing
S16-2001,P14-2131,0,0.0205355,"semantics (VSMs) represent words as points in a high-dimensional space. Similar words are represented by points that are close together in the space. VSMs are typically trained on a corpus in an unsupervised way; the goal is for words that occur in similar contexts to be assigned similar representations. The context of a word in a corpus is often defined as the set of words that occur in a small window around the word of interest (Lund and Burgess, 1996; Turney and Pantel, 2010). VSM representations have been shown to be useful in improving the performance of NLP systems (Turian et al., 2010; Bansal et al., 2014) as well as in predicting cognitive measures such as similarity judgments and semantic priming (Jones et al., 2006; Hill et al., 2015). man is to woman as king is to . The system is expected to infer the relation between the first two words—man and woman—and find a word that stands in the same relation to king. When this task is solved using the offset method, there is no explicit set of relations that the system is trained to identify. We simply subtract the vector for man from the vector for woman and add it to king. If the offset woman − man represents an abstract gender feature, adding tha"
S16-2001,E12-1004,0,0.0692466,"Missing"
S16-2001,P13-2010,0,0.0519242,"Missing"
S16-2001,W14-1618,0,0.469328,"solves the analogy task by simply returning the nearest neighbor of b, ignoring a and a∗ altogether. MODAL : Modal auxiliaries such as must or can quantify over relevant possible worlds (Kripke, 1959). Consider, for example, the following sentences: (7) a. b. Anne must go to bed early. Anne can go to bed early. Assuming deontic modality, such as the statement of a rule, (7-a) means that in all worlds in which the rule is obeyed, Anne goes to bed early, whereas (7-b) means that there exists at least one world consistent with the speaker’s orders in which she goes to bed early. Multiplication: Levy and Goldberg (2014) point out that the word x∗ that is closest to a∗ − a + b in terms of cosine similarity is the one that maximizes the following expression: MODAL VERB : Verbs such as request and forbid can be paraphrased using modal auxiliaries: he allowed me to stay up late is similar in meaning to he said I can stay up late. It is plausible to argue that allow is existential and increasing, just like can. 3 arg max (cos(x0 , a∗ ) − cos(x0 , a) + cos(x0 , b)) 0 x (2) They report that replacing addition with multiplication improves accuracy on the analogy task: Evaluation cos(x0 , a∗ ) cos(x0 , b) x cos(x0 ,"
S16-2001,W15-4002,0,0.0261367,"Missing"
S16-2001,Q15-1016,0,0.238727,"nvestigate how the quality of the representations is affected by the size of the training corpus. A large and constantly expanding range of VSM architectures have been proposed in the literature (Mikolov et al., 2013a; Pennington et al., 2014; Turney and Pantel, 2010). Instead of exploring the full range of architectures, the present study will focus on the skip-gram model, implemented in word2vec (Mikolov et al., 2013b). This model has been argued to perform either better than or on a par with competing architectures, depending on the task and on hyperparameter settings (Baroni et al., 2014; Levy et al., 2015). Particularly pertinent to our purposes, Levy et al. (2015) find that the skip-gram model tends to recover formal linguistic features more accurately than traditional distributional models. 2 I NC . Existential (5) a. b. Everybody went out to a death metal concert last night. Everybody went out last night. By contrast, in decreasing quantifiers such as nobody the truth of broader predicates entails the truth of narrower ones: (6) Quantificational force We focus on universal and existential quantificational words, which can be translated into firstorder logic using a universal (∀) or existenti"
S16-2001,D15-1003,0,0.0316767,"Missing"
S16-2001,N13-1090,0,0.875512,"o king. If the offset woman − man represents an abstract gender feature, adding that offset to king should lead us to queen (Figure 1). In the rest of this paper, we describe the set of analogy problems that we used to evaluate the VSMs’ representation of quantificational features, and explore how accuracy is affected by the con1 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 1–11, Berlin, Germany, August 11-12, 2016. queen Universal woman? king PERSON OBJECT man PLACE TIME Figure 1: Using the vector offset method to solve the analogy task (Mikolov et al., 2013c). MODAL MODAL V. somebody something somewhere sometimes can allow nobody nothing nowhere never cannot forbid (2) Everybody smiles: ∀x.person(x) → smiles(x) (3) Nobody smiles: ∀x.person(x) → ¬smiles(x) Somebody is existential: (4) Somebody smiles: ∃x.person(x) ∧ smiles(x) English has quantificational expressions that don’t fall into either category (three people, most things). Those are usually not encoded as a single English word, and are therefore not considered in this paper. 2.2 Polarity Quantifiers that can be expressed as a single word are in general either increasing or decreasing. A q"
S16-2001,W13-3209,0,0.0697813,"Missing"
S16-2001,D14-1162,0,0.0906918,"rsal Table 1: All of the words tested in the experiments (I NC = Increasing, D EC = Decreasing). text windows used to construct the VSM. We then report two experiments that examine the robustness of the results. First, we determine whether the level of performance that we expect from the VSMs is reasonable, by testing how well humans solve the same analogy problems. Second, we investigate how the quality of the representations is affected by the size of the training corpus. A large and constantly expanding range of VSM architectures have been proposed in the literature (Mikolov et al., 2013a; Pennington et al., 2014; Turney and Pantel, 2010). Instead of exploring the full range of architectures, the present study will focus on the skip-gram model, implemented in word2vec (Mikolov et al., 2013b). This model has been argued to perform either better than or on a par with competing architectures, depending on the task and on hyperparameter settings (Baroni et al., 2014; Levy et al., 2015). Particularly pertinent to our purposes, Levy et al. (2015) find that the skip-gram model tends to recover formal linguistic features more accurately than traditional distributional models. 2 I NC . Existential (5) a. b. Ev"
S16-2001,J15-4004,0,0.0142612,"in the space. VSMs are typically trained on a corpus in an unsupervised way; the goal is for words that occur in similar contexts to be assigned similar representations. The context of a word in a corpus is often defined as the set of words that occur in a small window around the word of interest (Lund and Burgess, 1996; Turney and Pantel, 2010). VSM representations have been shown to be useful in improving the performance of NLP systems (Turian et al., 2010; Bansal et al., 2014) as well as in predicting cognitive measures such as similarity judgments and semantic priming (Jones et al., 2006; Hill et al., 2015). man is to woman as king is to . The system is expected to infer the relation between the first two words—man and woman—and find a word that stands in the same relation to king. When this task is solved using the offset method, there is no explicit set of relations that the system is trained to identify. We simply subtract the vector for man from the vector for woman and add it to king. If the offset woman − man represents an abstract gender feature, adding that offset to king should lead us to queen (Figure 1). In the rest of this paper, we describe the set of analogy problems that we used t"
S16-2001,P10-1040,0,0.036897,"ce models of lexical semantics (VSMs) represent words as points in a high-dimensional space. Similar words are represented by points that are close together in the space. VSMs are typically trained on a corpus in an unsupervised way; the goal is for words that occur in similar contexts to be assigned similar representations. The context of a word in a corpus is often defined as the set of words that occur in a small window around the word of interest (Lund and Burgess, 1996; Turney and Pantel, 2010). VSM representations have been shown to be useful in improving the performance of NLP systems (Turian et al., 2010; Bansal et al., 2014) as well as in predicting cognitive measures such as similarity judgments and semantic priming (Jones et al., 2006; Hill et al., 2015). man is to woman as king is to . The system is expected to infer the relation between the first two words—man and woman—and find a word that stands in the same relation to king. When this task is solved using the offset method, there is no explicit set of relations that the system is trained to identify. We simply subtract the vector for man from the vector for woman and add it to king. If the offset woman − man represents an abstract gend"
S16-2001,J06-3003,0,0.150843,"Missing"
S16-2001,Q13-1015,0,\N,Missing
S19-1026,J99-2004,0,0.0608644,"Missing"
S19-1026,W15-2712,0,0.0230209,"cture constant. We ask whether the linguistic properties implicitly captured by pretraining objectives measurably affect the types of linguistic information encoded in the learned representations. To this end, we explore whether qualitatively different objectives lead to demonstrably different sentence representations. We focus our analysis on function words because they play a key role in compositional meaning—e.g., introducing and identifying discourse referents or representing relationships between entities or ideas—and are not yet considered to be well-modeled by distributional semantics (Bernardi et al., 2015). Our results suggest that different pretraining objectives give rise to differences in function word comprehension; for instance, we see that natural language inference helps understanding negation, and CCG supertagging helps recognizing meaningful sentence boundaries. However, overall, we find that the observed differences are not always straightforwardly interpretable, and further investigation is needed to determine which specific aspects of pretraining tasks yield good representations of function words. The analyses we present contribute new results in an ongoing line of research aimed at"
S19-1026,N19-1423,0,0.201655,"n of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguistic properties implicitl"
S19-1026,W16-2524,0,0.0505528,"aining and probing datasets. A regression analysis shows that vocabulary overlap overall is not a significant predictor of performance on the probing set (p = 0.39). No single probing set performance was significantly affected by vocabulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use simil"
S19-1026,W17-5401,0,0.0333637,"Missing"
S19-1026,W18-5426,0,0.050884,"bulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper compr"
S19-1026,N18-1038,0,0.0609676,"Missing"
S19-1026,W17-5405,0,0.026707,"Missing"
S19-1026,C18-1198,0,0.038925,"probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentencec"
S19-1026,N18-1202,0,0.406307,"elps the comprehension of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguisti"
S19-1026,N18-2082,1,0.879934,"Missing"
S19-1026,W18-5441,1,0.897134,"Missing"
S19-1026,P18-1079,0,0.0486921,"language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the rec"
S19-1026,W18-5409,0,0.0502653,"Missing"
S19-1026,P18-1018,0,0.0235763,"s none of them correctly (0/6). Here is an example of a numeric usage of below that the NLI model answered correctly but the image model answered incorrectly: P: Only those whose incomes do not exceed 125 percent of the federal poverty level qualify . . . H: Those whose incomes are below 125 percent qualify . . . (P→H) The image model’s bias towards the spatial usage is intuitive, since the numeric usage of below (i.e., as a counterpart to exceed) is difficult to learn from visual clues only. This concrete-abstract duality, which is not specific to below but common to most other prepositions (Schneider et al., 2018), may partially explain why the image-caption model behaves so differently from all other models, which are not trained on a multimodal objective. 4.3 Data Size and Genre Effects As can be seen from the varying sizes of the pretraining dataset reported in Figure 1, seeing more data during pretraining does not imply better performance on probing tasks. Also, as noted before, the fact that pretraining can hurt performance suggests that if the task is not the “right” task, adding more datapoints during pretraining can lead models to learn counterproductive representations. Another potential confo"
S19-1026,W17-5410,0,0.0296726,"Missing"
S19-1026,W17-2625,0,0.063243,"Missing"
S19-1026,W18-5446,1,0.80362,"o preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentenceclassification format. The primary difference in our work is that we focus specifically on the understanding of function words in context. We also present a suite of several tasks, but each one focuses on a particular structure, whereas tasks proposed in the works above generally aggregate multiple phenomena. Each of our tasks isolates each function word type and employ a targeted modification strategy that gives us a more narrowlyfocused, informative scope of analysis. 6 Conclusion We propose a new challenge set of nine tasks th"
S19-1026,I17-1100,1,0.878632,"Missing"
S19-1026,N18-1101,1,0.788304,"dex (σ = 2) and rounding to the nearest integer. 2.3 NLI-Based Tasks Our NLI-based probing tasks ask whether the choice of function word affects the inferences licensed by a sentence. These tasks consist of a pair of sentences—a premise p and a hypothesis h— and ask whether or not p entails h. We exploit the label changes induced by a targeted mutation of 3 We use WikiText instead of BWB because adjacent sentences in BWB are not logically contiguous and therefore may not be from the same discourse context. the sentence pairs taken from the Multi-genre Natural Language Inference dataset (MNLI, Williams et al., 2018). The rationale is that, if a change to a single function word in the premise changes the entailment label, that function word must play a significant role in the semantics of the sentence. Prepositions We manually curate a list of prepositions (see Appendix D) that are likely to be swapped with each other without affecting the grammaticality of the sentence. We generate mutated NLI pairs by finding occurrences of the prepositions in our list and randomly replacing them with other prepositions in the list. Our list consists of a set of locatives4 and several other manually-selected preposition"
S19-1026,Q17-1027,1,0.86489,"Missing"
S19-1026,P18-2100,0,0.0613273,"Missing"
W14-2002,D09-1034,0,0.390769,"competition among potential outcomes should result in increased processing load at the point at which the competing parses are still valid (McRae et al., 1998; Tabor and Tanenhaus, 1999). This contrasts with the entropy reduction hypothesis, according to which processing cost arises when competition is resolved. Intuitively, the two hypotheses make inversely correlated predictions: on average, there will be less competition following words that reduce entropy. A recent study found that reading times on wi correlated positively with entropy following wi , providing support for this hypothesis (Roark et al., 2009). The fourth hypothesis we consider, which we term the commitment hypothesis, is derived from Predictable linguistic elements are processed faster than unpredictable ones. Specifically, processing load on an element A in context C is linearly correlated with its surprisal, − log2 P (A|C) (Smith and Levy, 2013). This suggests that readers maintain expectations as to the upcoming elements: likely elements are accessed or constructed in advance of being read. While there is substantial amount of work on the effect of predictability on processing difficulty, the role (if any) of the distribution o"
W14-2002,P03-1054,0,0.0325577,"Missing"
W14-2002,J93-2004,0,0.0452611,"n for the recurrence is given by H = (I − A)−1 h (5) 3.2 where I is the identity matrix. The entropy after the first n words of the sentence, Hwn , can be calculated by applying Equation 5 to the grammar formed by intersecting the original grammar with the prefix w1 , . . . , wn (i.e., considering only the parses that are compatible with the words encountered so far) (Hale, 2006). Two points are worth noting about these equations. First, Equation 5 shows that calculating the entropy of a PCFG requires inverting the matrix Constructing the grammar We used a PCFG induced from the Penn Treebank (Marcus et al., 1993). As mentioned above, the grammar was mostly unlexicalized; however, in order for the predictions to depend on the identity of the verb, the grammar had to contain lexically specific rules for each verb. We discuss these rules at end of this section. The Penn Treebank tag set is often expanded by adding to each node’s tag an annotation of the 14 node’s parent, e.g., marking an NP whose parent is a VP as NP VP (Klein and Manning, 2003). While systematic parent annotation would have increased the size of the grammar dramatically, we did take the following minimal steps to improve parsing accurac"
W16-2503,P14-1023,0,0.0158632,"5 .00 .71 .60 Base to gerund .66 .67 .52 .37 .24 .00 .71 .64 Gerund to past .57 .63 .17 .25 .06 .00 .46 .15 Base to third person .60 .67 .20 .32 .07 .00 .69 .40 Adj. to adverb .33 .34 .22 .14 .05 .00 .23 .16 Adj. to comparative .86 .86 .36 .50 .00 .00 .59 .17 Adj. to superlative .59 .69 .03 .19 .00 .00 .43 .15 Adj. un− prefixation .38 .39 .17 .12 .01 .00 .36 .24 b) ) y− dd nl (A d d se ev er se (O te Va n R R ev er d− op po si b Ad Ad illa .00 −a .62 d Analogy problems: We use the analogy dataset proposed by Mikolov et al. (2013a). This dataset, which has become a standard VSM evaluation set (Baroni et al., 2014; Faruqui et al., 2015; Schnabel et al., 2015; Zhai et al., 2016), contains 14 categories; see Table 1 for a full list. A number of these categories, sometimes referred to as “syntactic”, test whether the structure of the space captures simple morphological relations, such as the relation between the base and gerund form of a verb (scream : screaming). Others evaluate the knowledge that the space encodes about the world, e.g., the relation between a country and its currency (latvia : lats). A final category that doesn’t fit neatly into either of those groups is the relation between masculine a"
W16-2503,N15-1184,0,0.0292603,"gerund .66 .67 .52 .37 .24 .00 .71 .64 Gerund to past .57 .63 .17 .25 .06 .00 .46 .15 Base to third person .60 .67 .20 .32 .07 .00 .69 .40 Adj. to adverb .33 .34 .22 .14 .05 .00 .23 .16 Adj. to comparative .86 .86 .36 .50 .00 .00 .59 .17 Adj. to superlative .59 .69 .03 .19 .00 .00 .43 .15 Adj. un− prefixation .38 .39 .17 .12 .01 .00 .36 .24 b) ) y− dd nl (A d d se ev er se (O te Va n R R ev er d− op po si b Ad Ad illa .00 −a .62 d Analogy problems: We use the analogy dataset proposed by Mikolov et al. (2013a). This dataset, which has become a standard VSM evaluation set (Baroni et al., 2014; Faruqui et al., 2015; Schnabel et al., 2015; Zhai et al., 2016), contains 14 categories; see Table 1 for a full list. A number of these categories, sometimes referred to as “syntactic”, test whether the structure of the space captures simple morphological relations, such as the relation between the base and gerund form of a verb (scream : screaming). Others evaluate the knowledge that the space encodes about the world, e.g., the relation between a country and its currency (latvia : lats). A final category that doesn’t fit neatly into either of those groups is the relation between masculine and feminine versions o"
W16-2503,N16-2002,0,0.507507,"lines, and therefore the most evidence for consistent offsets. We suggest that future studies employing the analogy task report the performance of the simple baselines we have suggested, in particular O NLYB and possibly also I GNORE - A . Other methods for evaluating the consistency of vector offsets may be less vulnerable to trivial responses and neighborhood structure, and should be considered instead of the offset method (Dunbar et al., 2015). Our results also highlight the difficulty in comparing spaces based on accuracy measures averaged across heterogeneous and unbalanced analogy sets (Gladkova et al., 2016). Spaces with similar overall accuracy can vary in their success on particular categories of analogies; effective representations of “world-knowledge” information are likely to be useful for different downstream tasks than effective representations of formal linguistic properties. Greater attention to the fine-grained strengths of particular spaces may lead to the development of new spaces that combine these strengths. Acknowledgments I thank Ewan Dunbar, Emmanuel Dupoux, Omer Levy and Benjamin Spector for comments and discussion. This research was supported by the European Research Council (g"
W16-2503,W14-1618,0,0.583755,"c). cosine similarity to the landing point. Formally, if the analogy is given by a : a∗ :: b : Introduction (1) where in our example a is debug, a∗ is debugging and b is scream, then the proposed answer to the analogy problem is Vector space models of semantics (VSMs) represent words as points in a high-dimensional space (Turney and Pantel, 2010). There is considerable interest in evaluating VSMs without needing to embed them in a complete NLP system. One such intrinsic evaluation strategy that has gained in popularity in recent years uses the offset approach to solving word analogy problems (Levy and Goldberg, 2014; Mikolov et al., 2013c; Mikolov et al., 2013a; Turney, 2012). This method assesses whether a linguistic relation — for example, between the base and gerund form of a verb (debug and debugging) — is consistently encoded as a particular linear offset in the space. If that is the case, estimating the offset using one pair of words related in a particular way should enable us to go back and forth between other pairs of words that are related in the same way, e.g., scream and screaming in the base-to-gerund case (Figure 1). Since VSMs are typically continuous spaces, adding the offset between debu"
W16-2503,S16-2001,1,0.838213,"nce on currencies and adjectives-to-adverbs was poor, while performance on capitals and comparatives was high. Although A DD and M ULTIPLY always outperformed the baselines, the margin varied widely across categories. The most striking case is the plurals category, where the accuracy of O NLY- B reached .70, and even A DD - OPPOSITE achieved Semantic spaces: In addition to comparing the performance of the analogy functions within a single VSM, we seek to understand to what extent this performance can differ across VSMs. To this end, we selected three VSMs out of the set of spaces evaluated by Linzen et al. (2016). All three spaces were produced by the skip-gram with negative sampling algorithm implemented in word2vec (Mikolov et al., 2013b), and were trained on the concatenation of ukWaC (Baroni et al., 2009) and a 2013 dump of the English Wikipedia. The spaces, which we refer to as s2 , s5 and s10 , differed only in their context window parameters. In s2 , the window consisted of two words on ei15 Space A DD A DD - I GNORE - A A DD - O NLY- B s2 s5 s10 .53 .6 .58 .41 .29 .26 .42 .36 .33 Table 2: Overall scores and the advantage of A DD over two of the baselines across spaces. Add Add − Ignore−a Add −"
W16-2503,N13-1090,0,0.930254,"ity tal.linzen@ens.fr Abstract debugging The offset method for solving word analogies has become a standard evaluation tool for vector-space semantic models: it is considered desirable for a space to represent semantic relations as consistent vector offsets. We show that the method’s reliance on cosine similarity conflates offset consistency with largely irrelevant neighborhood structure, and propose simple baselines that should be used to improve the utility of the method in vector space evaluation. 1 screaming? debug scream Figure 1: Using the vector offset method to solve the analogy task (Mikolov et al., 2013c). cosine similarity to the landing point. Formally, if the analogy is given by a : a∗ :: b : Introduction (1) where in our example a is debug, a∗ is debugging and b is scream, then the proposed answer to the analogy problem is Vector space models of semantics (VSMs) represent words as points in a high-dimensional space (Turney and Pantel, 2010). There is considerable interest in evaluating VSMs without needing to embed them in a complete NLP system. One such intrinsic evaluation strategy that has gained in popularity in recent years uses the offset approach to solving word analogy problems ("
W16-2503,D15-1036,0,0.10213,"7 .24 .00 .71 .64 Gerund to past .57 .63 .17 .25 .06 .00 .46 .15 Base to third person .60 .67 .20 .32 .07 .00 .69 .40 Adj. to adverb .33 .34 .22 .14 .05 .00 .23 .16 Adj. to comparative .86 .86 .36 .50 .00 .00 .59 .17 Adj. to superlative .59 .69 .03 .19 .00 .00 .43 .15 Adj. un− prefixation .38 .39 .17 .12 .01 .00 .36 .24 b) ) y− dd nl (A d d se ev er se (O te Va n R R ev er d− op po si b Ad Ad illa .00 −a .62 d Analogy problems: We use the analogy dataset proposed by Mikolov et al. (2013a). This dataset, which has become a standard VSM evaluation set (Baroni et al., 2014; Faruqui et al., 2015; Schnabel et al., 2015; Zhai et al., 2016), contains 14 categories; see Table 1 for a full list. A number of these categories, sometimes referred to as “syntactic”, test whether the structure of the space captures simple morphological relations, such as the relation between the base and gerund form of a verb (scream : screaming). Others evaluate the knowledge that the space encodes about the world, e.g., the relation between a country and its currency (latvia : lats). A final category that doesn’t fit neatly into either of those groups is the relation between masculine and feminine versions of the same concept (gro"
W16-2503,Q15-1016,0,\N,Missing
W16-2513,W16-2502,0,0.0704796,"sult human raters asked to quantify these relations must carry out some interpretations of their own with respect to the task, in order to settle upon a judgment schema and apply that schema to rate word pairs. The fact that the definition of the relation structure is left to the annotator’s judgment introduces interannotator variability as well as potentially undesirable properties of human similarity judgments: for example, the fact that they are not symmetric (Tversky, 1977). The subjectivity of this task, and the involvement of the conscious reasoning process needed to arrive at a rating (Batchkarov et al., 2016), raise the question: to what extent does the relation structure that emerges from such rating tasks reliably reflect the relation structure that underlies human language understanding? After all, humans process language effortlessly, and natural language comprehension does not require reasoning about how similar or related words are. This does not mean that the brain does not perform computations reflecting relations between words—evidence suggests that such computations occur constantly in language processing, but that these computations occur on a subconscious level (Kutas and Federmeier, 2"
W16-2513,W09-3207,0,0.826536,"Missing"
W16-2513,J15-4004,0,0.0682331,"ween pairs of word vectors mirror relations between the words that correspond to those vectors. This evaluation method requires us to select a word relation metric that can serve as ground truth, and it requires us to identify the particular types of relations that we would like our models to represent accurately. Typical approaches to VSM evaluation use human annotations as ground truth: in particular, similarity ratings for pairs of words. Some evaluation datasets focus on similarity per se: hotscalding would rate highly, while antonyms like hot-cold and associates like hot-stove would not (Hill et al., 2015). Others do not distinguish similarity from other types of relations: synonyms, antonyms and associates can all receive high ratings (Bruni et al., 2014). While the distinction between similarity and relatedness is important, it represents only a preliminary step toward a more precise understanding of what we mean—and what we should mean—when 72 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 72–77, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics exploring the ability of various VSMs to predict this measure, and discu"
W16-2513,W13-2608,0,0.130757,"eted as reflecting the strength of the relation. Since priming results provide us with a humangenerated quantification of relations between word pairs, without requiring participants to make conscious decisions about relatedness—the task that participants are performing is unrelated to the question of relatedness—this measure is a strong candidate for tapping into subconscious properties of word relations in the human brain. Several studies have already shown correspondence between priming magnitude and VSM measures of relation such as cosine similarity or neighbor rank (Mandera et al., 2016; Lapesa and Evert, 2013; Jones et al., 2006; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009; McDonald and Brew, 2004). These positive results suggest that some of the implicit relation structure in the human brain is already reflected in current vector space models, and that it is in fact feasible to evaluate relation structure of VSMs by testing their ability to predict this implicit human measure. However, to our knowledge, there has not yet been an effort to identify or tailor a priming dataset such that it is ideally suited to evaluation of VSMs. Experimental setup Cognitive measurements Most previous work ha"
W16-2513,P04-1003,0,0.182804,"enerated quantification of relations between word pairs, without requiring participants to make conscious decisions about relatedness—the task that participants are performing is unrelated to the question of relatedness—this measure is a strong candidate for tapping into subconscious properties of word relations in the human brain. Several studies have already shown correspondence between priming magnitude and VSM measures of relation such as cosine similarity or neighbor rank (Mandera et al., 2016; Lapesa and Evert, 2013; Jones et al., 2006; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009; McDonald and Brew, 2004). These positive results suggest that some of the implicit relation structure in the human brain is already reflected in current vector space models, and that it is in fact feasible to evaluate relation structure of VSMs by testing their ability to predict this implicit human measure. However, to our knowledge, there has not yet been an effort to identify or tailor a priming dataset such that it is ideally suited to evaluation of VSMs. Experimental setup Cognitive measurements Most previous work has modeled small priming datasets. By contrast, we follow Mandera et al. (2016) in taking advantag"
W16-2513,J07-2002,0,0.587442,"Missing"
W16-2513,U11-1007,0,0.0351926,"Missing"
W16-2513,D14-1162,0,0.0842216,"et word. We assess the usefulness of each of the methods for evaluating VSMs, in order to identify the methodological choices that generate optimal data for evaluation. A second advantage of the SPP is that it contains annotations of the relation types of the word pairs; this property can allow for finer-grained analyses that focus on relations of particular interest, as we will discuss in greater detail below. 3.2 Vector-space models We trained four word-level VSMs for testing: skip-gram (Mikolov et al., 2013) with window sizes of 5 and 15 words (referred to as SG5 and SG15 below) and GloVe (Pennington et al., 2014) with window sizes of 5 and 15 words (Gl5 and Gl15). All models were trained on a concatenation of English Wikipedia and English GigaWord using their default parameters and dimensionality of 100. A fifth model (referred to as SG5n) was gen73 erated by adding uniform random noise U(-2,2) to the vectors of the SG5 model, as an example of a model that we would expect to perform poorly. 3.3 Evaluation We evaluated the VSMs by fitting linear regression models to the human response times, with cosine similarity between prime and target as the predictor of interest.1 As a simple baseline model, we en"
W16-2513,D15-1036,0,0.0802724,"Evaluation We evaluated the VSMs by fitting linear regression models to the human response times, with cosine similarity between prime and target as the predictor of interest.1 As a simple baseline model, we entered only word frequency as a predictor. Word frequency is widely recognized as a strong predictor of reaction time in language tasks (Rubenstein et al., 1970). While it is only one among the factors known to affect the speed of word recognition (Balota et al., 2004), it is by far the most important, and unlike factors such as word length, it is represented in many vector space models (Schnabel et al., 2015), making it all the more important to control for here. 4 4.1 Relation Example pair Synonym Antonym Forward phrasal associate Script Category Supraordinate Instrument Functional property Backward phrasal associate Perceptual property Action presume, assume asleep, awake human, being ambulance, emergency celery, carrot disaster, earthquake rake, leaves airplane, fly lobe, ear fire, hot quench, thirst Table 1: Annotated relations in SPP nullifies that improvement. This suggests that the additional variance accounted for by the four normal VSMs is indeed a reflection of their quality. 4.2 Results"
W19-0101,L18-1012,0,0.232987,"findings is that what appears to be an effect of the predictablity of wt+1 is a confound driven by the relationship between the predictability of wt+1 and an underlying property of wt . Angele et al. (2015) hypothesized that the property of wt that is confounded with the predictability of wt+1 is the reader’s uncertainty about the words that could follow wt , but they did not test this hypothesis. The present paper directly evaluates the relation between successor surprisal and uncertainty estimated from a single RNN language model (Gulordava et al., 2018). We use a selfpaced reading corpus (Futrell et al., 2018), in which parafoveal preview is unavailable. To anticipate our results, we do not find evidence that the effect of successor surprisal can be reduced to uncertainty. We then explore the hypothesis that processing limitations, which lead to uncertainty 1 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 1-7. New York City, New York, January 3-6, 2019 being calculated over a restricted number of probable words rather than over the entire vocabulary, could account for these conflicting results, with similarly negative results. We conclude that uncertainty is unlikely t"
W19-0101,N18-1108,1,0.919581,"es to readers, then, the only possible explanation for these findings is that what appears to be an effect of the predictablity of wt+1 is a confound driven by the relationship between the predictability of wt+1 and an underlying property of wt . Angele et al. (2015) hypothesized that the property of wt that is confounded with the predictability of wt+1 is the reader’s uncertainty about the words that could follow wt , but they did not test this hypothesis. The present paper directly evaluates the relation between successor surprisal and uncertainty estimated from a single RNN language model (Gulordava et al., 2018). We use a selfpaced reading corpus (Futrell et al., 2018), in which parafoveal preview is unavailable. To anticipate our results, we do not find evidence that the effect of successor surprisal can be reduced to uncertainty. We then explore the hypothesis that processing limitations, which lead to uncertainty 1 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 1-7. New York City, New York, January 3-6, 2019 being calculated over a restricted number of probable words rather than over the entire vocabulary, could account for these conflicting results, with similarly ne"
W19-0101,N01-1021,0,0.276628,"ty 1 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 1-7. New York City, New York, January 3-6, 2019 being calculated over a restricted number of probable words rather than over the entire vocabulary, could account for these conflicting results, with similarly negative results. We conclude that uncertainty is unlikely to be the only explanation for successor surprisal effects. 2 Surprisal and entropy The relationship between the reading time at word wt and the conditional probability of wt is logarithmic (Smith and Levy, 2013); in other words, if we use surprisal (Hale, 2001) as our probability measure: surprisal(wt ) = log P(wt |w1...t 1) Figure 1: Successor surprisal plotted against entropy for each word in the Natural Stories Corpus. The Pearson correlation is 0.45, providing empirical validation of the theoretically strong limit-case relation between entropy and successor surprisal. (1) then there is a linear correlation between RT(wt ) and surprisal(wt ). Surprisal has been shown to be a strong predictor of reading time in linear regression models (e.g., Demberg and Keller, 2008; Roark et al., 2009). Successor surprisal is simply the surprisal of the next obs"
W19-0101,D09-1034,0,0.21706,"rithmic (Smith and Levy, 2013); in other words, if we use surprisal (Hale, 2001) as our probability measure: surprisal(wt ) = log P(wt |w1...t 1) Figure 1: Successor surprisal plotted against entropy for each word in the Natural Stories Corpus. The Pearson correlation is 0.45, providing empirical validation of the theoretically strong limit-case relation between entropy and successor surprisal. (1) then there is a linear correlation between RT(wt ) and surprisal(wt ). Surprisal has been shown to be a strong predictor of reading time in linear regression models (e.g., Demberg and Keller, 2008; Roark et al., 2009). Successor surprisal is simply the surprisal of the next observation in a sequence: succ. surprisal(wt ) = corpus: ˆ )⇡ H(T (6) t=1 |T | X 1 = surprisal(wt+1 ) |T| log P(wt+1 |w1...t ) (2) = surprisal(wt+1 ) |T | X 1 log P(wt+1 |w1...t ) |T| (7) t=1 (3) Therefore, if uncertainty over possible continuations influences reading time, then successor surprisal could be correlated with reading time simply due to its relationship with corpus-level entropy. Importantly for the present study, if the relationship to uncertainty is the sole underlying reason that successor surprisal can predict reading"
W19-0101,W16-4104,1,0.886251,"Missing"
W19-0101,W18-5423,0,0.0122261,"ulordava et al. (2018) on 90 million words from the English Wikipedia. The model had two LSTM layers with 650 hidden units each, 650-dimensional word embeddings, a dropout rate of 0.2 and batch size 128, and was trained for 40 epochs (with early stopping). Unlike grammar-based language models, RNN language models do not explicitly construct syntactic dependencies, which are essential in human sentence comprehension. However, recent work has shown that RNN language models are nevertheless sensitive to the probability of syntactic structures (Linzen et al., 2016; van Schijndel and Linzen, 2018; Wilcox et al., 2018), tentatively suggesting that they are an adequate substitute for modeling human reading behavior. Importantly, they have the added benefit that all our measures of interest are easy to calculate using Equations 1, 2, and 5 on the model’s softmax layer, which provides a conditional probability distribution over the upcoming word given the preceding words. Successor surprisal predicts reading time: Before testing whether entropy can account for the effectiveness of successor surprisal in predicting reading time, we first verified that our successor surprisal measure was positively correlated wi"
