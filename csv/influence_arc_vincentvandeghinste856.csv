2005.mtsummit-ebmt.17,2005.mtsummit-ebmt.1,0,0.177709,"on. This approach was also used by (Veale and Way, 1997), (Nirenburg et al. 1994), and (Brown, 1996). Extending the resources and integrating new languages using post-editing facilities. Adopting semi-automated techniques for adapting the system to different translation needs. Taking into account real user needs, especially 135 as far as the post-editing facilities are concerned. Figure 1: General System Flow This paper describes the approach of the Centre for Computational Linguistics within the METIS consortium. Other approaches can be found in (Markantonatou et al., 2005, this volume) and (Badia et al., 2005, this volume). The experiments in this article are part of the investigations in the breaking of the sentence-internal barriers. We use noun phrase (NP) translation as a test case. Our NP translation system differs from the approach explained in (Sato, 1993), in that we do not use parallel corpora, but a bilingual dictionary, and that our system is not domain specific. We also use a different weighing mechanism (cf. section 2.3.2). Dutch is used as a source language with the partsof-speech tagset of (Van Eynde, 2004). English is used as a target language, the British National Corpus (BNC) as"
2005.mtsummit-ebmt.17,C96-1030,0,0.0452547,"ources than to create a large enough parallel corpus that links the source language with the target language. METIS-I aimed at constructing free text translations by relying on pattern matching techniques and by retrieving the basic stock for translations from large monolingual corpora. METIS-II aims at further enhancing the system’s performance and adaptability by: Breaking sentence-internal barriers: the system will retrieve pieces of sentences (chunks) and will recombine them to produce a final translation. This approach was also used by (Veale and Way, 1997), (Nirenburg et al. 1994), and (Brown, 1996). Extending the resources and integrating new languages using post-editing facilities. Adopting semi-automated techniques for adapting the system to different translation needs. Taking into account real user needs, especially 135 as far as the post-editing facilities are concerned. Figure 1: General System Flow This paper describes the approach of the Centre for Computational Linguistics within the METIS consortium. Other approaches can be found in (Markantonatou et al., 2005, this volume) and (Badia et al., 2005, this volume). The experiments in this article are part of the investigations in"
2005.mtsummit-ebmt.17,2005.mtsummit-ebmt.3,0,0.219868,"Missing"
2005.mtsummit-ebmt.17,2003.eamt-1.7,0,0.105814,"e corpus, but not of any parallel corpora. In the current paper, we present the results of the first experiments with our approach (CCL) within the METIS consortium : the translation of noun phrases from Dutch to English, using the British National Corpus as a target language corpus. Future research is planned along similar lines for the sentence as is presented here for the noun phrase. 1 Introduction: Background of METIS-II The METIS approach differs from other known statistical or example-based approaches to machine translation in that it does not make use of parallel corpora (or bitexts) (Dologlou et al., 2003). It is conceived as a system to be used in those circumstances in which other MT-systems that are around cannot be used, for example, because there are no sufficiently large parallel corpora available, at least not in the given domain (be it a specific subdomain, such as the automotive domain, or the domain of free language) and/or for a given language pair. The latter will often be the case in the European context when smaller languages are involved. Constructing a rule based system would take too much time (and therefore be too costly). An alternative solution would be to use a hybrid syste"
2005.mtsummit-ebmt.17,1999.tc-1.8,0,0.0122471,"-internal barriers. We use noun phrase (NP) translation as a test case. Our NP translation system differs from the approach explained in (Sato, 1993), in that we do not use parallel corpora, but a bilingual dictionary, and that our system is not domain specific. We also use a different weighing mechanism (cf. section 2.3.2). Dutch is used as a source language with the partsof-speech tagset of (Van Eynde, 2004). English is used as a target language, the British National Corpus (BNC) as target-language corpus with the CLAWS5 tagset. A reason why not to use the world wide web as a resource like (Grefenstette, 1999) is that our corpus needs to be preprocessed (tagged, chunked, lemmatized) and our target language is English from native speakers. For a more extensive description of the METIS system see (Dirix et al., 2005, this volume). 2 System Description In this section we describe our prototype system, which is used in the experiments in section 3, and which is implemented in perl 5.8.5 (Wall, 2004). In figure 1, we present the general system flow (at the sentence level). The prototype we use is part of this general system as it translates noun phrase chunks. First we describe how the source language a"
2005.mtsummit-ebmt.17,2005.mtsummit-ebmt.12,0,0.152168,"Missing"
2005.mtsummit-ebmt.17,1993.tmi-1.5,0,0.0198202,"tion needs. Taking into account real user needs, especially 135 as far as the post-editing facilities are concerned. Figure 1: General System Flow This paper describes the approach of the Centre for Computational Linguistics within the METIS consortium. Other approaches can be found in (Markantonatou et al., 2005, this volume) and (Badia et al., 2005, this volume). The experiments in this article are part of the investigations in the breaking of the sentence-internal barriers. We use noun phrase (NP) translation as a test case. Our NP translation system differs from the approach explained in (Sato, 1993), in that we do not use parallel corpora, but a bilingual dictionary, and that our system is not domain specific. We also use a different weighing mechanism (cf. section 2.3.2). Dutch is used as a source language with the partsof-speech tagset of (Van Eynde, 2004). English is used as a target language, the British National Corpus (BNC) as target-language corpus with the CLAWS5 tagset. A reason why not to use the world wide web as a resource like (Grefenstette, 1999) is that our corpus needs to be preprocessed (tagged, chunked, lemmatized) and our target language is English from native speakers"
2005.mtsummit-ebmt.17,vandeghinste-2002-lexicon,1,0.761945,"get language tagset CLAWS5 is described on the UCREL website (University Centre for Computer Corpus Research on Language), http://www.comp.lancs.ac.uk/ucrel/claws5tags.html. Example LID() ADJ(prenom,basis) N(soort,ev,stan) If tokens are tagged as proper nouns in the source language, keep them as they are. If there are no translation alternatives, set the weight for the translated entry to 1. Check if the tokens are compounds. If this is the case, then translate the compounds’ modifier and head instead of the token as a whole. Here we use the same hybrid decompounding/compounding module as in (Vandeghinste, 2002), which is used in its decompounding mode. It takes a word (lemma or token) as its input and generates the word parts plus a confidence value. The modifier and the head are considered as separate tokens for the rest of the processing, and they are treated like dictionary entries which contain one word on the source into AT0 AJ0 NN0 or NN1 By combining the partial tag from the dictionary and the tag mapping rules, we can reduce a number of ambiguities which would otherwise arise. 2.3 Target Language Generation Generating the target language by using the BNC as a data-set of examples is a rather"
2005.mtsummit-ebmt.17,W04-1015,1,0.818393,"Missing"
2005.mtsummit-ebmt.17,vandeghinste-tjong-kim-sang-2004-using,1,0.878612,"Missing"
2005.mtsummit-ebmt.17,J03-3004,0,0.0220323,"clude that the approach adopted in our system works reasonably well for the translation of noun phrases. As this is work in progress (initial version of the code, the dictionary and the weighing system), we expect our system to perform better in future versions. NP translation is a substantial part of full sentence translation, but it is not safe to assume that because our approach works for noun phrase translation, it will work for full sentence translation. In NP translation from Dutch to English, there are not many word order issues to solve. Translating VPs is already much more difficult (Way and Gough, 2003), and we want to translate full sentences. There are also no agreement issues to solve, which certainly would be the case when translating full sentences (like the agreement between the subject and the verb). But still, as the approach seems promising, we plan to use the same strategy when implementing our full sentence translation system, although many issues will have to be solved during the process. 5 The Near and Not Too Distant Future In the near future, we plan to implement a full sentence translation system. In order to do so, there are a number of tasks which need to be executed. Among"
2005.mtsummit-ebmt.17,oostdijk-etal-2002-experiences,0,0.0327371,"Missing"
2005.mtsummit-ebmt.17,P02-1040,0,0.0760596,"Missing"
2005.mtsummit-ebmt.17,A00-1031,0,\N,Missing
2005.mtsummit-ebmt.6,2005.mtsummit-ebmt.3,0,0.183118,"Missing"
2005.mtsummit-ebmt.6,2003.eamt-1.7,0,0.0606471,"nd tools for both source and target language, making use of a target-language (TL) corpus, but not of any parallel corpora. In the current paper, we discuss the view of our team on the general philosophy and outline of the METIS-II system. 1 Introduction: Background of METIS-II The METIS-II project is an example-based machine translation project, which in principle does not make use of parallel corpora. As most other known example-based machine translation (and statistical) systems make use of parallel corpora or bitexts, our system is a new approach towards the automated translation problem (Dologlou et al., 2003), although e.g. Grefenstette (1999) made use of the world wide web in combination with a bilingual lexicon to translate compounds from Spanish and German to English. We deviced our system to be used in those circumstances where other machine translation systems are not available or of insufficient quality, because of lack of sufficiently large parallel corpora, in general or for the given domain, or because of the unavailability of the desired language pair. This is often the case in the European context as there is a high number of smaller languages. Building a rule-based system for language"
2005.mtsummit-ebmt.6,2005.mtsummit-ebmt.12,0,0.0732116,"emmatised. So we need a chunker and lemmatiser for English as well. The approach described below differs from the one adopted in METIS-I in that sentences are cut up in smaller chunks; linguistic information is also used outside the mapping rules; the TL corpus is indexed in different ways in order to increase the time efficiency; a general-purpose working prototype is built. In a first stage, the consortium partners conduct separate experiments on different ways of chunking (no chunking, grammatical chunking, n-grams), indexing, and creating a search engine. Other approaches can be found in (Markantonatou et al., 2005) and (Badia et al., 2005). METIS-II (like METIS-I) targets the construction of free text translations making use of patternmatching techniques and target-language retrieval from a large monolingual TL corpus. The system’s performance and adaptability is enhanced by: breaking sentence-internal barriers: the system retrieves pieces of sentences (chunks) and recombines them to produce a final translation; extending the resources and integrating new languages; using post-editing facilities; adopting semi-automated techniques for adapting the system to different translation needs; taking into accou"
2005.mtsummit-ebmt.6,2005.mtsummit-ebmt.17,1,0.854048,"assigned probability scores, and it depends on these scores which translation is favoured. These scores also determine how the translated string is presented to the end user for treatment during post-editing; unreliable or doubtful translations are marked as such. Before post-editing takes place, postprocessing has been taken care of by the system itself (automatic ‘adjustment’ of agreement, morphological generation of terms and the like). In the next sections, we will describe of which modules METIS-II consists, and the requirements that are already clear (as we are still experimenting [cf. (Vandeghinste et al., 2005)], several things are still unclear). 3 General concepts Before the various modules are described, some more general concepts should be described as these play an important role in our system. 3.1 Universal data format The idea is to have one universal data format for all the data that go through the system. It is an XML format that can be read and produced by all the modules and tools involved. Each single module picks the parts it is interested in and adds further information when needed. The representation can be piped through the different processes and visualised in the GUI of the user en"
2005.mtsummit-ebmt.6,vandeghinste-tjong-kim-sang-2004-using,1,0.883813,"Missing"
2005.mtsummit-ebmt.6,2005.mtsummit-ebmt.1,0,0.0999422,"nd lemmatiser for English as well. The approach described below differs from the one adopted in METIS-I in that sentences are cut up in smaller chunks; linguistic information is also used outside the mapping rules; the TL corpus is indexed in different ways in order to increase the time efficiency; a general-purpose working prototype is built. In a first stage, the consortium partners conduct separate experiments on different ways of chunking (no chunking, grammatical chunking, n-grams), indexing, and creating a search engine. Other approaches can be found in (Markantonatou et al., 2005) and (Badia et al., 2005). METIS-II (like METIS-I) targets the construction of free text translations making use of patternmatching techniques and target-language retrieval from a large monolingual TL corpus. The system’s performance and adaptability is enhanced by: breaking sentence-internal barriers: the system retrieves pieces of sentences (chunks) and recombines them to produce a final translation; extending the resources and integrating new languages; using post-editing facilities; adopting semi-automated techniques for adapting the system to different translation needs; taking into account real user needs, espec"
2007.tc-1.2,C92-3126,0,0.0763252,"on together with the METIS system and the MATADOR system. A somewhat different approach using ideas from both SMT, RBMT, and EBMT is called Data-oriented Translation (DOT), which was first proposed by Poutsma (1998), and the first large scale implementation of this approach was done by Hearne (2005). DOT still requires parallel data, but this time, it concerns parallel treebanks, in which alignments have been made on several levels in the trees. By using linguistically motivated trees, combined with using translation examples, and statistical techniques like data oriented parsing (Scha, 1990; Bod, 1992), this approach borrows from the three different MT paradigms. The prototype presented in this paper is based on the METIS-II approach, which will be described in the next section. 2 The METIS-II approach The aim of the METIS-II approach is to allow development of MT systems for low resource languages. Therefore, we restricted ourselves to using only limited tools, which are available for lots of languages, or which can be easily adapted to the languages in focus. The METIS-II approach also tries addressing some of the weak points of the classic approaches. Only a limited set of rules is used,"
2007.tc-1.2,2006.amta-papers.3,0,0.0136966,"ge parser and a translation dictionary, but no transfer rules or complex interlingual representation. On the target side, rich symbolic resources like lexical semantics, categorial variations and subcategorization frames are used to overgenerate multiple structural variations from a syntactic dependency representation of the source language sentence, where all terminal nodes are translated by the dictionary. The overgeneration is constrained by several statistical target language models, including surface n-grams and structural n-grams. Context-based Machine Translation (CBMT) as described by Carbonell et al. (2006) is another approach somewhat similar to the METIS approach. It does not require parallel corpora either and relies heavily on the target language side. It has been implemented for Spanish to English translation, and requires an extensive target language corpus, and a full-form dictionary. It does not contain transfer rules 3 or interlingual representations, but instead relies on long n-grams. The principle is to produce many long n-gram candidate translations by finding those long ngrams that contain as many as possible of the potential word and phrase translations from the dictionary, and as"
2007.tc-1.2,2007.mtsummit-papers.10,0,0.0431523,"Missing"
2007.tc-1.2,J90-1003,0,0.123082,"r that language, resulting in a target language treebank. For Dutch as target language, we use the Lassy treebank (van Noord et al., 2006) and the Alpino treebank, (http://www.let.rug.nl/~vannoord/trees/) which are both publicly available. For English as TL, we use the Redwoods treebank (Oepen et al., 2002). We extend these treebanks with the respective sides of the parallel data, and will possibly extend them with more automatically annotated monolingual treebanks, as the need arises. Lexical selection amongst several translation alternatives is based on co-occurrence metrics (Dunning, 1993; Church and Hanks, 1990; Evert 2004; Evert and Krenn, 2004), and frequency metrics taking into account the syntactic environment of the word (which are similar to what we already did in Dirix et al. (2006)). This allows us to decide which of the translation alternatives for e.g. an adjective are most likely to go together with a specific noun, etc. 10 Target language generation needs to be performed based on the obtained target language trees. In the target language corpus database, we store (sub-)tree structure patterns combined with surface string information like word order for these trees, allowing us to generat"
2007.tc-1.2,copestake-flickinger-2000-open,0,0.0163437,"Missing"
2007.tc-1.2,2005.mtsummit-ebmt.6,1,0.851837,"artial matches, and how to solve overlapping partial matches and recombine the target side of the mapping fragments. This is not the case for SMT, in which the generated sentence will be based on 2 statistics derived from the aligned parallel corpus, making abstraction of the cases which are contained in the parallel corpus. In recent years, hybrid machine translation systems have been starting to emerge. Within the METIS-II-consortium the idea arose to avoid some of the problematic issues of the previous approaches, and develop a prototype for a new translation method (Dologlou et al., 2003; Dirix et al., 2005; Vandeghinste et al., 2006), which relies heavily on the target language generation side, combining techniques from RBMT, SMT, and EBMT. The system was implemented for four language pairs: Dutch to English, Modern Greek to English, Spanish to English, and German to English. A more detailed description is given in section 2. Another hybrid machine translation system is the Matador system (Habash and Dorr, 2002; Habash, 2003, 2004), which is somewhat similar to the METIS-II approach, in that it does not require parallel data. It translates from Spanish to English, and relies heavily on target l"
2007.tc-1.2,2003.eamt-1.7,0,0.0308196,"s done in the case of partial matches, and how to solve overlapping partial matches and recombine the target side of the mapping fragments. This is not the case for SMT, in which the generated sentence will be based on 2 statistics derived from the aligned parallel corpus, making abstraction of the cases which are contained in the parallel corpus. In recent years, hybrid machine translation systems have been starting to emerge. Within the METIS-II-consortium the idea arose to avoid some of the problematic issues of the previous approaches, and develop a prototype for a new translation method (Dologlou et al., 2003; Dirix et al., 2005; Vandeghinste et al., 2006), which relies heavily on the target language generation side, combining techniques from RBMT, SMT, and EBMT. The system was implemented for four language pairs: Dutch to English, Modern Greek to English, Spanish to English, and German to English. A more detailed description is given in section 2. Another hybrid machine translation system is the Matador system (Habash and Dorr, 2002; Habash, 2003, 2004), which is somewhat similar to the METIS-II approach, in that it does not require parallel data. It translates from Spanish to English, and relies"
2007.tc-1.2,J93-1003,0,0.0551807,"and grammar for that language, resulting in a target language treebank. For Dutch as target language, we use the Lassy treebank (van Noord et al., 2006) and the Alpino treebank, (http://www.let.rug.nl/~vannoord/trees/) which are both publicly available. For English as TL, we use the Redwoods treebank (Oepen et al., 2002). We extend these treebanks with the respective sides of the parallel data, and will possibly extend them with more automatically annotated monolingual treebanks, as the need arises. Lexical selection amongst several translation alternatives is based on co-occurrence metrics (Dunning, 1993; Church and Hanks, 1990; Evert 2004; Evert and Krenn, 2004), and frequency metrics taking into account the syntactic environment of the word (which are similar to what we already did in Dirix et al. (2006)). This allows us to decide which of the translation alternatives for e.g. an adjective are most likely to go together with a specific noun, etc. 10 Target language generation needs to be performed based on the obtained target language trees. In the target language corpus database, we store (sub-)tree structure patterns combined with surface string information like word order for these trees"
2007.tc-1.2,P03-1011,0,0.0193151,"containing words, phrases, clauses, and full sentences), which is based on two sources: manual entries and automatically extracted information coming from parallel corpora and translation memories. We use a Dutch-English dictionary from the METIS-II project. We are collecting parallel corpora like Europarl (Koehn, 2005), Acquis Communautaire (Steinberger et al., 2006), and the Dutch Parallel Corpus (which will be available soon). A translation company provides us with real translation memories and translated texts. 7 Figure 1: Architecture of the prototype 8 We apply sub-sentential alignment (Gildea, 2003; Tiedemann, 2003) on these parallel data so we can automatically extract dictionary entries at word, phrase, clause and sentence level, which will be added to the already existing dictionaries. 2. We make use of structure mapping rules, which are also based on two sources: manual entries (avoiding the black-box of statistical MT systems) and automatically extracted information coming form parallel corpora and translation memories, as is done in Lavoie et al. (2002), Probst et al., (2002), and Quirk et al. (2005). We generate parallel parse forests, and we automatically extract structure mappi"
2007.tc-1.2,2003.mtsummit-papers.20,0,0.022188,"ortium the idea arose to avoid some of the problematic issues of the previous approaches, and develop a prototype for a new translation method (Dologlou et al., 2003; Dirix et al., 2005; Vandeghinste et al., 2006), which relies heavily on the target language generation side, combining techniques from RBMT, SMT, and EBMT. The system was implemented for four language pairs: Dutch to English, Modern Greek to English, Spanish to English, and German to English. A more detailed description is given in section 2. Another hybrid machine translation system is the Matador system (Habash and Dorr, 2002; Habash, 2003, 2004), which is somewhat similar to the METIS-II approach, in that it does not require parallel data. It translates from Spanish to English, and relies heavily on target language generation. It is aimed at language pairs lacking resource symmetry. It employs symbolic and statistical target language resources, and requires a source language parser and a translation dictionary, but no transfer rules or complex interlingual representation. On the target side, rich symbolic resources like lexical semantics, categorial variations and subcategorization frames are used to overgenerate multiple stru"
2007.tc-1.2,habash-dorr-2002-handling,0,0.0173907,"ithin the METIS-II-consortium the idea arose to avoid some of the problematic issues of the previous approaches, and develop a prototype for a new translation method (Dologlou et al., 2003; Dirix et al., 2005; Vandeghinste et al., 2006), which relies heavily on the target language generation side, combining techniques from RBMT, SMT, and EBMT. The system was implemented for four language pairs: Dutch to English, Modern Greek to English, Spanish to English, and German to English. A more detailed description is given in section 2. Another hybrid machine translation system is the Matador system (Habash and Dorr, 2002; Habash, 2003, 2004), which is somewhat similar to the METIS-II approach, in that it does not require parallel data. It translates from Spanish to English, and relies heavily on target language generation. It is aimed at language pairs lacking resource symmetry. It employs symbolic and statistical target language resources, and requires a source language parser and a translation dictionary, but no transfer rules or complex interlingual representation. On the target side, rich symbolic resources like lexical semantics, categorial variations and subcategorization frames are used to overgenerate"
2007.tc-1.2,W02-1610,0,0.0191466,"es us with real translation memories and translated texts. 7 Figure 1: Architecture of the prototype 8 We apply sub-sentential alignment (Gildea, 2003; Tiedemann, 2003) on these parallel data so we can automatically extract dictionary entries at word, phrase, clause and sentence level, which will be added to the already existing dictionaries. 2. We make use of structure mapping rules, which are also based on two sources: manual entries (avoiding the black-box of statistical MT systems) and automatically extracted information coming form parallel corpora and translation memories, as is done in Lavoie et al. (2002), Probst et al., (2002), and Quirk et al. (2005). We generate parallel parse forests, and we automatically extract structure mapping rules, that describe the transformation from the source language structure onto the target language structure. These automatically extracted rules can be augmented with manually defined rules to address remaining translation issues. When a source language sentence is analysed, this can result in several parse trees. We match these trees and their subtrees with the source language side of the dictionary / parallel corpus / translation memory. By abstracting over s"
2007.tc-1.2,2005.mtsummit-papers.11,0,0.0264473,"sides this, they are based on the HPSG paradigm (Pollard and Sag, 1987, 1994), and so is Alpino, which will result in comparable structures for both source language and target language trees. To go from source to target language we use two paths: 1. We make use of a translation dictionary (containing words, phrases, clauses, and full sentences), which is based on two sources: manual entries and automatically extracted information coming from parallel corpora and translation memories. We use a Dutch-English dictionary from the METIS-II project. We are collecting parallel corpora like Europarl (Koehn, 2005), Acquis Communautaire (Steinberger et al., 2006), and the Dutch Parallel Corpus (which will be available soon). A translation company provides us with real translation memories and translated texts. 7 Figure 1: Architecture of the prototype 8 We apply sub-sentential alignment (Gildea, 2003; Tiedemann, 2003) on these parallel data so we can automatically extract dictionary entries at word, phrase, clause and sentence level, which will be added to the already existing dictionaries. 2. We make use of structure mapping rules, which are also based on two sources: manual entries (avoiding the black"
2007.tc-1.2,P05-1034,0,0.0118094,"ed texts. 7 Figure 1: Architecture of the prototype 8 We apply sub-sentential alignment (Gildea, 2003; Tiedemann, 2003) on these parallel data so we can automatically extract dictionary entries at word, phrase, clause and sentence level, which will be added to the already existing dictionaries. 2. We make use of structure mapping rules, which are also based on two sources: manual entries (avoiding the black-box of statistical MT systems) and automatically extracted information coming form parallel corpora and translation memories, as is done in Lavoie et al. (2002), Probst et al., (2002), and Quirk et al. (2005). We generate parallel parse forests, and we automatically extract structure mapping rules, that describe the transformation from the source language structure onto the target language structure. These automatically extracted rules can be augmented with manually defined rules to address remaining translation issues. When a source language sentence is analysed, this can result in several parse trees. We match these trees and their subtrees with the source language side of the dictionary / parallel corpus / translation memory. By abstracting over some features, and by allowing partial matches, r"
2007.tc-1.2,steinberger-etal-2006-jrc,0,0.0167958,"SG paradigm (Pollard and Sag, 1987, 1994), and so is Alpino, which will result in comparable structures for both source language and target language trees. To go from source to target language we use two paths: 1. We make use of a translation dictionary (containing words, phrases, clauses, and full sentences), which is based on two sources: manual entries and automatically extracted information coming from parallel corpora and translation memories. We use a Dutch-English dictionary from the METIS-II project. We are collecting parallel corpora like Europarl (Koehn, 2005), Acquis Communautaire (Steinberger et al., 2006), and the Dutch Parallel Corpus (which will be available soon). A translation company provides us with real translation memories and translated texts. 7 Figure 1: Architecture of the prototype 8 We apply sub-sentential alignment (Gildea, 2003; Tiedemann, 2003) on these parallel data so we can automatically extract dictionary entries at word, phrase, clause and sentence level, which will be added to the already existing dictionaries. 2. We make use of structure mapping rules, which are also based on two sources: manual entries (avoiding the black-box of statistical MT systems) and automatically"
2007.tc-1.2,vandeghinste-etal-2006-metis,1,0.886575,"Missing"
2007.tc-1.2,2006.jeptalnrecital-invite.2,0,0.0508636,"Missing"
2007.tc-1.2,van-noord-etal-2006-syntactic,1,0.775604,"Missing"
2007.tc-1.2,C00-2092,0,\N,Missing
2007.tc-1.2,P01-1025,0,\N,Missing
2007.tmi-papers.7,boutsis-etal-2000-robust,0,\N,Missing
2007.tmi-papers.7,P02-1040,0,\N,Missing
2007.tmi-papers.7,2006.eamt-1.30,0,\N,Missing
2009.eamt-1.21,C92-3126,0,0.0436585,"nd moving words and phrases. The post-editor can choose amongst several translation alternatives for the sentence, or for certain parts of the sentence. When a sentence is accepted the post-editing information is fed back into the system’s databases, updating the weights of both the translation model and the target language generation model. 2.2 Related Research The hybrid MT system described in the previous section is similar to the Data-Oriented Translation (DOT) approach, which was first proposed by Poutsma (1998) and further researched by Hearne (2005). DOT uses Data-Oriented Parse Trees (Bod, 1992), whereas we use either rule-based parsers based on a set of linguistic rules and a stochastic disambiguation component or we use stochastic parsers trained on a manually parsed or corrected treebank. The DOT approach only uses small corpora and a limited domain, whereas we intend to use large corpora and a general domain (news). The target language generation approach is somewhat similar to the feature templates used by the translation candidate reranking component of Velldal (2007), although there are some important differences: Velldal’s feature templates can have a higher depth, whereas th"
2009.eamt-1.21,P03-1054,0,0.0321446,"Missing"
2009.eamt-1.21,2005.mtsummit-papers.11,0,0.0874032,"existing tools and resources to set up an MT architecture much like a classic rulebased transfer system. Instead of manually designing the rules, we intend to derive them from large parallel and monolingual (uncorrected) treebanks. The system requires a source language parser and a parallel treebank, aligned from the sentence level up to the word level (Och and Ney, 2003), including sub-sentential alignment (Tiedemann, 2003; Tinsley et al., 2007, Macken and Daelemans, 2008). To get a parallel treebank we parse both the source and target language components of parallel corpora a` la Europarl (Koehn, 2005). Each tree pair, sub-tree pair or word pair presents an example translation pair, and becomes a dictionary entry. This way we are removing the conceptual distinction between a dictionary and a parallel corpus, like Vandeghinste (2007). In a similar fashion, but making abstraction of the concrete words, we derive a set of transfer rules from the available alignments. A translation model is built by counting the frequencies of occurrence of all these alignments. The source language sentence is syntactically parsed, and the parse tree (and its sub-trees) is matched with the source language side"
2009.eamt-1.21,oostdijk-etal-2002-experiences,0,0.0249675,"Missing"
2009.eamt-1.21,P02-1040,0,0.0831416,"ute to the translation quality in good or bad sense that are not part of the target language model. Section 4.1 describes the methodology that is used for the experiment, and section 4.2 describes the evaluation results. 4.1 Methodology In a way, we are translating from Dutch to Dutch, only evaluating the ordering mechanism used in the target language generation component. We tested the quality of the output of the target language generation component by comparing it to the input sentence from which the bag of bags originates, which serves as a reference translation when evaluating with BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006). Additionally we also measured the number of exact matches: those cases in which the output sentence is identical to the input sentence. We have constructed a test set of 575 real-life sentences from a real translation context that were parsed with Alpino and converted into bags. We have several test conditions in two dimensions: 1. Corpus size: expressed in number of sentences. The treebank consists of several subcorpora, and we tested the system while gradually adding these sub-corpora. The size of these sub-corpora serves as data poin"
2009.eamt-1.21,2007.mtsummit-papers.62,0,0.0158411,"tion We are developing a data-driven hybrid approach towards machine translation, reusing as much as c 2009 European Association for Machine Translation. possible already existing tools and resources to set up an MT architecture much like a classic rulebased transfer system. Instead of manually designing the rules, we intend to derive them from large parallel and monolingual (uncorrected) treebanks. The system requires a source language parser and a parallel treebank, aligned from the sentence level up to the word level (Och and Ney, 2003), including sub-sentential alignment (Tiedemann, 2003; Tinsley et al., 2007, Macken and Daelemans, 2008). To get a parallel treebank we parse both the source and target language components of parallel corpora a` la Europarl (Koehn, 2005). Each tree pair, sub-tree pair or word pair presents an example translation pair, and becomes a dictionary entry. This way we are removing the conceptual distinction between a dictionary and a parallel corpus, like Vandeghinste (2007). In a similar fashion, but making abstraction of the concrete words, we derive a set of transfer rules from the available alignments. A translation model is built by counting the frequencies of occurren"
2009.eamt-1.21,2007.tc-1.2,1,0.807312,"nks. The system requires a source language parser and a parallel treebank, aligned from the sentence level up to the word level (Och and Ney, 2003), including sub-sentential alignment (Tiedemann, 2003; Tinsley et al., 2007, Macken and Daelemans, 2008). To get a parallel treebank we parse both the source and target language components of parallel corpora a` la Europarl (Koehn, 2005). Each tree pair, sub-tree pair or word pair presents an example translation pair, and becomes a dictionary entry. This way we are removing the conceptual distinction between a dictionary and a parallel corpus, like Vandeghinste (2007). In a similar fashion, but making abstraction of the concrete words, we derive a set of transfer rules from the available alignments. A translation model is built by counting the frequencies of occurrence of all these alignments. The source language sentence is syntactically parsed, and the parse tree (and its sub-trees) is matched with the source language side parse trees of the dictionary/parallel treebank. The retrieved target fragments are then restructured according to the information in the transfer rules resulting in a target language bag of bags, which is structured like a parse tree,"
2009.eamt-1.21,van-noord-etal-2006-syntactic,1,0.889272,"Missing"
2009.eamt-1.21,2006.jeptalnrecital-invite.2,0,0.138134,"Missing"
2009.eamt-1.21,2006.amta-papers.25,0,0.0249317,"re not part of the target language model. Section 4.1 describes the methodology that is used for the experiment, and section 4.2 describes the evaluation results. 4.1 Methodology In a way, we are translating from Dutch to Dutch, only evaluating the ordering mechanism used in the target language generation component. We tested the quality of the output of the target language generation component by comparing it to the input sentence from which the bag of bags originates, which serves as a reference translation when evaluating with BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006). Additionally we also measured the number of exact matches: those cases in which the output sentence is identical to the input sentence. We have constructed a test set of 575 real-life sentences from a real translation context that were parsed with Alpino and converted into bags. We have several test conditions in two dimensions: 1. Corpus size: expressed in number of sentences. The treebank consists of several subcorpora, and we tested the system while gradually adding these sub-corpora. The size of these sub-corpora serves as data points on the X-axis in figures 3, 4, 5, 6, and 7. 2. Abstra"
2009.eamt-1.21,C00-2092,0,\N,Missing
2009.eamt-1.21,J03-1002,0,\N,Missing
2010.eamt-1.38,2009.mtsummit-posters.2,0,0.0127759,"nduced from a parallel corpus. 2 Related Work We compare the transfer component described in this paper with the transfer component of Vandeghinste and Martens (2009). The general approach towards MT is quite similar to Data-Oriented translation (DOT) (Poutsma, 1998; Hearne, 2005), differing in the fact that we use rule-based or probabilistic context-free parsers whereas they use Data-Oriented Parsing (Bod, 1992), and the DOT approach was only tested on small corpora and a limited domain, whereas we intend a general news domain using large corpora. There are also similarities with the work of Ambati et al. (2009). They use synchronous contextfree grammars (SCFGs) (Aho and Ullman, 1969), which limit the depth of the transfer rules to 2, whereas the approach described in this paper does not set a limit to the maximum depth of a transfer rule, just like the synchronous tree-substitution grammars (STSGs), as described in Zhang et al. (2007). The difference between our system and STSGs is the fact that we build a target language tree without using any ordering information, since this is handled in the decoding step: the target language generation component (Vandeghinste, 2009). Our general approach is also"
2010.eamt-1.38,C92-3126,0,0.0316149,"syntactic analysis, syntactic transfer rules and a dictionary (lexical transfer rules), and a target language generation component. Both syntactic and lexical transfer rules are automatically induced from a parallel corpus. 2 Related Work We compare the transfer component described in this paper with the transfer component of Vandeghinste and Martens (2009). The general approach towards MT is quite similar to Data-Oriented translation (DOT) (Poutsma, 1998; Hearne, 2005), differing in the fact that we use rule-based or probabilistic context-free parsers whereas they use Data-Oriented Parsing (Bod, 1992), and the DOT approach was only tested on small corpora and a limited domain, whereas we intend a general news domain using large corpora. There are also similarities with the work of Ambati et al. (2009). They use synchronous contextfree grammars (SCFGs) (Aho and Ullman, 1969), which limit the depth of the transfer rules to 2, whereas the approach described in this paper does not set a limit to the maximum depth of a transfer rule, just like the synchronous tree-substitution grammars (STSGs), as described in Zhang et al. (2007). The difference between our system and STSGs is the fact that we"
2010.eamt-1.38,de-marneffe-etal-2006-generating,0,0.0540595,"Missing"
2010.eamt-1.38,W04-3303,0,0.0758069,"Missing"
2010.eamt-1.38,J03-1002,0,0.00742626,"dency tree (de Marneffe et al., 2006). Both parsers are freely available. 3.2 Preprocessing the parallel corpus The system was trained on the sentence-aligned Europarl corpus version 3 (Koehn, 2005). The source language parser is used to parse the source side of the parallel corpus in preprocessing, as well as the input sentence during actual translation. The target language parser is only used to preprocess the target side of the parallel corpus. This results in a parallel treebank, on which more details can be found in Tiedemann and Kotz´e (2009a). This treebank is word aligned with GIZA++ (Och and Ney, 2003) and node aligned using a discriminative approach to tree alignment (Tiedemann and Kotz´e, 2009b). 3.3 Bottom-up transfer The transfer component takes the source language parse tree and matches the nodes in that tree with nodes on the source language side of the parallel treebank. The corresponding target side fragments are recombined into target language trees. All possible output trees of this component are merged into a target language forest. Vandeghinste and Martens (2009) describe a top-down transfer component which leads to unsatisfactory results. We have investigated a bottomup transfe"
2010.eamt-1.38,P03-1054,0,0.00739348,"nto a target language forest that represents all possible target language parses that are considered translation candidates. The target language generation component turns this forest into a ranked set of sentences, each with their weight. 3.1 Syntactic Analysis The system reuses existing parsers for both source and target language analysis. As we are translating from Dutch to English, the system uses the Alpino parser (Van Noord, 2006) for Dutch, which outputs results in an xml-format combining phrase structure information with dependency information; and the system uses the Stanford parser (Klein and Manning, 2003) for English, which gives a phrase structure tree and an additional dependency tree (de Marneffe et al., 2006). Both parsers are freely available. 3.2 Preprocessing the parallel corpus The system was trained on the sentence-aligned Europarl corpus version 3 (Koehn, 2005). The source language parser is used to parse the source side of the parallel corpus in preprocessing, as well as the input sentence during actual translation. The target language parser is only used to preprocess the target side of the parallel corpus. This results in a parallel treebank, on which more details can be found in"
2010.eamt-1.38,P07-2045,0,0.00923959,"classical parsing strategies which build tree structures up from the bottom. 5 Evaluation We evaluated our system, using well-known automated MT metrics, like BLEU (Papineni et al. 2002), NIST (Doddington 2002), and TER (Snover et al. 2006), as well as WER (word error rate), PER (position independent word error rate), and CER (character error rate). We have used the same evaluation test set as Vandeghinste and Martens (2009) consisting of 500 Dutch sentences, with two reference translations for each sentence. To give an idea about the difficulty of the test set, it scored 29.96 BLEU on Moses (Koehn et al. 2007) trained on the same sentences of Europarl as used in our system and 38.82 BLEU on Google translate. We evaluated the bottom-up system in three conditions: 1. Smallbeam: In target language generation, we use a beam size of 10, a cutoff factor of 50, a maxcomb of 100 and a maxperm of 100 2. Largebeam: In target language generation, we use a beam size of 100, cutoff factor of 500, maxcomb of 200 and a maxperm of 200 3. Dummy: Only bottom-up transfer of matching words, as described in section 4.2. Source word order is retained and the target language generation module favours orders that are clos"
2010.eamt-1.38,W09-3212,1,0.113627,"ropean Association for Machine Translation. eration component, which determines the output sentences. The system can also be considered a rule-based transfer system, as it conforms to the general architecture of a rule-based system, using source language syntactic analysis, syntactic transfer rules and a dictionary (lexical transfer rules), and a target language generation component. Both syntactic and lexical transfer rules are automatically induced from a parallel corpus. 2 Related Work We compare the transfer component described in this paper with the transfer component of Vandeghinste and Martens (2009). The general approach towards MT is quite similar to Data-Oriented translation (DOT) (Poutsma, 1998; Hearne, 2005), differing in the fact that we use rule-based or probabilistic context-free parsers whereas they use Data-Oriented Parsing (Bod, 1992), and the DOT approach was only tested on small corpora and a limited domain, whereas we intend a general news domain using large corpora. There are also similarities with the work of Ambati et al. (2009). They use synchronous contextfree grammars (SCFGs) (Aho and Ullman, 1969), which limit the depth of the transfer rules to 2, whereas the approach"
2010.eamt-1.38,P02-1040,0,0.0793027,"ce have translations that are siblings in the target language. So, when no other transfer rule is available, it selects the target language node label that most corresponds to the source language parent, and then guesses which of the target language child nodes is likely to be the head of that phrase, based on what labels are usually heads for that type of phrase. Translating from the bottom-up in this manner is closely related to classical parsing strategies which build tree structures up from the bottom. 5 Evaluation We evaluated our system, using well-known automated MT metrics, like BLEU (Papineni et al. 2002), NIST (Doddington 2002), and TER (Snover et al. 2006), as well as WER (word error rate), PER (position independent word error rate), and CER (character error rate). We have used the same evaluation test set as Vandeghinste and Martens (2009) consisting of 500 Dutch sentences, with two reference translations for each sentence. To give an idea about the difficulty of the test set, it scored 29.96 BLEU on Moses (Koehn et al. 2007) trained on the same sentences of Europarl as used in our system and 38.82 BLEU on Google translate. We evaluated the bottom-up system in three conditions: 1. Smallbeam"
2010.eamt-1.38,2006.amta-papers.25,0,0.0107348,"guage. So, when no other transfer rule is available, it selects the target language node label that most corresponds to the source language parent, and then guesses which of the target language child nodes is likely to be the head of that phrase, based on what labels are usually heads for that type of phrase. Translating from the bottom-up in this manner is closely related to classical parsing strategies which build tree structures up from the bottom. 5 Evaluation We evaluated our system, using well-known automated MT metrics, like BLEU (Papineni et al. 2002), NIST (Doddington 2002), and TER (Snover et al. 2006), as well as WER (word error rate), PER (position independent word error rate), and CER (character error rate). We have used the same evaluation test set as Vandeghinste and Martens (2009) consisting of 500 Dutch sentences, with two reference translations for each sentence. To give an idea about the difficulty of the test set, it scored 29.96 BLEU on Moses (Koehn et al. 2007) trained on the same sentences of Europarl as used in our system and 38.82 BLEU on Google translate. We evaluated the bottom-up system in three conditions: 1. Smallbeam: In target language generation, we use a beam size of"
2010.eamt-1.38,W09-4206,0,0.303575,"Missing"
2010.eamt-1.38,2009.eamt-1.21,1,0.903986,"imilarities with the work of Ambati et al. (2009). They use synchronous contextfree grammars (SCFGs) (Aho and Ullman, 1969), which limit the depth of the transfer rules to 2, whereas the approach described in this paper does not set a limit to the maximum depth of a transfer rule, just like the synchronous tree-substitution grammars (STSGs), as described in Zhang et al. (2007). The difference between our system and STSGs is the fact that we build a target language tree without using any ordering information, since this is handled in the decoding step: the target language generation component (Vandeghinste, 2009). Our general approach is also similar to the example-based MT-engine described by Kurohashi (2009), differing in the fact that Kurohashi uses dependency trees and we combine information from phrase structure trees and dependency trees. 3 System Description The example-based machine translation system has an architecture very similar to that of rulebased transfer systems. An input sentence is analyzed by a source language parser. The source language parse tree is converted by the transfer component into a target language forest that represents all possible target language parses that are consi"
2010.eamt-1.38,2006.jeptalnrecital-invite.2,0,0.0302893,"Missing"
2010.eamt-1.38,2007.mtsummit-papers.71,0,0.03615,"probabilistic context-free parsers whereas they use Data-Oriented Parsing (Bod, 1992), and the DOT approach was only tested on small corpora and a limited domain, whereas we intend a general news domain using large corpora. There are also similarities with the work of Ambati et al. (2009). They use synchronous contextfree grammars (SCFGs) (Aho and Ullman, 1969), which limit the depth of the transfer rules to 2, whereas the approach described in this paper does not set a limit to the maximum depth of a transfer rule, just like the synchronous tree-substitution grammars (STSGs), as described in Zhang et al. (2007). The difference between our system and STSGs is the fact that we build a target language tree without using any ordering information, since this is handled in the decoding step: the target language generation component (Vandeghinste, 2009). Our general approach is also similar to the example-based MT-engine described by Kurohashi (2009), differing in the fact that Kurohashi uses dependency trees and we combine information from phrase structure trees and dependency trees. 3 System Description The example-based machine translation system has an architecture very similar to that of rulebased tra"
2010.eamt-1.38,C00-2092,0,\N,Missing
2010.eamt-1.38,2005.mtsummit-papers.11,0,\N,Missing
2011.eamt-1.10,C10-2043,0,0.052329,"Missing"
2011.eamt-1.10,J10-4005,0,0.0160707,"nt@ccl.kuleuven.be Driven by competition, the translation industry integrated new MT systems with the widely used TM technology. Today, as one of several approaches, SMT systems prove to be successful, especially when they are integrated in postediting workflows and are trained with TM data (He et al., 2010). While the translation industry follows the scientific developments of MT closely, it faces its own specific problems. Although there is much effort put into scientific research topics in the field of SMT (Yamada and Knight, 2000; Och and Ney, 2002; Koehn et al., 2003; Koehn et al., 2007; Koehn, 2010) this paper introduces the XML markup problem in SMT-CAT integration and proposes practical solutions. Section 2 presents background information about XML in translation and post-editing workflows, and explains the challenges XML markup brings. Section 3 refers to related work and motivates this paper. Section 4 introduces several methods to handle the XML markup. Section 5 reports and analyzes the experiments that were conducted. Section 6 concludes, looking into the possibilities for future work. 2 XML in Translation Workflows An important challenge in SMT-CAT integration is the use of a TM"
2011.eamt-1.10,N03-1017,0,0.0230304,"niversiteit Leuven Leuven, Belgium vincent@ccl.kuleuven.be Driven by competition, the translation industry integrated new MT systems with the widely used TM technology. Today, as one of several approaches, SMT systems prove to be successful, especially when they are integrated in postediting workflows and are trained with TM data (He et al., 2010). While the translation industry follows the scientific developments of MT closely, it faces its own specific problems. Although there is much effort put into scientific research topics in the field of SMT (Yamada and Knight, 2000; Och and Ney, 2002; Koehn et al., 2003; Koehn et al., 2007; Koehn, 2010) this paper introduces the XML markup problem in SMT-CAT integration and proposes practical solutions. Section 2 presents background information about XML in translation and post-editing workflows, and explains the challenges XML markup brings. Section 3 refers to related work and motivates this paper. Section 4 introduces several methods to handle the XML markup. Section 5 reports and analyzes the experiments that were conducted. Section 6 concludes, looking into the possibilities for future work. 2 XML in Translation Workflows An important challenge in SMT-C"
2011.eamt-1.10,leplus-etal-2004-weather,0,0.0511783,"Missing"
2011.eamt-1.10,2002.tmi-tutorials.2,0,0.0339497,"uistics Katholike Universiteit Leuven Leuven, Belgium vincent@ccl.kuleuven.be Driven by competition, the translation industry integrated new MT systems with the widely used TM technology. Today, as one of several approaches, SMT systems prove to be successful, especially when they are integrated in postediting workflows and are trained with TM data (He et al., 2010). While the translation industry follows the scientific developments of MT closely, it faces its own specific problems. Although there is much effort put into scientific research topics in the field of SMT (Yamada and Knight, 2000; Och and Ney, 2002; Koehn et al., 2003; Koehn et al., 2007; Koehn, 2010) this paper introduces the XML markup problem in SMT-CAT integration and proposes practical solutions. Section 2 presents background information about XML in translation and post-editing workflows, and explains the challenges XML markup brings. Section 3 refers to related work and motivates this paper. Section 4 introduces several methods to handle the XML markup. Section 5 reports and analyzes the experiments that were conducted. Section 6 concludes, looking into the possibilities for future work. 2 XML in Translation Workflows An importan"
2011.eamt-1.10,J03-1002,0,0.00473803,"of tag content retrieval and reordering methods. 5.1 Corpora, System and Evaluation From this data 912 and 871 pairs of segments are extracted respectively as test sets, leaving 400.000 and 399.489 fragments as training data for the SMT system. As these TMs do not contain any duplicate translation pairs, there is no overlap between the test set and the training set. The TMX exports are cleaned from TMX markup prior to the training, leaving two aligned files per TM (on the sentence level), for source and target segments. For the SMT system, we use the Moses toolkit consisting of Moses, GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The LMs were trained with five-grams, applying interpolation, Kneser-Ney discounting and 5.2 Results Table 2 shows the scores obtained by the proposed methods for the two language pairs. Table 2: Automatic evaluation scores for English-Spanish and English-French. 59 The most striking outcome is how the tags ZHUH KDQGOHGEWKHIXOOWRNHQL]aWLRQ´ PHWKRG The systems actually handle the XML tags quite well, making no mistakes on the XML structure itself. However, when the tags included words or SKUDVHV WKH VVWHPV SURYLGH XQQHFHVVDU´ translations (for phra"
2011.eamt-1.10,W05-0909,0,0.026219,"ments and Analysis In the experiments we use two TM exports (TMX) from the automotive domain for the language pairs English-Spanish and English-French. These TMs include domain specific data and are heavily tagged with XML. 41.145 segments out of 400.912 (En-Sp) included one or more tags. 36.540 segments out of 400.360 (En-Fr) included one or more tags. Table 1: Different training sets represented with number of sentence pairs used for both language pairs. To evaluate the SMT results, we use automatic evaluation metrics such as BLEU (Papineni et al., 2000), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) and a human translator for judging the MT outputs for tag reordering. The training data, the input, the output, and the reference files are all tokenized (with the Moses tokenizer) and all the tags in all outputs of different methods are nRUPDOL]HGWR#WDJ#´to avoid possible score differences caused by comparing different type of output and reference translations regarding the XML tags. Figure 3: Representation of tag content retrieval and reordering methods. 5.1 Corpora, System and Evaluation From this data 912 and 871 pairs of segments are extracted respectively as test sets, leaving 400"
2011.eamt-1.10,P02-1040,0,0.080328,"Missing"
2011.eamt-1.10,C00-2172,0,0.0253333,"Missing"
2011.eamt-1.10,2010.eamt-1.23,0,0.252902,"Missing"
2011.eamt-1.10,P01-1067,0,0.194941,"Missing"
2011.eamt-1.10,C04-1114,0,0.0580564,"Missing"
2011.eamt-1.10,J07-3002,0,0.0532308,"Missing"
2014.tc-1.11,E14-1022,0,0.0111658,"or characters (one may compare a pair of words based on their characters). Trees are structures with hierarchically linked nodes. For instance, parse trees structure a sentence into its syntactic constituents, assign part-of-speech tags to words, and assign dependency relations to constituents (for instance subject). An example of a parse tree is shown in Figure 1. Figure 1: Parse tree with syntactic constituents and part-of-speech tags 2.1. Strings Methods acting upon flat sequences are called string-based. They include methods like Levenshtein distance (Levenshtein 1966) and percent match (Bloodgood and Strauss 2014). Levenshtein distance is one of the most commonly applied matching methods. It looks for the shortest path to transform one sequence into another sequence through deletions, insertions and substitutions. This shortest path is expressed as a distance. The distance can be converted 3 One example is the tool Similis (http://www.similis.org), which determines constituents such as noun phrases in sentences and allows for retrieving TM sentences which share constituents with the query sentence. 4 http://www.ccl.kuleuven.be/scate 91 Translating and The Computer 36 into a score by normalizing on the"
2014.tc-1.11,D11-1047,0,0.0198914,"rtest path to convert one tree to another one by deleting, inserting, and substituting nodes. The second method looks for the easiest way to combine the two trees into a single one by adding empty nodes. When applying these methods to parse trees, node comparison may involve several features on a node, such as word form, lemma (for instance, the infinitive of a verb, the singular form of a noun), part-of-speech tag (for instance, verb) or constituent (for instance, noun phrase). Tree-based methods have also been proposed in the literature on example-based machine translation, see for instance Cromieres and Kurohashi (2011). Figure 2 provides an example of a parse tree alignment based on the shortest tree alignment distance. In this example, the parse trees have a stronger match than the sentences in Table 1, thanks to the fact that the trees contain similar syntactic information: not only some words match, but also some constituent labels and part-of-speech tags. 5 Figure 2: Aligned parse trees 5 The VBD node matches the VBN node as the tags both refer to verbs and the surrounding nodes also match. 92 Translating and The Computer 36 2.3. Trees as strings Linguistic knowledge can be exploited in both string-base"
2014.tc-1.11,de-marneffe-etal-2006-generating,0,0.0207012,"Missing"
2014.tc-1.11,2014.eamt-1.2,0,0.457174,"Missing"
2014.tc-1.11,2010.jec-1.4,0,0.116047,"Missing"
2014.tc-1.11,2010.amta-papers.2,0,0.0665622,"Missing"
2014.tc-1.11,P08-1023,0,0.0802276,"Missing"
2014.tc-1.11,P02-1040,0,0.0882887,"Missing"
2014.tc-1.11,2012.amta-papers.26,0,0.0241821,"Missing"
2014.tc-1.11,2006.amta-papers.25,0,0.17746,"Missing"
2015.eamt-1.21,2005.mtsummit-papers.29,0,0.0506644,"r comparing any pair of sequences or trees (not necessarily sentences and their parse trees), for fuzzy matching in a TM, or for comparing machine translation (MT) output to a reference translation. As pointed out by Simard and Fujita (2012), the third type, MT automatic evaluation metrics, can also be used in the context of TMs, both as fuzzy matching metric and as metric for comparing the translation of a fuzzy match with the desired translation. Some matching methods specifically support the integration of fuzzy matches within an MT system (examplebased or statistical MT); see for instance Aramaki et al. (2005), Smith and Clark (2009), Zhechev and van Genabith (2010), Ma et al. (2011). Some matching methods are linguistically unaware. Levenshtein distance (Levenshtein, 1966), which calculates the effort needed to convert one sequence into another using the operations insertion, deletion and substitution, is the most commonly used fuzzy matching method (Bloodgood and Strauss, 2014). Tree edit distance (Klein, 1998) applies this principle to trees; another tree comparison method is tree alignment (Jiang et al., 1995).3 To allow using string-based matching methods on trees, there are several ways of co"
2015.eamt-1.21,E14-1022,0,0.0115955,"etric for comparing the translation of a fuzzy match with the desired translation. Some matching methods specifically support the integration of fuzzy matches within an MT system (examplebased or statistical MT); see for instance Aramaki et al. (2005), Smith and Clark (2009), Zhechev and van Genabith (2010), Ma et al. (2011). Some matching methods are linguistically unaware. Levenshtein distance (Levenshtein, 1966), which calculates the effort needed to convert one sequence into another using the operations insertion, deletion and substitution, is the most commonly used fuzzy matching method (Bloodgood and Strauss, 2014). Tree edit distance (Klein, 1998) applies this principle to trees; another tree comparison method is tree alignment (Jiang et al., 1995).3 To allow using string-based matching methods on trees, there are several ways of converting trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods specifically designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compar"
2015.eamt-1.21,comelles-etal-2014-verta,0,0.0501746,"Missing"
2015.eamt-1.21,W14-3348,0,0.0139981,"TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-specific rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine."
2015.eamt-1.21,W14-3350,0,0.010984,"(2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-specific rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine. Different types of metrics can be combined in order to join their strengths. For instance, the Asiya toolkit (Gim´enez and M´arquez, 2010) contains a large number of matching metrics of different origins and applies them for MT evaluation. An optimal metric set is determined by progressively adding metrics to the set if that increases the quality of the translation. 3 3.1 Independent variables In Sections 3."
2015.eamt-1.21,2014.tc-1.10,0,0.0130369,"metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-specific rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine. Different types of metrics can be combined in order to join their strengths. For instance, the Asiya toolkit (Gim´enez and M´arquez, 2010) contains a large number of matching metrics of different origins and applies them for MT evaluation. An optimal metric set is determined by progressively adding metrics to the set if that increases the quality of the translation. 3 3.1 Independent variables In Sections 3.1.1, 3.1.2, and 3.1.3, we describe the independent variables of our ex"
2015.eamt-1.21,2005.mtsummit-papers.11,0,0.0366604,"Missing"
2015.eamt-1.21,2010.amta-papers.2,0,0.0310282,"to select candidate sentences in the TM which are likely to reach a minimal matching threshold when submitting them to a fuzzy matching metric, in order to increase the speed of matching. A candidate sentence is a sentence which shares one or more ngrams of a minimal length N with Q, and which shares enough ngrams with Q so as to cover the latter sufficiently. The implementation of the filter uses a suffix array (Manber and Myers, 1993), which allows for a very efficient search for sentences sharing ngrams with Q.11 This approach is similar to the one used in the context of fuzzy matching by Koehn and Senellart (2010). In order to measure the usefulness of the AQC filter, we measured the tradeoff between the gain 11 We used the SALM toolkit (Zhang and Vogel, 2006) for building and consulting suffix arrays in our experiment. 156 in speed and the loss of potentially useful matches. We used a sample of about 30,000 English-Dutch sentence pairs selected from Europarl (Koehn, 2005), and a threshold of 0.2. After applying a leave-one-out test, which consists of considering each Si in the sample as a Q and comparing it to all the other Si in the sample, it appeared that the AQC filter selected about 9 candidate s"
2015.eamt-1.21,W05-0904,0,0.0452946,") and based on post-order tree traversal. Examples of matching methods specifically designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by"
2015.eamt-1.21,P11-1023,0,0.0469165,"racter-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-specific rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are"
2015.eamt-1.21,P11-1124,0,0.0506553,"Missing"
2015.eamt-1.21,2012.amta-papers.26,0,0.0356387,"Missing"
2015.eamt-1.21,2006.amta-papers.25,0,0.0609844,"g trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods specifically designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output"
2015.eamt-1.21,W14-3354,0,0.0441942,"Missing"
2015.eamt-1.21,2010.amta-papers.19,0,0.0273764,"Missing"
2015.eamt-1.21,P02-1040,0,0.0967231,"matching methods on trees, there are several ways of converting trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods specifically designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Met"
augustinus-etal-2012-example,bouma-kloosterman-2002-querying,0,\N,Missing
augustinus-etal-2012-example,J93-2004,0,\N,Missing
augustinus-etal-2012-example,van-noord-etal-2006-syntactic,1,\N,Missing
augustinus-etal-2012-example,stepanek-pajas-2010-querying,0,\N,Missing
augustinus-etal-2012-example,P05-3009,0,\N,Missing
augustinus-etal-2012-example,2006.jeptalnrecital-invite.2,0,\N,Missing
augustinus-etal-2012-example,kotze-etal-2012-large,1,\N,Missing
augustinus-etal-2012-example,Y05-1001,0,\N,Missing
augustinus-etal-2012-example,W07-1503,0,\N,Missing
augustinus-etal-2012-example,U04-1019,0,\N,Missing
kotze-etal-2012-large,N07-1051,0,\N,Missing
kotze-etal-2012-large,J93-1004,0,\N,Missing
kotze-etal-2012-large,de-marneffe-etal-2006-generating,0,\N,Missing
kotze-etal-2012-large,tiedemann-2010-lingua,1,\N,Missing
kotze-etal-2012-large,C08-1139,0,\N,Missing
kotze-etal-2012-large,C08-1038,0,\N,Missing
kotze-etal-2012-large,W06-1661,0,\N,Missing
kotze-etal-2012-large,J93-2003,0,\N,Missing
kotze-etal-2012-large,W01-1406,0,\N,Missing
kotze-etal-2012-large,2001.mtsummit-ebmt.4,0,\N,Missing
kotze-etal-2012-large,C04-1154,0,\N,Missing
kotze-etal-2012-large,C90-3045,0,\N,Missing
kotze-etal-2012-large,P03-1054,0,\N,Missing
kotze-etal-2012-large,P02-1040,0,\N,Missing
kotze-etal-2012-large,P10-1032,0,\N,Missing
kotze-etal-2012-large,2010.eamt-1.38,1,\N,Missing
kotze-etal-2012-large,2006.jeptalnrecital-invite.2,0,\N,Missing
kotze-etal-2012-large,P07-2045,0,\N,Missing
kotze-etal-2012-large,2006.amta-papers.15,0,\N,Missing
kotze-etal-2012-large,W08-0411,0,\N,Missing
kotze-etal-2012-large,P09-1063,0,\N,Missing
kotze-etal-2012-large,P03-2041,0,\N,Missing
kotze-etal-2012-large,J03-1002,0,\N,Missing
kotze-etal-2012-large,2009.eamt-1.21,1,\N,Missing
kotze-etal-2012-large,J10-2004,0,\N,Missing
kotze-etal-2012-large,2005.mtsummit-papers.11,0,\N,Missing
kotze-etal-2012-large,D12-1079,0,\N,Missing
kotze-etal-2012-large,W09-4206,1,\N,Missing
kotze-etal-2012-large,W90-0102,0,\N,Missing
L16-1107,augustinus-etal-2012-example,1,0.789643,"Missing"
L16-1107,W13-5638,1,0.839951,"Missing"
L16-1107,W08-1301,0,0.0474946,"Missing"
L16-1107,de-marneffe-etal-2006-generating,0,0.0123446,"Missing"
L16-1107,eiselen-puttkammer-2014-developing,0,0.0162899,"ilable tools. Compared to well-resourced languages such as English and Dutch, however, there are fewer wellperforming tools available for Afrikaans. In this paper, we describe the development of an Afrikaans treebank (Section 2.), a dependency parser (Section 3.), and a tool for querying the treebank (Section 4.). 2. Treebank The Afrikaans part of the NCHLT1 Annotated Text Corpora (Puttkammer et al., 2014) was used as a basis for the development of the treebank. This corpus was selected because it had already been annotated with part-of-speech (POS) tags and word lemmas in a previous project (Eiselen and Puttkammer, 2014). The scope of work described below thus involved additional annotation for word dependencies within sentence context given the pre-existing POS tags and lemmas. 1 South African National Centre for Human Language Technologies 2.1. Development The NCHLT text corpus, consisting mainly of government domain documents, contains a number of incomplete sentence fragments which could not be annotated sensibly for dependency structure. The first action performed was filtering the corpus so that only complete sentences were annotated. The POS annotation in the NCHLT corpus was based on a fine-grained ta"
L16-1107,petrov-etal-2012-universal,0,0.027676,"n be seen in Table 1. Subset Tokens/Words Lines/Sentences NCHLT corpus before filtering training set 55386 2610 test set 5834 329 NCHLT corpus after filtering training set 43895 1663 test set 5381 271 Table 1: NCHLT corpus division before and after filtering for valid sentences. As some of the information in the original POS tag set may be superfluous for determining the sentence dependency structure and the additional granularity increased the difficulty and hence the reliability of the manual part of the annotation task, the POS tag set was simplified to a largely universal set of POS tags (Petrov et al., 2012). Table 2 contains the resulting tag set used with the number of original tags that was mapped to each simplified tag. For the dependency relation annotation, a subset of the 677 POS tag ADJ ADP ADV CONJ DET NOUN NUM PRON PRT PUNCT VERB X Afr. tags 6 1 19 2 2 19 13 32 9 4 15 24 Description Adjectives Prepositions Adverbs Conjunctions Determiners Nouns (including proper nouns) Numerals Pronouns Particles Punctuation Verbs (including auxiliary verbs) Catch-all class (including abbreviations and interjections amongst others) Table 2: The POS tag set used with the number of sub-classes mapped from"
L16-1107,E12-2021,0,0.0577136,"Missing"
L16-1107,D07-1099,0,0.0297303,"Missing"
L16-1564,augustinus-etal-2012-example,1,0.611322,"eries are not shown at this stage. In the advanced search mode, users can adapt the XPath expressions in order to refine or generalize the search instruction. If a pair of parse trees matches the Dutch and English query, Poly-GrETEL checks whether the matching parts are aligned. 4 In the example, we are looking for non-lexical construc//node[@cat=&quot;VP&quot;and node[@rel=&quot;hd&quot; and @pos=&quot;VB&quot;] and node[@rel=&quot;dobj&quot; and @cat=&quot;NP&quot; and node[@rel=&quot;hd&quot; and @pos=&quot;NN&quot;]]] For more information about the two other search options, see the GrETEL manual (http://gretel.ccl.kuleuven. be/project) and Augustinus et al. (2012). 3551 6. Results If Poly-GrETEL finds matching constructions in the parallel treebank, they are presented to the user as a list of sentence pairs. Some example results are given in (4–5). The a-sentences are matches for the source side and the b-sentences are the corresponding construction in the target side. The parts in bold are the aligned matches. (4) a. The reasons for adjustments are, for example, to improve employment... b. De redenen voor de aanpassingen zijn bijvoorbeeld het verbeteren van de werkgelegenheid... (5) a. The Commission agrees to strengthen this political message. b. De"
L16-1564,W13-5638,1,0.845176,"Missing"
L16-1564,de-marneffe-etal-2006-generating,0,0.0855378,"Missing"
L16-1564,P03-1054,0,0.00842396,"parallel treebank We extracted the Dutch and English sentences from the Europarl parallel corpus (Koehn, 2005), version 7.3 Table 1 presents statistics about the corpus. Dutch English Words 38,859,141 40,077,179 Sentences 1,607,423 1,607,423 Table 1: Statistics about the Europarl parallel corpus We parsed the Dutch side with Alpino (van Noord, 2006), a dependency parser that also assigns phrase structure labels and outputs XML trees (Alpino-XML format) that are isomorphous to the syntax trees, which makes them easily queryable with XPath. We parsed the English side using the Stanford parser (Klein and Manning, 2003), added the dependency labels (de Marneffe et al., 2006), and converted the bracketed phrasestructure tree and the dependency labels into Alpino-XML format. We added word alignment information by applying GIZA++ (Och and Ney, 2003), as well as node alignment between parallel trees provided by the Dublin Subtree Aligner (Zhechev, 2009), which creates node alignments in a relatively straightforward manner, using lexical probabilities derived by GIZA++. Nodes adhering to wellformedness rules are aligned. An alternative alignment, which we intend to include in future versions, is produced by Lingu"
L16-1564,J03-1002,0,0.00517357,"7,423 Table 1: Statistics about the Europarl parallel corpus We parsed the Dutch side with Alpino (van Noord, 2006), a dependency parser that also assigns phrase structure labels and outputs XML trees (Alpino-XML format) that are isomorphous to the syntax trees, which makes them easily queryable with XPath. We parsed the English side using the Stanford parser (Klein and Manning, 2003), added the dependency labels (de Marneffe et al., 2006), and converted the bracketed phrasestructure tree and the dependency labels into Alpino-XML format. We added word alignment information by applying GIZA++ (Och and Ney, 2003), as well as node alignment between parallel trees provided by the Dublin Subtree Aligner (Zhechev, 2009), which creates node alignments in a relatively straightforward manner, using lexical probabilities derived by GIZA++. Nodes adhering to wellformedness rules are aligned. An alternative alignment, which we intend to include in future versions, is produced by Lingua-Align (Tiedemann, 2010), a discriminative tree aligner trained on a small parallel treebank with manual alignments. 4. Querying the treebank Previous implementations of GrETEL allow users to look up Dutch constructions, either by"
L16-1564,tiedemann-2010-lingua,0,0.015198,"he dependency labels (de Marneffe et al., 2006), and converted the bracketed phrasestructure tree and the dependency labels into Alpino-XML format. We added word alignment information by applying GIZA++ (Och and Ney, 2003), as well as node alignment between parallel trees provided by the Dublin Subtree Aligner (Zhechev, 2009), which creates node alignments in a relatively straightforward manner, using lexical probabilities derived by GIZA++. Nodes adhering to wellformedness rules are aligned. An alternative alignment, which we intend to include in future versions, is produced by Lingua-Align (Tiedemann, 2010), a discriminative tree aligner trained on a small parallel treebank with manual alignments. 4. Querying the treebank Previous implementations of GrETEL allow users to look up Dutch constructions, either by example or using XPath queries. When lookup is example-based, the input example is parsed using the same parser that is used for the creation of the treebank, and the user indicates the relevant and irrelevant parts in the parse tree of the example. This information is 3 The corpus was downloaded http://www.statmt.org/europarl. from Figure 1: Flow chart of the processing steps for exampleba"
L16-1564,2006.jeptalnrecital-invite.2,0,0.0419013,"Missing"
L16-1564,2015.eamt-1.47,1,0.893278,"Missing"
L16-1564,volk-etal-2014-innovations,0,0.0336825,"Missing"
L16-1564,P13-1054,0,0.0479186,"Missing"
L16-1564,tiedemann-2012-parallel,0,0.0312289,"comes in the form of links between words, constituents or dependencies. For instance, the SMULTRON treebank contains handcrafted alignments between nodes in parse trees. The PaCoMT system (Vandeghinste et al., 2013) and the SCATE project (Vandeghinste et al., 2015) make use of large sets of automatically created parse trees and subtree alignment links to induce synchronous grammars for machine translation. Depending on the available linguistic annotation and alignment, and on the expressiveness of the query language, the parallel corpus may be queried in different ways. The OPUS environment (Tiedemann, 2012) uses the Corpus Workbench (Evert and Hardie, 2011) for querying with regular expressions and part-of-speech tags. The Stockholm TreeAligner (Lundborg et al., 2007) and the INESS Search 3549 platform (Meurer, 2012) allow querying parse tree nodes, e.g. by specifying the type of constituent. Queries typically relate to the source language only. Some query tools allow for a cross-language search, i.e. the specification of constraints in both the source and the target language. The Stockholm TreeAligner and the INESS Search platform add an alignment condition to this cross-language search: parse"
L16-1564,2005.mtsummit-papers.11,0,\N,Missing
oostdijk-etal-2008-coi,schuurman-2008-spatiotemporal,1,\N,Missing
oostdijk-etal-2008-coi,reynaert-2006-corpus,1,\N,Missing
oostdijk-etal-2008-coi,oostdijk-boves-2006-user,1,\N,Missing
oostdijk-etal-2008-coi,W97-1502,0,\N,Missing
oostdijk-etal-2008-coi,van-den-bosch-etal-2006-transferring,1,\N,Missing
oostdijk-etal-2008-coi,van-noord-etal-2006-syntactic,1,\N,Missing
oostdijk-etal-2008-coi,W07-1513,1,\N,Missing
oostdijk-etal-2008-coi,2006.jeptalnrecital-invite.2,1,\N,Missing
oostdijk-etal-2008-coi,W03-2414,1,\N,Missing
S16-2017,P02-1040,0,0.0949164,"he WAI-NOT corpus, which were manually translated to Beta and Sclera pictographs by Vandeghinste et al. (2015). To the original tuning corpus, we added five more hand-picked messages from the corpus that included a polysemous word, that had at least two pictographs linked to at least two of its synsets. Biasing the tuning corpus like this was necessary, since the original set had very few ambiguous words. We used the local hill climber algorithm as described in Vandeghinste et al. (2015), which varies the parameter values when running the Text-toPictograph translation script. The BLEU metric (Papineni et al., 2002) was used as an indicator of relative improvement. In order to maximize the BLEU score, we ran five trials of the local hill climbing algorithm, until BLEU converged onto a fixed score. Each trial was run with random initialization values, and varied the values between certain boundaries. From these trials, we took the best scoring parameter values. (WER) and Position-independent word Error Rate (PER).11 We have added significance levels for the BLEU and NIST scores, by comparing the no WSD condition with the WSD condition. Significance was calculated using bootstrap resampling (Koehn, 2004)."
S16-2017,W15-5119,1,0.874025,"Missing"
S16-2017,P05-1048,0,0.0167665,"h is then semiautomatically converted into pictographs. However, they do not provide automatic translation aids based on linguistic knowledge to properly disambiguate lexical ambiguities, which can lead to erroneous translation (Vandeghinste, 2012). There is contradictory evidence that Natural Language Processing tools and Information Retrieval tasks benefit from WSD. Within the field of Machine Translation, Dagan and Itai (1994) and Vickrey et al. (2005) show that proper incorporation of WSD leads to an increase in translation performance for automatic translation systems. On the other hand, Carpuat and Wu (2005) argue that it is difficult, at the least, to use standard WSD models to obtain significant improvements to statistical Machine Translation systems, even when supervised WSD models are used. In later research, Carpuat and Wu (2007) 4 Description of the tools The following sections describe the architecture of the Text-to-Pictograph translation system (section 3.1) and the WSD tool (section 3.2). 3.1 The Text-to-Pictograph translation system The Text-to-Pictograph translation system translates text into a series of Beta or Sclera pictographs, cf. Vandeghinste et al. (2015) and Sevens et al. (20"
S16-2017,vandeghinste-schuurman-2014-linking,1,0.893947,"Missing"
S16-2017,D07-1007,0,0.0398642,"ndeghinste, 2012). There is contradictory evidence that Natural Language Processing tools and Information Retrieval tasks benefit from WSD. Within the field of Machine Translation, Dagan and Itai (1994) and Vickrey et al. (2005) show that proper incorporation of WSD leads to an increase in translation performance for automatic translation systems. On the other hand, Carpuat and Wu (2005) argue that it is difficult, at the least, to use standard WSD models to obtain significant improvements to statistical Machine Translation systems, even when supervised WSD models are used. In later research, Carpuat and Wu (2007) 4 Description of the tools The following sections describe the architecture of the Text-to-Pictograph translation system (section 3.1) and the WSD tool (section 3.2). 3.1 The Text-to-Pictograph translation system The Text-to-Pictograph translation system translates text into a series of Beta or Sclera pictographs, cf. Vandeghinste et al. (2015) and Sevens et al. (2015a). The source text first undergoes shallow linguistic processing, consisting of several sub-processes, such as tokenization, part-of-speech tagging, and lemmatization. For each word in the source text, the system then returns al"
S16-2017,J94-4003,0,0.254121,"was not evaluated. Quite similar to the Text-to-Pictograph translation system are SymWriter4 and Blissymbols (Hehner et al., 1983). These systems allow users to insert arbitrary text, which is then semiautomatically converted into pictographs. However, they do not provide automatic translation aids based on linguistic knowledge to properly disambiguate lexical ambiguities, which can lead to erroneous translation (Vandeghinste, 2012). There is contradictory evidence that Natural Language Processing tools and Information Retrieval tasks benefit from WSD. Within the field of Machine Translation, Dagan and Itai (1994) and Vickrey et al. (2005) show that proper incorporation of WSD leads to an increase in translation performance for automatic translation systems. On the other hand, Carpuat and Wu (2005) argue that it is difficult, at the least, to use standard WSD models to obtain significant improvements to statistical Machine Translation systems, even when supervised WSD models are used. In later research, Carpuat and Wu (2007) 4 Description of the tools The following sections describe the architecture of the Text-to-Pictograph translation system (section 3.1) and the WSD tool (section 3.2). 3.1 The Text-"
S16-2017,H05-1097,0,0.045417,"imilar to the Text-to-Pictograph translation system are SymWriter4 and Blissymbols (Hehner et al., 1983). These systems allow users to insert arbitrary text, which is then semiautomatically converted into pictographs. However, they do not provide automatic translation aids based on linguistic knowledge to properly disambiguate lexical ambiguities, which can lead to erroneous translation (Vandeghinste, 2012). There is contradictory evidence that Natural Language Processing tools and Information Retrieval tasks benefit from WSD. Within the field of Machine Translation, Dagan and Itai (1994) and Vickrey et al. (2005) show that proper incorporation of WSD leads to an increase in translation performance for automatic translation systems. On the other hand, Carpuat and Wu (2005) argue that it is difficult, at the least, to use standard WSD models to obtain significant improvements to statistical Machine Translation systems, even when supervised WSD models are used. In later research, Carpuat and Wu (2007) 4 Description of the tools The following sections describe the architecture of the Text-to-Pictograph translation system (section 3.1) and the WSD tool (section 3.2). 3.1 The Text-to-Pictograph translation"
S16-2017,W04-3250,0,0.0300277,"et al., 2002) was used as an indicator of relative improvement. In order to maximize the BLEU score, we ran five trials of the local hill climbing algorithm, until BLEU converged onto a fixed score. Each trial was run with random initialization values, and varied the values between certain boundaries. From these trials, we took the best scoring parameter values. (WER) and Position-independent word Error Rate (PER).11 We have added significance levels for the BLEU and NIST scores, by comparing the no WSD condition with the WSD condition. Significance was calculated using bootstrap resampling (Koehn, 2004). The results are presented in Table 1.12 Significant improvements were made for Beta and Sclera (in the BLEU condition). The observation that WSD does not more significantly improve the evaluation results can be explained by the fact that the evaluation set is small and does not contain many polysemous words with multiple senses which are linked to a pictograph in the evaluation set. Only six examples were found. For that reason, we selected another 20 sentences from the WAI-NOT corpus that contain a word that has at least two pictographs attached to at least two of its synsets (belonging to"
S16-2017,vossen-etal-2008-integrating,0,0.0854467,"Missing"
S16-2017,P07-1005,0,\N,Missing
S16-2017,vossen-etal-2012-dutchsemcor,0,\N,Missing
S16-2017,W15-4711,1,\N,Missing
schuurman-vandeghinste-2010-cultural,C08-2024,0,\N,Missing
schuurman-vandeghinste-2010-cultural,schuurman-2008-spatiotemporal,1,\N,Missing
schuurman-vandeghinste-2010-cultural,2010.eamt-1.38,1,\N,Missing
schuurman-vandeghinste-2010-cultural,2009.eamt-1.21,1,\N,Missing
schuurman-vandeghinste-2010-cultural,lenci-etal-2002-multilingual,0,\N,Missing
schuurman-vandeghinste-2010-cultural,schuurman-etal-2010-interacting,1,\N,Missing
schuurman-vandeghinste-2010-cultural,mani-etal-2008-spatialml,0,\N,Missing
van-den-bosch-etal-2006-transferring,W96-0102,0,\N,Missing
van-den-bosch-etal-2006-transferring,zavrel-daelemans-2000-bootstrapping,0,\N,Missing
van-den-bosch-etal-2006-transferring,P03-1062,1,\N,Missing
van-den-bosch-etal-2006-transferring,J95-4004,0,\N,Missing
van-den-bosch-etal-2006-transferring,J01-2002,0,\N,Missing
van-noord-etal-2006-syntactic,W00-1505,0,\N,Missing
van-noord-etal-2006-syntactic,bouma-kloosterman-2002-querying,0,\N,Missing
van-noord-etal-2006-syntactic,W97-1502,0,\N,Missing
van-noord-etal-2006-syntactic,C00-2144,0,\N,Missing
van-noord-etal-2006-syntactic,2006.jeptalnrecital-invite.2,1,\N,Missing
van-noord-etal-2006-syntactic,vandeghinste-etal-2006-metis,1,\N,Missing
van-noord-etal-2006-syntactic,I05-7011,0,\N,Missing
van-noord-etal-2006-syntactic,W03-2414,1,\N,Missing
vandeghinste-2002-lexicon,van-eynde-etal-2000-part,0,\N,Missing
vandeghinste-etal-2006-metis,oostdijk-etal-2002-experiences,0,\N,Missing
vandeghinste-etal-2006-metis,boutsis-etal-2000-robust,0,\N,Missing
vandeghinste-etal-2006-metis,2005.mtsummit-ebmt.12,1,\N,Missing
vandeghinste-etal-2006-metis,P02-1040,0,\N,Missing
vandeghinste-etal-2006-metis,alsina-etal-2002-catcg,1,\N,Missing
vandeghinste-etal-2006-metis,W98-1231,1,\N,Missing
vandeghinste-etal-2008-evaluation,habash-dorr-2002-handling,0,\N,Missing
vandeghinste-etal-2008-evaluation,P02-1040,0,\N,Missing
vandeghinste-etal-2008-evaluation,A00-1031,0,\N,Missing
vandeghinste-etal-2008-evaluation,N03-1017,0,\N,Missing
vandeghinste-etal-2008-evaluation,N03-1013,0,\N,Missing
vandeghinste-etal-2008-evaluation,2003.mtsummit-systems.9,0,\N,Missing
vandeghinste-etal-2008-evaluation,2005.mtsummit-papers.11,0,\N,Missing
vandeghinste-etal-2008-evaluation,2003.eamt-1.7,1,\N,Missing
vandeghinste-etal-2008-evaluation,2007.mtsummit-papers.74,0,\N,Missing
vandeghinste-etal-2008-evaluation,vandeghinste-etal-2006-metis,1,\N,Missing
vandeghinste-schuurman-2014-linking,W08-2116,0,\N,Missing
vandeghinste-schuurman-2014-linking,A00-1031,0,\N,Missing
vandeghinste-schuurman-2014-linking,vandeghinste-2002-lexicon,1,\N,Missing
vandeghinste-tjong-kim-sang-2004-using,J93-1004,0,\N,Missing
vandeghinste-tjong-kim-sang-2004-using,oostdijk-etal-2002-experiences,0,\N,Missing
vandeghinste-tjong-kim-sang-2004-using,vandeghinste-2002-lexicon,1,\N,Missing
W04-1015,A00-1031,0,0.094406,"Missing"
W04-1015,oostdijk-etal-2002-experiences,0,0.107547,"Missing"
W04-1015,vandeghinste-tjong-kim-sang-2004-using,1,0.812281,"Missing"
W04-1015,vandeghinste-2002-lexicon,1,0.820164,"lternative of the sentence it is checked whether the word can be reduced. The words are sent to a WordSplitter-module, which takes a word as its input and checks if it is a compound by trying to split it up in two parts: the modifier and the head. This is done by lexicon lookup of both parts. If this is possible, it is checked whether the modifier and the head can be recompounded according to the word formation rules for Dutch (Booij and van Santen, 1995), (Haeseryn et al., 1997). This is done by sending the modifier and the head to a WordBuilding-module, which is described in more detail in (Vandeghinste, 2002). This is a hybrid module combining the compounding rules with statistical information about the frequency of compounds with the samen head, the frequency of compounds with the same modifier, and the number of different compounds with the same head. Only if this module allows the recomposition of the modifier and the head, the word can be considered to be a compound, and it can potentially be reduced to its head, removing the modifier. If the words occur in a database which contains a list of compounds which should not be split up, the word cannot be reduced. For example, the word voetbal [E:"
W10-3713,W08-1301,0,\N,Missing
W10-3713,2010.eamt-1.38,1,\N,Missing
W10-3713,2005.mtsummit-papers.11,0,\N,Missing
W10-3713,C10-2093,1,\N,Missing
W10-3713,W04-0408,0,\N,Missing
W13-5638,augustinus-etal-2012-example,1,0.776083,"Missing"
W13-5638,W10-3713,1,0.800099,"GrETEL 2.0, which will be able to deal with larger treebanks, and which will also be able to deal with XML formats other than Alpino XML. We plan to make the various layers less intertwined to make adaptation for other languages, treebanks, and/or data formats possible. The major challenge, however, is making the system usable for very large treebanks, such as the LASSY large corpus (500M tokens). In the next implementation the PostgreSQL database will be replaced by a native XML database, such as BaseX,10 and we will make use of the Varro Treebank Indexer, cf. ‘version 2’ in Figure 3. Varro (Martens and Vandeghinste, 2010), (Martens, 2011) is a toolkit and algorithm for indexing regular structures in treebanks. Subtrees are listed in the Varro Treebank Index, which indicates where they are located and what their frequency is. With this preprocessing step we intend to allow for faster treebank mining and, we hope, to query large (huge) treebanks in a timespan acceptable for the intended users, i.e. descriptive linguists. http://www.postgresql.org/ http://www.let.rug.nl/vannoord/Lassy/alpino_ds.dtd 10 http://basex.org 8 9 Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Link"
W13-5638,oostdijk-etal-2002-experiences,1,0.751238,"Missing"
W13-5638,P05-3009,0,0.0246587,"eebank is the obvious resource to use, rather than a corpus just annotated for part of speech. The construction in mind may also determine which treebank is consulted. The LASSY treebank contains written Dutch, whereas the CGN treebank contains (transcribed) spoken language. For this example, we will use CGN, as case studies on the LASSY treebank were already described in previous work (Augustinus et al., 2012). We will furthermore indicate the differences between the updated version of GrETEL and the previous release. Work related to our approach is the now deceased Linguist’s Search Engine (Resnik and Elkiss, 2005), a tool that also used example-based querying; and the TIGER Corpus Navigator (Hellmann et al., 2010), which is a SemanticWeb system used to classify and retrieve sentences from the TIGER corpus on the basis of abstract linguistic concepts. 1 2 http://www.clarin.eu http://nederbooms.ccl.kuleuven.be/eng/gretel Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 424 of 474] Natural language example The user provides a relevant natural language example, containing the syntactic construction (s)he is looking"
W13-5638,2006.jeptalnrecital-invite.2,0,0.290445,"Missing"
W14-5815,J03-1002,0,0.00531325,"Missing"
W14-5815,2005.mtsummit-papers.11,0,\N,Missing
W14-5815,C12-3030,0,\N,Missing
W14-5815,vandeghinste-schuurman-2014-linking,1,\N,Missing
W14-5815,I08-2091,0,\N,Missing
W15-1010,P05-1074,0,0.148799,"Missing"
W15-1010,E14-1022,0,0.0195439,"nd on semantics-based tree alignment will hint at the potential of using additional sources of linguistic information, such as lexical semantics and semantic roles, for fuzzy matching. 2 Background The principle of fuzzy matching in a translation memory can be applied to flat sequences or to trees, and either be applied in a linguistically unaware way or involve some degree of linguistic knowledge. Fuzzy matching may be performed using classical sequence comparison metrics like Levenshtein distance (Levenshtein, 1966) or other metrics specifically designed for fuzzy matching, like the ones of Bloodgood and Strauss (2014). It may also be ap61 Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–64, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics plied using MT evaluation metrics like TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2014), which were originally designed to compare MT output with one or more reference translations. In this respect, it should be noted that fuzzy matching is performed at the sub-segment level, as it determines matching parts, while MT evaluation is performed on the segment level (Ca"
W15-1010,W10-1751,0,0.0602764,"Missing"
W15-1010,W14-3348,0,0.0136153,"in a linguistically unaware way or involve some degree of linguistic knowledge. Fuzzy matching may be performed using classical sequence comparison metrics like Levenshtein distance (Levenshtein, 1966) or other metrics specifically designed for fuzzy matching, like the ones of Bloodgood and Strauss (2014). It may also be ap61 Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–64, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics plied using MT evaluation metrics like TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2014), which were originally designed to compare MT output with one or more reference translations. In this respect, it should be noted that fuzzy matching is performed at the sub-segment level, as it determines matching parts, while MT evaluation is performed on the segment level (Callison-Burch et al., 2012). However, evaluating MT output at the subsegment level may also be helpful, for instance to determine whether specific parts are translated better than other ones. As for the quality of fuzzy matching metrics, combined metrics appear to perform better than individual ones. For instance, Vanal"
W15-1010,2011.mtsummit-papers.52,0,0.0490988,"Missing"
W15-1010,D08-1008,0,0.0704218,"Missing"
W15-1010,N03-1017,0,0.0526753,"Missing"
W15-1010,2010.jec-1.4,0,0.0234622,"llison-Burch et al., 2012). However, evaluating MT output at the subsegment level may also be helpful, for instance to determine whether specific parts are translated better than other ones. As for the quality of fuzzy matching metrics, combined metrics appear to perform better than individual ones. For instance, Vanallemeersch and Vandeghinste (2015) combine linguistically unaware with syntactically oriented metrics using regression trees. In recent years, there has been increasing interest in integrating fuzzy matches with SMT. An example of a linguistically unaware approach is described by Koehn and Senellart (2010), who pretranslate sentences before decoding, using the word alignment between the matching source sentence in the translation memory and its translation. Instead of using the translation of matched parts for pretranslation, the parts and their translation may also be used for enriching a phrase table, as shown by Simard and Isabelle (2009). An example of a linguistically aware integration approach is described by Zhechev and van Genabith (2010), who pretranslate sentences using the node alignment between the parse trees of the source and target sentences in the translation memory. He et al. ("
W15-1010,C10-1081,0,0.0456923,"Missing"
W15-1010,P11-1023,0,0.068757,"Missing"
W15-1010,meyers-etal-2004-annotating,0,0.0551578,"Missing"
W15-1010,J05-1004,0,0.0196143,"Missing"
W15-1010,schuurman-etal-2010-interacting,0,0.0492477,"Missing"
W15-1010,2009.mtsummit-papers.14,0,0.0471078,"nste (2015) combine linguistically unaware with syntactically oriented metrics using regression trees. In recent years, there has been increasing interest in integrating fuzzy matches with SMT. An example of a linguistically unaware approach is described by Koehn and Senellart (2010), who pretranslate sentences before decoding, using the word alignment between the matching source sentence in the translation memory and its translation. Instead of using the translation of matched parts for pretranslation, the parts and their translation may also be used for enriching a phrase table, as shown by Simard and Isabelle (2009). An example of a linguistically aware integration approach is described by Zhechev and van Genabith (2010), who pretranslate sentences using the node alignment between the parse trees of the source and target sentences in the translation memory. He et al. (2011) apply linguistic knowledge on matching parts during – instead of before – decoding, for instance semantic knowledge. As indicated above, pretranslation using fuzzy matching involves word alignment or tree alignment. The latter may be based on syntactic information in the trees, but may also involve semantic roles (Vanallemeersch, 2012"
W15-1010,2006.amta-papers.25,0,0.0559942,"to trees, and either be applied in a linguistically unaware way or involve some degree of linguistic knowledge. Fuzzy matching may be performed using classical sequence comparison metrics like Levenshtein distance (Levenshtein, 1966) or other metrics specifically designed for fuzzy matching, like the ones of Bloodgood and Strauss (2014). It may also be ap61 Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–64, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics plied using MT evaluation metrics like TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2014), which were originally designed to compare MT output with one or more reference translations. In this respect, it should be noted that fuzzy matching is performed at the sub-segment level, as it determines matching parts, while MT evaluation is performed on the segment level (Callison-Burch et al., 2012). However, evaluating MT output at the subsegment level may also be helpful, for instance to determine whether specific parts are translated better than other ones. As for the quality of fuzzy matching metrics, combined metrics appear to perform better th"
W15-1010,W15-4920,1,0.797332,"2014), which were originally designed to compare MT output with one or more reference translations. In this respect, it should be noted that fuzzy matching is performed at the sub-segment level, as it determines matching parts, while MT evaluation is performed on the segment level (Callison-Burch et al., 2012). However, evaluating MT output at the subsegment level may also be helpful, for instance to determine whether specific parts are translated better than other ones. As for the quality of fuzzy matching metrics, combined metrics appear to perform better than individual ones. For instance, Vanallemeersch and Vandeghinste (2015) combine linguistically unaware with syntactically oriented metrics using regression trees. In recent years, there has been increasing interest in integrating fuzzy matches with SMT. An example of a linguistically unaware approach is described by Koehn and Senellart (2010), who pretranslate sentences before decoding, using the word alignment between the matching source sentence in the translation memory and its translation. Instead of using the translation of matched parts for pretranslation, the parts and their translation may also be used for enriching a phrase table, as shown by Simard and"
W15-1010,2006.jeptalnrecital-invite.2,0,0.0713089,"Missing"
W15-1010,N09-2004,0,0.0376063,"Missing"
W15-1010,2010.amta-papers.19,0,0.0458478,"Missing"
W15-1010,tiedemann-2010-lingua,0,\N,Missing
W15-1010,W12-3102,0,\N,Missing
W15-1010,W11-1003,0,\N,Missing
W15-1010,W11-2136,0,\N,Missing
W15-4711,vossen-etal-2008-integrating,0,0.544748,"Missing"
W15-4711,W98-0718,0,0.148152,"text as a means of written communication (Keskinen et al., 2012). Within the Able to Include framework,1 a EU project aiming to improve the living conditions of people with IDD, we developed a Pictograph-toText translation system. It provides help in constructing Dutch textual messages by allowing the user to input a series of pictographs and translates these messages into NL. English and Spanish versions of the tool are currently in development. It 1 2 Related work Our task shares elements with regular machine translation between natural languages and with Natural Language Generation (NLG). Jing (1998) retrieves the semantic concepts from WordNet and maps them to appropriate words to produce large amounts of lexical paraphrases for a specific application domain. Similar to our approach, Liu (2003) uses statistical language models as a solution to the word inflection problem, as there may exist multiple forms for a concept constituent. The language model re-scores all inflection forms in order to generate the best hypothesis in the output. Our solution is specifically tailored towards translation from pictographs into text. A number of pictograph-based input interfaces can be found in the li"
W15-4711,2005.mtsummit-papers.11,0,0.0203663,"etto (Vossen et al., 2008) database. Vandeghinste and Schuurman (2014) manually linked 5710 Sclera and 2746 Beta pictographs to Dutch synsets (groupings of synonymous words) in Cornetto. The direct route contains specific rules for appropriately dealing with pronouns (as pictographs for pronouns exist in Sclera and Beta) and contains one-on-one mappings between pictographs and individual lemmas in a dictionary. 5.2 5.4 Decoding We performed Viterbi-decoding based on a trigram language model, trained with the SRILM toolkit on a very large corpus. The Dutch training corpus consists of Europarl (Koehn, 2005), CGN (Oostdijk et al., 2003), CLEF (Peters and Braschler, 2001), DGT-TM (Steinberger et al., 2012) and Wikipedia.9 Architecture of the system When a pictograph is selected, its synset is retrieved, and from this synset we retrieve all the 7 Tuning the parameters 8 9 http://www.linguatools.de/disco/ 73 http://tst-centrale.org/producten/corpora/sonar-corpus/ http://en.wikipedia.org/wiki/ 6 Condition Sclera Baseline Rev. lem. Direct Synsets Articles Beta Baseline Rev. lem. Direct Synsets Articles Preliminary results We present results for Sclera-to-Dutch and Betato-Dutch. The test set consists o"
W15-4711,steinberger-etal-2012-dgt,0,0.023988,"Missing"
W15-4711,vandeghinste-schuurman-2014-linking,1,0.352892,"al language text was built. The system’s general architecture is outlined in section 5.2. It introduces a set of parameters, which were tuned on a training corpus (section 5.3). Finally, as explained in section 5.4, an optimal NL string is selected. 5.1 Linking pictographs to natural language text Pictographs are connected to NL words through a semantic route and a direct route. The semantic route concerns the use of WordNets, which are a core component of both the Textto-Pictograph and the Pictograph-to-Text translation systems. For Dutch, we used the Cornetto (Vossen et al., 2008) database. Vandeghinste and Schuurman (2014) manually linked 5710 Sclera and 2746 Beta pictographs to Dutch synsets (groupings of synonymous words) in Cornetto. The direct route contains specific rules for appropriately dealing with pronouns (as pictographs for pronouns exist in Sclera and Beta) and contains one-on-one mappings between pictographs and individual lemmas in a dictionary. 5.2 5.4 Decoding We performed Viterbi-decoding based on a trigram language model, trained with the SRILM toolkit on a very large corpus. The Dutch training corpus consists of Europarl (Koehn, 2005), CGN (Oostdijk et al., 2003), CLEF (Peters and Braschler,"
W15-4711,oostdijk-etal-2002-experiences,1,\N,Missing
W15-4711,H93-1040,0,\N,Missing
W15-4711,1993.mtsummit-1.24,0,\N,Missing
W15-4920,2005.mtsummit-papers.29,0,0.0686987,"or comparing any pair of sequences or trees (not necessarily sentences and their parse trees), for fuzzy matching in a TM, or for comparing machine translation (MT) output to a reference translation. As pointed out by Simard and Fujita (2012), the third type, MT automatic evaluation metrics, can also be used in the context of TMs, both as fuzzy matching metric and as metric for comparing the translation of a fuzzy match with the desired translation. Some matching methods speciﬁcally support the integration of fuzzy matches within an MT system (examplebased or statistical MT); see for instance Aramaki et al. (2005), Smith and Clark (2009), Zhechev and van Genabith (2010), Ma et al. (2011). Some matching methods are linguistically unaware. Levenshtein distance (Levenshtein, 1966), which calculates the effort needed to convert one sequence into another using the operations insertion, deletion and substitution, is the most commonly used fuzzy matching method (Bloodgood and Strauss, 2014). Tree edit distance (Klein, 1998) applies this principle to trees; another tree comparison method is tree alignment (Jiang et al., 1995).3 To allow using string-based matching methods on trees, there are several ways of co"
W15-4920,E14-1022,0,0.265086,"metric for comparing the translation of a fuzzy match with the desired translation. Some matching methods speciﬁcally support the integration of fuzzy matches within an MT system (examplebased or statistical MT); see for instance Aramaki et al. (2005), Smith and Clark (2009), Zhechev and van Genabith (2010), Ma et al. (2011). Some matching methods are linguistically unaware. Levenshtein distance (Levenshtein, 1966), which calculates the effort needed to convert one sequence into another using the operations insertion, deletion and substitution, is the most commonly used fuzzy matching method (Bloodgood and Strauss, 2014). Tree edit distance (Klein, 1998) applies this principle to trees; another tree comparison method is tree alignment (Jiang et al., 1995).3 To allow using string-based matching methods on trees, there are several ways of converting trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods speciﬁcally designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compare"
W15-4920,comelles-etal-2014-verta,0,0.0508164,"Missing"
W15-4920,W14-3348,0,0.0207968,"TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-speciﬁc rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine. D"
W15-4920,W14-3350,0,0.0111083,"(2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-speciﬁc rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine. Different types of metrics can be combined in order to join their strengths. For instance, the Asiya toolkit (Gim´enez and M´arquez, 2010) contains a large number of matching metrics of different origins and applies them for MT evaluation. An optimal metric set is determined by progressively adding metrics to the set if that increases the quality of the translation. 3 3.1 Independent variables In Sections 3."
W15-4920,2014.tc-1.10,0,0.331762,"metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-speciﬁc rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are based on multiple linguistic layers. The layers are assigned weights or combined using a support vector machine. Different types of metrics can be combined in order to join their strengths. For instance, the Asiya toolkit (Gim´enez and M´arquez, 2010) contains a large number of matching metrics of different origins and applies them for MT evaluation. An optimal metric set is determined by progressively adding metrics to the set if that increases the quality of the translation. 3 3.1 Independent variables In Sections 3.1.1, 3.1.2, and 3.1.3, we describe the independent variables of our ex"
W15-4920,2005.mtsummit-papers.11,0,0.0307708,"Missing"
W15-4920,2010.amta-papers.2,0,0.279924,"e is to select candidate sentences in the TM which are likely to reach a minimal matching threshold when submitting them to a fuzzy matching metric, in order to increase the speed of matching. A candidate sentence is a sentence which shares one or more ngrams of a minimal length N with Q, and which shares enough ngrams with Q so as to cover the latter sufﬁciently. The implementation of the ﬁlter uses a sufﬁx array (Manber and Myers, 1993), which allows for a very efﬁcient search for sentences sharing ngrams with Q.11 This approach is similar to the one used in the context of fuzzy matching by Koehn and Senellart (2010). In order to measure the usefulness of the AQC ﬁlter, we measured the tradeoff between the gain 11 We used the SALM toolkit (Zhang and Vogel, 2006) for building and consulting sufﬁx arrays in our experiment. 156 in speed and the loss of potentially useful matches. We used a sample of about 30,000 English-Dutch sentence pairs selected from Europarl (Koehn, 2005), and a threshold of 0.2. After applying a leave-one-out test, which consists of considering each Si in the sample as a Q and comparing it to all the other Si in the sample, it appeared that the AQC ﬁlter selected about 9 candidate sent"
W15-4920,W05-0904,0,0.0385718,"8) and based on post-order tree traversal. Examples of matching methods speciﬁcally designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by"
W15-4920,P11-1023,0,0.0851123,"racter-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output to one or more reference translations, not only by comparing word forms, but also through shallow linguistic knowledge, i.e. by calculating the stem of words (in some cases using language-speciﬁc rules), and by using lists with function words, synonyms and paraphrases. Some MT evaluation metrics, such as VERTa (Comelles et al., 2014) and LAYERED (Gautam and Bhattacharyya, 2014), and some fuzzy matching methods, like the one of Gupta et al. (2014), are"
W15-4920,P11-1124,0,0.0503343,"Missing"
W15-4920,2012.amta-papers.26,0,0.162878,"Missing"
W15-4920,2006.amta-papers.25,0,0.0954129,"ng trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods speciﬁcally designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Meteor (Denkowski and Lavie, 2014). The latter aligns MT output"
W15-4920,W14-3354,0,0.0525414,"Missing"
W15-4920,2010.amta-papers.19,0,0.449994,"Missing"
W15-4920,P02-1040,0,0.0999752,"matching methods on trees, there are several ways of converting trees into strings without information loss, as described in Li et al. (2008), who applies a method designed by Pr¨ufer (1918) and based on post-order tree traversal. Examples of matching methods speciﬁcally designed for fuzzy matching are percent match and ngram precision (Bloodgood and Strauss, 2014), which act on unigrams and longer ngrams. Baldwin (2010) compares bag-of-words fuzzy matching metrics with order-sensitive metrics, and wordbased with character-based metrics. Examples of well-known MT evaluation metrics are BLEU (Papineni et al., 2002) and TER, i.e. Translation Error Rate4 (Snover et al., 2006). Linguistically aware matching methods make use of several layers of information. The ”subtree metric” of Liu and Gildea (2005) compares subtrees of phrase structure trees. We devised a similar method, shared partial subtree matching, described in Section 3.1.2. Matching can also involve dependency structures, as in the approach of Smith and Clark (2009), head word chains (Liu and Gildea, 2005), semantic roles, as in the HMEANT metric (Lo and Wu, 2011), and semantically similar words or paraphrases, as in the MT evaluation metric Met"
W15-5119,W08-2116,0,0.0746772,"Missing"
W15-5119,vandeghinste-schuurman-2014-linking,1,0.553241,"pictograph, namely carrot soup. 4. Linking pictographs to other WordNets WordNets, lexical-semantic databases, are an essential component of the Text-to-Pictograph translation system. For the original Dutch system, Cornetto [14, 15] was used. Its English and Spanish counterparts are Princeton WordNet 3.0 [1]15 and the Spanish Multilingual Central Repository (MCR) 3.0 [16].16 WordNets contain synsets (groupings of synonyms that have an abstract, usually numeric identifier, see Figure 3) and are designed in such way that each synset is connected to one or more lemmas. Vandeghinste and Schuurman [17] manually linked 5710 Sclera pictographs and 2760 Beta pictographs to Dutch synsets in Cornetto.17 An essential step in building Text-to-Pictograph translation systems for other languages is making sure that the pictographs are connected to (sets of) words in those languages. 3. Pictographic languages Mihalcea and Leong [8] note that complex and abstract concepts (such as democracy) are not always easy to depict. Some characteristics of natural languages may not be present in the pictographic languages.11 Usually, no distinction between singular and plural is made. Tense, aspect, and inflectio"
W15-5119,W14-5815,1,0.900752,".si.ehu.es/web/MCR/ 17 As a Cornetto license can no longer be obtained, the authors will transfer these links to the Open Source Dutch WordNet (http://wordpress.let.vupr.nl/odwn/). 14 The cultural differences remain. 11 We use the term pictographic language in order to refer to the combination of individual pictographs, that belong to a specific pictograph set, into a larger meaningful structure. 12 There are some exceptions. Beta, for instance, contains the Dutch articles. 111 Vossen et al. [19], filling the database with more than 80000 links between Dutch and English synsets. Sevens et al. [18] showed that a considerable amount of the original links were highly erroneous, making them not yet very reliable for multilingual processing. By using these equivalence relations, we would risk assigning pictographs to unrelated synsets in Princeton WordNet 3.0. In the case of a Dutch synset being wrongly connected to an English synset, writing a message in English would allow the system to generate pictographs that depict another concept. Therefore, we used the filtered,19 more reliable connections that were established by Sevens et al. [18]. As a result, it became possible to automatically"
W15-5119,W15-4711,1,0.805901,"Missing"
W15-5119,H93-1040,0,\N,Missing
W15-5119,1993.mtsummit-1.24,0,\N,Missing
W15-5119,vossen-etal-2008-integrating,0,\N,Missing
