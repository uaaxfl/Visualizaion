2018.gwc-1.5,I08-1041,0,0.098455,"ngle words and the component structure for phrases included in RuWordNet. The described procedure of transformation highlights the specific features of each type of thesaurus representations. 1 Introduction WordNet thesaurus is one of the popular language resources for natural language processing (Fellbaum, 1998). The projects for creating WordNet-like resources have been initiated for many languages in the world (Vossen, 1998; Bond and Paik, 2012). Other thesaurus models are rarely discussed, created and used in NLP. In several works, S.Szpakowicz and coauthors (Jarmasz and Szpakowicz, 2004; Aman and Szpakowicz, 2008; Kennedy and Szpakowicz, 2008) evaluated two versions of Roget’s thesaurus in several applications. Borin and colleagues (Borin and Forsberg, 2009; Borin et al. 2013) compared the structure of the Swedish thesaurus Saldo with the WordNet structure. In (Borin et al., 2014) automatic generation of Swedish Roget’s thesaurus and its comparing with the existing Roget-style thesaurus for Swedish is discussed. For the Russian language, RuThes thesaurus has been created more than fifteen years ago (Loukachevitch and Dobrov, 2002). It was utilized in various information-retrieval and NLP applications"
2018.gwc-1.5,W04-2214,0,0.092867,"очивать (to sleep) because if someone dreams something, then this someone is sleeping. Domain relations Since relations in such thesauri as WordNet are mostly generic (hyponym-hypernym), there exists a so-called ""tennis problem"" (Miller, 1998), which is that synsets from the same domain (for example, related to tennis: tennis player, racket, court) are very far from each other in the WordNet hierarchy. To solve this problem in part, a hierarchical system of domains (domains)3 has been proposed, and WordNet synsets were semi-automatically assigned to one or more domains (Magnini, Pianta, 2000; Bentivogli et al., 2004). This domain system is now partially transferred to RuWordNet. The mechanism of introducing domains for the RuWordNet synsets was as follows. The existing domain system for Princeton WordNet was taken. First, the domain list was refined: the subject areas that were not presented in the RuWordNet thesaurus were removed (i.e. Heraldry), and several new domains were added. For example, domain labels corresponding to world religions and some confessions were introduced. Currently, RuWordNet has 156 domains. The domains labels can be considered as a list of categories for a knowledge-based categor"
2018.gwc-1.5,borin-etal-2014-bring,0,0.060317,"Missing"
2018.gwc-1.5,E14-2026,0,0.0678202,"Missing"
2018.gwc-1.5,galieva-etal-2017-russian,0,0.0283916,"v, 2010), text categorization (Loukachevitch and Dobrov, 2015), detecting Russian paraphrases (Loukachevitch et al., 2017), etc. Using the RuThes model for the concept representation, several domain-specific thesauri have been created for NLP and domain-specific information-retrieval applications including Sociopolitical thesaurus (Loukachevitch and Dobrov, 2015), Ontology on Natural Sciences and Technology (Dobrov and Loukachevitch, 2006), Banking thesaurus (Nokel and Loukachevitch, 2016) and others. Currently, RuThes concepts provide a basis for creating the Tatar Socio-Political Thesaurus (Galieva et al., 2017). In 2013, RuThes was partially published for non-commercial use (Loukachevitch et al., 2014). But people would like to have a large Russian wordnet. Therefore, we have initiated a transforming procedure from the published version of RuThes (RuThes-lite) to the largest Russian WordNet (RuWordNet 1 ), which we describe in this paper. This transformation allows us to show similarities and differences between two resources in a detailed way. RuWordNet currently includes 115 thousand unique words and phrases. 1 http://ruwordnet.ru/en/ The structure of this paper is as follows. In Section 2, we des"
2018.gwc-1.5,W17-1902,0,0.0605059,"Missing"
2018.gwc-1.5,W14-0121,1,0.9079,"Kennedy and Szpakowicz, 2008) evaluated two versions of Roget’s thesaurus in several applications. Borin and colleagues (Borin and Forsberg, 2009; Borin et al. 2013) compared the structure of the Swedish thesaurus Saldo with the WordNet structure. In (Borin et al., 2014) automatic generation of Swedish Roget’s thesaurus and its comparing with the existing Roget-style thesaurus for Swedish is discussed. For the Russian language, RuThes thesaurus has been created more than fifteen years ago (Loukachevitch and Dobrov, 2002). It was utilized in various information-retrieval and NLP applications (Loukachevitch and Dobrov, 2014). RuThes was successfully evaluated in text summarization (Mani et al., 2002), text clustering (Dobrov and Pavlov, 2010), text categorization (Loukachevitch and Dobrov, 2015), detecting Russian paraphrases (Loukachevitch et al., 2017), etc. Using the RuThes model for the concept representation, several domain-specific thesauri have been created for NLP and domain-specific information-retrieval applications including Sociopolitical thesaurus (Loukachevitch and Dobrov, 2015), Ontology on Natural Sciences and Technology (Dobrov and Loukachevitch, 2006), Banking thesaurus (Nokel and Loukachevitch,"
2018.gwc-1.5,loukachevitch-alekseev-2014-summarizing,1,0.874004,"Missing"
2018.gwc-1.5,loukachevitch-gerasimova-2017-human,1,0.785531,"word synonyms. Numerous examples of multisynonyms can be found also in English and can be met in WordNet. For example, plant  industrial plant, platform  political platform, park  car park  parking lot. But in RuThes, multisynonyms were specially searched and added. RuThes also includes multiword expressions with so called relational idiosyncrasy, that is multiword expressions that look like compositional ones but they have specificity in relations with other single words and/or expressions, which usually means that these expressions denote some important concepts, entities or situations (Loukachevitch and Gerasimova, 2017). For example, such phrase as дорожное движение (road traffic) seems to be compositional one, but it has hyponyms: левостороннее движение (left-hand traffic) and правостороннее движение (right-hand traffic): the existence of such hyponyms cannot be inferred from its component words. Currently, all multiword expressions (54 thousands of 115 thousand entries) of RuThes-lite were transferred to RuWordNet. In such a way, it is possible to say that RuWordNet contains the maximal share of phrases in synsets among other WordNet-like resources. It means that the representation of phrases in RuWordNet"
2018.gwc-1.5,magnini-cavaglia-2000-integrating,0,0.119086,"Missing"
2018.gwc-1.5,W16-1806,1,0.766139,"evitch and Dobrov, 2014). RuThes was successfully evaluated in text summarization (Mani et al., 2002), text clustering (Dobrov and Pavlov, 2010), text categorization (Loukachevitch and Dobrov, 2015), detecting Russian paraphrases (Loukachevitch et al., 2017), etc. Using the RuThes model for the concept representation, several domain-specific thesauri have been created for NLP and domain-specific information-retrieval applications including Sociopolitical thesaurus (Loukachevitch and Dobrov, 2015), Ontology on Natural Sciences and Technology (Dobrov and Loukachevitch, 2006), Banking thesaurus (Nokel and Loukachevitch, 2016) and others. Currently, RuThes concepts provide a basis for creating the Tatar Socio-Political Thesaurus (Galieva et al., 2017). In 2013, RuThes was partially published for non-commercial use (Loukachevitch et al., 2014). But people would like to have a large Russian wordnet. Therefore, we have initiated a transforming procedure from the published version of RuThes (RuThes-lite) to the largest Russian WordNet (RuWordNet 1 ), which we describe in this paper. This transformation allows us to show similarities and differences between two resources in a detailed way. RuWordNet currently includes 1"
2018.gwc-1.5,W07-1710,0,0.0487643,"t synsets: (1) -&gt; (4). For example, domain “Art” is described as RuThes concept Art with full expansion, which adds to the Art domain all hyponyms, parts, dependent concepts obtained by logical inference using the properties of transitivity and inheritance (Section 3.1). As a result, “Art” concepts comprise more than 700 RuThes concepts, including Jazzman, Piece of painting, Harp, etc. Then RuWordNet synsets originated from these RuThes concepts were also assigned to the Art domain. 5.3 Derivational relations For RuWordNet, the derivational relations were also introduced (Leseva et al., 2015; Pala and Hlaváčková, 2007, Piasecki, et al, 2012). These relations are lexical, that is established between lexical entries. At the moment, these relations are established for those words that have the same beginning of the word (without prefixes). The derivation relations were established between words if two conditions were fulfilled:   the words have the same beginnings, these words refer to concepts that either have a direct relationship in the RuThes thesaurus or the relationship can be derived from the properties of transitivity and inheritance established in RuThes. For example, for the word аренда (lease), t"
2019.gwc-1.3,S07-1002,0,0.0907451,"Missing"
2019.gwc-1.3,L18-1723,0,0.0254651,"ives. They constructed networks based on a distributional thesaurus over eight different time windows, clustered these networks and compared these clusters to identify the emergence of novel senses. The performance of the method has been evaluated manually as well as by comparison with WordNet and a list of slang words. But Mitra et al. did not check if WordNet misses some senses. The task of revising and verifying of resources is important for developers of WordNet-like resources. Some ontological tools have been proposed to check consistency of relations in WordNet (Guarino and Welty, 2004; Alvez et al., 2018). Some authors report about revision of mistakes and inconsistencies in their wordnets in the process of linking the wordnet and English WordNet (Cristea et al., 2004; Rudnicka et al., 2012). Rambousek et al. (2018) consider a crowdsourcing tool allowing a user of Czech wordnet to report errors. Users may propose an update of any data value. These suggestions can be approved or rejected by editors. Also visualization tools can help to find problems in wordnets (Piasecki et al. 2013; Johannsen et al., 2011). Loukachevitch (2019) proposed to use embedding-based word similarities to find possible"
2019.gwc-1.3,W11-2501,0,0.0463394,"Missing"
2019.gwc-1.3,P13-1133,0,0.0155794,"tic resources as WordNet. The method of verification procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. We calculated such word similarities on the basis of a Russian news collection and Russian wordnet (RuWordNet). We applied the procedure to more than 30 thousand words and found some serious errors in word sense description, including incorrect or absent relations or missed main senses of ambiguous words. 1 Introduction Large lexical-semantic resources such as Princeton WordNet (Fellbaum, 1998) and wordnets created for other languages (Bond and Foster, 2013) are important instruments for natural language processing. Developing and maintaining such resources requires special efforts, because it is difficult to find errors or gaps in structures consisting of thousands lexical units and relations between them. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidental"
2019.gwc-1.3,2018.gwc-1.5,1,0.709566,"ty Moscow, Russia Louk_nat@mail.ru Ekaterina Parkhomenko Lomonosov Moscow State University Moscow, Russia parkat13@yandex.ru In this paper, we consider an approach how to use embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2016). But we can use them in the opposite way: to utilize embedding-based similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. The structure of the paper is as follows. Section 2 is devoted to related work. In Section 3 we briefly present RuWordNet. Section 4 describes the procedure of calculating two types of word similarities based on thesaurus and a corpus. In Section 5 we analyze discrepancies between thesaurus-based and corpus-based word similarities, which can appear because of different reasons. In Section 6 we study groupings of distributionally similar words to an initial word using the thesaurus. Abstract In this paper we consider an approach to verification of large lexical-semantic resources as WordNet."
2019.gwc-1.3,W14-0154,0,0.0269761,"Missing"
2019.gwc-1.3,P19-1577,1,0.116236,"check consistency of relations in WordNet (Guarino and Welty, 2004; Alvez et al., 2018). Some authors report about revision of mistakes and inconsistencies in their wordnets in the process of linking the wordnet and English WordNet (Cristea et al., 2004; Rudnicka et al., 2012). Rambousek et al. (2018) consider a crowdsourcing tool allowing a user of Czech wordnet to report errors. Users may propose an update of any data value. These suggestions can be approved or rejected by editors. Also visualization tools can help to find problems in wordnets (Piasecki et al. 2013; Johannsen et al., 2011). Loukachevitch (2019) proposed to use embedding-based word similarities to find possible mistakes or inconsistencies in a WordNet-like thesaurus. In the current paper we provide some additional details for the (Loukachevitch, 2019) study. 3 RuWordNet RuWordNet was created on the basis of another Russian thesaurus RuThes in 2016, which was developed as a tool for natural language processing during more than 20 years (Loukachevitch and Dobrov, 2002). Currently, the published version of RuWordNet includes 110 thousand Russian words and expressions. The important feature of RuWordNet (and its source RuThes), which is"
2019.gwc-1.3,Y11-1028,0,0.0497096,"Missing"
2019.gwc-1.3,Q16-1003,0,0.0125322,"difficult to find errors or gaps in structures consisting of thousands lexical units and relations between them. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder, Palmer, 2004; Bond, Wang, 2014). 2 Related Work In (Lau et al. 2014), the task of finding unattested senses in a dictionary is studied. At first, they apply the method of word sense induction based on LDA topic modeling. Each extracted sense is represented to top-N words in the constructed topics. To compute the similarity between a sense and a topic, the words in the definition are converted into the probability distr"
2019.gwc-1.3,W11-2508,0,0.0287472,"nse and a topic, the words in the definition are converted into the probability distribution. Then two probability distributions (gloss-based and topic-based) are compared using the JensenShannon divergence. It was found that the proposed novelty measure could identify target lemmas with high- and medium-frequency novel senses. But the authors evaluated their method using word sense definitions in the Macmillan 1 1 http://ruwordnet.ru/en/ dictionary and did not check the quality of relations presented in a thesaurus. A series of works was devoted to studies of semantic changes in word senses (Gulordava and Baroni, 2011; Mitra et al., 2015; Frermann, Lapata, 2016) . Gulordava and Baroni (2011) study semantic change of words using Google ngram corpus. They compared frequencies and distributional models based on word bigrams in 60s and 90s. They found that significant growth in frequency often reveals the appearance of a novel sense. Also it was found that sometimes the senses of words do not change but the context of their use changed significantly. For example, the context of word parent considerably change in 90s because of the most frequent collocation single parent family. In (Mitra et al., 2015), the aut"
2019.gwc-1.3,P14-1025,0,0.0120389,"gaps in structures consisting of thousands lexical units and relations between them. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder, Palmer, 2004; Bond, Wang, 2014). 2 Related Work In (Lau et al. 2014), the task of finding unattested senses in a dictionary is studied. At first, they apply the method of word sense induction based on LDA topic modeling. Each extracted sense is represented to top-N words in the constructed topics. To compute the similarity between a sense and a topic, the words in the definition are converted into the probability distribution. Then two p"
2019.gwc-1.3,W11-4643,0,0.0249871,"hen two probability distributions (gloss-based and topic-based) are compared using the JensenShannon divergence. It was found that the proposed novelty measure could identify target lemmas with high- and medium-frequency novel senses. But the authors evaluated their method using word sense definitions in the Macmillan 1 1 http://ruwordnet.ru/en/ dictionary and did not check the quality of relations presented in a thesaurus. A series of works was devoted to studies of semantic changes in word senses (Gulordava and Baroni, 2011; Mitra et al., 2015; Frermann, Lapata, 2016) . Gulordava and Baroni (2011) study semantic change of words using Google ngram corpus. They compared frequencies and distributional models based on word bigrams in 60s and 90s. They found that significant growth in frequency often reveals the appearance of a novel sense. Also it was found that sometimes the senses of words do not change but the context of their use changed significantly. For example, the context of word parent considerably change in 90s because of the most frequent collocation single parent family. In (Mitra et al., 2015), the authors study the detection of word sense changes by analyzing digitized books"
2019.gwc-1.3,P06-1101,0,0.100993,"sense description, including incorrect or absent relations or missed main senses of ambiguous words. 1 Introduction Large lexical-semantic resources such as Princeton WordNet (Fellbaum, 1998) and wordnets created for other languages (Bond and Foster, 2013) are important instruments for natural language processing. Developing and maintaining such resources requires special efforts, because it is difficult to find errors or gaps in structures consisting of thousands lexical units and relations between them. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder, Palmer, 2004; B"
2019.gwc-1.3,W04-0811,0,0.0371464,"Missing"
2019.gwc-1.9,P13-1133,0,0.0330307,"formed into RuWordNet, having the WordNet-like structure. Now, the RuThes English data are utilized to establish matching from the RuWordNet synsets to the WordNet synsets. 1 Introduction The Princeton WordNet thesaurus (Fellbaum, 1998, Miller, 1998) created for the English language is one of the most popular linguistic resources used in natural language processing. In many countries their own projects on creating WordNet-like resources (wordnets) for national languages have been initiated (Vossen, 1998). The Open Multilingual WordNet project is currently being developed (Bond and Paik, 2012; Bond and Foster, 2013; Rudnicka et al., 2017). The goal of the project is to link together the existing wordnets created for different languages with an open license 1 . To connect a new language to the project, it is necessary to associate synsets of this language with WordNet synsets and present the data in the required format. Sources of links of a specific wordnet to English synsets of Princeton WordNet can be different (Vossen, 1998; Piante et al., 2002). Some wordnets have been developed with semiautomatic translation of Princeton WordNet synsets, and therefore these links exist from the 1 http: // compling."
2019.gwc-1.9,2018.gwc-1.5,1,0.724625,"ivalents, inaccurate descriptions of Polish word senses could be corrected. Maziarz et al. (2013) provide quantitative characteristics of the established relations: the Ihyponymy relation was the most frequent link between synsets of WordNet and plWordNet. This can be explained by the existence of a large number of lexical and cultural lacunae, greater lexicalization of the category of gender in the Polish language (for example, for the names of roles, posts of people), the use of diminutive names in Polish, etc. 3 RuWordNet Thesaurus The Russian wordnet RuWordNet (Loukachevitch et al., 2016; Loukachevitch et al., 2018) has been created on the basis of another Russian thesaurus RuThes in 2016 (Loukachevitch, Dobrov, 2002). Main units of RuThes are concepts, each concept has a monosemous and clear name and the set of text entries that convey the corresponding concept in texts. The text entries of a concept can include single words of different parts of speech, multiword expressions and also compositional phrases, with the same meaning. To represent bilingual data, the RuThes concept has the English name of concept and the set of English text entries with the same variety of text entries. To create RuWordNet,"
2019.gwc-1.9,R13-1058,0,0.0162925,"WordNet ) and WordNet was performed in 2012 (Rudnicka et al., 2012). To establish links, the following set of interlingual (I) relationships was used: Isynonymy, I-hyponymy, I-hyperonymy, Imeronymy, I-holonymy, I-quasi-synonymy (near synonymy), I- inter-register synonymy. The latter relation is established when the synsets in Polish and English have the same meaning, but refer to different language registers. The matching between the Polish and English synsets was performed manually. In the process of searching for equivalents, inaccurate descriptions of Polish word senses could be corrected. Maziarz et al. (2013) provide quantitative characteristics of the established relations: the Ihyponymy relation was the most frequent link between synsets of WordNet and plWordNet. This can be explained by the existence of a large number of lexical and cultural lacunae, greater lexicalization of the category of gender in the Polish language (for example, for the names of roles, posts of people), the use of diminutive names in Polish, etc. 3 RuWordNet Thesaurus The Russian wordnet RuWordNet (Loukachevitch et al., 2016; Loukachevitch et al., 2018) has been created on the basis of another Russian thesaurus RuThes in"
2019.gwc-1.9,C12-2101,0,0.0283448,"analysis is needed to choose the most appropriate synset. The second type of problems is associated with the absence of lexicalized means of naming a concept denoted by the English synset. In such cases, an additional synset is introduced into the Romanian wordnet, which contains a nonlexicalized expression. The next type of problems stems from the fact that the word sense system in the English WordNet is more fractional than in the Romanian wordnet. In such cases, new senses were entered into the Romanian wordnet. Linking between Polish wordnet (plWordNet ) and WordNet was performed in 2012 (Rudnicka et al., 2012). To establish links, the following set of interlingual (I) relationships was used: Isynonymy, I-hyponymy, I-hyperonymy, Imeronymy, I-holonymy, I-quasi-synonymy (near synonymy), I- inter-register synonymy. The latter relation is established when the synsets in Polish and English have the same meaning, but refer to different language registers. The matching between the Polish and English synsets was performed manually. In the process of searching for equivalents, inaccurate descriptions of Polish word senses could be corrected. Maziarz et al. (2013) provide quantitative characteristics of the e"
2020.coling-main.276,Q17-1010,0,0.188394,"Missing"
2020.coling-main.276,S15-2151,0,0.175007,"b.com/skoltech-nlp/diachronic-wordnets 3095 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the inp"
2020.coling-main.276,S16-1168,0,0.0933984,"achronic-wordnets 3095 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it"
2020.coling-main.276,N19-1423,0,0.0157609,"Missing"
2020.coling-main.276,S16-1208,0,0.0209482,"arch is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity. Another example is TALN team (Espinosa-Anke et al., 2016) which also makes use of the definition by extracting noun and verb phrases for candidates generation. This scenario may be unrealistic for manual annotation because annotators are writing a definition for a new word and adding new words to the taxonomy simultaneously. Having a list of candidates would not only speed up the annotation process but also identify the range of possible senses. Moreover, it is possible that not yet included words may have no definition in any other sources: they could be very rare (“apparatchik”, “falanga”), relatively new (“selfie”, “hashtag”) or come from a narro"
2020.coling-main.276,W14-0142,0,0.0201251,"or the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable lexical resources up-to-date. In this paper, we analyse the approaches to automatic enrichment of wordnets. Formally, the goal of the Taxonomy Enrichment task is as follows: given word"
2020.coling-main.276,S15-2049,0,0.0902803,"training and evaluating taxonomy enrichment models and describe a technique of creating such datasets for other languages. 1 Introduction Nowadays, construction and maintenance of lexical resources (ontologies, knowledge bases, thesauri) have become essential for the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assi"
2020.coling-main.276,N06-1025,0,0.0492605,"e of creating such datasets for other languages. 1 Introduction Nowadays, construction and maintenance of lexical resources (ontologies, knowledge bases, thesauri) have become essential for the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable le"
2020.coling-main.276,K17-3009,0,0.0670342,"Missing"
2020.coling-main.276,K17-3001,0,0.0420954,"Missing"
2020.coling-main.276,S16-1210,0,0.162228,"deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity. Another example is TALN team (Espinosa-Anke et al., 2016) which also makes use of the definition by extracting noun and verb phrases for candidates generation. This scenario may be unrealistic for manual annotation because annotators are writing a definition for a new word and adding new words to the taxonomy simultaneously. Having a list of candidates would not only speed up the annotation process but also identify the range of possible senses. Moreov"
2020.coling-main.276,J13-3007,0,0.181293,"5 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the"
2021.gwc-1.15,P19-1474,1,0.89603,"Missing"
2021.gwc-1.15,S18-1152,0,0.0324629,"Missing"
2021.gwc-1.15,S18-1116,0,0.0484942,"Missing"
2021.gwc-1.15,Q17-1010,0,0.050124,"resentations are complementary to the distributional word embeddings, as they capture the hypo-hypernymy relations from graphs. We expect that models using graph representations could be beneficial for the taxonomy enrichment task in combination with distributed word vector representations or on their own. We check our hypothesis on several models which make use of graph structures: node2vec (Grover and Leskovec, 2016), Poincar´e embeddings (Nickel and Kiela, 2017) and GCN autoencoder (Kipf and Welling, 2016a) and compare it with an approach of Nikishina et al. (2020b) which applies fastText (Bojanowski et al., 2017) and features from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction"
2021.gwc-1.15,S15-2151,0,0.0260509,"res from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCol"
2021.gwc-1.15,S16-1168,0,0.019997,"All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) w"
2021.gwc-1.15,2020.acl-main.747,0,0.0185525,"Missing"
2021.gwc-1.15,S16-1169,0,0.0500915,"Missing"
2021.gwc-1.15,S18-1151,0,0.0377813,"Missing"
2021.gwc-1.15,S15-2049,0,0.031679,"resources, taxonomies are graph structures. Combining word embeddings with graph structure of taxonomy could be of use for predicting taxonomic relations. In this paper we compare several approaches for attaching new words to the existing taxonomy which are based on the graph representations with the one that relies on fastText embeddings. We test all methods on Russian and English datasets, but they could be also applied to other wordnets and languages. 1 Introduction Taxonomic structures are often used for the downstream tasks like lexical entailment (Herrera et al., 2005), entity linking (Moro and Navigli, 2015), named entity recognition (Negri and Magnini, 2004). Therefore, they always need to be up-todate and to keep up with the language change. Moreover, with the rapid growth of lexical resources for specific domains it becomes more and more important to develop systems that could automatically enrich the existing knowledge bases with new words or at least facilitate the manual taxonomy extension process. In this paper we tackle the taxonomy enrichment task which aims at associating new words (words not present in a taxonomy) with the appropriate hypernym synsets from the taxonomy. For instance, t"
2021.gwc-1.15,2020.coling-main.276,1,0.820805,"h representations. We assume that graph-based representations are complementary to the distributional word embeddings, as they capture the hypo-hypernymy relations from graphs. We expect that models using graph representations could be beneficial for the taxonomy enrichment task in combination with distributed word vector representations or on their own. We check our hypothesis on several models which make use of graph structures: node2vec (Grover and Leskovec, 2016), Poincar´e embeddings (Nickel and Kiela, 2017) and GCN autoencoder (Kipf and Welling, 2016a) and compare it with an approach of Nikishina et al. (2020b) which applies fastText (Bojanowski et al., 2017) and features from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. T"
2021.gwc-1.15,D14-1162,0,0.0885801,"ted to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) word embeddings are used by most of participants. Bernier-Colborne and Barri`ere (2018) predict the likelihood of the relationship between an input word and a candidate using word2vec (Mikolov et al., 2013) embeddings. Word2vec is used by Berend et al. (2018) to compute features to train a logistic regression classifier. Maldonado and Klubiˇcka (2018) simply consider top-10 closest associates from the Skipgram word2vec model as hypernym candidates. Pre-trained GloVe embeddings (Pennington et al., 2014) are also used by Shwartz et al. (2016) to initialize embeddings for their LSTM-based Hypernymy Detection model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al.,"
2021.gwc-1.15,S16-1202,0,0.0175458,"used by most of participants. Bernier-Colborne and Barri`ere (2018) predict the likelihood of the relationship between an input word and a candidate using word2vec (Mikolov et al., 2013) embeddings. Word2vec is used by Berend et al. (2018) to compute features to train a logistic regression classifier. Maldonado and Klubiˇcka (2018) simply consider top-10 closest associates from the Skipgram word2vec model as hypernym candidates. Pre-trained GloVe embeddings (Pennington et al., 2014) are also used by Shwartz et al. (2016) to initialize embeddings for their LSTM-based Hypernymy Detection model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al., 2020a) also apply word embeddings to find the correct hypernyms in the existing taxonomy. For instance, Tanev and Rotondi (2016) c"
2021.gwc-1.15,2020.emnlp-main.594,0,0.0815842,"Missing"
2021.gwc-1.15,P16-1226,0,0.0319294,"Missing"
2021.gwc-1.15,S16-1210,0,0.0138621,"n model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al., 2020a) also apply word embeddings to find the correct hypernyms in the existing taxonomy. For instance, Tanev and Rotondi (2016) compute a definition vector for the input word by comparing it with the definition vectors of the candidates from a wordnet using cosine similarity. Kunilovskaya et al. (2020) train word2vec embeddings from scratch and cast the task as a classification problem. Arefyev et al. (2020) compare the approach based on XLM-R model (Conneau et al., 2020) with the word2vec “hypernyms of co-hyponyms” method. It considers nearest neighbours as co-hyponyms and takes their hypernyms as candidate synsets. Summing up, the usage of distributed word vector representations is a simple yet efficient approach to"
2021.gwc-1.15,J13-3007,0,0.0380202,"ibution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) word embeddings are used"
2021.gwc-1.23,N09-1003,0,0.112438,"semantically close were also close according to the thesaurus. In other cases, additions to the thesaurus were proposed. 1 Introduction Semantic proximity of words is an important parameter required in various tasks of natural language processing. It can be estimated in different ways: by corpus, using distributional methods (Mikolov 2013, Bojanowski et al., 2017), expert assessments; from psychosemantic experiments; using thesauri such as WordNet (Fellbaum, 1998). The general concept of semantic similarity can be subdivided to paradigmatic (taxonomical) similarity and semantic associations (Agirre et al., 2009; Hill et al., 2015; Kliegr and Zamazal, 2018; Majewska et al., 2020). Paradigmatic similarity can be defined in terms of shared superordinate category or shared semantic features. SeNatalia Loukachevitch Lomonosov Moscow State University, Moscow, Russia, louk_nat@mail.ru mantic associations correspond to co-occurrence (syntagmatic relations) in texts. To study automatic methods of word similarity calculation, specialized datasets are created. Some researchers try to create datasets distinguishing different subtypes of semantic similarity of words, which requires additional efforts and guideli"
2021.gwc-1.23,L18-1723,0,0.0611333,"results of the experiment. 2 Related work The paper concerns two directions of studies: revision and updating existing lexical-semantic resources for natural language processing (thesauri) and studies on relation types exploited by native speakers in word association experiments. 2.1 Revision of Existing Lexical Semantic Resources Procedures for revising and verifying resources are important for the developers of WordNet-like resources. Some ontological tools have been proposed to check the consistency of relationships in WordNet (Guarino and Welty, 2004; Alvez et al., 2018). Rambousek et al. (2018) considered a crowdsourcing tool allowing a user of the Czech wordnet to report errors. Users may propose an update of any data value. These suggestions can be approved or rejected by editors. Visualization tools can also help to find problems in wordnets (Piasecki et al. 2013; Johannsen et al. 2011). Cristea et al. (2004) and Rudnicka et al. (2012) reported on the revision of mistakes and inconsistencies in their wordnets in the process of linking the wordnet and the English WordNet. McCrae et al. (2019) discussed a new project: Open-Source WordNet for English, which is based on the Princeton"
2021.gwc-1.23,W18-4109,0,0.0918377,"Missing"
2021.gwc-1.23,Q17-1010,0,0.0597305,"Missing"
2021.gwc-1.23,J15-4004,0,0.0142106,"ere also close according to the thesaurus. In other cases, additions to the thesaurus were proposed. 1 Introduction Semantic proximity of words is an important parameter required in various tasks of natural language processing. It can be estimated in different ways: by corpus, using distributional methods (Mikolov 2013, Bojanowski et al., 2017), expert assessments; from psychosemantic experiments; using thesauri such as WordNet (Fellbaum, 1998). The general concept of semantic similarity can be subdivided to paradigmatic (taxonomical) similarity and semantic associations (Agirre et al., 2009; Hill et al., 2015; Kliegr and Zamazal, 2018; Majewska et al., 2020). Paradigmatic similarity can be defined in terms of shared superordinate category or shared semantic features. SeNatalia Loukachevitch Lomonosov Moscow State University, Moscow, Russia, louk_nat@mail.ru mantic associations correspond to co-occurrence (syntagmatic relations) in texts. To study automatic methods of word similarity calculation, specialized datasets are created. Some researchers try to create datasets distinguishing different subtypes of semantic similarity of words, which requires additional efforts and guidelines. Agirre et al."
2021.gwc-1.23,W11-4643,0,0.0430821,"Missing"
2021.gwc-1.23,2018.gwc-1.5,1,0.746222,"word interchangeability in sentences. We can also try to use human scores of word semantic similarity to assess the quality of descriptions in electronic lexical-semantic resources (thesauri). Such resources are built on the basis of synsets – sets of synonyms – linked by semantic relations such as hyponymy, hypernymy, antonymy, and some others. The automatic use of thesauri requires high quality descriptions of semantic senses and semantic relations between them. In this paper, we compare the results of a survey of respondents and the similarity of words according to the RuWordNet thesaurus (Loukachevitch et al., 2018) for the Russian language. Currently, the published RuWordNet version comprises about 110 thousand Russian words and expressions. A new version of RuWordNet is being prepared and RuWordNet data are tested from different points of view. In the psychosemantic experiment the respondents were asked to list synonyms for stimuli words without any guidelines. We found that their answers mainly contain paradigmatically similar words, practically without words related via any other similarity relationships, which makes it possible to check the taxonomic structure of the thesaurus. The paper is structur"
2021.gwc-1.23,P19-1577,1,0.93166,"and inconsistencies in their wordnets in the process of linking the wordnet and the English WordNet. McCrae et al. (2019) discussed a new project: Open-Source WordNet for English, which is based on the Princeton WordNet. This project has already fixed errors found in the current version of WordNet, including spelling mistakes in definitions and examples. Some problematic issues were reported (for example, synset duplicates, missed or incorrect relationships) for further revision. Recently, verification and enrichment methods have been systematically developed for the RuWordNet thesaurus. In (Loukachevitch, 2019), the following method for enriching the RuWordNet thesaurus was proposed. For a large text corpus, words are searched for which 20 words closest in the corpus (based on the standard method for evaluating the semantic similarity of words) are located far from each other in the thesaurus. The distance between words in the thesaurus is the length of the shortest path between them in the graph of semantic relations. For found words with such properties, the reasons for such discrepancy are analyzed. The analysis of the data presented in (Loukachevitch, 2019) was continued in (Bayrasheva, 2019). I"
2021.gwc-1.23,2020.lrec-1.705,0,0.0510627,"Missing"
2021.gwc-1.23,2019.gwc-1.31,0,0.0167623,"consistency of relationships in WordNet (Guarino and Welty, 2004; Alvez et al., 2018). Rambousek et al. (2018) considered a crowdsourcing tool allowing a user of the Czech wordnet to report errors. Users may propose an update of any data value. These suggestions can be approved or rejected by editors. Visualization tools can also help to find problems in wordnets (Piasecki et al. 2013; Johannsen et al. 2011). Cristea et al. (2004) and Rudnicka et al. (2012) reported on the revision of mistakes and inconsistencies in their wordnets in the process of linking the wordnet and the English WordNet. McCrae et al. (2019) discussed a new project: Open-Source WordNet for English, which is based on the Princeton WordNet. This project has already fixed errors found in the current version of WordNet, including spelling mistakes in definitions and examples. Some problematic issues were reported (for example, synset duplicates, missed or incorrect relationships) for further revision. Recently, verification and enrichment methods have been systematically developed for the RuWordNet thesaurus. In (Loukachevitch, 2019), the following method for enriching the RuWordNet thesaurus was proposed. For a large text corpus, wo"
2021.gwc-1.23,C12-2101,0,0.0301045,"ocedures for revising and verifying resources are important for the developers of WordNet-like resources. Some ontological tools have been proposed to check the consistency of relationships in WordNet (Guarino and Welty, 2004; Alvez et al., 2018). Rambousek et al. (2018) considered a crowdsourcing tool allowing a user of the Czech wordnet to report errors. Users may propose an update of any data value. These suggestions can be approved or rejected by editors. Visualization tools can also help to find problems in wordnets (Piasecki et al. 2013; Johannsen et al. 2011). Cristea et al. (2004) and Rudnicka et al. (2012) reported on the revision of mistakes and inconsistencies in their wordnets in the process of linking the wordnet and the English WordNet. McCrae et al. (2019) discussed a new project: Open-Source WordNet for English, which is based on the Princeton WordNet. This project has already fixed errors found in the current version of WordNet, including spelling mistakes in definitions and examples. Some problematic issues were reported (for example, synset duplicates, missed or incorrect relationships) for further revision. Recently, verification and enrichment methods have been systematically develo"
C12-1037,P11-2103,0,0.101726,"исок оценочной лексики для русского языка. 595 1 Introduction Over the last few years a lot of efforts were made to solve sentiment analysis tasks in different domains. Automated approaches to sentiment analysis can be useful for state bodies and politicians, companies, and ordinary users. Most of these efforts concern English, where a lot of resources and tools for natural language processing and especially for sentiment analysis exist. One of the important tasks, considered as a basis for sentiment analysis of documents written in a specific language, is a creation of its sentiment lexicon (Abdul-Mageed et al., 2011; Peres-Rosas et al., 2012). Usually authors try to gather general sentiment lexicons for their languages. However a lot of researchers stress the differences between sentiment lexicons in specific domains. For example, “must-see” is a strongly opinionated word in the movie domain, but neutral in the digital camera domain (Blitzer et al., 2007). For these reasons, supervised learning algorithms trained in one domain and applied to other domains demonstrate considerable decrease in the performance (Ponomareva & Thelwall, 2012; Read & Carroll, 2009; Taboada et al., 2011). To overcome this issue"
C12-1037,D08-1014,0,0.0264533,"Missing"
C12-1037,P07-1056,0,0.37584,"rces and tools for natural language processing and especially for sentiment analysis exist. One of the important tasks, considered as a basis for sentiment analysis of documents written in a specific language, is a creation of its sentiment lexicon (Abdul-Mageed et al., 2011; Peres-Rosas et al., 2012). Usually authors try to gather general sentiment lexicons for their languages. However a lot of researchers stress the differences between sentiment lexicons in specific domains. For example, “must-see” is a strongly opinionated word in the movie domain, but neutral in the digital camera domain (Blitzer et al., 2007). For these reasons, supervised learning algorithms trained in one domain and applied to other domains demonstrate considerable decrease in the performance (Ponomareva & Thelwall, 2012; Read & Carroll, 2009; Taboada et al., 2011). To overcome this issue various adaptation methods are proposed, like ensembles of classifiers (Aue & Gamon, 2005) or graph-based approaches (Wu et al., 2009). Nevertheless such approaches usually do not work well for domains whose lexicons differ significantly and recent studies are focused on bridging the gap between domainspecific words (Pan et al, 2010). Indeed, s"
C12-1037,P11-1014,0,0.0193127,"ecessary). This sentiment lexicon is clean enough to be used in various sentiment analysis tasks. This meta-domain list of sentiment words consists of words really used in users’ reviews and its creation does not require any dictionary resources. We plan to make it available for further research in sentiment analysis of Russian texts. 6 6.1 Lexicon evaluation on the cross-domain sentiment classification task Experimental setup To evaluate usefulness of our meta-domain sentiment list we test it in the cross-domain sentiment classification task as described for example in (Blitzer et al., 2007; Bollegala et al., 2011; Pan et al., 2010). In these studies the dataset consisting of Amazon product reviews for four different product types (books (B), DVDs (D), electronics (E) and kitchen appliances (K)) is used. There are 1000 positive and 1000 negative reviews selected randomly and labeled for each domain. Domain-adaptation algorithms are trained on the one domain (source domain) and tested on the other domain (target domain). We do not compare our approach with these approaches because we do not make any efforts to adapt a classifier to a new domain. We use the similar setup to show the 605 generalization ab"
C12-1037,D09-1062,0,0.0252877,"To overcome this issue various adaptation methods are proposed, like ensembles of classifiers (Aue & Gamon, 2005) or graph-based approaches (Wu et al., 2009). Nevertheless such approaches usually do not work well for domains whose lexicons differ significantly and recent studies are focused on bridging the gap between domainspecific words (Pan et al, 2010). Indeed, sentiment lexicons adapted to a particular domain or topic have been shown to improve task performance in a number of applications, including opinion retrieval (Jijkoun et al., 2010), and expression-level sentiment classification (Choi & Cardie, 2009). In addition sentiment word extraction from a text collection enables to find slang and non-vocabulary words, which can be strong sentiment predictors. Stressing the differences in sentiment lexicons between domains, one should understand that domains can form clusters of similar domains. So a lot of sentiment words relevant to various product domains are not relevant to the political domain or the general news domain and vice versa. For example, such words as evil or villain are not applicable to all product domains. Therefore we suppose that gathering a specialized sentiment lexicon for the"
C12-1037,P97-1023,0,0.156505,"Missing"
C12-1037,P10-1060,0,0.0461649,"Missing"
C12-1037,W06-1642,0,0.222009,"Missing"
C12-1037,P07-1123,0,0.115161,"Missing"
C12-1037,perez-rosas-etal-2012-learning,0,0.418197,"Missing"
C12-1037,J11-1002,0,0.0226428,"Missing"
C12-1037,W11-1704,0,0.0492008,"Missing"
C12-1037,P09-2080,0,0.0102237,"ot of researchers stress the differences between sentiment lexicons in specific domains. For example, “must-see” is a strongly opinionated word in the movie domain, but neutral in the digital camera domain (Blitzer et al., 2007). For these reasons, supervised learning algorithms trained in one domain and applied to other domains demonstrate considerable decrease in the performance (Ponomareva & Thelwall, 2012; Read & Carroll, 2009; Taboada et al., 2011). To overcome this issue various adaptation methods are proposed, like ensembles of classifiers (Aue & Gamon, 2005) or graph-based approaches (Wu et al., 2009). Nevertheless such approaches usually do not work well for domains whose lexicons differ significantly and recent studies are focused on bridging the gap between domainspecific words (Pan et al, 2010). Indeed, sentiment lexicons adapted to a particular domain or topic have been shown to improve task performance in a number of applications, including opinion retrieval (Jijkoun et al., 2010), and expression-level sentiment classification (Choi & Cardie, 2009). In addition sentiment word extraction from a text collection enables to find slang and non-vocabulary words, which can be strong sentime"
C12-1037,J11-2001,0,\N,Missing
C12-3010,P11-2103,0,0.0516371,"Missing"
C12-3010,D08-1014,0,0.0324675,"Missing"
C12-3010,P07-1056,0,0.145223,"Missing"
C12-3010,W11-4003,1,0.83202,"Missing"
C12-3010,W06-1642,0,0.103299,"Missing"
C12-3010,P07-1123,0,0.0613719,"Missing"
C12-3010,perez-rosas-etal-2012-learning,0,0.0222247,"Missing"
C12-3010,J11-1002,0,0.0880949,"Missing"
C12-3010,W11-1704,0,0.0349364,"Missing"
C12-3010,J11-2001,0,\N,Missing
L16-1186,baccianella-etal-2010-sentiwordnet,0,0.145613,"Missing"
L16-1186,P07-1056,0,0.17589,"Missing"
L16-1186,C12-1037,1,0.873931,"ution of messages according to polarity classes in the SentiRuEval-2016 datasets After the label propagation, the whole volume of the RuThes thesaurus entries obtained positive, negative, or neutral labels. Thus, the Extended Connotation Lexicon was generated. The Pattern Connotation Lexicon and the most probable entries of the Extended Connotation lexicon were analyzed by a linguist to enrich the RuSentiLex lexicon. 4.3. Extraction of sentiment words from Twitter To extract sentiment words from Twitter, we applied the supervised model of sentiment word extraction trained on the movie domain (Chetviorkin and Loukachevitch 2012). The size of the Russian tweet collection was 1 M+ of unlabeled tweets. This model is based on several text collections: a collection with the high concentration of sentiment words, a contrast domain-specific collection, a contrast domainindependent collection (e.g. general news collection). Thus, taking into account statistical distributions of words in such collections, it is possible to distinguish specific sentiment words, used in a collection under analysis (tweets in this case) (Chetviorkin and Loukachevitch, 2013). As a result, the words extracted from Twitter were ordered in decreasin"
L16-1186,W13-2403,1,0.898365,"Missing"
L16-1186,W14-2612,1,0.803447,"Missing"
L16-1186,D09-1062,0,0.0181921,"ohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009; Lau et al., 2011). But some works (Mansour et al., 2013) show that the combination of multi-domain training data in supervised sentiment analysis improves the performance of a classifier in each domain under study. It proves the existence of a relatively stable set of general sentiment words and expressions with relatively stable sentiment orientations. Besides, as was shown in (Mohammad et al., 2013), features based on publicly available sentiment vocabularies are useful for improvement of supervised sentiment analysis systems. Thus, for any natural language it is useful to have a publicly"
L16-1186,P13-1174,0,0.106758,"in this paper differs from the existing Russian sentiment lexicons with the coverage and expert quality. Besides, the sentiment ambiguity of Russian words is described. 3. RuSentiLex Lexicon The RuSentiLex lexicon is an alphabet-ordered Russian sentiment vocabulary. It contains the following types of Russian sentiment-related words: - words from general Russian for that at least one sense has a positive or negative polarity what means that it conveys negative/positive opinion (excellent) or negative/positive emotion, (sadness); 1 - non-opinionated words with negative or positive connotations (Feng et al., 2013) such as unemployment, terrorism, disease, cancer, explosion, spam etc. Further we will call them facts; - slang and curse words from Twitter. http://www.cir.ru/SentiLexicon/ProductSentiRus.txt NRC Emotion Lexicon translated in Russian via Google Translate (NRC) 3 http://linis-crowd.org/ 2 Thus, in RuSentiLex all words and their senses are considered from three points of views: - polarity: negative, positive or neutral - source: opinion, emotion or non-opinionated fact, - sentiment differences between word senses. If a word has different sentiment orientations or sources in its different sense"
L16-1186,R13-1055,0,0.0197628,"glish, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009; Lau et al., 2011). But some works (Mansour et al., 2013) show that the combination of multi-domain training data in supervised sentiment analysis improves the performance of a classifier in each domain under study. It proves the existence of a relatively stable set of general sentiment words and expressions with relatively stable sentiment orientations. Besides, as was shown in (Mohammad et al., 2013), features based on publicly available sentiment vocabularies are useful for improvement of supervised sentiment analysis systems. Thus, for any natural language it is useful to have a publicly available, manually crafted general sentiment lexicon, whi"
L16-1186,S13-2053,0,0.07872,"by the participants of the SentiRuEval-2016 Twitter reputation monitoring shared task and allowed them to achieve high results. Keywords: sentiment analysis, sentiment lexicon, connotation, lexical ambiguity, thesaurus 1. Introduction Automatic sentiment analysis is useful in many practical applications, such as analysis of users&apos; reviews, posts in social networks, newspaper articles, etc. Sentiment lexicons are important components of sentiment analysis systems. They can be applied in lexicon-based approaches (Taboada et al., 2011) or be sources of features in the machine-learning framework (Mohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009;"
L16-1186,E14-1010,0,0.0151954,"newspaper articles, etc. Sentiment lexicons are important components of sentiment analysis systems. They can be applied in lexicon-based approaches (Taboada et al., 2011) or be sources of features in the machine-learning framework (Mohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009; Lau et al., 2011). But some works (Mansour et al., 2013) show that the combination of multi-domain training data in supervised sentiment analysis improves the performance of a classifier in each domain under study. It proves the existence of a relatively stable set of general sentiment words and expressions with relatively stable sentiment orientations. Besides, a"
L16-1186,N15-1159,0,0.0958427,"the SentiRuEval-2016 Twitter reputation monitoring shared task and allowed them to achieve high results. Keywords: sentiment analysis, sentiment lexicon, connotation, lexical ambiguity, thesaurus 1. Introduction Automatic sentiment analysis is useful in many practical applications, such as analysis of users&apos; reviews, posts in social networks, newspaper articles, etc. Sentiment lexicons are important components of sentiment analysis systems. They can be applied in lexicon-based approaches (Taboada et al., 2011) or be sources of features in the machine-learning framework (Mohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009; Lau et al., 2011). But some"
L16-1186,J11-2001,0,0.032382,"in machine-learning approaches. In this role, the RuSentiLex lexicon was utilized by the participants of the SentiRuEval-2016 Twitter reputation monitoring shared task and allowed them to achieve high results. Keywords: sentiment analysis, sentiment lexicon, connotation, lexical ambiguity, thesaurus 1. Introduction Automatic sentiment analysis is useful in many practical applications, such as analysis of users&apos; reviews, posts in social networks, newspaper articles, etc. Sentiment lexicons are important components of sentiment analysis systems. They can be applied in lexicon-based approaches (Taboada et al., 2011) or be sources of features in the machine-learning framework (Mohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extractio"
L16-1186,H05-1044,0,0.251926,"entiment analysis, sentiment lexicon, connotation, lexical ambiguity, thesaurus 1. Introduction Automatic sentiment analysis is useful in many practical applications, such as analysis of users&apos; reviews, posts in social networks, newspaper articles, etc. Sentiment lexicons are important components of sentiment analysis systems. They can be applied in lexicon-based approaches (Taboada et al., 2011) or be sources of features in the machine-learning framework (Mohammad et al., 2013; Severyn & Moschitti, 2015). For English, there are many sentiment lexicons, which were manually created by experts (Wilson et al., 2005) or by crowdsourcing (Mohammad & Turney, 2013). For other languages, automatic approaches for generating sentiment lexicons have been proposed (Chetviorkin & Loukachevitch, 2012; Perez-Rosas et al., 2012; San Vicente et al., 2014; Yang et al., 2013). It is well-known that sentiment vocabularies can depend on a specific domain, therefore, a lot of works are devoted to extraction of domain-specific sentiment lexicons (Blitzer et al., 2007; Choi & Cardie, 2009; Lau et al., 2011). But some works (Mansour et al., 2013) show that the combination of multi-domain training data in supervised sentiment"
L16-1186,R15-1092,0,0.0523925,"Missing"
L16-1186,perez-rosas-etal-2012-learning,0,\N,Missing
L16-1186,W14-0121,1,\N,Missing
loukachevitch-2012-automatic,zhang-etal-2008-comparative,0,\N,Missing
loukachevitch-2012-automatic,C04-1087,0,\N,Missing
loukachevitch-2012-automatic,C00-1077,0,\N,Missing
loukachevitch-2012-automatic,P03-2020,0,\N,Missing
loukachevitch-2012-automatic,dobrov-loukachevitch-2006-development,0,\N,Missing
loukachevitch-2012-automatic,baroni-bernardini-2004-bootcat,0,\N,Missing
loukachevitch-2012-automatic,P06-2084,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,J02-4004,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,C00-1072,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,P10-1084,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,P02-1047,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,W04-1013,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,P07-3015,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,C12-1098,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,P13-2024,0,\N,Missing
loukachevitch-alekseev-2014-summarizing,W14-0121,1,\N,Missing
loukachevitch-dobrov-2002-evaluation,E99-1011,0,\N,Missing
loukachevitch-dobrov-2004-development,loukachevitch-dobrov-2002-evaluation,1,\N,Missing
loukachevitch-dobrov-2004-development-ontologies,loukachevitch-dobrov-2002-evaluation,1,\N,Missing
P19-1577,S07-1002,0,0.202371,"can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. RuWordNet was created on the basis of another Russian thesaurus RuThes in 2016, which was developed as a tool for natural language processing during more than 20 years (Loukachevitch and Dobrov, 2002). Currently, the published version of RuWordNet includes 110 thousand Russian words and expressions. 2 Related Work Word sense induction approaches (Agirre and Soroa, 2007; Navigli, 2009; Lau et al., 2014; Panchenko et al., 2018) try to induce senses of ambiguous words from their contexts in a large corpus. Sometimes such approaches can find new senses not described in any lexical resources. But the results of these methods are rarely intended to 1 http://ruwordnet.ru/en/ 5773 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5773–5779 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics . improve the sense representation in a specific semantic resource. Lau et al. (2014) study"
P19-1577,W11-2501,0,0.0583755,"d accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. RuWordNet was created on the basis of another Russian thesaurus RuThes in 2016, which was developed as a tool for natural language processing during more than 20 years (Loukachevitch and Dobrov, 2002). Currently, the published version of RuWordNet includes 110 thousand Russian words and expressions. 2 Related Work"
P19-1577,P13-1133,0,0.0378958,"ersity Leninskie Gory, 1/4, Moscow, Russia louk_nat@mail.ru Abstract In this paper we discuss the usefulness of applying a checking procedure to existing thesauri. The procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. We applied the procedure to more than 30 thousand words of the Russian wordnet and found some serious errors in word sense description, including inaccurate relationships and missing senses of ambiguous words. 1 Introduction Large thesauri such as Princeton WordNet (Fellbaum, 1998) and wordnets created for other languages (Bond and Foster, 2013) are important instruments for natural language processing. Developing and maintaining such resources is a very expensive and time-consuming procedure. At the same time, contemporary computational systems, which can translate texts with almost human quality (Castilho et al., 2017), cannot automatically create such thesauri from scratch providing a structure somehow similar to resources created by professionals (CamachoCollados, 2017; Camacho-Collados et al., 2018). But if such a thesaurus exists, the developers should have approaches to maintain and improve it. In previous works, various metho"
P19-1577,W14-0154,0,0.269085,"and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. RuWordNet was created on the basis of another Russian thesaurus RuThes in 2016, which was devel"
P19-1577,1985.tmi-1.4,0,0.556704,"Missing"
P19-1577,Y11-1028,0,0.745925,"Missing"
P19-1577,Q16-1003,0,0.278721,"2018). But if such a thesaurus exists, the developers should have approaches to maintain and improve it. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study"
P19-1577,W11-2508,0,0.163145,"he similarity between a sense and a topic, the words in the definition are converted into the probability distribution. Then two probability distributions (gloss-based and topic-based) are compared using the Jensen-Shannon divergence. It was found that the proposed novelty measure could identify target lemmas with high- and medium-frequency novel senses. But the authors evaluated their method using word sense definitions in the Macmillan dictionary2 and did not check the quality of relations presented in a thesaurus. A series of works was devoted to studies of semantic changes in word senses (Gulordava and Baroni, 2011; Mitra et al., 2015; Frermann and Lapata, 2016), Gulordava and Baroni, 2011) study semantic change of words using Google n-gram corpus. They compared frequencies and distributional models based on word bigrams in 60s and 90s. They found that significant growth in frequency often reveals the appearance of a novel sense. Also it was found that sometimes the senses of words do not change but the context of their use changed significantly. In (Mitra et al., 2015), the authors study the detection of word sense changes by analyzing digitized books archives. They constructed networks based on a dist"
P19-1577,P14-1025,0,0.346929,"urus exists, the developers should have approaches to maintain and improve it. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities f"
P19-1577,2018.gwc-1.5,1,0.358216,"c annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. RuWordNet was created on the basis of another Russian thesaurus RuThes in 2016, which was developed as a tool for natural language processing during more than 20 years (Loukachevitch and Dobrov, 2002). Currently, the published version of RuWordNet includes 110 thousand Russian words and expressions. 2 Related Work Word sense induction approaches (Agirre and Soroa, 2007; Navigli, 2009; Lau et al., 2014; Panchenko et al., 2018) try to induce senses of ambiguous words from their contexts in a large corpus. Sometimes such approaches can find new senses not described in any lexical resources. Bu"
P19-1577,P06-1101,0,0.215064,"ng. Developing and maintaining such resources is a very expensive and time-consuming procedure. At the same time, contemporary computational systems, which can translate texts with almost human quality (Castilho et al., 2017), cannot automatically create such thesauri from scratch providing a structure somehow similar to resources created by professionals (CamachoCollados, 2017; Camacho-Collados et al., 2018). But if such a thesaurus exists, the developers should have approaches to maintain and improve it. In previous works, various methods on lexical enrichment of thesauri have been studied (Snow et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palme"
P19-1577,W04-0811,0,0.0764028,"now et al., 2006; Navigli and Ponzetto, 2012). But another issue was not practically discussed: how to find mistakes in existing thesaurus descriptions: incorrect relations or missed significant senses of ambiguous words, which were not included accidentally or appeared recently. In fact, it is much more difficult to reveal missed and novel senses or wrong relations, if compared to detect novel words (Frermann and Lapata, 2016; Lau et al., 2014). So it is known that such missed senses are often found during semantic annotation of a corpus and this is an additional problem for such annotation (Snyder and Palmer, 2004; Bond and Wang, 2014). In this paper, we consider an approach that uses embedding models to reveal problems in a thesaurus. Previously, distributional and embedding methods were evaluated in comparison with manual data (Baroni and Lenci, 2011; Panchenko et al., 2015). But we can use them in the opposite way: to utilize embeddingbased similarities and try to detect some problems in a thesaurus. We study such similarities for more than 30 thousand words presented in Russian wordnet RuWordNet (Loukachevitch et al., 2018)1. RuWordNet was created on the basis of another Russian thesaurus RuThes in"
R11-1103,C00-1077,0,0.0276401,"ubical Mutual Information (MI3). This feature is a modification of MI feature. In corpora research it was shown that this feature better orders low frequent phrases (Daille et. al., 1998): MI3 (ab) = log ( N  freq 3 (ab) ) freq (a)  freq (b) Insideness. Insideness is calculated as the inverse ratio between the phrase frequency and the maximal frequency of a three-word expression comprising the given phrase. Inside (ab) = freq (*ab*) freq (ab) This feature is intended to reveal truncated word sequences – parts of real terms. The similar phenomenon is modeled by C-value feature, described in (Maynard and Ananiadou, 2000). 3.2 Features Based on Internet Search An important characteristic of a domain term is “termhood” that is relevance to the domain (Kageura and Umino, 1996). The known way to estimate “termhood” is comparative analysis of a given text collection with a contrast text collection. The huge collection of Internet texts can serve as such a contrast collection. In previous research the Web was used for developing domain specific corpora (Penas et.al., 2001; Baroni and Bernardini, 2004). (Turney, 2003) exploits the Web to obtain the most important domain terms using so called coherence feature, ranki"
R11-1103,C04-1087,0,0.0734208,"Missing"
R11-1103,P06-2084,0,0.0704857,"Missing"
R11-1103,P03-2020,0,0.0823651,"Missing"
R11-1103,baroni-bernardini-2004-bootcat,0,0.0422083,"truncated word sequences – parts of real terms. The similar phenomenon is modeled by C-value feature, described in (Maynard and Ananiadou, 2000). 3.2 Features Based on Internet Search An important characteristic of a domain term is “termhood” that is relevance to the domain (Kageura and Umino, 1996). The known way to estimate “termhood” is comparative analysis of a given text collection with a contrast text collection. The huge collection of Internet texts can serve as such a contrast collection. In previous research the Web was used for developing domain specific corpora (Penas et.al., 2001; Baroni and Bernardini, 2004). (Turney, 2003) exploits the Web to obtain the most important domain terms using so called coherence feature, ranking higher term candidates that cooccur with other candidates in Web documents. In our study we extract several phrase features from the Web and combine them with other types of features (collection-based and thesaurusbased). We obtain Internet-based features using xml-interface of Russian Search Engine Yandex on the basis of specially formulated queries. For our experiments we utilised so-called search snippets - short fragments of texts explaining search results. Use of Internet"
R11-1103,zhang-etal-2008-comparative,0,0.0756442,"Missing"
R11-1103,dobrov-loukachevitch-2006-development,0,\N,Missing
R19-1118,P16-1032,0,0.0329018,"Missing"
R19-1118,N15-1146,0,0.143018,"ork The task of attitude recognition toward named entities or events, including opinion holder identification from full texts did not attract much attention. In 2014, the TAC evaluation conference in Knowledge Base Population (KBP) track included so-called sentiment track (Ellis et al., 2014). The task was to find all the cases where a query entity (sentiment holder) holds a positive or negative sentiment about another entity (sentiment target). Thus, this task was formulated as a query-based retrieval of entity-sentiment from relevant documents and focused only on query entities2 . MPQA 3.0 (Deng and Wiebe, 2015b) is a corpus of analytical articles with annotated opinion expressions (towards entities and events). The annotation is sentence-based. For example, in the sentence «When the Imam issued the fatwa against Salman Rushdie for insulting the Prophet ...», Imam is negative to Salman Rushdie but is positive to the Prophet. In paper (Choi et al., 2016), authors studied the approach to the recovery of the documents attitudes between subjects mentioned in the text. The approach considers such features as frequency of a named entity in the text, relatedness between entities, direct-indirect speech, et"
R19-1118,W09-2415,0,0.0307018,"closest F-measure and conclude that the task still remains complicated. Each attitude may be considered in terms of related article context, or sentence. The sentence consists of words which could be gathered and treated as an embedding, where each word repre2 https://tac.nist.gov/2014/KBP/ Sentiment/index.html sents a feature vector. Convolving embedded sentence representation by a set of different filters, in paper (Zeng et al., 2014) authors implemented and trained the Convolutional Neural Network (CNN) model for the relation classification task. Being applied for the SemEval-2010 Task 8 (Hendrickx et al., 2009), the obtained model significantly outperformed the results of other participants. This idea was developed further in terms of max-pooling operation (Zeng et al., 2015). This is an operation, which is applied to the convolved by filters data and extracts the maximal values within each convolution. However, for the relation classification task, original max-pooling reduces information extremely rapid and blurs significant relation aspects. Authors proposed to treat each convolution in parts. The division into parts depends on attitude entities: inner (between entities), and outer. This approach"
R19-1118,C16-1139,0,0.0138219,"is process refers to a single sentence case described in Section 5.2 with the following modifications: • Each minibatch presented as a sequence of n bags bi = hEs , yi, where Es is a set of embedded sentences (Section 5.2); For features, we use randomly initialized vectors. Table 7 provides parameter values of each embedding described above. Type Parameters POS vsize Distance vsize Tokens hsize, lt i Words hsize, lw , wi Values 5 5 3 17, 10 147 · 103 , 103 , 20 5.2 • Result output vector {ol }nl=1 obtained by an application of max-pooling operation over separately convolved context sentences (Jiang et al., 2016); 6 Table 7: Embedding parameters, where vsize is the size of embedding vectors. 1. Composing a minibatch I = {b1 , . . . , bn } where bi = {s1 , . . . , sm }; 2. Performing a forward propagation through the network; the result is a vector {ok }qk=1 , where ok ∈ Rc , and q = n · m; 3. Computing cross entropy loss for output: lk = c X log p(yi |ok,j ; θ), k ∈ 1 . . . q (1) j=1 n 4. Composing cost  vector {cost  i }i=1 , where costi = max li·g . . . l(i+1)·g is a maximal loss within i’th bag; 5. Using cost to update hidden variables set; Datasets and Experimential Setup We consider the problem"
R19-1118,P09-1113,0,0.0809958,"Missing"
R19-1118,J05-1004,0,0.061683,"Negative opinion pairs (avg./doc.) Avg. dist. between named entities within a sentence in words Value 73 1361 105.75 18.64 8.71 9.93 Table 2: Quantitative characteristics of the RuSentiFrames entries. 10.2 Table 1: Attitude statistics of RuSentRel-v1.1 corpus. 3.2 RuSentiFrames Lexicon The RuSentiFrames4 lexicon describes sentiments and connotations conveyed with a predicate in a verbal or nominal form (Rashkin et al., 2016; Klenner and Amsler, 2016). The structure of the frames includes the set of predicate-specific roles and frame dimensions. For role designation, the approach of PropBank (Palmer et al., 2005) is used. In this approach, individual verb’s semantic arguments are numbered, beginning with zero. For a particular verb, Arg0 is generally the argument exhibiting features of a Prototypical Agent (Dowty, 1991), while Arg1 is a Prototypical Patient or Theme. In the main part of the frame, the following dimensions are described: • the attitude of the author of the text towards mentioned participants; • positive or negative sentiment between participants; The created frames are associated not only with a single entry but with a &quot;family&quot; of related words and expressions, which have the same atti"
R19-1118,P16-1030,0,0.0234859,"proposed approach. Table 1 describes the corpus statistics. Parameter Number of documents Total opinion pairs Sentences (avg./doc.) Opinion pairs (avg./doc.) Positive opinion pairs (avg./doc.) Negative opinion pairs (avg./doc.) Avg. dist. between named entities within a sentence in words Value 73 1361 105.75 18.64 8.71 9.93 Table 2: Quantitative characteristics of the RuSentiFrames entries. 10.2 Table 1: Attitude statistics of RuSentRel-v1.1 corpus. 3.2 RuSentiFrames Lexicon The RuSentiFrames4 lexicon describes sentiments and connotations conveyed with a predicate in a verbal or nominal form (Rashkin et al., 2016; Klenner and Amsler, 2016). The structure of the frames includes the set of predicate-specific roles and frame dimensions. For role designation, the approach of PropBank (Palmer et al., 2005) is used. In this approach, individual verb’s semantic arguments are numbered, beginning with zero. For a particular verb, Arg0 is generally the argument exhibiting features of a Prototypical Agent (Dowty, 1991), while Arg1 is a Prototypical Patient or Theme. In the main part of the frame, the following dimensions are described: • the attitude of the author of the text towards mentioned participants; • po"
R19-1118,P02-1053,0,0.0223726,"Missing"
R19-1118,D15-1203,0,0.416243,"oint AI Center, PDMI RAS, St. Petersburg, Russia kolyarus@yandex.ru, louk_nat@mail.ru, elvtutubalina@kpfu.ru Abstract When relation extraction is performed automatically using machine learning approaches, this complexity results in a lack of training examples. One technique that helps to accomplish this task is distant supervision (DS), initially proposed by (Mintz et al., 2009). It assumes to extract and label data by relying on assumptions based on a prepared knowledge base. Although many methods have been proposed in such domains as sentiment analysis and relation extraction (Turney, 2002; Zeng et al., 2015), the domain of sentiment attitude extraction remains understudied. This paper describes a new approach to distant supervision for extracting sentiment attitudes between named entities mentioned in texts. It is worth noting that DS faces the problem of wrong labels, which becomes a reason of noisy labeled data. To address the shortcomings of noisy labeling, in this paper we exploit two primary sources of automatic annotation: News articles often convey attitudes between the mentioned subjects, which is essential for understanding the described situation. In this paper, we describe a new approa"
R19-1118,C14-1220,0,0.0554746,"Missing"
R19-1118,L16-1461,0,0.022652,"le 1 describes the corpus statistics. Parameter Number of documents Total opinion pairs Sentences (avg./doc.) Opinion pairs (avg./doc.) Positive opinion pairs (avg./doc.) Negative opinion pairs (avg./doc.) Avg. dist. between named entities within a sentence in words Value 73 1361 105.75 18.64 8.71 9.93 Table 2: Quantitative characteristics of the RuSentiFrames entries. 10.2 Table 1: Attitude statistics of RuSentRel-v1.1 corpus. 3.2 RuSentiFrames Lexicon The RuSentiFrames4 lexicon describes sentiments and connotations conveyed with a predicate in a verbal or nominal form (Rashkin et al., 2016; Klenner and Amsler, 2016). The structure of the frames includes the set of predicate-specific roles and frame dimensions. For role designation, the approach of PropBank (Palmer et al., 2005) is used. In this approach, individual verb’s semantic arguments are numbered, beginning with zero. For a particular verb, Arg0 is generally the argument exhibiting features of a Prototypical Agent (Dowty, 1991), while Arg1 is a Prototypical Patient or Theme. In the main part of the frame, the following dimensions are described: • the attitude of the author of the text towards mentioned participants; • positive or negative sentimen"
R19-1128,N16-1030,0,0.358315,"aset for two different NER systems. The corpus of over 850 000 tokens includes a large amount of NVD 1114 Proceedings of Recent Advances in Natural Language Processing, pages 1114–1120, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_128 descriptions and also some MS Bulletins and Metasploit Framework’s descriptions. The corpus was auto-labeled by a special algorithm. The labeling scheme is BIO. The two NER systems trained by Gasmi et al. (2018) are a CRF-model (CRFSuite implementation by Okazaki (2007)) and a neural network (NN) based model LSTMCRF (as suggested by Lample et al. (2016)). The NN-based model combines bidirectional LSTM, word2vec models as a source of pre-trained word embeddings and CRFs as an output layer. The only work that discusses the NER task for unstructured Russian text from cybersecurity domain is (Mazharov and Dobrov, 2018). The authors train their NER system on an early version of Sec_col collection, which was annotated with a CRF-classifier trained on news documents (Mozharova and Loukachevitch, 2016a). 3 Labeled Corpus Construction The source of the text for our corpus is Sec_col collection. It consists of 2000 texts (posts and forum publications)"
R19-1128,P16-1101,0,0.0609078,"Missing"
R19-1128,W18-5605,0,0.123286,"Missing"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
W11-4003,W06-1642,0,\N,Missing
W11-4003,P10-1060,0,\N,Missing
W11-4003,D09-1062,0,\N,Missing
W11-4003,P02-1053,0,\N,Missing
W13-2403,C12-1037,1,0.905842,"Missing"
W13-2403,H93-1012,0,0.40592,"Missing"
W13-2403,P07-1123,0,0.0206853,"petition in Russian. Several test and train collections were created for such tasks as sentiment classification in blogs and newswire, opinion retrieval. The paper describes the state of the art in sentiment analysis in Russian, collection characteristics, track tasks and evaluation metrics. 1 Introduction Sentiment analysis of natural language texts is one of the fast-developing technologies of natural language processing. Many lexical resources and tools were created for sentiment analysis in English. But lately a lot of research work was initiated for sentiment analysis in other languages (Mihalcea et al., 2007; Abdul-Mageed et al., 2011; P´erez-Rosas et al., 2012). The development of sentiment analysis in Russian previously did not attract a lot of attention at international conferences. Besides, until recently, the interest to sentiment analysis within Russia was connected only with election campaigns. But now there is a considerable interest to sentiment analysis within Russia both from the research community and from the industry. Therefore during the last years, two workshops on the evaluation of sentiment analysis systems were organized within the framework of Russian Information Retrieval Sem"
W13-2403,J11-2001,0,0.0148248,"ggregating scores of sentiment word and operator sequences (Kuznetsova et al., 2013). The second and third results in this task were obtained by a rule-based system with comparably small sentiment dictionaries but a rich rule set based on syntactic analysis (Panicheva, 2013). An interesting conclusion is that the size of sentiment dictionaries can be compensated with various syntactic rules, which allows handling the variety of situations in expressing sentiment. The results of this task can be compared with one of the recent studies on lexicon-based methods for sentiment analysis in English (Taboada et al., 2011). The text fragments in the paper and 3.3 Query-based retrieval of opinionated blog posts For several years TREC Blog tracks were connected with opinion finding and processing of blog data (Ounis et al., 2007; Macdonald et al., 2008; Ounis et al., 2008; Macdonald et al., 2010; Ounis et al., 2011). During the research cycles within these initiatives, the following sentiment analysis tasks were considered: • Opinion finding (blog post) retrieval task, • Polarised opinion finding (blog post) retrieval task. The query-based retrieval of opinions from blogs was one of the basic tasks for the TREC B"
W13-2403,S10-1014,0,0.0393047,"Missing"
W13-2403,S12-1035,0,0.0684636,"Missing"
W13-2403,perez-rosas-etal-2012-learning,0,0.070976,"Missing"
W13-2403,P11-2103,0,\N,Missing
W13-2403,W11-1704,0,\N,Missing
W14-0121,W04-2214,0,0.297873,"rate easily into the general language, and can be widely discussed in mass media. Besides, such a scope of concepts facilitates the application of RuThes in specialized subdomains of the broad socio-political domain. Examples of such concepts in RuThes include: EMERGENCY LOAN, TAX EXEMPTION, IMPORT TAX, DEMOGRAPHIC INDICATOR etc. In fact, we subdivide the whole scope of RuThes concepts to:  General Lexicon comprising concepts that can be met in various specific domains. In this, General Lexicon approximately corresponds to the Factotum domain in the Wordnet domain set (Gonzalez et al., 2012; Bentivogli et al., 2004),  and Socio-political Thesaurus containing thematically oriented lexemes and multiword expressions as well as domainspecific terms of the broad sociopolitical domain. After a concept has been introduced, an expert searches for all possible synonyms or derivative synonyms (that is derivates preserving the sense of an initial word), single words and phrases that can be associated with this concept. For example, a concept ДУШЕВНОЕ СТРАДАНИЕ (wound in the soul) has more than 20 text entries including such as: боль, боль в душе, в душе на- болело, душа болит, душа саднит, душевная пытка, душевная"
W14-0121,dobrov-etal-2004-russian,1,0.743065,"econd, 200 thousand words in a dictionary form (so called lemmas) ordered in decreasing frequency were extracted from the document frequency list of information-retrieval system RUSSIA (www.uisrussia.msu.ru/), in which contemporary Russian legal documents and newspaper materials are stored (2 million documents). The contemporary usage of these lemmas (distinct from proper names) was checked out during ten years of work mainly in news collections of online news services. In combination with other techniques we applied RuThes in tasks of Russian Information Retrieval Evaluation Seminar (ROMIP) (Dobrov et al., 2004). So in 2007 we tested our knowledge-based text categorization system in ROMIP text categorization evaluation (Ageev et al., 2008a). The task was to automatically classify documents of 1.5 mln. webpages using 247 categories (Russian part of DMOZ categories www.dmoz.org). The training collection included 300 thousand documents with DMOZ category labels. For every category, we created a Boolean expression over a relative small number of “supporting” concepts of the thesaurus. After that initial Boolean expressions were expanded on the basis of properties of the thesaurus relations. Final Boolean"
W14-0121,J02-2001,0,0.0637971,"for different parts of speech, for description of old and new names of the same entities, specific word usage in different dialects of the language or text genres (moke - donkey, nose - nozzle) etc. This is due to the fact that the basic relation in WordNet is the synonymy, based on the principle of substitution of one for another in sentences (Fellbaum, 1998). Some of new wordnets enhance the diversity of lexical relations between words to describe mainly their derivational links (Azarowa, 2008; Derwojedowa et al., 2008; Maziarz et al., 2013; Bosch et al., 2008). However, it was supposed in (Edmonds and Hirst, 2002; Hirst, 2009) that a fine-grained hierarchy is inappropriate as a model for the relationship between the senses of near-synonyms in a lexicon for any practical use in tasks such as machine translation and other applications. They assert that, ""what is required is a very coarsegrained conceptual hierarchy so that whole clusters of near-synonyms are mapped to a single node: their core meaning”. If to look at information-retrieval thesauri as representative sources of the terminology and domain knowledge one can see that most standards and guidelines for information-retrieval thesauri constructi"
W14-0121,J06-1001,0,\N,Missing
W14-0121,dobrov-loukachevitch-2006-development,0,\N,Missing
W14-2612,D09-1063,0,0.024498,"ches for creation of a sentiment lexicon in a specific language: dictionary-based methods and corpus-based methods. 67 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 67–72, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Linguistics multilingual bootstrapping technique for building tweet sentiment lexicons was introduced. This method is used as a baseline in our work. Dictionary-based methods for various languages have received a lot of attention in the literature (P´erez-Rosas et al., 2012; Mohammad et al., 2009; Clematide and Klenner, 2010), but the main problem of such approaches is that it is difficult to apply them to processing social media. The reason is that short informal texts contain a lot of misspellings and out-of-vocabulary words. 3 Data For the experiments in this paper we use several collections in two domains: movie review collection in Russian for training and fine-tuning of the proposed algorithms and Twitter collections for evaluation and demonstration of robustness in Russian and English languages. Movie domain. The movie review dataset collected from the online service imhonet.ru"
W14-2612,W11-0705,0,0.0243339,"ary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A large body of research has been focused on Twitter sentiment analysis during the previous several years (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011). In (Chen et al., 2012) authors propose an optimization framework for extraction of opinion expressions from tweets. Using extracted lexicons authors were able to improve the tweet sentiment classification quality. Our approach is based on similar assumptions (like consistency relations), but we do not use any syntactic parsers and dictionary resources. In (Volkova et al., 2013) a new 4 Method for sentiment word extraction In this section we introduce an algorithm for sentiment lexicon extraction, which is inspired by the method described in (Chetviorkin and Loukachevitch,"
W14-2612,perez-rosas-etal-2012-learning,0,0.255162,"Missing"
W14-2612,C10-2005,0,0.0426052,"in a collection. This model is based on the Ising spin model of magnetism and is built upon Markov random field framework, using various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A large body of research has been focused on Twitter sentiment analysis during the previous several years (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011). In (Chen et al., 2012) authors propose an optimization framework for extraction of opinion expressions from tweets. Using extracted lexicons authors were able to improve the tweet sentiment classification quality. Our approach is based on similar assumptions (like consistency relations), but we do not use any syntactic parsers and dictionary resources. In (Volkova et al., 2013) a new 4 Method for sentiment word extraction In this se"
W14-2612,E09-1077,0,0.0331135,"In such approaches various statistical and linguistic features are used to discriminate opinion words from all other words (He et al., 2008; Jijkoun et al., 2010). Another important group of approaches, which can be both dictionary-based and corpus-based are graph-based methods. In (Velikovich et al., 2010) a new method for constructing a lexical network was proposed, which aggregates the huge amount of unlabeled data. Then the graph propagation algorithm was used. Several other researchers utilized the graph or label propagation techniques for solving the problem of opinion word extraction (Rao and Ravichandran, 2009; Speriosu et al., 2011). In (Takamura et al., 2005) authors describe a probabilistic model for assigning polarity to each word in a collection. This model is based on the Ising spin model of magnetism and is built upon Markov random field framework, using various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construc"
W14-2612,W11-2207,0,0.0176434,"statistical and linguistic features are used to discriminate opinion words from all other words (He et al., 2008; Jijkoun et al., 2010). Another important group of approaches, which can be both dictionary-based and corpus-based are graph-based methods. In (Velikovich et al., 2010) a new method for constructing a lexical network was proposed, which aggregates the huge amount of unlabeled data. Then the graph propagation algorithm was used. Several other researchers utilized the graph or label propagation techniques for solving the problem of opinion word extraction (Rao and Ravichandran, 2009; Speriosu et al., 2011). In (Takamura et al., 2005) authors describe a probabilistic model for assigning polarity to each word in a collection. This model is based on the Ising spin model of magnetism and is built upon Markov random field framework, using various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A l"
W14-2612,P05-1017,0,0.323814,"eatures are used to discriminate opinion words from all other words (He et al., 2008; Jijkoun et al., 2010). Another important group of approaches, which can be both dictionary-based and corpus-based are graph-based methods. In (Velikovich et al., 2010) a new method for constructing a lexical network was proposed, which aggregates the huge amount of unlabeled data. Then the graph propagation algorithm was used. Several other researchers utilized the graph or label propagation techniques for solving the problem of opinion word extraction (Rao and Ravichandran, 2009; Speriosu et al., 2011). In (Takamura et al., 2005) authors describe a probabilistic model for assigning polarity to each word in a collection. This model is based on the Ising spin model of magnetism and is built upon Markov random field framework, using various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A large body of research has be"
W14-2612,C12-1037,1,0.613121,"imited number of natural language processing tools and resources are available. Then to demonstrate the robustness of the method and to compare the results with the other approaches we used it for English. The current research can be separated into two steps. We start with a special supervised model based on statistical and linguistic features of sentiment words, which is trained and evaluated in the movie domain. Then this model is utilized for extraction of sentiment words from unlabeled Twitter datasets, which are preliminary filtered using the domain-independent lexicons: ProductSentiRus (Chetviorkin and Loukachevitch, 2012) for Russian and MPQA (Wilson et al., 2005) for English. In the second step an algorithm for polarity classification of extracted sentiment words is introduced. It is built using the Markov random field framework and uses only information contained in text collections. To evaluate the quality of the created lexicons extrinsically, we conduct the experiments on the tweet subjectivity and polarity classification tasks using various lexicons. The key advantage of the proposed two-step algorithm is that once trained it can be utilized to different domains and languages with minor modifications. To"
W14-2612,N10-1119,0,0.0191791,"t al., 2013). All tweets in DEV and TEST collections are manually labeled by subjectivity and polarity using the Mechanical Turk with five workers (majority voting). This data was used for development and evaluation in (Volkova et al., 2013). Corpus-based methods are more suitable for processing social media data. In such approaches various statistical and linguistic features are used to discriminate opinion words from all other words (He et al., 2008; Jijkoun et al., 2010). Another important group of approaches, which can be both dictionary-based and corpus-based are graph-based methods. In (Velikovich et al., 2010) a new method for constructing a lexical network was proposed, which aggregates the huge amount of unlabeled data. Then the graph propagation algorithm was used. Several other researchers utilized the graph or label propagation techniques for solving the problem of opinion word extraction (Rao and Ravichandran, 2009; Speriosu et al., 2011). In (Takamura et al., 2005) authors describe a probabilistic model for assigning polarity to each word in a collection. This model is based on the Ising spin model of magnetism and is built upon Markov random field framework, using various dictionary-based a"
W14-2612,C10-2028,0,0.0425634,"is built upon Markov random field framework, using various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A large body of research has been focused on Twitter sentiment analysis during the previous several years (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011). In (Chen et al., 2012) authors propose an optimization framework for extraction of opinion expressions from tweets. Using extracted lexicons authors were able to improve the tweet sentiment classification quality. Our approach is based on similar assumptions (like consistency relations), but we do not use any syntactic parsers and dictionary resources. In (Volkova et al., 2013) a new 4 Method for sentiment word extraction In this section we introduce an algorithm for sentiment lexicon extraction, which is"
W14-2612,P13-2090,0,0.0762724,"language: 1M+ of unlabeled tweets (UNL) for extraction of sentiment lexicons, 2K labeled tweets for development data (DEV), and 2K labeled tweets for evaluation (TEST). DEV dataset is used to find the best combination of various lexicons for processing Twitter data and TEST for evaluating the quality of constructed lexicons. The UNL dataset in Russian was collected during one day using Twitter API. These tweets contain various topics without any filtering. Only strict duplicates and retweets were removed from the dataset. The similar collection for English was downloaded using the links from (Volkova et al., 2013). All tweets in DEV and TEST collections are manually labeled by subjectivity and polarity using the Mechanical Turk with five workers (majority voting). This data was used for development and evaluation in (Volkova et al., 2013). Corpus-based methods are more suitable for processing social media data. In such approaches various statistical and linguistic features are used to discriminate opinion words from all other words (He et al., 2008; Jijkoun et al., 2010). Another important group of approaches, which can be both dictionary-based and corpus-based are graph-based methods. In (Velikovich e"
W14-2612,P11-1016,0,0.025216,"sing various dictionary-based and linguistic features. In our research, unlike (Takamura et al., 2005) we use only information contained in a text collection without any external dictionary resources (due to the lack of necessary resources for Russian). Our advantage is that we use only potential domainspecific sentiment words during the construction of the network. A large body of research has been focused on Twitter sentiment analysis during the previous several years (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011). In (Chen et al., 2012) authors propose an optimization framework for extraction of opinion expressions from tweets. Using extracted lexicons authors were able to improve the tweet sentiment classification quality. Our approach is based on similar assumptions (like consistency relations), but we do not use any syntactic parsers and dictionary resources. In (Volkova et al., 2013) a new 4 Method for sentiment word extraction In this section we introduce an algorithm for sentiment lexicon extraction, which is inspired by the method described in (Chetvior"
W14-2612,H05-1044,0,0.109535,"urces are available. Then to demonstrate the robustness of the method and to compare the results with the other approaches we used it for English. The current research can be separated into two steps. We start with a special supervised model based on statistical and linguistic features of sentiment words, which is trained and evaluated in the movie domain. Then this model is utilized for extraction of sentiment words from unlabeled Twitter datasets, which are preliminary filtered using the domain-independent lexicons: ProductSentiRus (Chetviorkin and Loukachevitch, 2012) for Russian and MPQA (Wilson et al., 2005) for English. In the second step an algorithm for polarity classification of extracted sentiment words is introduced. It is built using the Markov random field framework and uses only information contained in text collections. To evaluate the quality of the created lexicons extrinsically, we conduct the experiments on the tweet subjectivity and polarity classification tasks using various lexicons. The key advantage of the proposed two-step algorithm is that once trained it can be utilized to different domains and languages with minor modifications. To demonstrate the ability of the proposed al"
W14-2612,P10-1060,0,0.0555711,"Missing"
W15-0901,D07-1109,0,0.50431,"Missing"
W15-0901,P05-1075,0,0.0376907,"So we decided to use also modification of this metric to consider the first 10 terms, no two of which are from the same set of similar unigrams and bigrams (this metric will be further called as TC-PMI-nSIM). 5 Integrating bigrams into topic models To compare proposed algorithm with the original one we extracted all bigrams found in each document of collections. For ranking bigrams we utilized Term Frequency (TF) or one of the following 16 word association measures: 1. Mutual Information (MI) (Church and Hanks, 1990); 2. Augmented MI (Zhang, 2008); 3. Normalized MI (Bouma, 2009); 4. True MI (Deane, 2005); 5. Cubic MI (Daille, 1995); 6. Symmetric Conditional Probability (Lopes and Silva, 1999); 7. Dice Coefficient (DC) (Smadja et al., 1996); 8. Modified DC (Kitamura and Matsumoto, 1996); 9. Gravity Count (Daudarviˇcius and Marcinkeviˇcien´e, 2003); 10. Simple Matching Coefficient (Daille, 1995); 11. 12. 13. 14. 15. 16. Kulczinsky Coefficient (Daille, 1995); Yule Coefficient (Daille, 1995); Jaccard Coefficient (Jaccard, 1901); T-Score; Chi Square; Loglikelihood Ratio (Dunning, 1993). Corpus Banking According to the results of (Lau et al., 2013) we decided to integrate top-1000 bigrams into all"
W15-0901,J93-1003,0,0.535809,"Missing"
W15-0901,P12-2023,0,0.0702533,"Missing"
W15-0901,C08-1044,0,0.0307528,"s belong to the first kind of methods. So, the first movement beyond “bag-of-words” assumption has been made by Wallach (2006), where the Bigram Topic Model was presented. In this model word probabilities are conditioned on the immediately preceding word. The LDA Collocation Model (Griffiths et al., 2007) extends the Bigram Topic Model by introducing a new set of variables and thereby giving a flexibility to generate both unigrams and bigrams. Wang et al. (2007) proposed the Topical N-Gram Model that adds a layer of complexity to allow the formation of bigrams to be determined by the context. Hu et al. (2008) proposed the Topical Word-Character Model challenging the assumption that the topic of an n-gram is determined by the topics of composite words within the collocation. This model is mainly suitable for Chinese language. Johnson (2010) established connection between LDA and Probabilistic Context-Free Grammars and proposed two probabilistic models combining insights from LDA and Adaptor Grammars 2 to integrate collocations and proper names into the topic model. While all these models have a theoretically elegant background, they are very complex and hard to compute on real datasets. For example"
W15-0901,P10-1117,0,0.0629178,"Missing"
W15-0901,W96-0107,0,0.166139,"Missing"
W15-0901,D11-1024,0,0.707837,"ard Label – fishing Table 1: Examples of incoherent and coherent topics Since involving experts is time-consuming and expensive, there were several attempts to propose a method for automatic evaluation of topic models quality that would go beyond perplexity and would be correlated with expert opinions. The formulation of such a problem is very complicated since experts can quite strongly disagree with each other. However, it was recently shown that it is possible to evaluate topic coherence automatically using word semantics with precision, almost coinciding with experts (Newman et al., 2010; Mimno et al., 2011). The proposed metric measures interpretability of topics based on human judgement (Newman et al., 2010). As topics are usually presented to users via their top-N topic terms, the topic coherence evaluates whether these top terms correspond to the topic or not. Newman et al. (2010) proposed an automated variation of the coherence score based on pointwise mutual information (TC-PMI): T C-P M I(t) = 10 j−1 X X j=2 i=1 log P (wj , wi ) , P (wj )P (wi ) (2) where (w1 , w2 , . . . , w10 ) are the top-10 terms in a topic, P (wi ) and P (wj ) are probabilities of unigrams wi and wj respectively, whil"
W15-0901,N10-1012,0,0.761797,"ationship between them and unigrams in topic models (such as citizen – citizen of country – citizen of union – European citizen – state citizen; categorization – document categorization – term categorization – text categorization). This allows us to create a novel method of integrating bigram collocations into topic models that does not consider bigrams being as “black boxes”, but maintains the relationship between unigrams and bigrams based on their component structure. The proposed algorithm leads to significant improvement of topic models quality measured in perplexity and topic coherence (Newman et al., 2010) 1 Proceedings of NAACL-HLT 2015, pages 1–9, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics without complications of the model. All experiments were carried out using PLSA algorithm and its modifications on four corpora of different domains and languages: the English part of Europarl parallel corpus, the English part of JRCAcquis parallel corpus, ACL Anthology Reference corpus, and Russian banking magazines. The rest of the paper is organized as follows. In the section 2 we focus on related work. Section 4 describes the datasets used in experiments, a"
W15-0901,J96-1001,0,0.424902,"milar unigrams and bigrams (this metric will be further called as TC-PMI-nSIM). 5 Integrating bigrams into topic models To compare proposed algorithm with the original one we extracted all bigrams found in each document of collections. For ranking bigrams we utilized Term Frequency (TF) or one of the following 16 word association measures: 1. Mutual Information (MI) (Church and Hanks, 1990); 2. Augmented MI (Zhang, 2008); 3. Normalized MI (Bouma, 2009); 4. True MI (Deane, 2005); 5. Cubic MI (Daille, 1995); 6. Symmetric Conditional Probability (Lopes and Silva, 1999); 7. Dice Coefficient (DC) (Smadja et al., 1996); 8. Modified DC (Kitamura and Matsumoto, 1996); 9. Gravity Count (Daudarviˇcius and Marcinkeviˇcien´e, 2003); 10. Simple Matching Coefficient (Daille, 1995); 11. 12. 13. 14. 15. 16. Kulczinsky Coefficient (Daille, 1995); Yule Coefficient (Daille, 1995); Jaccard Coefficient (Jaccard, 1901); T-Score; Chi Square; Loglikelihood Ratio (Dunning, 1993). Corpus Banking According to the results of (Lau et al., 2013) we decided to integrate top-1000 bigrams into all topic models under consideration. We should note that in all experiments described in the paper we fixed the number of topics and the numb"
W15-0901,D12-1087,0,0.0250274,"ies of unigrams wi and wj respectively, while P (wj , wi ) is the probability of bigram (wj , wi ). The final measure of topic coherence is calculated by averaging T C-P M I(t) measure by all topics t. This score is proven to demonstrate high correlation with human judgement (Newman et al., 2010). The proposed metric considers only top-10 words in each topic since they usually provide enough information to form the subject of the topic and distinguishing features from other topics. Topic coherence is becoming more widely used to evaluate topic model quality along with perplexity. For example, Stevens et al. (2012) showed that this metric is 5 strongly correlated with expert estimates. Also Andrzejewski et al. (2011) simply used it for evaluating topic model quality. Following the approach proposed by (Mimno et al., 2011) we compute probabilities by dividing the number of documents where the unigram or bigram occurred by the number of documents in the collection. To avoid optimistically high values we use external corpus for this purpose – namely, Russian and English Wikipedia. We should note that we do not consider another variation of topic coherence based on log conditional probability (TC-LCP) propo"
W15-0901,P09-2075,0,0.0351058,"h, 2006; Griffiths et al., 2007; Wang et al., 2007). Introduction Topic modeling is one of the latest applications of machine learning techniques to natural language processing. Topic models identify which topics relate to each document and which words form each topic. Each topic is defined as a multinomial distribution over terms and each document is defined as multinomial distribution over topics (Blei et al., 2003). Topic models have achieved noticeable success in various areas such as information retrieval (Wei and Croft, 2006), including such applications as multi-document summarization (Wang et al., 2009), text clustering and categorization (Zhou The paper proposes a novel approach taking into account bigram collocations and relationship between them and unigrams in topic models (such as citizen – citizen of country – citizen of union – European citizen – state citizen; categorization – document categorization – term categorization – text categorization). This allows us to create a novel method of integrating bigram collocations into topic models that does not consider bigrams being as “black boxes”, but maintains the relationship between unigrams and bigrams based on their component structure"
W15-0901,C10-1143,0,0.0423631,"Missing"
W15-0901,J90-1003,0,\N,Missing
W15-1819,D07-1109,0,0.0434298,"l language processing. Topic models identify which topics relate to each document and which words form each topic. Each topic is defined as a multinomial distribution over terms and each document is defined as multinomial distribution over topics (Blei et al., 2003). Topic models have achieved noticeable success in various areas such as information retrieval (Wei and Croft, 2006), including such applications as multi-document summarization (Wang et al., 2009), text clustering and categorization (Zhou et al., 2009), and other natural language processing tasks such as word sense disambiguation (Boyd-Graber et al., 2007), machine translation (Eidelman et al., 2012). Among most Natalia Loukachevitch Lomonosov Moscow State University, Russian Federation louk nat@mail.ru well-known models are Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which is based on Dirichlet prior distribution, and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), which is not connected with any parametric prior distribution. One of the main drawback of the topic models is that they utilize “bag-of-words” model that discards word order and is based on the word independence assumption. There are numerous studies, wher"
W15-1819,J93-1003,0,0.142207,"Missing"
W15-1819,P12-2023,0,0.0291382,"ch topics relate to each document and which words form each topic. Each topic is defined as a multinomial distribution over terms and each document is defined as multinomial distribution over topics (Blei et al., 2003). Topic models have achieved noticeable success in various areas such as information retrieval (Wei and Croft, 2006), including such applications as multi-document summarization (Wang et al., 2009), text clustering and categorization (Zhou et al., 2009), and other natural language processing tasks such as word sense disambiguation (Boyd-Graber et al., 2007), machine translation (Eidelman et al., 2012). Among most Natalia Loukachevitch Lomonosov Moscow State University, Russian Federation louk nat@mail.ru well-known models are Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which is based on Dirichlet prior distribution, and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), which is not connected with any parametric prior distribution. One of the main drawback of the topic models is that they utilize “bag-of-words” model that discards word order and is based on the word independence assumption. There are numerous studies, where the integration of collocations, n-grams, i"
W15-1819,C08-1044,0,0.029076,"es belong to the first kind of methods. So, the first movement beyond “bag-ofwords” assumption has been made by Wallach (2006), where the Bigram Topic Model was presented. In this model word probabilities are conditioned on the immediately preceding word. The LDA Collocation Model (Griffiths et al., 2007) extends the Bigram Topic Model by introducing a new set of variables and thereby giving a flexibility to generate both unigrams and bigrams. Wang et al. (2007) proposed the Topical N-Gram Model that adds a layer of complexity to allow the formation of bigrams to be determined by the context. Hu et al. (2008) proposed the Topical WordCharacter Model challenging the assumption that the topic of an n-gram is determined by the topics of composite words within the collocation. This model is mainly suitable for Chinese language. Johnson (2010) established connection between LDA and Probabilistic Context-Free Grammars and proposed two probabilistic models combining insights from LDA and Adaptor Grammars to integrate collocations and proper names into the topic model. While all these models have a theoretically elegant background, they are very complex and hard to compute on real datasets. For example, B"
W15-1819,P10-1117,0,0.0397289,"Missing"
W15-1819,W96-0107,0,0.102916,"c will be further called as TC-PMI-nSIM). 5 Integrating bigrams into topic models To compare proposed algorithm with the original one we extracted all bigrams found in each document of collections. For ranking bigrams we utilized Term Frequency (TF) or one of the following 19 word association measures: 1. Mutual Information (MI) (Church and Hanks, 1990); 2. Augmented MI (Zhang, 2008); 3. Normalized MI (Bouma, 2009); 4. True MI (Deane, 2005); 5. Cubic MI (Daille, 1995); 6. Symmetric Conditional Probability (Lopes and Silva, 1999); 7. Dice Coefficient (DC) (Smadja et al., 1996); 8. Modified DC (Kitamura and Matsumoto, 1996); 9. Lexical Cohesion (Park et al., 2002); 10. Gravity Count (Daudarviˇcius and Marcinkeviˇcien´e, 2003); 11. Simple Matching Coefficient (Daille, 1995); 12. Kulczinsky Coefficient (Daille, 1995); 13. Ochiai Coefficient (Daille, 1995); 14. Yule Coefficient (Daille, 1995); 15. Jaccard Coefficient (Jaccard, 1901); 16. T-Score; 17. Z-Score; 18. Chi Square; 19. Loglikelihood Ratio (Dunning, 1993). According to the results of Lau et al. (2013) we decided to integrate top-1000 bigrams into all topic models under consideration. We should note that in all experiments described in the paper we fixed th"
W15-1819,N10-1012,0,0.628165,"ationship between them and unigrams in topic models (such as citizen – citizen of country – citizen of union – European citizen – state citizen; categorization – document categorization – term categorization – text categorization). This allows us to create a novel method of integrating bigram collocations into topic models that does not consider bigrams being as “black boxes”, but maintains the relationship between unigrams and bigrams based on their component structure. The proposed algorithm leads to significant improvement of topic models quality measured in perplexity and topic coherence (Newman et al., 2010) without complications of the model. All experiments were carried out using PLSA algorithm and its modifications on four corpora of different domains and languages: the English part of Europarl parallel corpus, the English part of JRC-Acquis parallel corpus, ACL Anthology Reference corpus, and Russian banking magazines. The rest of the paper is organized as follows. In the section 2 we focus on related work. Section 3 Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 145 proposes a novel algorithm PLSA-SIM that incorporates bigrams and similarities between"
W15-1819,C02-1142,0,0.0320429,"ing bigrams into topic models To compare proposed algorithm with the original one we extracted all bigrams found in each document of collections. For ranking bigrams we utilized Term Frequency (TF) or one of the following 19 word association measures: 1. Mutual Information (MI) (Church and Hanks, 1990); 2. Augmented MI (Zhang, 2008); 3. Normalized MI (Bouma, 2009); 4. True MI (Deane, 2005); 5. Cubic MI (Daille, 1995); 6. Symmetric Conditional Probability (Lopes and Silva, 1999); 7. Dice Coefficient (DC) (Smadja et al., 1996); 8. Modified DC (Kitamura and Matsumoto, 1996); 9. Lexical Cohesion (Park et al., 2002); 10. Gravity Count (Daudarviˇcius and Marcinkeviˇcien´e, 2003); 11. Simple Matching Coefficient (Daille, 1995); 12. Kulczinsky Coefficient (Daille, 1995); 13. Ochiai Coefficient (Daille, 1995); 14. Yule Coefficient (Daille, 1995); 15. Jaccard Coefficient (Jaccard, 1901); 16. T-Score; 17. Z-Score; 18. Chi Square; 19. Loglikelihood Ratio (Dunning, 1993). According to the results of Lau et al. (2013) we decided to integrate top-1000 bigrams into all topic models under consideration. We should note that in all experiments described in the paper we fixed the number of topics and the number of iter"
W15-1819,J96-1001,0,0.296222,"nonsimilar terms in topics (this metric will be further called as TC-PMI-nSIM). 5 Integrating bigrams into topic models To compare proposed algorithm with the original one we extracted all bigrams found in each document of collections. For ranking bigrams we utilized Term Frequency (TF) or one of the following 19 word association measures: 1. Mutual Information (MI) (Church and Hanks, 1990); 2. Augmented MI (Zhang, 2008); 3. Normalized MI (Bouma, 2009); 4. True MI (Deane, 2005); 5. Cubic MI (Daille, 1995); 6. Symmetric Conditional Probability (Lopes and Silva, 1999); 7. Dice Coefficient (DC) (Smadja et al., 1996); 8. Modified DC (Kitamura and Matsumoto, 1996); 9. Lexical Cohesion (Park et al., 2002); 10. Gravity Count (Daudarviˇcius and Marcinkeviˇcien´e, 2003); 11. Simple Matching Coefficient (Daille, 1995); 12. Kulczinsky Coefficient (Daille, 1995); 13. Ochiai Coefficient (Daille, 1995); 14. Yule Coefficient (Daille, 1995); 15. Jaccard Coefficient (Jaccard, 1901); 16. T-Score; 17. Z-Score; 18. Chi Square; 19. Loglikelihood Ratio (Dunning, 1993). According to the results of Lau et al. (2013) we decided to integrate top-1000 bigrams into all topic models under consideration. We should note that in all"
W15-1819,D12-1087,0,0.0264435,"ities of unigrams wi and w j respectively, while P(w j , wi ) is the probability of bigram (w j , wi ). The final measure of topic coherence is calculated by averaging TC-PMI(t) measure by all topics t. This score is proven to demonstrate high correlation with human judgement (Newman et al., 2010). The proposed metric considers only top10 words in each topic since they usually provide enough information to form the subject of the topic and distinguishing features from other topics. Topic coherence is becoming more widely used to evaluate topic model quality along with perplexity. For example, Stevens et al. (2012) showed that this metric is strongly correlated with expert estimates. Also Andrzejewski et al. (2011) simply used it for evaluating topic model quality. Following the approach proposed by Mimno et al. (2011) we compute probabilities by dividing the number of documents where the unigram or bigram occurred by the number of documents in the collection. To avoid optimistically high values we use external corpus for this purpose – namely, Russian and English Wikipedia. We should note that we do not consider another variation of topic coherence based on log conditional probability (TC-LCP) proposed"
W15-1819,P09-2075,0,0.0288122,"ions, when integrated into PLSA-SIM algorithm. 1 Introduction Topic modeling is one of the latest applications of machine learning techniques to the natural language processing. Topic models identify which topics relate to each document and which words form each topic. Each topic is defined as a multinomial distribution over terms and each document is defined as multinomial distribution over topics (Blei et al., 2003). Topic models have achieved noticeable success in various areas such as information retrieval (Wei and Croft, 2006), including such applications as multi-document summarization (Wang et al., 2009), text clustering and categorization (Zhou et al., 2009), and other natural language processing tasks such as word sense disambiguation (Boyd-Graber et al., 2007), machine translation (Eidelman et al., 2012). Among most Natalia Loukachevitch Lomonosov Moscow State University, Russian Federation louk nat@mail.ru well-known models are Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which is based on Dirichlet prior distribution, and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), which is not connected with any parametric prior distribution. One of the main drawback of the"
W15-1819,C10-1143,0,0.0482144,"Missing"
W15-1819,D11-1024,0,0.212494,"ard Label – fishing Table 1: Examples of incoherent and coherent topics Since involving experts is time-consuming and expensive, there were several attempts to propose a method for automatic evaluation of topic models quality that would go beyond perplexity and would be correlated with expert opinions. The formulation of such a problem is very complicated since experts can quite strongly disagree with each other. However, it was recently shown that it is possible to evaluate topic coherence automatically using word semantics with precision, almost coinciding with experts (Newman et al., 2010; Mimno et al., 2011). The proposed metric measures interpretability of topics based on human judgeProceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 148 ment (Newman et al., 2010). As topics are usually presented to users via their top-N topic terms, the topic coherence evaluates whether these top terms correspond to the topic or not. Newman et al. (2010) proposed an automated variation of the coherence score based on pointwise mutual information (TC-PMI): TC-PMI(t) = j−1 10 X X j=2 P(w j , wi ) log , P(w )P(w ) j i i=1 (2) where (w1 , w2 , . . . , w10 ) are the top-10 terms in"
W15-1819,J90-1003,0,\N,Missing
W15-1819,P05-1075,0,\N,Missing
W15-2003,J98-1006,0,0.431287,"Missing"
W15-2003,agirre-de-lacalle-2004-publicly,0,0.0908256,"Missing"
W15-2003,P98-2127,0,0.273339,"Missing"
W15-2003,W14-0121,1,0.925659,"rthy et al., 2007). In such experiments mainly WordNet-like resources are studied. In (Mohammad, Hirst, 2006), the Macquarie Thesaurus serves as a basis for the predominant sense identification. In this paper we present our experiments demonstrating how unambiguous multiword expressions can help to reveal the most frequent sense if they are allowed to be included in a thesaurus. The experiments are based on newlypublished Thesaurus of Russian language RuThes-lite, which has been developed since 1994 and was applied in a number of tasks of natural language processing and information retrieval (Loukachevitch and Dobrov, 2014). This paper is organized as follows. Section 2 compares our study with related works. In Section 3, we describe the main principles of RuThes-lite linguistic ontology construction. Section 4 is devoted to the manual analysis of the distribution of word senses described in RuThes, which is performed on the basis of Russian news flow provided by Yandex news service. Section 5 describes the experiments on supervised prediction of the most frequent sense of an ambiguous word. Abstract The paper describes a supervised approach for the detection of the most frequent senses of words on the basis of"
W15-2003,P04-1036,0,0.117745,"Missing"
W15-2003,J07-4005,0,0.0609947,"Missing"
W15-2003,mihalcea-2002-bootstrapping,0,0.0273282,"Missing"
W15-2003,E14-2026,0,0.0660948,"Missing"
W15-2003,P14-1096,0,0.0696024,"Missing"
W15-2003,E06-1016,0,0.0558649,"Missing"
W15-2003,H05-1053,0,0.0656243,"Missing"
W15-2003,W04-0811,0,0.0290558,"Missing"
W15-2003,W14-0132,0,0.0244419,"r example, a concept such as Ь (falling asleep at the wheel) is introduced because it denotes a specific important situation in road traffic, has an &quot;interesting&quot; text entry я я (falling asleep while driving). Also, this concept has an &quot;interesting&quot; relation to concept (road accident) (Loukachevitch and Dobrov, 2014). The word Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015 22 not have the possibility to create a sense-tagged corpus based on RuThes senses. In addition, as it was indicated in (Petrolito and Bond, 2014), in sense-labeling most time and efforts are spent on adding new word senses to a source resource. Another problem of a sense-labeled corpus is that it fixes the described sets of senses, and it is impossible to automatically update them for a new version of a thesaurus. To verify the coverage of lexical senses described in RuThes, the most important issue is to check that at least frequent senses have been already described. With this aim, it is not necessary to label all senses of a word in a large text collection, it is enough to check out senses in a randomly selected sample of word usage"
W15-2003,S07-1000,0,\N,Missing
W15-2003,C98-2122,0,\N,Missing
W15-2003,P14-1025,0,\N,Missing
W15-5314,W13-2403,1,0.881268,"Missing"
W15-5314,E12-2021,0,0.0349664,"ANCE, COMFORT, COSTS, GENERAL. The aspect categories with their sentiment scores (positive, negative, both, or absent) were also attached to the whole review. The participants were to solve one or more of the following tasks in two domains: automatic extraction of explicit aspects, automatic extraction of all aspect terms, extraction of sentiments towards explicit aspects, automatic categorization of explicit aspects into aspect categories, and sentiment analysis of the whole review according to aspect categories. The labeling of training and test data was conducted with BRAT annotating tool (Stenetorp et al., 2012). Aspects in each review were labeled by a single linguist under inspection of a supervisor. Besides, to check up the quality of aspect labeling, In (Gupta, 2013; Tutubalina and Ivanov, 2014; Zhang et al., 2012), extraction of so-called technical problems mentioned by users in reviews was discussed. Technical problems can also be considered as specific types of sentiment-oriented facts. Besides, some non-opinionated words can have negative or positive associations (connotations (Feng et al., 2013)) that their appearance in a text can imply relevant sentiment, e.g., word hair has usually the ne"
W15-5314,W14-4509,0,0.0125479,"lve one or more of the following tasks in two domains: automatic extraction of explicit aspects, automatic extraction of all aspect terms, extraction of sentiments towards explicit aspects, automatic categorization of explicit aspects into aspect categories, and sentiment analysis of the whole review according to aspect categories. The labeling of training and test data was conducted with BRAT annotating tool (Stenetorp et al., 2012). Aspects in each review were labeled by a single linguist under inspection of a supervisor. Besides, to check up the quality of aspect labeling, In (Gupta, 2013; Tutubalina and Ivanov, 2014; Zhang et al., 2012), extraction of so-called technical problems mentioned by users in reviews was discussed. Technical problems can also be considered as specific types of sentiment-oriented facts. Besides, some non-opinionated words can have negative or positive associations (connotations (Feng et al., 2013)) that their appearance in a text can imply relevant sentiment, e.g., word hair has usually the negative connotation in the restaurant domain (hair on the plate). The dataset created by Ganu et al. (2009) was used as a basis for aspect-based review analysis evaluation at SemEval in 2014"
W15-5314,P13-1174,0,0.023446,"categories. The labeling of training and test data was conducted with BRAT annotating tool (Stenetorp et al., 2012). Aspects in each review were labeled by a single linguist under inspection of a supervisor. Besides, to check up the quality of aspect labeling, In (Gupta, 2013; Tutubalina and Ivanov, 2014; Zhang et al., 2012), extraction of so-called technical problems mentioned by users in reviews was discussed. Technical problems can also be considered as specific types of sentiment-oriented facts. Besides, some non-opinionated words can have negative or positive associations (connotations (Feng et al., 2013)) that their appearance in a text can imply relevant sentiment, e.g., word hair has usually the negative connotation in the restaurant domain (hair on the plate). The dataset created by Ganu et al. (2009) was used as a basis for aspect-based review analysis evaluation at SemEval in 2014 (Pontiki et al., 2014). The dataset included isolated, out of context sentences in two domains: restaurants and laptops. The set of aspect categories for restaurants included: FOOD, SERVICE, PRICE, AMBIENCE, ANECDOTES/MISCELLANEOUS. In 2015 SemEval evaluations of the aspectbased sentiment analysis of reviews wa"
W15-5314,I11-1131,0,0.117467,"neutral). Each sentence is assigned to one or more aspect categories together with a polarity label for each category. 90 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 90–95, Hissar, Bulgaria, 10–11 September 2015. Hu and Liu (2004) created the product review dataset containing 100 reviews for each of five electronics products. They labeled terms naming aspects (e.g., voice dialing) together with their sentiment strength scores. They found that aspects can be expressed explicitly or implicitly, as the size aspect in the sentence it fits in a pocket nicely. Zhang and Liu (2011) argue that there are many types of expressions that do not bear sentiments on their own, but they imply sentiment in specific contexts. One such type of expressions involves resources, which are important for many application domains. For example, money is a resource in probably every domain, gas is a resource in the car domain, and ink is a resource in the printer domain. An expression containing a quantifier (some, more, large, small, etc.) in combination with a resource term may often look like a reference to an objective fact but, in practice, it often implies a specific sentiment. Figure"
W15-5314,S14-2004,0,0.0208235,"Zhang et al., 2012), extraction of so-called technical problems mentioned by users in reviews was discussed. Technical problems can also be considered as specific types of sentiment-oriented facts. Besides, some non-opinionated words can have negative or positive associations (connotations (Feng et al., 2013)) that their appearance in a text can imply relevant sentiment, e.g., word hair has usually the negative connotation in the restaurant domain (hair on the plate). The dataset created by Ganu et al. (2009) was used as a basis for aspect-based review analysis evaluation at SemEval in 2014 (Pontiki et al., 2014). The dataset included isolated, out of context sentences in two domains: restaurants and laptops. The set of aspect categories for restaurants included: FOOD, SERVICE, PRICE, AMBIENCE, ANECDOTES/MISCELLANEOUS. In 2015 SemEval evaluations of the aspectbased sentiment analysis of reviews was focused on entire reviews (Pontiki et al., 2015). Aspect categories of terms became more complicated and now consist of Entity-Attribute pairs (E-A), for example FOOD-PRICE, FOOD-QUALITY. In both cases, only explicit aspects (comprising named entities, common nouns, or multiword noun groups) were labeled an"
W15-5314,S15-2082,0,0.0333181,"Missing"
W15-5314,H05-2017,0,\N,Missing
W15-5314,H05-1043,0,\N,Missing
W16-1806,D07-1109,0,0.0449135,"-ITER that allows the incorporation of the most suitable ngrams into topic models. The experiments of integrating ngrams and multiword terms conducted on five text collections in different languages and domains demonstrate a significant improvement in all the metrics under consideration. 1 Introduction Topic models, such as PLSA (Hofmann, 1999) and LDA (Blei et al., 2003), have shown great success in discovering latent topics in text collections. They have considerable applications in the information retrieval, text clustering and categorization (Zhou et al., 2009), word sense disambiguation (Boyd-Graber et al., 2007), etc. However, these unsupervised models may not produce topics that conform to the user’s existing knowledge (Mimno et al., 2011). One key reason is that the objective functions of topic models do not correlate well with human judgements (Chang et al., 2009). Therefore, it is often necessary to incorporate semantic knowledge into topic models to improve the model’s performance. Recent work has shown that interactive human feedback (Hu et al., 2011) and information about words (BoydGraber et al., 2007) can improve the inferred topic quality. Another key limitation of the original algorithms i"
W16-1806,W15-0901,1,0.903147,"ressions into topic models is that similar ngrams sharing the same words (e.g, hidden – hidden layer – hidden Markov model – number of hidden units) often belong to the same topics, under one important condition that they often co-occur within the same texts. To implement the approach, we introduce the sets of similar ngrams and words: S = {Sw }, where Sw is the set of ngrams similar to w, that S S is Sw = {w ( w1 . . . wn )}, where of theoretical interest since they are very complex and hard to compute on real datasets. The second type of methods includes those proposed in (Lau et al., 2013; Nokel and Loukachevitch, 2015). These works are also limited to bigrams. Nokel and Loukachevitch (2015) extend the first work and propose the PLSA-SIM algorithm, which integrates top-ranked bigrams and maintains the relationships between bigrams sharing the same words. The authors achieve an improvement in topic model quality. Our first method in the paper extends the PLSASIM algorithm (Nokel and Loukachevitch, 2015) by switching to ngrams and the more widespread LDA model. Also we propose a novel iterative LDA-ITER algorithm that allows the automatic choice of the most appropriate ngrams for further integration into topic"
W16-1806,N15-1074,0,0.0119345,"rporated knowledge by Must-Link and Cannot-Link primitives represented by a Dirichlet Forest prior. These primitives were then used in (Petterson et al., 2010; Newman et al., 2011), where similar words are encouraged to have similar topic distributions. However, all such methods incorporate knowledge in a hard and topicindependent way, which is a simplification since two words that are similar in one topic are not necessarily of equal importance for another topic. Also several works seek to utilize the domainindependent knowledge available in online dictionaries or thesauri (such as WordNet) (Xie et al., 2015). We argue that this knowledge may be insufficient in the particular text corpus. Our current work proposes an approach to maintain the relationships between ngrams, sharing the same words. Our method does not require any complication of the original LDA model and just gives advice on whether ngrams and words can be in the same topics or not. 3 n w1 ...wn :∃i:wi =w w is the lemmatized word, and w1 . . . wn is the lemmatized ngram. While adding ngrams to the vocabulary as single tokens, we decrease the frequencies of unigram components by the frequencies of encompassing ngrams in each document"
W16-1806,P11-1026,0,0.0234387,"nsiderable applications in the information retrieval, text clustering and categorization (Zhou et al., 2009), word sense disambiguation (Boyd-Graber et al., 2007), etc. However, these unsupervised models may not produce topics that conform to the user’s existing knowledge (Mimno et al., 2011). One key reason is that the objective functions of topic models do not correlate well with human judgements (Chang et al., 2009). Therefore, it is often necessary to incorporate semantic knowledge into topic models to improve the model’s performance. Recent work has shown that interactive human feedback (Hu et al., 2011) and information about words (BoydGraber et al., 2007) can improve the inferred topic quality. Another key limitation of the original algorithms is that they rely on a “bag-of-words“ as2 Related work The idea of using ngrams in topic models is not a novel one. Two kinds of methods are proposed to deal with this problem: the creation of a unified topic model and preliminary extraction of collocations for further integration into topic models. Most studies belong to the first kind of methods and are limited to bigrams: i.e, the Bigram Topic Model (Wallach, 2006) and LDA Collocation Model (Griffi"
W16-1806,D11-1024,0,0.0309371,"rms conducted on five text collections in different languages and domains demonstrate a significant improvement in all the metrics under consideration. 1 Introduction Topic models, such as PLSA (Hofmann, 1999) and LDA (Blei et al., 2003), have shown great success in discovering latent topics in text collections. They have considerable applications in the information retrieval, text clustering and categorization (Zhou et al., 2009), word sense disambiguation (Boyd-Graber et al., 2007), etc. However, these unsupervised models may not produce topics that conform to the user’s existing knowledge (Mimno et al., 2011). One key reason is that the objective functions of topic models do not correlate well with human judgements (Chang et al., 2009). Therefore, it is often necessary to incorporate semantic knowledge into topic models to improve the model’s performance. Recent work has shown that interactive human feedback (Hu et al., 2011) and information about words (BoydGraber et al., 2007) can improve the inferred topic quality. Another key limitation of the original algorithms is that they rely on a “bag-of-words“ as2 Related work The idea of using ngrams in topic models is not a novel one. Two kinds of met"
W16-1806,N10-1012,0,0.0940073,"Missing"
