1997.tmi-1.24,1994.amta-1.23,0,0.0290068,"Missing"
1997.tmi-1.24,W97-0812,1,0.764642,"Missing"
1997.tmi-1.24,P95-1026,0,0.194909,"Missing"
2009.mtsummit-papers.7,W08-0309,1,0.714167,"cal machine 1 2 http://eur-lex.europa.eu/ http://europa.eu/eurovoc/ translation system, and very little additional processing is needed. It is hard to quantify how much training data is needed to achieve a minimum level of performance. This depends on the expansiveness of the domain and the language pair. Typically, tens of millions of words give decent performance: For instance, systems trained on the 30–40 million word Europarl corpus are competitive with commercial systems, typically better on this domain and even close in performance when translating related material such as news stories (Callison-Burch et al., 2008). The JRC-Acquis corpus is large enough to expect decent translation performance within its domain, but on the other hand, the domain is also very specific. Translation models trained on such legal texts do not necessarily perform well on other domains. 3.2 Tuning and Test Sets Since we develop machine translation systems for 462 language pairs, we wanted to have a common tuning and testing environment. Hence, we extracted from part of the corpus subset where sentences are aligned one-to-one across all languages. First, we identified all documents that exist for all languages. This is a set of"
2009.mtsummit-papers.7,D08-1089,0,0.0147934,"present when choosing candidate translation phrases. We have also included corpus size as a factor as the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to"
2009.mtsummit-papers.7,W09-0431,0,0.0369666,"tion performance more often than not. This is not the case for other languages. See Table 7 for summary statistics for English and French as pivot. When using English as pivot, we find not much difference (BLEU diverges by up to 2 points) for about a third of language pairs, for another third there are significant gains (2-5 points) and for another third even larger gains (5-10 points). However, using French as pivot generally decreases performance, only for a sixth of language pairs there is not much difference. English as pivot has also shown to be beneficial for Arabic–Chinese translation (Habash and Hu, 2009). We find it hard to claim that this is due to linguistic reasons, but rather an artifact of the data set we are using. It is likely that most of the text was originally authored in English. 6.2 Multi-Pivot Translation While pivoting through any language but English does generally lead to worse translations, it does constitute an alternative translation path. A recent trend in statistical machine translation is to combine the output of different MT systems in form of a consensus translation. In multi-pivot translation, we combine the direct translation system with several pivot systems, a nove"
2009.mtsummit-papers.7,P07-2045,1,0.0121715,"age. This gave us a set of 12,322 sentences aligned across all 22 languages of the corpus. We split this set into three parts, a tuning set for parameter optimization, a development test set for experimentation and a final test set to report translation performance. Since these sets contain many short and a few very long sentences, we reduced the tuning set further, by requiring that all sentences are between 8 and 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better"
2009.mtsummit-papers.7,P02-1040,0,0.101453,"nd 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better than Bulgarian–Hungarian (24.7). French Input LE CONSEIL DE LA COMMUNAUTE´ ´ ´ ECONOMIQUE EUROPEENNE, consid´erant que l’instauration d’une politique commune des transports comporte entre autres l’´etablissement de r`egles communes applicables aux transports internationaux de marchandises par route, ex´ecut´es au d´epart ou a` destination du territoire d’un e´ tat membre, ou traversant le territoire d’un ou plusieu"
2009.mtsummit-papers.7,N07-1029,0,0.0423619,"Missing"
2009.mtsummit-papers.7,D08-1078,1,0.937108,"s the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to provide a wide perspective on the challenges facing machine translation and provide strong mot"
2009.mtsummit-papers.7,steinberger-etal-2006-jrc,1,0.296887,"Missing"
2009.mtsummit-papers.7,P07-1108,0,\N,Missing
balahur-etal-2010-sentiment,J94-2004,0,\N,Missing
balahur-etal-2010-sentiment,W03-1014,0,\N,Missing
balahur-etal-2010-sentiment,S07-1013,0,\N,Missing
balahur-etal-2010-sentiment,C04-1200,0,\N,Missing
balahur-etal-2010-sentiment,strapparava-valitutti-2004-wordnet,0,\N,Missing
balahur-etal-2010-sentiment,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
balahur-etal-2014-resource,banea-etal-2008-bootstrapping,0,\N,Missing
balahur-etal-2014-resource,W12-3709,1,\N,Missing
balahur-etal-2014-resource,P02-1040,0,\N,Missing
balahur-etal-2014-resource,P11-1016,0,\N,Missing
balahur-etal-2014-resource,P10-1094,0,\N,Missing
balahur-etal-2014-resource,P05-2008,0,\N,Missing
balahur-etal-2014-resource,N03-1017,0,\N,Missing
balahur-etal-2014-resource,C10-1004,0,\N,Missing
balahur-etal-2014-resource,W11-1704,1,\N,Missing
balahur-etal-2014-resource,P10-1061,0,\N,Missing
balahur-etal-2014-resource,pak-paroubek-2010-twitter,0,\N,Missing
balahur-etal-2014-resource,S13-2052,0,\N,Missing
C08-3001,P03-1044,0,0.0292624,"ndidate locations by looking up in the texts all place, province, region and country names found in a multilingual gazetteer (including name variants). The weights of the locations are then based on the place name significance (e.g., a capital city scores higher than a village) and on the place name hierarchy (i.e. if the province or region to which the place belongs are also mentioned in the text, it scores higher). 4 Pattern Acquisition For pattern acquisition, we deploy a weakly supervised bootstrapping algorithm (Tanev and OezdenWennerberg, 2008) similar in spirit to the one described in (Yangarber, 2003), which involves some manual validation. Contrary to other approaches, the learning phase exploits the knowledge to which cluster the news items belong. Intuitively, this guarantees better precision of the learned patterns. In particular, for each event-specific semantic role (e.g. killed), a separate cycle of learning iterations is executed (usually up to three) in order to learn 1-slot extraction patterns. Each cluster includes articles from different sources about the same news story. Therefore, we assume that each entity appears in the same semantic role (actor, victim, injured) in the con"
C94-1008,E93-1022,0,\N,Missing
C94-1008,C92-4190,0,\N,Missing
C94-1008,P92-1026,0,\N,Missing
E12-2006,W08-0309,0,0.0339232,"Moses servers, it allows us to mitigate the workload of each single instance reducing translation time of each single article and to improve translation quality. 3.1 Translation Quality To evaluate the translation performance of ONTS, we run a set of experiments where we translate a test set for each language pair using our system and Google Translate. Lack of human translated parallel titles obliges us to test only the content based model. For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al., 2010), for French and Italian the news test sets presented in (Callison-Burch et al., 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus. For the other languages we use 2,000 sentences which are not news but a mixture of JRCAcquis, Europarl and DGT-TM data. It is not guarantee that our test sets are not part of the training data of Google Translate. Each test set is translated by Google Translate - Translator Toolkit, and by our system. Bleu score is used to evaluate the performance of both systems. Results, see Table 1, show that Google Translate produces better translation for th"
E12-2006,W11-2123,0,0.0120413,"1 0.424 0.334 0.395 Table 1: Automatic evaluation. 4 Technical Implementation The translation service is made of two components: the connection module and the Moses server. The connection module is a servlet implemented in Java. It receives the RSS files, isolates each single news article, identifies each source language and pre-processes it. Each news item is split into sentences, each sentence is tokenized, lowercased, passed through a statistical compound word splitter, (Koehn and Knight, 2003), and the named entity annotator module. For language modelling we use the KenLM implementation, (Heafield, 2011). According to the language, the correct Moses servers, title and content, are fed in a multithread manner. We use the multi-thread version of Moses (Haddow, 2010). When all the sentences of each article are translated, the inverse process is run: they are detokenized, recased, and untranslated/unknown words are listed. The translated title and content of each article are uploaded into the RSS file and it is passed to the next modules. The full system including the translation modules is running in a 2xQuad-Core with Intel Hyper-threading Technology processors with 48GB of memory. It is our in"
E12-2006,2005.mtsummit-papers.11,0,0.00728069,"er day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the target side. According to our experiments, in terms of performance the hierarchical model 26 performs better than PBSMT and syntax (18.31, 18.09, 17.62 Bleu points), but in terms of translation speed PBSMT is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence). Although, the hierarchica"
E12-2006,N03-1017,0,0.00454265,"most suitable SMT system for our requirements? The main goal of our system is to help the user understand the content of an article. This means that a translated article is evaluated positively even if it is not perfect in the target language. Dealing with such a large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the tar"
E12-2006,P07-2045,0,0.00743392,"Missing"
E12-2006,W11-2132,0,0.0371863,"Missing"
E12-2006,W06-1606,0,0.0501559,"Missing"
E12-2006,2009.mtsummit-posters.15,0,0.0159142,"h training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3 , Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Boˇ jar and Zabokrtsk´ y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5 . This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, we tried to extend the available corpora. For Farsi, we applied the methodology p"
E12-2006,steinberger-etal-2006-jrc,1,0.893926,"Missing"
E12-2006,E03-1076,0,\N,Missing
E12-2006,W09-0401,0,\N,Missing
E12-2006,W10-1703,0,\N,Missing
E12-2006,J07-2003,0,\N,Missing
hajlaoui-etal-2014-dcep,J93-1004,0,\N,Missing
hajlaoui-etal-2014-dcep,steinberger-etal-2006-jrc,1,\N,Missing
hajlaoui-etal-2014-dcep,D07-1091,0,\N,Missing
hajlaoui-etal-2014-dcep,P07-2045,0,\N,Missing
hajlaoui-etal-2014-dcep,P11-2031,0,\N,Missing
hajlaoui-etal-2014-dcep,2005.mtsummit-papers.11,0,\N,Missing
hajlaoui-etal-2014-dcep,rosen-vavrin-2012-building,0,\N,Missing
hajlaoui-etal-2014-dcep,2009.mtsummit-papers.7,1,\N,Missing
hajlaoui-etal-2014-dcep,P03-1021,0,\N,Missing
jacquet-etal-2014-clustering,R11-1015,1,\N,Missing
jacquet-etal-2014-clustering,R13-1031,1,\N,Missing
jacquet-etal-2014-clustering,kokkinakis-dannells-2006-recognizing,0,\N,Missing
kucuk-etal-2014-named,D09-1026,0,\N,Missing
kucuk-etal-2014-named,P12-1055,0,\N,Missing
kucuk-etal-2014-named,P11-1037,0,\N,Missing
kucuk-etal-2014-named,P11-3019,0,\N,Missing
kucuk-etal-2014-named,D11-1141,0,\N,Missing
kucuk-etal-2014-named,W13-2413,0,\N,Missing
kucuk-etal-2014-named,R13-1011,0,\N,Missing
kucuk-etal-2014-named,C96-1079,0,\N,Missing
L16-1084,N13-1073,0,0.0211141,"ach written form from rt2 . For a given language pair, the corresponding parallel corpus is the concatenation of all translations t from all the entities/pages p. These Wikipedia tables are also used for evaluation purposes (cf Section 5.1.). As a consequence, the 1.7 million expansions and 0.4 million acronyms on which the approach is applied were removed from the parallel corpora. There were about 300,000 training examples for German–English and French–English, and about 170,000 for Italian–English. Word alignments with manyto-one links were generated using the unsupervised fast_align tool (Dyer et al., 2013) in both directions and combined with the grow-diag-final-and symmetrization heuristic (Koehn et al., 2003). Lexical translation tables for the three language pairs in both directions where extracted with a tool from the Moses translation toolkit (Koehn et al., 2007). Tables contain maximum likelihood probability estimated for the conditional word translation probabilities p(En|{F r, De, It}) and p({F r, De, It}|En). Our T ransM od matrix is constructed based on the concatenation of these tables. 532 Baseline Monolingual ExpAgg Multilingual ExpAgg Cosine measure TokAgg TransTokAgg All aggregat"
L16-1084,R13-1031,1,0.89455,"United Nattions’ when referring to United Nations), inﬂections (e.g. Birleşmiş Milletler’in where the inﬂection suﬃx ’in is added to the Turkish equivalence of ‘United Nations’) and variants in other scripts (e.g. Russian Cyrillic Организация Объединённых Наций). One daily media monitoring task consists in recognising new names and in determining automatically whether they are a new name or whether they might be a spelling variant of a name encountered before. We aim at addressing this task by creating a daily updated resource containing multi-word entities, their acronyms and their variants. Ehrmann et al. (2013) developed a method handling variants at monolingual level, meaning that there were separate clusters and identiﬁers for each language. In this paper we address this task at the multilingual level. Additionally to the complexity of the monolingual task, we have to address expression translations, increasing acronym ambiguity, and larger numbers of expressions referring to the same entity. The ECOWAS/CEDEAO example mentioned previously shows how the same conceptual entity can both have diﬀerent variants and diﬀerent acronyms in diﬀerent languages. Figure 1 shows that we can neither assume that"
L16-1084,jacquet-etal-2014-clustering,1,0.712174,"Missing"
L16-1084,N03-1017,0,0.0112962,"Missing"
L16-1084,P07-2045,0,0.010615,"Missing"
L16-1084,kokkinakis-dannells-2006-recognizing,0,0.031401,"ng resembles sense inventory organisation, which can eventually serve as reference for disambiguation. We report here on the ﬁrst two aspects. With regard to acronym extraction, existing work almost exclusively focuses on English biomedical literature (Schwartz and Hearst, 2003; Okazaki and Ananiadou, 2006; James et al., 2001; Wren and Garner, 2002; Adar, 2004; Chang et al., 2002; David and Turney, 2005). Results are good and the extraction-recognition step can be considered a mature technology for this combination of domain and language. However, there is very little work on other languages: Kokkinakis and Dannélls (2006) investigate the speciﬁcity of Swedish, Siklósi et al. (2014) carry out Hungarian abbreviation processing, both on medical texts. Kompara (2010) and Hahn et al. (2005) seem to be the only ones to work with acronyms across languages, with preliminary work on Slovene, English and Italian for the former, and acronym alignment across English, German, Portuguese and Spanish based on an inter-lingua for the latter. As mentioned previously, the variety and the number of acronyms is very large so that it is useful to organise the acronym dataset on a semantic basis by grouping related variants under t"
L16-1084,R11-1015,1,0.889273,"Missing"
P10-2070,W04-3219,0,0.0241606,"compression. We survey below work along these lines. Although our approach is related to sentence compression (Knight and Marcu, 2002; Clarke and Lapata, 2008), it is subtly different. Firstly, we reduce the number of terms to be used in the summary at a global level, not at a local per-sentence level. Secondly, we directly exploit the resulting structures from the SVD making the last generation step fully aware of previous processing stages, as opposed to tackling the problem of sentence compression in isolation. A similar approach to our sentence reconstruction method has been developed by Quirk et al. (2004) for paraphrase generation. In their work, training and test sets contain sentence pairs that are composed of two different proper English sentences and a paraphrase of a source sentence is generated by finding the optimal path through a paraphrases lattice. Finally, it is worth mentioning that we are aware of the ‘capsule overview’ summaries proposed by Boguraev and Kennedy (1997) which is similar to our TSR (see below), however, as opposed to their emphasis on a suitable browsing interface rather than producing a readable summary, we precisely attempt the latter. 3 3.1 Term Summary Represent"
P10-2070,W97-0702,0,0.113435,"ing the last generation step fully aware of previous processing stages, as opposed to tackling the problem of sentence compression in isolation. A similar approach to our sentence reconstruction method has been developed by Quirk et al. (2004) for paraphrase generation. In their work, training and test sets contain sentence pairs that are composed of two different proper English sentences and a paraphrase of a source sentence is generated by finding the optimal path through a paraphrases lattice. Finally, it is worth mentioning that we are aware of the ‘capsule overview’ summaries proposed by Boguraev and Kennedy (1997) which is similar to our TSR (see below), however, as opposed to their emphasis on a suitable browsing interface rather than producing a readable summary, we precisely attempt the latter. 3 3.1 Term Summary Representation To explain how a term summary representation (TSR) is produced, we first need to define two concepts: salience score of a given term and salience threshold. Salience score for each term in matrix A is given by the magnitude of the corresponding vector in the matrix resulting from the dot product of the matrix of left singular vectors with the diagonal matrix of singular value"
P10-2070,2002.tmi-tutorials.2,0,0.0200853,"on describes a probabilistic approach to the reconstruction problem. We adopt the noisychannel framework that has been widely used in a number of other NLP applications. Our interpretation of the noisy channel consists of looking at a stemmed string without stopwords and imagining that it was originally a long string and that someone removed or stemmed some text from it. In our framework, reconstruction consists of identifying the original long string. To model our interpretation of the noisy channel, we make use of one of the most popular classes of SMT systems: the Phrase Based Model (PBM) (Zens et al., 2002; Och and Ney, 2001; Koehn et al., 2003). It is an extension of the noisychannel model and was introduced by Brown et al. (1994), using phrases rather than words. In PBM, a source sentence f is segmented into a sequence of I phrases f I = [f1 , f2 , . . . fI ] and the same is done for the target sentence e, where the notion of phrase is not related to any grammatical assumption; a phrase is an n-gram. The best translation ebest of f is obtained by: ebest = arg max p(e|f ) = arg max e e I Y In our reconstruction problem, the difference between the source and target sentences is not in terms of"
P10-2070,I05-2047,0,0.0308449,"r the underlying logic and assumptions, but it also provides a neat framework for developing systems based on clean and extendable designs. For instance, Gong and Liu (2002) proposed a method based on Latent Semantic Analysis (LSA) and later J. Steinberger et al. (2007) showed that solely by enhancing the first source interpretation phase, one is already able to produce better summaries. There has been limited work on the last summary generation phase due to the fact that it is unarguably a very challenging problem. The vast 2 Related Work There is a large body of literature on summarization (Hovy, 2005; Erkan and Radev, 2004; Kupiec et al., 1995). The most closely related work to the approach presented hereby is work on summarization attempting to go beyond simple sentence ex1 http://www.nist.gov/tac/ 382 Proceedings of the ACL 2010 Conference Short Papers, pages 382–386, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics ing a succinct summary representation comprised only of salient terms – Term Summary Representation (TSR). Then this TSR is passed on to another module which attempts to produce complete sentences. The module for sentence reconstruction is d"
P10-2070,N03-1017,0,0.00276555,"28 0.372 0.083 0.131 0.298 0.083 0.104 Table 2: Summarization results on TAC’09 data (initial summaries). 4 Noisy-channel model for sentence reconstruction start position of the source phrase that was translated into the ith target phrase, and bi−1 denotes the end position of the source phrase translated into the (i−1th ) target phrase. pLM (ei |e1 . . . ei−1 ) is the language model probability that is based on the Markov chain assumption. It assigns a higher probability to fluent/grammatical sentences. λφ , λLM and λd are used to give a different weight to each element (for more details see (Koehn et al., 2003)). This section describes a probabilistic approach to the reconstruction problem. We adopt the noisychannel framework that has been widely used in a number of other NLP applications. Our interpretation of the noisy channel consists of looking at a stemmed string without stopwords and imagining that it was originally a long string and that someone removed or stemmed some text from it. In our framework, reconstruction consists of identifying the original long string. To model our interpretation of the noisy channel, we make use of one of the most popular classes of SMT systems: the Phrase Based"
P10-2070,P07-2045,0,0.00581514,"tter performing system than a typical LSA system based on sentence selection. Hence, we find these results very encouraging. Finally, we admittedly note that by applying a percentage cutoff on the initial term set and further performing the sentence reconstruction we gain in content coverage, to a certain extent, on the expense of sentence readability. • Source Sentence: “royal mail ha doubl profit 321 million huge fall number letter post” • Target Sentence: “royal mail has doubled its profits to 321 million despite a huge fall in the number of letters being posted” In this work we use Moses (Koehn et al., 2007), a complete phrase-based translation toolkit for academic purposes. It provides all the state-of-theart components needed to create a phrase-based machine translation system. It contains different modules to preprocess data, train the Language Models and the Translation Models. 5 Experimental Results For our experiments we made use of the TAC 2009 data which conveniently contains humanproduced summaries against which we could evaluate the output of our system (NIST, 2009). To begin our inquiry we carried out a phase of exploratory data analysis, in which we measured the average number of sent"
P10-2070,N03-1020,0,0.456388,"epresentations without recurring to simple sentence extraction and aiming at more human-like summaries. This decision is also motivated by empirical evidence from TAC 2009 data (see table 1) showing that human summaries contain on average more and shorter sentences than the system summaries. The intuition behind this is that, by containing more sentences, a summary is able to capture more of the important content from the source. Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data1 yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. Taking into account that our approach is completely unsupervised and language-independent, we find our preliminary results encouraging. The remainder of the paper is organised as follows: in the next section we briefly survey the related work, in §3 we describe our approach to summarization, in §4 we explain how we tackle the generation step, in §5 we present and discuss our experimental results and towards the end we conclude and give pointers to future work. The main focus of this work is to investigate robust wa"
P10-2070,P02-1038,0,\N,Missing
pajzs-etal-2014-media,zaghouani-etal-2010-adapting,1,\N,Missing
pajzs-etal-2014-media,pouliquen-etal-2006-geocoding,1,\N,Missing
pajzs-etal-2014-media,W05-1106,0,\N,Missing
pajzs-etal-2014-media,W03-1306,0,\N,Missing
pajzs-etal-2014-media,R11-1035,1,\N,Missing
pajzs-etal-2014-media,szarvas-etal-2006-highly,0,\N,Missing
pouliquen-etal-2006-geocoding,E99-1001,0,\N,Missing
R11-1015,W03-2201,0,0.0676395,"Missing"
R11-1015,buchholz-van-den-bosch-2000-integrating,0,0.0606041,"Missing"
R11-1015,maurel-2008-prolexbase,0,0.0258307,"Missing"
R11-1015,wentland-etal-2008-building,0,\N,Missing
R11-1015,toral-etal-2008-named,0,\N,Missing
R11-1017,P03-2031,0,0.0300022,"ng the comparability of NER system results across languages; morever, as named entity recognition systems are domainsensitive, it could be relevant to evaluate multilingual NER systems on equivalent tasks. The remainder of the paper is organized as follows. We introduce related work (section 2), then present our NE projection method (section 3), report the results (section 4) and finally conclude and propose some elements for future work (section 5). 2 Related Work Regarding the automatic acquisition of NE annotated corpora, some work investigates how to constitute monolingual annotated data (An et al., 2003; Nothman et al., 2008). With respect to parallel corpora, their exploitation has been growing in recent years, showing their usefulness in various NLP tasks like word sense disambiguation or cross-lingual tagging (refer to the state of art presented by Bentivolgi and Pianta (2005)). With respect to cross-lingual knowledge induction, multiple work addressed the challenge of automatic parallel treebank building (Lavie et al., 2008; Hwa et al., 2005), whereas (Padó and Lapata, 2009; Bentivogli and Pianta, 2005) explored semantic information projection. Several researchers investigated named enti"
R11-1017,U08-1016,0,0.0554916,"ity of NER system results across languages; morever, as named entity recognition systems are domainsensitive, it could be relevant to evaluate multilingual NER systems on equivalent tasks. The remainder of the paper is organized as follows. We introduce related work (section 2), then present our NE projection method (section 3), report the results (section 4) and finally conclude and propose some elements for future work (section 5). 2 Related Work Regarding the automatic acquisition of NE annotated corpora, some work investigates how to constitute monolingual annotated data (An et al., 2003; Nothman et al., 2008). With respect to parallel corpora, their exploitation has been growing in recent years, showing their usefulness in various NLP tasks like word sense disambiguation or cross-lingual tagging (refer to the state of art presented by Bentivolgi and Pianta (2005)). With respect to cross-lingual knowledge induction, multiple work addressed the challenge of automatic parallel treebank building (Lavie et al., 2008; Hwa et al., 2005), whereas (Padó and Lapata, 2009; Bentivogli and Pianta, 2005) explored semantic information projection. Several researchers investigated named entity annotation and paral"
R11-1017,J82-2005,0,0.610215,"Missing"
R11-1017,W09-3025,1,0.889123,"Missing"
R11-1017,W08-0305,1,0.883649,"Missing"
R11-1017,W10-1830,0,0.0225763,"s tools via robust (and noisy) annotation projection. More recently, Ma (2010) applied a cotraining algorithm on unlabelled bilingual data (English-Chinese), showing that NE taggers can complement and improve each other while working together on parallel corpora. Samy et al. (2005) developed a named entity recognizer for Arabic, leveraging an Arabic-Spanish parallel corpus aligned at sentence level and POS tagged. With a slightly different goal, Klementiev and Roth (2008) proposed an algorithm for crosslingual multiword NE discovery in a bilingual weakly temporally aligned corpus. The work of Volk et al. (2010) on combining parallel treebanks and geo-tagging showed similar results to what we offer, with the difference that they focused on the location type only and worked with a bilingual French-German corpus. Finally, Shah et al. ( 2010) designed a Machine Translation-based approach to NER which includes a NE annotation projection phase based on word alignment. These approaches aimed at developing/improving NER systems and parallel annotated corpora seemed to be a positive side-effect of these experiments. In comparison, our work differs from that mentioned here in that we aim at developing an anno"
R11-1017,H01-1035,0,0.235233,"d entity recognition (NER). Within a development or training framework, annotated corpora are used as models from which machine learning systems, or computational linguists, can infer rules and decision criteria; within an evaluation framework, they are used as a gold standard to assess systems’ performances and help to guide their quality improvement, e.g. via non-regression tests. During the last decade, several named entity (NE) annotated corpora were built, thanks to a 118 Proceedings of Recent Advances in Natural Language Processing, pages 118–124, Hissar, Bulgaria, 12-14 September 2011. Yarowsky et al. (2001) carried out some pioneer experiments, investigating the feasibility of annotation projection over four NLP tasks, including named entity recognition. The goal was to automatically induce stand-alone text analysis tools via robust (and noisy) annotation projection. More recently, Ma (2010) applied a cotraining algorithm on unlabelled bilingual data (English-Chinese), showing that NE taggers can complement and improve each other while working together on parallel corpora. Samy et al. (2005) developed a named entity recognizer for Arabic, leveraging an Arabic-Spanish parallel corpus aligned at s"
R11-1017,H05-1108,0,\N,Missing
R11-1017,W09-0401,0,\N,Missing
R11-1017,P07-2045,0,\N,Missing
R11-1017,W08-0411,0,\N,Missing
R11-1035,P07-1107,0,0.0236191,"ltidocument multilingual13 summarization systems (Turchi et al., 2010).14 Our approach for integrating a coreference resolver into an LSA-based summarization system draws on the method put forward by (Steinberger et al., 2007). The intuition behind this choice is that in addition to capturing pure lexical cooccurrence the extended system is also capable of capturing entity co-occurrence which takes the summarization process to a more semanticallyaware level. 5 Related work Representatives of machine learning work on coreference are (Ng and Cardie, 2002; Luo, 2007) for supervised learning and (Haghighi and Klein, 2007) for unsupervised. In more recent work, (Stoyanov et al., 2009) provides a comprehensive discussion of the state of the art coupled with extensive experiments on the standard corpora for English: MUC-6, MUC7, ACE-2, ACE-3, ACE-4 and ACE-5. Recasens and Hovy (2010) explore the impact on coreference resolution performance by varying several prominent contextual factors; they measure performance across corpora, languages, annotation schemes and preprocessing. However, their set of languages consisted of English and Spanish only. The most closely related experiment to ours is that of the SemEval-2"
R11-1035,M98-1029,0,0.124329,"Missing"
R11-1035,N07-1010,0,0.0218615,"cly available corpus12 for evaluating multidocument multilingual13 summarization systems (Turchi et al., 2010).14 Our approach for integrating a coreference resolver into an LSA-based summarization system draws on the method put forward by (Steinberger et al., 2007). The intuition behind this choice is that in addition to capturing pure lexical cooccurrence the extended system is also capable of capturing entity co-occurrence which takes the summarization process to a more semanticallyaware level. 5 Related work Representatives of machine learning work on coreference are (Ng and Cardie, 2002; Luo, 2007) for supervised learning and (Haghighi and Klein, 2007) for unsupervised. In more recent work, (Stoyanov et al., 2009) provides a comprehensive discussion of the state of the art coupled with extensive experiments on the standard corpora for English: MUC-6, MUC7, ACE-2, ACE-3, ACE-4 and ACE-5. Recasens and Hovy (2010) explore the impact on coreference resolution performance by varying several prominent contextual factors; they measure performance across corpora, languages, annotation schemes and preprocessing. However, their set of languages consisted of English and Spanish only. The most clos"
R11-1035,P02-1014,0,0.0209732,"ibed above on a publicly available corpus12 for evaluating multidocument multilingual13 summarization systems (Turchi et al., 2010).14 Our approach for integrating a coreference resolver into an LSA-based summarization system draws on the method put forward by (Steinberger et al., 2007). The intuition behind this choice is that in addition to capturing pure lexical cooccurrence the extended system is also capable of capturing entity co-occurrence which takes the summarization process to a more semanticallyaware level. 5 Related work Representatives of machine learning work on coreference are (Ng and Cardie, 2002; Luo, 2007) for supervised learning and (Haghighi and Klein, 2007) for unsupervised. In more recent work, (Stoyanov et al., 2009) provides a comprehensive discussion of the state of the art coupled with extensive experiments on the standard corpora for English: MUC-6, MUC7, ACE-2, ACE-3, ACE-4 and ACE-5. Recasens and Hovy (2010) explore the impact on coreference resolution performance by varying several prominent contextual factors; they measure performance across corpora, languages, annotation schemes and preprocessing. However, their set of languages consisted of English and Spanish only. T"
R11-1035,J04-3003,0,0.0312897,"Missing"
R11-1035,N06-1025,0,0.0454887,"Missing"
R11-1035,P10-1144,0,0.0118247,"hat in addition to capturing pure lexical cooccurrence the extended system is also capable of capturing entity co-occurrence which takes the summarization process to a more semanticallyaware level. 5 Related work Representatives of machine learning work on coreference are (Ng and Cardie, 2002; Luo, 2007) for supervised learning and (Haghighi and Klein, 2007) for unsupervised. In more recent work, (Stoyanov et al., 2009) provides a comprehensive discussion of the state of the art coupled with extensive experiments on the standard corpora for English: MUC-6, MUC7, ACE-2, ACE-3, ACE-4 and ACE-5. Recasens and Hovy (2010) explore the impact on coreference resolution performance by varying several prominent contextual factors; they measure performance across corpora, languages, annotation schemes and preprocessing. However, their set of languages consisted of English and Spanish only. The most closely related experiment to ours is that of the SemEval-2010 task 1 (Recasens et al., 2010), which covered coreference evaluation on six languages. 4.2.1 Experimental Results The experimental results are presented in table 5. Each summary score is computed by first calculating the intersection of sentences selected by t"
R11-1035,W09-2411,0,0.064013,"Missing"
R11-1035,P09-1074,0,0.0649835,"great part due to the availability of annotated corpora such as MUC-6/7 (Hirschman, 1998), ACE-2/3/4/5 (NIST, 2004), GNOME (Poesio et al., 2004) and large-scale crowdsourcing efforts like Phrase Detectives.1 One of the big advantages of machine learning approaches is that they are reasonably easy to reproduce given that the set of input features are documented well, since there are many good open-source platforms for machine learning (e.g., WEKA2 ) and machine-learning-based coreference (e.g., BART3 (Versley et al., 2008)). However, intrinsic evaluations can pose problems. As pointed out by (Stoyanov et al., 2009) 1 http://www.phrasedetectives.org. http://www.cs.waikato.ac.nz/ml/weka/. 3 http://www.bart-coref.org/. 2 4 http://emm.newsbrief.eu/overview.html 254 Proceedings of Recent Advances in Natural Language Processing, pages 254–260, Hissar, Bulgaria, 12-14 September 2011. groups or religions (e.g., Iraqi, Latin-American, Parisian, Berber, Catholic), and a variety of other strings that may indicate that the adjacent uppercase words are a person (e.g., XX-year-old, has declared, deceased). These lists are mostly produced using empirical methods or machine learning, but they are always manually verifi"
R11-1035,P08-4003,0,0.0215873,"the English language (Ng and Cardie, 2002; Ponzetto and Strube, 2006; Luo, 2007). This is in great part due to the availability of annotated corpora such as MUC-6/7 (Hirschman, 1998), ACE-2/3/4/5 (NIST, 2004), GNOME (Poesio et al., 2004) and large-scale crowdsourcing efforts like Phrase Detectives.1 One of the big advantages of machine learning approaches is that they are reasonably easy to reproduce given that the set of input features are documented well, since there are many good open-source platforms for machine learning (e.g., WEKA2 ) and machine-learning-based coreference (e.g., BART3 (Versley et al., 2008)). However, intrinsic evaluations can pose problems. As pointed out by (Stoyanov et al., 2009) 1 http://www.phrasedetectives.org. http://www.cs.waikato.ac.nz/ml/weka/. 3 http://www.bart-coref.org/. 2 4 http://emm.newsbrief.eu/overview.html 254 Proceedings of Recent Advances in Natural Language Processing, pages 254–260, Hissar, Bulgaria, 12-14 September 2011. groups or religions (e.g., Iraqi, Latin-American, Parisian, Berber, Catholic), and a variety of other strings that may indicate that the adjacent uppercase words are a person (e.g., XX-year-old, has declared, deceased). These lists are mo"
R11-1035,S10-1001,0,\N,Missing
R11-1113,C04-1200,0,0.20149,"Missing"
R11-1113,W06-0301,0,0.0251127,"Missing"
R11-1113,P07-1123,0,0.0363773,"Missing"
R11-1113,W02-1011,0,0.0152932,"Missing"
R11-1113,W11-1704,1,0.580069,"not usually available for other types of texts then reviews, or they are almost exclusively available for English. Sentiment dictionaries are also mostly available for English only or, if they exist for other languages, they are not comparable, in the sense that they have been developed for different purposes, have different sizes, are based on different definitions of what sentiment or opinion means. We addressed the resource bottleneck for sentiment dictionaries, by developing highly multilingual and comparable sentiment dictionaries having similar sizes and based on a common specification (Steinberger et al., 2011). Our sentiment system is simply based on counting subjective terms around entity mentions (mainly persons and organizations). Evaluating its performance in more languages would multiply the annotation efforts. In this paper we propose using parallel corpora to automatically project annotations from English. We study the subjectivity of the entity-centered sentiment annotation and evaluate our sentiment system in seven languages (English, Spanish, French, German, Czech, Italian and Hungarian). As a side effect this evaluation serves as a task-based evaluation of the quality of the sentiment di"
R11-1113,balahur-etal-2010-sentiment,1,0.543102,"Missing"
R11-1113,P06-1134,0,0.0603856,"Missing"
R11-1113,banea-etal-2008-bootstrapping,0,0.0868364,"Missing"
R11-1113,C00-1044,0,0.0404557,"Missing"
R11-1113,P09-1027,0,\N,Missing
R13-1031,kokkinakis-dannells-2006-recognizing,0,0.822642,"Missing"
R13-1031,W01-0516,0,0.0923378,"Missing"
R13-1031,R11-1015,1,\N,Missing
steinberger-etal-2006-jrc,J93-1004,0,\N,Missing
steinberger-etal-2006-jrc,moore-2002-fast,0,\N,Missing
steinberger-etal-2006-jrc,P06-2035,0,\N,Missing
steinberger-etal-2006-jrc,2005.mtsummit-papers.11,0,\N,Missing
steinberger-etal-2006-jrc,civera-juan-2006-bilingual,0,\N,Missing
steinberger-etal-2006-jrc,erjavec-2004-multext,1,\N,Missing
steinberger-etal-2012-dgt,R11-1015,1,\N,Missing
steinberger-etal-2012-dgt,steinberger-etal-2006-jrc,1,\N,Missing
steinberger-etal-2012-dgt,H01-1035,0,\N,Missing
steinberger-etal-2012-dgt,2005.mtsummit-papers.11,0,\N,Missing
steinberger-etal-2012-dgt,steinberger-etal-2012-jrc,1,\N,Missing
steinberger-etal-2012-dgt,2009.mtsummit-papers.7,1,\N,Missing
steinberger-etal-2012-dgt,macklovitch-etal-2000-transsearch,0,\N,Missing
steinberger-etal-2012-jrc,R11-1015,1,\N,Missing
steinberger-etal-2012-jrc,steinberger-etal-2006-jrc,1,\N,Missing
steinberger-etal-2012-jrc,steinberger-etal-2012-dgt,1,\N,Missing
steinberger-etal-2012-jrc,W08-1408,1,\N,Missing
W08-1408,wayne-2000-multilingual,0,0.0830951,"Missing"
W08-1408,pouliquen-etal-2006-geocoding,1,\N,Missing
W08-1408,C04-1138,1,\N,Missing
W09-4602,pouliquen-etal-2006-geocoding,1,\N,Missing
W09-4602,W08-1408,1,\N,Missing
W11-1704,balahur-etal-2010-sentiment,1,0.796551,"Missing"
W11-1704,banea-etal-2008-bootstrapping,0,0.149358,"ctive language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. Since word ambiguity can appear (Opinion Finder does not mark word senses), they filter as correct translations only the most frequent words. The problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the Web. Another approach in obtaining subjectivity lexicons for other languages than English was explored in Banea et al. (2008b). To this aim, the authors perform three different experiments, with good results. In the first one, they automatically translate the annotations of the MPQA corpus and thus obtain subjectivity annotated sentences in Romanian. In the second approach, they use the automatically translated entries in the Opinion Finder lexicon to annotate a set of sentences in Romanian. In the last experiment, they reverse the direction of translation and verify the assumption that subjective language can be translated and thus new subjectivity lexicons can be obtained for languages with no such resources. Fin"
W11-1704,D08-1014,0,0.0716055,"ctive language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. Since word ambiguity can appear (Opinion Finder does not mark word senses), they filter as correct translations only the most frequent words. The problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the Web. Another approach in obtaining subjectivity lexicons for other languages than English was explored in Banea et al. (2008b). To this aim, the authors perform three different experiments, with good results. In the first one, they automatically translate the annotations of the MPQA corpus and thus obtain subjectivity annotated sentences in Romanian. In the second approach, they use the automatically translated entries in the Opinion Finder lexicon to annotate a set of sentences in Romanian. In the last experiment, they reverse the direction of translation and verify the assumption that subjective language can be translated and thus new subjectivity lexicons can be obtained for languages with no such resources. Fin"
W11-1704,C10-1004,0,0.181087,"es and a training set of words. They start with a set of 60 words pertaining to the categories of noun, verb, adjective and adverb obtained by translating words in the Opinion Finder lexicon. Translations are filtered using a measure of similarity to the original words, based on Latent Semantic Analysis (Landauer and Dumais, 1997) scores. Wan (2008) uses co-training to classify un-annotated Chinese reviews using a corpus of annotated English reviews. He first translates the English reviews into Chinese and subsequently back to English. He then performs co-training using all generated corpora. Banea et al. (2010) translate the MPQA corpus into five other languages (some with a similar ethimology, others with a very different structure). Subsequently, they expand the feature space used in a Naive Bayes classifier using the same data translated to 2 or 3 other languages. Their conclusion is that expanding the feature space with data from other languages performs almost as well as training a classifier for just one language on a large set of training data. 3 Approach Overview Our approach to dictionary creation starts with semiautomatic way of colleting subjective terms in English and Spanish. These pivo"
W11-1704,esuli-sebastiani-2006-sentiwordnet,0,0.00323747,"emiautomatic way of colleting subjective terms in English and Spanish. These pivot language dictionaries are then projected to other languages. The 3rd language dictionaries are formed by the overlap of the translations (triangulation). The lists are then manually filtered and expanded, either by other relevant terms or by their morphological variants, to gain a wider coverage. 30 3.1 Gathering Subjective Terms We started with analysing the available English dictionaries of subjective terms: General Inquirer (Stone et al., 1966), WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOp (Cerini et al., 2007). Additionally, we used the resource of opinion words with associated polarity from Balahur et al. (2009), which we denote as JRC Tonality Dictionary. The positive effect of distinguishing two levels of intensity was shown in (Balahur et al., 2010). We followed the idea and each of the emloyed resources was mapped to four categories: positive, negative, highly positive and highly negative. We also got inspired by the results reported in that paper and we selected as the base dictionaries the combination of MicroWNOp and JRC Tonality Dictionary which gave the be"
W11-1704,W06-0301,0,0.0170717,"om the automatic learning of the sentiment vocabulary, the triangulation process, the expansion of the dictionaries in size and regarding morphological inflections. Section 4 presents a number of results regarding dictionary creation using simple translation versus triangulation, morphological expansion and inter-annotator agreement. Section 5 summarises, concludes and points to future work. 2 Related Work Most of the work in obtaining subjectivity lexicons was done for English. However, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages. Kim and Hovy (2006) use a machine translation system and subsequently use a subjectivity analysis system that was developed for English. Mihalcea et al. (2007) propose a method to learn multilingual subjective language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. Since word ambiguity can appear (Opinion Finder does not mark word senses), they filter as correct translations only the most frequent words. The problem of translating multi-word expressions is solved by translating word-b"
W11-1704,P07-1123,0,0.270533,"morphological inflections. Section 4 presents a number of results regarding dictionary creation using simple translation versus triangulation, morphological expansion and inter-annotator agreement. Section 5 summarises, concludes and points to future work. 2 Related Work Most of the work in obtaining subjectivity lexicons was done for English. However, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages. Kim and Hovy (2006) use a machine translation system and subsequently use a subjectivity analysis system that was developed for English. Mihalcea et al. (2007) propose a method to learn multilingual subjective language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. Since word ambiguity can appear (Opinion Finder does not mark word senses), they filter as correct translations only the most frequent words. The problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the Web. Another approach in obtaining subjectivity lexicons for"
W11-1704,W03-1014,0,0.029016,"oved at this step terms that are more likely to describe bad or good news content, rather than a sentiment towards an entity. In addition, we manually collected English diminishers (e.g. less or approximately), intensifiers (e.g. very or indeed) and invertors (e.g. not or barely). The English terms were translated to Spanish and the same filtering was performed. We extended all English and Spanish lists with the missing morphological variants of the terms. 3.2 Automatic Learning of Subjective Terms We decided to expand our subjective term lists by using automatic term extraction, inspired by (Riloff and Wiebe, 2003). We look at the problem of acquisition of subjective terms as learning of semantic classes. Since we wanted to do this for two different languages, namely English and Spanish, the multilingual term extraction algorithm Ontopopulis (Tanev et al., 2010) was a natural choice. Ontopopulis performs weakly supervised learning of semantic dictionaries using distributional similarity. The algorithm takes on its input a small set of seed terms for each semantic class, which is to be learnt, and an unannotated text corpus. For example, if we want to learn the semantic class land vehicles, we can use th"
W11-1704,strapparava-valitutti-2004-wordnet,0,0.00814991,"ur approach to dictionary creation starts with semiautomatic way of colleting subjective terms in English and Spanish. These pivot language dictionaries are then projected to other languages. The 3rd language dictionaries are formed by the overlap of the translations (triangulation). The lists are then manually filtered and expanded, either by other relevant terms or by their morphological variants, to gain a wider coverage. 30 3.1 Gathering Subjective Terms We started with analysing the available English dictionaries of subjective terms: General Inquirer (Stone et al., 1966), WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOp (Cerini et al., 2007). Additionally, we used the resource of opinion words with associated polarity from Balahur et al. (2009), which we denote as JRC Tonality Dictionary. The positive effect of distinguishing two levels of intensity was shown in (Balahur et al., 2010). We followed the idea and each of the emloyed resources was mapped to four categories: positive, negative, highly positive and highly negative. We also got inspired by the results reported in that paper and we selected as the base dictionaries the combination of MicroWNOp an"
W11-1704,H05-1044,0,0.0382703,"ogical expansion and inter-annotator agreement. Section 5 summarises, concludes and points to future work. 2 Related Work Most of the work in obtaining subjectivity lexicons was done for English. However, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages. Kim and Hovy (2006) use a machine translation system and subsequently use a subjectivity analysis system that was developed for English. Mihalcea et al. (2007) propose a method to learn multilingual subjective language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. Since word ambiguity can appear (Opinion Finder does not mark word senses), they filter as correct translations only the most frequent words. The problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the Web. Another approach in obtaining subjectivity lexicons for other languages than English was explored in Banea et al. (2008b). To this aim, the authors perform three different experiments, with good results. I"
W11-1704,P09-1027,0,\N,Missing
W14-1309,R13-1011,0,0.00591036,"erforms NER and named entity normalization on tweets. An unsupervised approach that performs only named entity extraction on tweets using resources like Wikipedia is described in (Li et al., 2012). A clustering-based approach for NER on microtexts is presented in (Jung, 2012), a lightweight filter based approach for NER on tweets is described in (de Oliveira et al., 2013), and a series of NER experiments on targeted tweets in Polish is presented in (Piskorski and Ehrmann, 2013). Finally, an adaptation of the ANNIE component of GATE framework to microblog texts, called TwitIE, is described in (Bontcheva et al., 2013). Table 1: NE Statistics on the Data Sets. NE Type Person Location Organization All PLOs Date Time Money Percent All NEs 3 Frequency in Tweet Set-1 Tweet Set-2 457 774 282 191 241 409 980 1,374 201 342 5 25 16 13 9 3 1,211 1,757 Named Entity Recognition Experiments The NER experiments are performed using the rule-based NER system (K¨uc¸u¨ k and Yazıcı, 2009) which makes use of a set of lexical resources, i.e., lists of person/location/organization names (henceforth referred to as PLOs), and patterns for the extraction of named entities (NEs) of type PLOs and time/date/money/percent expressions"
W14-1309,C12-1150,0,0.242149,"Missing"
W14-1309,P11-1038,0,0.0715256,"Missing"
W14-1309,W13-2413,0,0.0108871,"arest Neighbor and CRF based classifiers are sequentially applied. In (Liu et al., 2012), a factor graph based approach is proposed that jointly performs NER and named entity normalization on tweets. An unsupervised approach that performs only named entity extraction on tweets using resources like Wikipedia is described in (Li et al., 2012). A clustering-based approach for NER on microtexts is presented in (Jung, 2012), a lightweight filter based approach for NER on tweets is described in (de Oliveira et al., 2013), and a series of NER experiments on targeted tweets in Polish is presented in (Piskorski and Ehrmann, 2013). Finally, an adaptation of the ANNIE component of GATE framework to microblog texts, called TwitIE, is described in (Bontcheva et al., 2013). Table 1: NE Statistics on the Data Sets. NE Type Person Location Organization All PLOs Date Time Money Percent All NEs 3 Frequency in Tweet Set-1 Tweet Set-2 457 774 282 191 241 409 980 1,374 201 342 5 25 16 13 9 3 1,211 1,757 Named Entity Recognition Experiments The NER experiments are performed using the rule-based NER system (K¨uc¸u¨ k and Yazıcı, 2009) which makes use of a set of lexical resources, i.e., lists of person/location/organization names ("
W14-1309,D12-1039,0,0.0291427,"Missing"
W14-1309,D11-1141,0,0.516435,"ity Recognition on Turkish Tweets ¨ ¸ uk ¨ and Ralf Steinberger Dilek Kuc European Commission, Joint Research Centre Via E. Fermi 2749 21027 Ispra (VA), Italy firstname.lastname@jrc.ec.europa.eu Abstract in (Ritter et al., 2012), named entities in tweets are used to complement the events extracted by an open domain event extraction system for Twitter. However, existing NER solutions for well-formed text types like news articles are reported to suffer from considerable performance degradations when they are ported to social media texts, mainly due to the peculiarities of this latter text type (Ritter et al., 2011). In this paper, we report on our NER experiments on Turkish tweets in order to determine facilitating and impeding factors during the development of a NER system for Turkish tweets which can be used in social media analysis applications. We carry out these experiments on two tweet data sets annotated with named entities. After the initial evaluation results of a rule-based NER system (K¨uc¸u¨ k and Yazıcı, 2009) on these data sets, we gradually present the performance results achieved by the extended versions of the system together with discussions of these results. For these experiments, we"
W14-1309,kucuk-etal-2014-named,1,0.515758,"Missing"
W14-1309,P11-1037,0,0.0644024,"presenting approaches for NER on microblog texts, especially on tweets in English. Among these studies, in (Ritter et al., 2011), a NER system tailored to 71 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 71–78, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for subsequent classification, using Freebase dictionaries. A hybrid approach to NER on tweets is presented in (Liu et al., 2011) where k-Nearest Neighbor and CRF based classifiers are sequentially applied. In (Liu et al., 2012), a factor graph based approach is proposed that jointly performs NER and named entity normalization on tweets. An unsupervised approach that performs only named entity extraction on tweets using resources like Wikipedia is described in (Li et al., 2012). A clustering-based approach for NER on microtexts is presented in (Jung, 2012), a lightweight filter based approach for NER on tweets is described in (de Oliveira et al., 2013), and a series of NER experiments on targeted tweets in Polish is pre"
W14-1309,P11-3019,0,0.0367085,"Missing"
W14-1309,P12-1055,0,0.502493,"ies, in (Ritter et al., 2011), a NER system tailored to 71 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 71–78, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for subsequent classification, using Freebase dictionaries. A hybrid approach to NER on tweets is presented in (Liu et al., 2011) where k-Nearest Neighbor and CRF based classifiers are sequentially applied. In (Liu et al., 2012), a factor graph based approach is proposed that jointly performs NER and named entity normalization on tweets. An unsupervised approach that performs only named entity extraction on tweets using resources like Wikipedia is described in (Li et al., 2012). A clustering-based approach for NER on microtexts is presented in (Jung, 2012), a lightweight filter based approach for NER on tweets is described in (de Oliveira et al., 2013), and a series of NER experiments on targeted tweets in Polish is presented in (Piskorski and Ehrmann, 2013). Finally, an adaptation of the ANNIE component of GATE fram"
W14-1309,R11-1015,1,\N,Missing
W17-1702,C02-1130,0,0.110243,"upervised learning, relying on large, often manually annotated corpora from which to extract and learn positive and negative features for a particular class of entity. Since such corpora are costly, attention has also turned to distant-supervision, which utilises existing structured resources (e.g. WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008), BabelNet (Navigli and Ponzetto, 2012), among others) to automatically generate ‘silver-standard’ annotated corpora, without incurring the cost associated with gaining access to manually annotated corpora (e.g. Fleischman and Hovy (2002), Ling and Weld (2012), Nothman et al. (2013)). We follow this general approach with the production of a large-scale automatically-created MWEntity resource extracted from BabelNet, used to distantly supervise our classifiers. Similarly, weaklysupervised systems use a bootstrapping technique to approach the same issue, starting with a few annotated examples and automatically expanding the corpus based on these ‘seed’ terms (e.g. Pasca et 3 Extraction of a Multi-Word Entity Silver-Standard Resource from BabelNet When addressing the MWEntity recognition task, some approaches are based on methods"
W17-1702,L16-1084,1,0.673279,"Missing"
W17-1702,D15-1103,0,0.0181481,". (2006), Ratinov and Roth (2009)). In this work, we are interested in drawing a distinction between the recognition of named entities and, most relevant to us, their classification. The task of entity classification has been approached largely through machine learning techniques, utilising both word-internal features (Durrett and Klein, 2014) and additional contextual information, such as dictionary definitions (Gangemi et al., 2012) and ‘lexical expansions’ (e.g. synonyms and derivationally related forms) extracted from WordNet, as well as co-occurrence statistics from external corpora (Del Corro et al., 2015). Very recent work has also moved towards multi-source learning, automatically retrieving additional semi-structured contextual information, such as webpage titles and URLs, through Web search (Vexler and Minkov, 2016). Much of the early work in the area of NERC was monolingual, often working on English data. As approaches have advanced, multilingual named entities have received more attention, though the reliance on large corpora often limits the possible coverage. In an attempt to overcome this bottleneck, Nothman et al. (2013) automatically classify Wikipedia articles into named entity type"
W17-1702,Q14-1037,0,0.0346112,"focus on MWEntity classification, thereby placing NERC in the context of the study of MWExpressions. 11 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 11–20, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics 2 Related Work al. (2006), Ratinov and Roth (2009)). In this work, we are interested in drawing a distinction between the recognition of named entities and, most relevant to us, their classification. The task of entity classification has been approached largely through machine learning techniques, utilising both word-internal features (Durrett and Klein, 2014) and additional contextual information, such as dictionary definitions (Gangemi et al., 2012) and ‘lexical expansions’ (e.g. synonyms and derivationally related forms) extracted from WordNet, as well as co-occurrence statistics from external corpora (Del Corro et al., 2015). Very recent work has also moved towards multi-source learning, automatically retrieving additional semi-structured contextual information, such as webpage titles and URLs, through Web search (Vexler and Minkov, 2016). Much of the early work in the area of NERC was monolingual, often working on English data. As approaches h"
W17-1702,sekine-etal-2002-extended,0,0.166937,"Missing"
W19-3714,W17-1412,1,0.742627,"Missing"
W19-3714,D10-1098,0,0.0136899,"to analyse the error logs in more detail to investigate possible improvements. Also, we observe that in almost all languages and topics, the best results are obtained by the JRC-TMA-CC-2 system, which is most likely correlated to a high recall. 4 Conclusions and Future Work Related Work NER systems are often the first step in event detection, question answering, information retrieval, co-reference resolution, topic modelling, etc. The first NER task was organised by (Grishman and Sundheim, 1996) in the Sixth Message Understanding Conference. Early NER systems were based on handcrafted rules (Chiticariu et al., 2010), lexicons, orthographic features and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning (Nadeau and Sekine, 2007). There are not many systems for NER that address inflected languages like the Slavic ones. Among the others, (Piskorski et al., 2007) tackled the task of matching morphological variants of names in Polish text by optimising string similar103 learn the inflection patterns, contain errors because the wildcards match too generously, and (f) updating and completing our list of geographical names as the coverage for different langua"
W19-3714,R11-1015,1,0.723178,"Missing"
W19-3714,pajzs-etal-2014-media,1,0.756076,"d Task was subdivided into three subtasks, namely, Entity Recognition, Normalisation and Linking, our contribution focused less on 102 cs ru bg pl JRC-TMA-CC-1 JRC-TMA-CC-2 JRC-TMA-CC-3 JRC-TMA-CC-4 62.0 61.6 58.0 58.0 73.6 72.0 73.7 73.5 73.2 72.4 73.8 74.2 56.13 54.8 50.9 57.4 Relaxed Exact JRC-TMA-CC-1 JRC-TMA-CC-2 JRC-TMA-CC-3 JRC-TMA-CC-4 55.6 54.3 55.3 55.3 68.7 66.2 68.2 68.0 67.3 66.7 67.6 67.9 48.6 45.5 48.4 49.6 Strict JRC-TMA-CC-1 JRC-TMA-CC-2 JRC-TMA-CC-3 JRC-TMA-CC-4 47.6 49.9 50.0 50.0 59.9 60.4 60.6 60.5 63.9 64.4 64.6 65.2 41.8 44.0 44.6 45.9 ity calculations for inflections. (Pajzs et al., 2014) experimented with name lemmatisation and inflection variant generation in the highly inflected and agglutinative language Hungarian. (Gareev et al., 2013) describes NER for the highly inflective Russian language. The first edition of the Shared Task on Slavic NER was organised in the context of BSNLP 2017 (Piskorski et al., 2017) Single language JRC-TMA-CC-1 JRC-TMA-CC-2 JRC-TMA-CC-3 JRC-TMA-CC-4 29.8 35.9 33.5 33.9 41.8 42.9 41.9 41.8 51.8 51.5 51.5 52.4 21.9 30.3 25.8 28.2 5 Cross-lingual JRC-TMA-CC-1 JRC-TMA-CC-2 JRC-TMA-CC-3 JRC-TMA-CC-4 24.7 29.7 26.4 27.3 AVERAGE ON BOTH T OPICS Phase L"
Y09-2019,esuli-sebastiani-2006-sentiwordnet,0,0.0298509,"ing to the polarity strength (the higher the negative score, the more negative the sentence and similarly, the higher the positive score, the more positive the sentence). Given that we are faced with the task of classifying opinion in a general context, we employed a simple, yet efficient approach, presented in (Balahur et al., 2009c). At the present moment, there are different lexicons for affect detection and opinion mining. In order to have a more extensive database of affect-related terms, in the following experiments we used WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOp (Cerini et al., 2007). Each of the employed resources were mapped to four categories, which were given different scores: positive (1), negative (-1), high positive (4) and high negative (-4). As shown in (Balahur et al., 2009c), these values performed better than the usual assignment of only positive (1) and negative (-1) values. First, the score of each of the blog posts was computed as sum of the values of the words identified; a positive score leads to the classification of the post as positive, whereas a final negative score leads to the system classifying the post as negative."
Y09-2019,I05-2047,0,0.0342729,"e present our experimental results and discuss the main issues (sec. 4); and finally, we conclude the paper and give pointers to future work. ? Special thanks to the EMM team for supporting this research. We also thank Ester Boldrini for her annotations of the corpus used in the experiments presented. Copyright 2009 by Alexandra Balahur, Mijail Kabadjov, Josef Steinberger, Ralf Steinberger, and Andr´es Montoyo 23rd Pacific Asia Conference on Language, Information and Computation, pages 606–613 606 2 Related Work Whilst there is abundant literature on text summarization (Kabadjov et al., 2009; Hovy, 2005; Erkan and Radev, 2004; Gong and Liu, 2002) and sentiment analysis (Balahur et al., 2009a; Pang and Lee, 2008; Riloff et al., 2005), there is still limited work at the intersection of these two areas (Stoyanov and Cardie, 2006). Initial research in opinion mining concentrated on news texts. Wiebe (1994) defines subjectivity based on Quirks idea of “private states” (states that are not open to verification) and distinguishes between objectivity and subjectivity on this criteria. Consequently, based on this definition, the Multi-Perspective Question Answering (MPQA) annotation schema and corpus"
Y09-2019,W04-1013,0,0.0209929,"Missing"
Y09-2019,N03-1020,0,0.178486,"Missing"
Y09-2019,P04-1035,0,0.0245929,"ivate states” (states that are not open to verification) and distinguishes between objectivity and subjectivity on this criteria. Consequently, based on this definition, the Multi-Perspective Question Answering (MPQA) annotation schema and corpus were created over news texts, distinguishing between the subjective/objective speech, as well as the polarity of text spans (Wiebe et al., 2005). Subsequently, different authors show that this initial discrimination is crucial for the sentiment task, improving results obtained when using only polarity classification for sentence-level opinion mining (Pang and Lee, 2004), as part of Opinion Information Retrieval (last three editions of the TREC Blog tracks, the TAC 2008 competition), Information Extraction (Riloff et al., 2005) and Question Answering (QA) (Stoyanov et al., 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. 3 Opinion Summarization In our opinion summarization experiments we adopt a standard approach by employing in tandem a sentiment classification system and a text summarizer. The output of the former"
Y09-2019,strapparava-valitutti-2004-wordnet,0,0.0944072,"e and negative) and a numerical value corresponding to the polarity strength (the higher the negative score, the more negative the sentence and similarly, the higher the positive score, the more positive the sentence). Given that we are faced with the task of classifying opinion in a general context, we employed a simple, yet efficient approach, presented in (Balahur et al., 2009c). At the present moment, there are different lexicons for affect detection and opinion mining. In order to have a more extensive database of affect-related terms, in the following experiments we used WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOp (Cerini et al., 2007). Each of the employed resources were mapped to four categories, which were given different scores: positive (1), negative (-1), high positive (4) and high negative (-4). As shown in (Balahur et al., 2009c), these values performed better than the usual assignment of only positive (1) and negative (-1) values. First, the score of each of the blog posts was computed as sum of the values of the words identified; a positive score leads to the classification of the post as positive, whereas a final negative score leads to t"
Y09-2019,J94-2004,0,0.0670595,"Copyright 2009 by Alexandra Balahur, Mijail Kabadjov, Josef Steinberger, Ralf Steinberger, and Andr´es Montoyo 23rd Pacific Asia Conference on Language, Information and Computation, pages 606–613 606 2 Related Work Whilst there is abundant literature on text summarization (Kabadjov et al., 2009; Hovy, 2005; Erkan and Radev, 2004; Gong and Liu, 2002) and sentiment analysis (Balahur et al., 2009a; Pang and Lee, 2008; Riloff et al., 2005), there is still limited work at the intersection of these two areas (Stoyanov and Cardie, 2006). Initial research in opinion mining concentrated on news texts. Wiebe (1994) defines subjectivity based on Quirks idea of “private states” (states that are not open to verification) and distinguishes between objectivity and subjectivity on this criteria. Consequently, based on this definition, the Multi-Perspective Question Answering (MPQA) annotation schema and corpus were created over news texts, distinguishing between the subjective/objective speech, as well as the polarity of text spans (Wiebe et al., 2005). Subsequently, different authors show that this initial discrimination is crucial for the sentiment task, improving results obtained when using only polarity c"
Y09-2019,W06-0302,0,\N,Missing
zaghouani-etal-2010-adapting,W98-1002,0,\N,Missing
zaghouani-etal-2010-adapting,W98-1006,0,\N,Missing
