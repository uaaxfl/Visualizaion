2020.sustainlp-1.21,P12-1024,0,0.033188,"arbitrary features of the input sequence. On the other hand, non-deterministic weighted automata (WFAs) are recurrent models that only use linear activation functions.Essentially, WFAs can be regarded as recurrent neural networks where the function that predicts the dynamic state representation from previous states is linear. For more details about the relations between linear activation RNNs and WFAs, we refer the reader to (Rabusseau et al., 2019). Several algorithms based on low rank matrix decompositions have been proposed (Hsu et al., 2009, 2012; Bailly et al., 2009; Balle et al., 2011; Cohen et al., 2012; Balle et al., 2014). In addition to being easily trainable, WFAs offer other advantages. The main advantage is that they are classical computer science models that have been intensively researched in the theoretical community. Because of this they are relatively well understood and we know how to efficiently perform important computations. For example, consider a WFA computing a distribution over strings, there are simple and efficient algorithms to compute marginal probabilities for prefixes, infixes and suffixes. Furthermore another advantage of these models is that there are well known an"
2020.sustainlp-1.21,P19-1594,1,0.845847,"d how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. We focus on the task of sequence classification and compare the performance of WFAs and CNNs trained under the same initial conditions, over five different data sets. To a certain extent a similar comparison between 159 Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, pages 159–163 c Online, November 20, 2020. 2020 Association for Computational Linguistics WFAs and neural models was made by (Quattoni and Carreras, 2019) in the context of language modeling. But to our knowledge this is the first empirical comparison of CNNs and WFAs for sequence classification. 2 2.1 WFAs for Sequence Classification Preliminaries: WFAs for sequence modelling We will use weighted finite state automata (WFAs) as elementary building blocks to build our sequence prediction model. More precisely, a WFA takes as input a sequence and outputs a real number, that is: f : Σ? → R where x = x1 · · · xn is sequence of length n over some finite alphabet Σ. We denote as Σ? the set of all finite sequences, and we use it as a domain of our fu"
2020.sustainlp-1.21,N15-1011,0,0.0268838,"imilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs. 1 Introduction In the latter years CNNs have been proposed as models for sequence classification and it has been shown that they can give competitive results, even when compared to more complex models (Kim, 2014; Zhang and Wallace, 2017; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Goldberg, 2016). They typically combine various convolutional filters with max-pooling layers. Because they have several interacting layers, it is in general is hard to interpret exactly what is it that they are learning. But most likely their success relies on the fact that their convolutional filters have the ability to capture arbitrary features of the input sequence. On the other hand, non-deterministic weighted automata (WFAs) are recurrent models that only use linear activation functions.Essentially, WFAs can be regarded as recurrent neural networks where the function that predicts the"
2020.sustainlp-1.21,P14-1062,0,0.0460366,"o these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs. 1 Introduction In the latter years CNNs have been proposed as models for sequence classification and it has been shown that they can give competitive results, even when compared to more complex models (Kim, 2014; Zhang and Wallace, 2017; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Goldberg, 2016). They typically combine various convolutional filters with max-pooling layers. Because they have several interacting layers, it is in general is hard to interpret exactly what is it that they are learning. But most likely their success relies on the fact that their convolutional filters have the ability to capture arbitrary features of the input sequence. On the other hand, non-deterministic weighted automata (WFAs) are recurrent models that only use linear activation functions.Essentially, WFAs can be regarded as recurrent neural networks where the f"
2020.sustainlp-1.21,D14-1181,0,0.123189,"to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs. 1 Introduction In the latter years CNNs have been proposed as models for sequence classification and it has been shown that they can give competitive results, even when compared to more complex models (Kim, 2014; Zhang and Wallace, 2017; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Goldberg, 2016). They typically combine various convolutional filters with max-pooling layers. Because they have several interacting layers, it is in general is hard to interpret exactly what is it that they are learning. But most likely their success relies on the fact that their convolutional filters have the ability to capture arbitrary features of the input sequence. On the other hand, non-deterministic weighted automata (WFAs) are recurrent models that only use linear activation functions.Essentially, WFAs can"
2020.sustainlp-1.21,P04-1035,0,0.0145925,"05). • SST-2: This is a sentiment treebank, where the task is to predict a positive or negative sentiment label. There are two classes and the average sentence length is 19. The total number of samples is 9,613 and the vocabulary size 16,185 (Socher et al., 2013). Figure 1: Performance as a function of the number of states of the model for the SUBJ dataset. • Subj: This is a subjectivity data set were the task is to predict if a sentence is subjective or objective.There are two classes and the average sentence length is 23. The total number of samples is 10,000 and the vocabulary size 21,323 (Pang and Lee, 2004). • TREC: This is a question classification data set. The task is to classify a question into six question types (e.g. a question about a location, a person, etc.). There are six classes and the average sentence length is 10. The total number of samples is 5,952 and the vocabulary size 9,592 (Li and Roth, 2002). • CR: This data set contains reviews written by customers about various products. The task is to predict the review is positive or negative. There are two classes and the average sentence length is 19. The total number of samples is 3,775 and the vocabulary size 5,340 (Hu and Liu, 2004"
2020.sustainlp-1.21,P05-1015,0,0.365575,"hand we realized that the simple modification of using the generative models to make a discriminative prediction resulted in good performance. 3 Experiments CNN 76.1 82.7 89.6 91.2 79.8 83.9 WFA 77.3 81.6 91.9 90.1 79.5 84.1 Table 1: Results of the WFA classifier against a baseline CNN. We conducted experiments on five sequence classification data sets: • MR: This is a movie review data set where the task is to classify a sentence as positive or negative review. There are two classes and the average sentence length is 20. The total number of samples is 106,662 and the vocabulary size 18,765 (Pang and Lee, 2005). • SST-2: This is a sentiment treebank, where the task is to predict a positive or negative sentiment label. There are two classes and the average sentence length is 19. The total number of samples is 9,613 and the vocabulary size 16,185 (Socher et al., 2013). Figure 1: Performance as a function of the number of states of the model for the SUBJ dataset. • Subj: This is a subjectivity data set were the task is to predict if a sentence is subjective or objective.There are two classes and the average sentence length is 23. The total number of samples is 10,000 and the vocabulary size 21,323 (Pan"
2020.sustainlp-1.21,I17-1026,0,0.02657,"gate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs. 1 Introduction In the latter years CNNs have been proposed as models for sequence classification and it has been shown that they can give competitive results, even when compared to more complex models (Kim, 2014; Zhang and Wallace, 2017; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Goldberg, 2016). They typically combine various convolutional filters with max-pooling layers. Because they have several interacting layers, it is in general is hard to interpret exactly what is it that they are learning. But most likely their success relies on the fact that their convolutional filters have the ability to capture arbitrary features of the input sequence. On the other hand, non-deterministic weighted automata (WFAs) are recurrent models that only use linear activation functions.Essentially, WFAs can be regarded as recurrent"
2021.findings-emnlp.246,2020.sustainlp-1.21,1,0.731551,"s and suffixes of the language, such that Hf (p, s) = f (p · s). A central result establishes that for a WFA that computes function f , with d dimensions and k = 1, the rank of Hf is d. This is because WFAs and linear RNNs factor the computation of f as products of prefix and suffix vectors, which are of dimension d. The reverse also holds: if a Hankel matrix Hf has rank d, then there is a WFA with d states that computes the associated f function. Next we describe spectral learning, which uses this result. See (Rabusseau et al., 2019) for further connections between WFAs and linear RNNs. See (Quattoni and Carreras, 2020) for an application of WFAs to NLP sentence classification tasks. 2.2 The Spectral Method Spectral learning is based on learning a low-rank Hankel matrix of the target distribution. Here we provide a high level description of the method; for a complete derivation and the theory justifying the algorithm we refer the reader to the works by Hsu et al. (2009) and Balle et al. (2014). At training, we are given sequences T from the distribution and we want to estimate f . We denote as fT (x) the empirical subsequence expectation of x in T .3 Using fT , the spectral method estimates a WFA A with d st"
2021.findings-emnlp.246,D09-1009,0,0.139237,"Missing"
2021.findings-emnlp.246,P11-1015,0,0.0210807,"s p × s obtained by concatenating a prefix p ∈ P with a suffix s ∈ S, such that p × s is observed in U For every phrase q ∈ Q ask the annotator to provide feedback, in the form of an indicator vector z ∈ {0, 1}k , where zl denotes that q can be a phrase of sentences of class l ∈ L Result: A  set of labeled phrases k (q, z) |q ∈ Q, z ∈ {0, 1} pling strategy to recent active learning methods for fine-tuning BERT-based sentence classifiers (Yuan et al., 2020), that also seek diversity in the sample. Data. We use two common datasets for sentence classification: the IMDB dataset of movie reviews (Maas et al., 2011), where the goal is to predict if a movie review is positive or negative; and the AG News dataset Zhang et al. (2015) of news articles headlines classified into four classes. The IMDB dataset has 17, 500 training examples, 7, 500 validation examples, and 25, 000 test examples. The AG News dataset has 110, 000 training examples, 10, 000 validation examples and 7, 600 test examples. We use the union of the training and validation as the unlabeled pool of examples. Evaluation. We report performance on the test partition. As an evaluation metric we use the F1 average between precision and recall."
2021.findings-emnlp.246,D18-1318,0,0.0271312,"tion feedback that it exploits. For example, fast with minimal annotation effort. To address the annotated data bottleneck, re- in text classification the annotations can consist of labels for complete texts, phrases, sentences, searchers have proposed active learning approaches that develop sampling strategies designed to min- features, rules or labeling functions (McCallum and Nigam, 1999; Settles et al., 2008; Druck et al., imize the number of annotations required to train a model (Settles, 2009; Wang and Shang, 2014; 2009; Ratner et al., 2017; Safranchik et al., 2020). Zhang et al., 2016; Siddhant and Lipton, 2018). In this paper we focus on the problem of training Most active learning proposals are based on two sequence classification models under an annotation 2890 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2890–2899 November 7–11, 2021. ©2021 Association for Computational Linguistics budget constraint and with no prior trained model for the task. This is sometimes referred as the cold start problem. We consider a setting in which the annotation feedback is at the level of phrases. Our goal is to develop an efficient algorithm that can answer the question: if one has"
2021.findings-emnlp.246,2020.emnlp-main.637,0,0.372866,"rategy might not work very well during the In the later years the field of NLP has witnessed first iterations of active learning, when the predicgreat progress on supervised machine learning methods for sequence classification. However, tions of the model are unstable. Furthermore, the model uncertainty criteria cannot be applied in the most of these methods require large amounts of first iteration, when no model has been trained and annotated training data. Because of this, whenever one needs to resort to other cold start sampling a new NLP application needs to be developed, data strategies (Yuan et al., 2020). To overcome the annotation becomes the main bottleneck in terms limitations of the uncertainty approach other reof cost and time. For example, a defense research analyst might wish to quickly train a text classi- searchers have proposed sampling strategies that attempt to maximize the diversity of the selected fier to detect emergent socio-political events in a samples (Shao et al., 2019). given conflict area. Since there might be only a Besides the selection strategy, another dimenfew experts on the subject their time will be costly. sion of an active learning method is the type of Therefor"
C14-1017,J92-4003,0,0.129333,"ervised representation learning requires the presence of supervised training data with the potential advantage that it can adapt the representation to the task at hand. Unsupervised approaches to learning representations mainly involve representations that are learned not for a specific task, rather a variety of tasks. These representations rely more on the property of abstractness and generalization. Further, unsupervised approaches can be roughly categorized into (a) clustering-based approaches that make use of clusters induced using a notion of distributed similarity, such as the method by Brown et al. (1992); (b) neural-network-based representations that focus on learning multilayer neural network in a way to extract features from the data (Morin and Bengio, 2005; Mnih and Hinton, 2007; Bengio and S´en´ecal, 2008; Mnih and Hinton, 2009); (c) pure distributional approaches that principally follow the distributional assumption that the words which share a set of contexts are similar (Sahlgren, 2006; Turney and Pantel, 2010; Dumais et al., 1988; Landauer et al., 1998; Lund et al., 1995; V¨ayrynen et al., 2007). We also induce lexical embeddings, but in our case we employ supervision. That is, we fol"
C14-1017,J93-2004,0,0.0453108,"constraints. One related area where bilinear operators are used to induce embeddings is distance metric learning. Weinberger and Saul (2009) used large-margin nearest neighbor methods to learn a non-sparse embedding, but these are computationally intensive and might not be suitable for large-scale tasks in NLP. 5 Experiments on Syntactic Relations We conducted a set of experiments to test the ability of our algorithm to learn bilexical operators for several linguistic relations. As supervised training data we use the gold standard dependencies of the WSJ training section of the Penn Treebank (Marcus et al., 1993). We consider the following relations: 165 Nouns given Adjective 80 85 78 80 76 pairwise accuracy pairwise accuracy Adjectives given Noun 90 75 70 65 60 unsupervised NN L1 L2 55 50 1e3 1e4 1e5 1e6 1e7 number of operations 74 72 70 unsupervised NN L1 L2 68 66 1e8 1e3 1e4 Objects given Verb 1e5 1e6 1e7 number of operations 1e8 Verbs given Object 62 80 60 75 pairwise accuracy pairwise accuracy 58 56 54 52 50 46 1e3 65 unsupervised NN L1 L2 48 1e4 1e5 1e6 1e7 number of operations 70 1e8 60 1e3 unsupervised NN L1 L2 1e4 1e5 1e6 1e7 number of operations 1e8 Figure 1: Pairwise accuracy with respect t"
C14-1017,H94-1048,0,0.586466,"the size, the nuclear-norm model performs much better. Roughly, 20 hidden dimensions are enough to obtain the most accurate performances (which result in ∼ 140, 000 operations for initial representaions of 2, 000 dimensions and 5, 000 modifier candidates). As an example of the type of predictions, Table 1 shows the most likely adjectives for some test nouns. 6 Experiments on PP Attachment We now switch to a standard classification task, prepositional phrase attachment, that we frame as a bilexical prediction task. We start from the formulation of the task as a binary classification problem by Ratnaparkhi et al. (1994): given a tuple x = hv, o, p, ni consisting of a verb v, noun object o, preposition 1 To obtain curves for each model type with respect to a range of number of operations, we first obtained the best model on validation data and then forced it to have at most k non-zero features or rank k by projecting, for a range of k values. 168 80 attachment accuracy 75 bilinear L1 bilinear L2 bilinear NN linear interpolated L1 interpolated L2 interpolated NN 70 65 60 55 for from with Figure 3: Attachment accuracies of linear, bilinear and interpolated models for three prepositions. p and noun n, decide if"
C14-1017,D11-1014,0,0.0488453,"extracted from unlabeled data, while the supervised step compresses the representation to be low-dimensional in a way that favors the the task at hand. Collobert and Weston (2008) present a neural network language model, where given a sentence, it performs a set of language processing tasks (from part of speech tagging, chunking, extracting named entity, extracting semantic roles and decisions on the correctness of the sentence) by using the learned representations. The representation itself is extracted from unlabeled corpora, while all the other tasks are jointly trained on labeled corpus. Socher et al. (2011) present a model based on recursive neural networks that learns vector space representations for words, multi-word phrases and sentences. Given a sentence with its syntactic structure, their model assings vector representations to each of the lexical tokens of the sentence, and then traverses the syntactic tree bottom-up, such that at each node a vector representation of the corresponding phrase is obtained by composing the vectors associated with the children. Bai et al. (2010) use a technique similar to ours, using bilinear forms with low-rank constraints. In their case, they explicitly look"
C14-1017,W09-1119,0,\N,Missing
C14-1017,P08-1068,1,\N,Missing
C14-1017,W09-1208,0,\N,Missing
C14-1017,P09-1116,0,\N,Missing
C18-1115,P17-1183,0,0.113652,"models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either the decoding algorithm chooses to delete a specific symbol or not to. A similar observation was made by Schnober et al. (2016), R"
C18-1115,P05-1022,0,0.0769671,"n placeholders that are deleted during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and n"
C18-1115,P12-1024,1,0.823958,"ng and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algor"
C18-1115,N13-1015,1,0.800426,"he D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algorithms, and neural net"
C18-1115,P14-2102,0,0.0217992,"orks best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequence labeling algorithms have been the mainstay for an array of pr"
C18-1115,D08-1113,0,0.0414885,"roach can be used with any sequence labeling algorithm and it works best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequ"
C18-1115,P02-1001,0,0.112997,"e between the two, as string transduction can re-write a string into a completely different string, while sequence labeling has a stronger notion of locality. As such, sequence labeling is considered an easier problem than general string transduction. Yet, we show in this paper how to exploit sequence labeling algorithms, with their flexibility and efficiency, to do general string transduction. 1361 3 Transduction as Insertion and Labeling Most approaches to string transduction involve inducing an alignment between symbols in the input and output strings (Knight and Graehl, 1998; Clark, 2001; Eisner, 2002; Azawi et al., 2013; Bailly et al., 2013). In an alignment, unaligned input symbols are called deletions, while unaligned output symbols are called insertions. It is challenging to jointly induce alignments and learn a transduction model. A second challenge is that, at prediction time, it is difficult to predict the insertions, as there can be an arbitrary number of them between any two input symbols. The prediction problem would be much simpler if the insertion positions were in place, because the model would only need to decide which symbol goes in each position. Our approach is based on th"
C18-1115,P16-2090,0,0.0418981,"Missing"
C18-1115,D15-1214,1,0.833557,"during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and neural networks. We tested our"
C18-1115,N16-1076,0,0.0888006,"he sentence in the target language can be shorter, longer, and contain a significant amount of re-ordering. Indeed, re-ordering is a challenge with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertio"
C18-1115,C16-1160,0,0.34387,"with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either t"
C18-1115,W16-2406,0,0.274929,"Missing"
C18-1115,W13-3507,1,0.894402,"nd the transformation of the final y to i to accommodate the new suffix). For example, the past tense of may is also may, and in this case, if the string may was also in the training set, may should retain its form and be transduced to may. With the context function mentioned above we would have the pair mayε (input sequence), mayD (output sequence) to learn from. During decoding, we apply the σ function on the input string and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recogn"
carreras-etal-2004-freeling,A92-1018,0,\N,Missing
carreras-etal-2004-freeling,A00-1031,0,\N,Missing
carreras-etal-2004-freeling,carreras-padro-2002-flexible,1,\N,Missing
carreras-padro-2002-flexible,W01-1013,1,\N,Missing
carreras-padro-2002-flexible,A97-2017,0,\N,Missing
D07-1015,E06-1011,0,0.0733024,"timization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapting the inside-outside algorithm to these models, partition functions and marginals can be computed for second-order projective structures, allowing log-linear and max-margin training to be applied via the framework developed in this paper. For higher-order non-projective parsing, however, computational complexity"
D07-1015,W07-2216,0,0.703877,"to a large-scale problem. We again show improved performance over the perceptron. The goal of our experiments is to give a rigorous comparative study of the marginal-based training algorithms and a highly-competitive baseline, the averaged perceptron, using the same feature sets for all approaches. We stress, however, that the purpose of this work is not to give competitive performance on the CoNLL data sets; this would require further engineering of the approach. Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneously by Smith and Smith (2007) and McDonald and Satta (2007); see Section 5 for more discussion. 2 2.1 Background Discriminative Dependency Parsing Dependency parsing is the task of mapping a sentence x to a dependency structure y. Given a sentence x with n words, a dependency for that sentence is a tuple (h, m) where h ∈ [0 . . . n] is the index of the head word in the sentence, and m ∈ [1 . . . n] is the index of a modifier word. The value h = 0 is a special root-symbol that may only appear as the head of a dependency. We use D(x) to refer to all possible dependencies for a sentence x: D(x) = {(h, m) : h ∈ [0 . . . n], m ∈ [1 . . . n]}. A dependency"
D07-1015,P05-1012,0,0.106724,"earl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a wellknown result in graph theory: Kirchhoff’s MatrixTree Theorem (Tutte, 1984). A na¨ıve application of the theorem yields O(n4 ) and O(n6 ) algorithms for computation of the partition func"
D07-1015,H05-1066,0,0.175733,"earl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a wellknown result in graph theory: Kirchhoff’s MatrixTree Theorem (Tutte, 1984). A na¨ıve application of the theorem yields O(n4 ) and O(n6 ) algorithms for computation of the partition func"
D07-1015,W06-2920,0,0.341918,"s, Bartlett et al. (2004) have provided a simple training algorithm, based on exponentiated-gradient (EG) updates, that requires computation of marginals and can thus be implemented within our framework. Both of these methods explicitly minimize the loss incurred when parsing the entire training set. This contrasts with the online learning algorithms used in previous work with spanning-tree models (McDonald et al., 2005b). We applied the above two marginal-based training algorithms to six languages with varying degrees of non-projectivity, using datasets obtained from the CoNLL-X shared task (Buchholz and Marsi, 2006). Our experimental framework compared three training approaches: log-linear models, max-margin models, and the averaged perceptron. Each of these was applied to both projective and non-projective parsing. Our results demonstrate that marginal-based training yields models which out141 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 141–150, Prague, June 2007. 2007 Association for Computational Linguistics Projective perform those trained using the averaged perceptron. In summary, the contributions of"
D07-1015,W06-2932,0,0.0181463,"raining-setting combination. The 3 Our algorithms also support labeled parsing (see Section 3.4). Initial experiments with labeled models showed the same trend that we report here for unlabeled parsing, so for simplicity we conducted extensive experiments only for unlabeled parsing. 4 The transformations were performed by running the projective parser with score +1 on correct dependencies and -1 otherwise: the resulting trees are guaranteed to be projective and to have a minimum loss with respect to the correct tree. Note that only the training sets were transformed. 5 It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features. Ara Dut Jap Slo Spa Tur Perceptron p np 71.74 71.84 77.17 78.83 91.90 91.78 78.02 78.66 81.19 80.02 71.22 71.70 Max-Margin p np 71.74 72.99 76.53 79.69 92.10 92.18 79.78 80.10 81.71 81.93 72.83 72.02 Log-Linear p np 73.11 73.67 76.23 79.55 91.68 91.49 78.24 79.66 81.75 81.57 72.26 72.62 Table 2: Test data results. The p and np columns show results with projective and non-projective training respectively. P E L Ara 71.74 72.99 73.67 Dut 78.83 79.69 79.55 Jap 91.78 92.18 91.49 Slo 78.66 80.10 79.66 Spa 81.19 81.93 81.57 Tur 71.70 72"
D07-1015,D07-1101,1,0.770405,"s work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapting the inside-outside algorithm to these models, partition functions and marginals can be computed for second-order projective structures, allowing log-linear and max-margin training to be applied via the framework developed in this paper. For higher-order non-projective parsing, however, computational complexity results (McDonald and Pereira, 2006; McDonald and Satta, 2007) indicate that exact solutions to the three inference problems of"
D07-1015,W04-2407,0,0.0142665,"les correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapti"
D07-1015,W02-1001,1,0.191066,"s that we report are for unlabeled dependency parsing.3 The non-projective models were trained on the CoNLL-X data in its original form. Since the projective models assume that the dependencies in the data are non-crossing, we created a second training set for each language where non-projective dependency structures were automatically transformed into projective structures. All projective models were trained on these new training sets.4 Our feature space is based on that of McDonald et al. (2005a).5 6.2 Results We performed experiments using three training algorithms: the averaged perceptron (Collins, 2002), log-linear training (via conjugate gradient descent), and max-margin training (via the EG algorithm). Each of these algorithms was trained using projective and non-projective methods, yielding six training settings per language. The different training algorithms have various meta-parameters, which we optimized on the validation set for each language/training-setting combination. The 3 Our algorithms also support labeled parsing (see Section 3.4). Initial experiments with labeled models showed the same trend that we report here for unlabeled parsing, so for simplicity we conducted extensive e"
D07-1015,dzeroski-etal-2006-towards,0,0.0167912,"Missing"
D07-1015,C96-1058,0,0.181095,"cy structure y ∗ (x; w) (see Eq. 1) can be computed. In this paper the motivation for solving Problems 2 and 3 arises from training algorithms for discriminative models. As we will describe in Section 4, both log-linear and max-margin models can be trained via methods that make direct use of algorithms for Problems 2 and 3. In the case of projective dependency structures (i.e., T (x) defined as Tps (x) or Tpm (x)), there are well-known algorithms for all three inference problems. Decoding can be carried out using Viterbistyle dynamic-programming algorithms, for example the O(n3 ) algorithm of Eisner (1996). Computation of the marginals and partition function can also be achieved in O(n3 ) time, using a variant of the inside-outside algorithm (Baker, 1979) applied to the Eisner (1996) data structures (Paskin, 2001). In the non-projective case (i.e., T (x) defined as s (x) or T m (x)), McDonald et al. (2005b) deTnp np scribe how the CLE algorithm (Chu and Liu, 1965; Edmonds, 1967) can be used for decoding. However, it is not possible to compute the marginals and partition function using the inside-outside algorithm. We next describe a method for computing these quantities in O(n3 ) time using mat"
D07-1015,P99-1069,0,0.0534465,"s y ∈ Tnp compute the partition function directly: construct a Laplacian matrix L(θ) for G0 and compute the minor L(0,0) (θ). Since this minor is also a determinant, the marginals can be obtained analogously to the single-root case. More concretely, this technique ˆ corresponds to defining the matrix L(θ) as Multiple Roots In the case of multiple roots, we can still compute the partition function and marginals efficiently. In fact, the derivation of this case is simpler than for single-root structures. Create an extended graph G0 145 4.1 Log-Linear Estimation In conditional log-linear models (Johnson et al., 1999; Lafferty et al., 2001), a distribution over parse trees for a sentence x is defined as follows: exp P (y |x; w) = nP o (h,m)∈y w · f (x, h, m) Z(x; w) (7) where Z(x; w) is the partition function, a sum over s (x), T m (x) or T m (x). Tps (x), Tnp p np We train the model using the approach described by Sha and Pereira (2003). Assume that we have a training set {(xi , yi )}N i=1 . The optimal parameters are taken to be w∗ = argminw L(w) where L(w) = −C convex function L(w). Let the margin for parse tree y on the i’th training example be defined as N X 1 log P (yi |xi ; w) + ||w||2 2 i=1 mi,y ("
D07-1015,P01-1042,0,0.0304567,"| . If obj has decreased 2 compared to last iteration, set η = η2 . Output: Parameter values w. Figure 2: The EG Algorithm for Max-Margin Estimation. The learning rate η is halved each time the dual objective function (see (Bartlett et al., 2004)) fails to increase. In our experiments we chose β = 9, which was found to work well during development of the algorithm. achieved using the inside-outside algorithm for projective structures, and the algorithms described in Section 3 for non-projective structures. 5 Related Work Global log-linear training has been used in the context of PCFG parsing (Johnson, 2001). Riezler et al. (2004) explore a similar application of log-linear models to LFG parsing. Max-margin learning 147 has been applied to PCFG parsing by Taskar et al. (2004b). They show that this problem has a QP dual of polynomial size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsoch"
D07-1015,N04-1013,0,0.0144885,"ecreased 2 compared to last iteration, set η = η2 . Output: Parameter values w. Figure 2: The EG Algorithm for Max-Margin Estimation. The learning rate η is halved each time the dual objective function (see (Bartlett et al., 2004)) fails to increase. In our experiments we chose β = 9, which was found to work well during development of the algorithm. achieved using the inside-outside algorithm for projective structures, and the algorithms described in Section 3 for non-projective structures. 5 Related Work Global log-linear training has been used in the context of PCFG parsing (Johnson, 2001). Riezler et al. (2004) explore a similar application of log-linear models to LFG parsing. Max-margin learning 147 has been applied to PCFG parsing by Taskar et al. (2004b). They show that this problem has a QP dual of polynomial size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004)"
D07-1015,N03-1028,0,0.040522,"In the case of multiple roots, we can still compute the partition function and marginals efficiently. In fact, the derivation of this case is simpler than for single-root structures. Create an extended graph G0 145 4.1 Log-Linear Estimation In conditional log-linear models (Johnson et al., 1999; Lafferty et al., 2001), a distribution over parse trees for a sentence x is defined as follows: exp P (y |x; w) = nP o (h,m)∈y w · f (x, h, m) Z(x; w) (7) where Z(x; w) is the partition function, a sum over s (x), T m (x) or T m (x). Tps (x), Tnp p np We train the model using the approach described by Sha and Pereira (2003). Assume that we have a training set {(xi , yi )}N i=1 . The optimal parameters are taken to be w∗ = argminw L(w) where L(w) = −C convex function L(w). Let the margin for parse tree y on the i’th training example be defined as N X 1 log P (yi |xi ; w) + ||w||2 2 i=1 mi,y (w) = The parameter C &gt; 0 is a constant dictating the level of regularization in the model. Since L(w) is a convex function, gradient descent methods can be used to search for the global minimum. Such methods typically involve repeated computation of the loss L(w) and gradient ∂L(w) ∂w , requiring efficient implementations of"
D07-1015,P05-1044,0,0.0141546,"ents, for example the set of all parse trees for a given sentence. Methods for summing over such structures include the inside-outside algorithm for probabilistic contextfree grammars (Baker, 1979), the forward-backward algorithm for hidden Markov models (Baum et al., 1970), and the belief-propagation algorithm for graphical models (Pearl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed usi"
D07-1015,D07-1014,0,0.706894,"plication of this algorithm to a large-scale problem. We again show improved performance over the perceptron. The goal of our experiments is to give a rigorous comparative study of the marginal-based training algorithms and a highly-competitive baseline, the averaged perceptron, using the same feature sets for all approaches. We stress, however, that the purpose of this work is not to give competitive performance on the CoNLL data sets; this would require further engineering of the approach. Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneously by Smith and Smith (2007) and McDonald and Satta (2007); see Section 5 for more discussion. 2 2.1 Background Discriminative Dependency Parsing Dependency parsing is the task of mapping a sentence x to a dependency structure y. Given a sentence x with n words, a dependency for that sentence is a tuple (h, m) where h ∈ [0 . . . n] is the index of the head word in the sentence, and m ∈ [1 . . . n] is the index of a modifier word. The value h = 0 is a special root-symbol that may only appear as the head of a dependency. We use D(x) to refer to all possible dependencies for a sentence x: D(x) = {(h, m) : h ∈ [0 . . . n], m"
D07-1015,W04-3201,0,0.551986,"probabilistic contextfree grammars (Baker, 1979), the forward-backward algorithm for hidden Markov models (Baum et al., 1970), and the belief-propagation algorithm for graphical models (Pearl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a"
D07-1015,W03-3023,0,0.0251568,"size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projectiv"
D07-1101,W06-2925,1,0.881244,"Missing"
D07-1101,W02-1001,0,0.167917,"x, h, m, c) is: • • • • • • • • • 3 dir · cpos(xh ) · cpos(xm ) · cpos(xc ) dir · cpos(xh ) · cpos(xc ) dir · cpos(xm ) · cpos(xc ) dir · form(xh ) · form(xc ) dir · form(xm ) · form(xc ) dir · cpos(xh ) · form(xc ) dir · cpos(xm ) · form(xc ) dir · form(xh ) · cpos(xc ) dir · form(xm ) · cpos(xc ) Experiments and Results We report experiments with higher-order models for the ten languages in the multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007).1 In all experiments, we trained our models using the averaged perceptron (Freund and Schapire, 1999), following the extension of Collins (2002) for structured prediction problems. To train models, we used “projectivized” versions of the training dependency trees.2 1 We are grateful to the providers of the treebanks that constituted the data for the shared task (Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; Bo¨ hmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003). 2 We obtained projective trees for training sentences by running the projective parser with an oracle model (that assigns a scor"
D07-1101,C96-1058,0,0.918284,"sentations of structures. The information included in the factors determines the type of features that the model can exploit. However, richer representations translate into higher complexity of the inference algorithms associated with the model. In dependency parsing, the basic first-order model is defined by a decomposition of a tree into headmodifier dependencies. Previous work extended this basic model to include second-order relations—i.e. dependencies that are adjacent to the main dependency of the factor. Specifically, these approaches considered sibling relations of the modifier token (Eisner, 1996; McDonald and Pereira, 2006). In this paper we extend the parsing model with other types of second-order relations. In particular, we incorporate relations between the head and modifier tokens and the children of the modifier. One paradigmatic case where the relations we consider are relevant is PP-attachment. For example, in “They sold 1,210 cars in the U.S.”, the ambiguity problem is to determine whether the preposition “in” (which governs “the U.S.”) is modifying “sold” or “cars”, the former being correct in this case. It is generally accepted that to solve the attachment decision it is ne"
D07-1101,W07-2416,0,0.0193143,"models for the ten languages in the multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007).1 In all experiments, we trained our models using the averaged perceptron (Freund and Schapire, 1999), following the extension of Collins (2002) for structured prediction problems. To train models, we used “projectivized” versions of the training dependency trees.2 1 We are grateful to the providers of the treebanks that constituted the data for the shared task (Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; Bo¨ hmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003). 2 We obtained projective trees for training sentences by running the projective parser with an oracle model (that assigns a score of +1 to correct dependencies and -1 otherwise). First-Order, no averaging First-Order Higher-Order, ch Higher-Order, ch cmo Higher-Order, ch cmi cmo Catalan 82.07 86.15 87.50 87.68 88.04 Czech 68.98 75.96 77.15 77.62 78.09 English 83.75 87.54 88.70 89.28 89.59 Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Average Table 2: Labeled attachment"
D07-1101,J93-2004,0,0.0368896,"nts with higher-order models for the ten languages in the multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007).1 In all experiments, we trained our models using the averaged perceptron (Freund and Schapire, 1999), following the extension of Collins (2002) for structured prediction problems. To train models, we used “projectivized” versions of the training dependency trees.2 1 We are grateful to the providers of the treebanks that constituted the data for the shared task (Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; Bo¨ hmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003). 2 We obtained projective trees for training sentences by running the projective parser with an oracle model (that assigns a score of +1 to correct dependencies and -1 otherwise). First-Order, no averaging First-Order Higher-Order, ch Higher-Order, ch cmo Higher-Order, ch cmi cmo Catalan 82.07 86.15 87.50 87.68 88.04 Czech 68.98 75.96 77.15 77.62 78.09 English 83.75 87.54 88.70 89.28 89.59 Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Average"
D07-1101,E06-1011,0,0.856382,"structures. The information included in the factors determines the type of features that the model can exploit. However, richer representations translate into higher complexity of the inference algorithms associated with the model. In dependency parsing, the basic first-order model is defined by a decomposition of a tree into headmodifier dependencies. Previous work extended this basic model to include second-order relations—i.e. dependencies that are adjacent to the main dependency of the factor. Specifically, these approaches considered sibling relations of the modifier token (Eisner, 1996; McDonald and Pereira, 2006). In this paper we extend the parsing model with other types of second-order relations. In particular, we incorporate relations between the head and modifier tokens and the children of the modifier. One paradigmatic case where the relations we consider are relevant is PP-attachment. For example, in “They sold 1,210 cars in the U.S.”, the ambiguity problem is to determine whether the preposition “in” (which governs “the U.S.”) is modifying “sold” or “cars”, the former being correct in this case. It is generally accepted that to solve the attachment decision it is necessary to look at the head n"
D07-1101,P05-1012,0,0.680905,"t token to participate in exactly one dependency. The second allows many dependencies involving the root token. For the singleroot case, it is necessary to treat the root token differently than other tokens. In the experiments, we used the single-root variant if sentences in the training set satisfy this property. Otherwise we used the multi-root variant. 2.2 Features The first-order features φ1 (x, h, m) are the exact same implementation as in previous CoNLL system (Carreras et al., 2006). In turn, those features 959 were inspired by successful previous work in firstorder dependency parsing (McDonald et al., 2005). The most basic feature patterns consider the surface form, part-of-speech, lemma and other morphosyntactic attributes of the head or the modifier of a dependency. The representation also considers complex features that exploit a variety of conjunctions of the forms and part-of-speech tags of the following items: the head and modifier; the head, modifier, and any token in between them; the head, modifier, and the two tokens following or preceding them. As for the second-order features, we again base our features with those of McDonald and Pereira (2006), who reported successful experiments wi"
D07-1101,H94-1048,0,\N,Missing
D07-1101,D07-1096,0,\N,Missing
D09-1021,W06-1606,0,0.29494,"ouple of respects: first, these criticisms is initially seen to the left of take, but after the adjunction this order is reversed; second, and more unusually, the treelet for seriously has been skipped over, with the result that the German words translated at this point (diese, kritik, and nehmen) form a non-contiguous sequence. More generally, we will allow any two 1 Introduction Syntax-based models for statistical machine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information i"
D09-1021,P96-1023,0,0.0611987,"here wm and sm are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules"
D09-1021,P05-1012,0,0.0634496,"the administration must be able to respond more quickly in future try hes gibt ⇒ there isi a correct alignment would be h(1, 1), (2, 2)i, specifying that there is aligned to es, and is is aligned to gibt (note that in many, but not all, cases ai = bi , i.e., a target language word is aligned to a single source language word). The alignment information in s-phrases will be useful in tying syntactic dependencies created in the target language to positions in the source language string. In particular, we will consider discriminative models (analogous to models for dependency parsing, e.g., see (McDonald et al., 2005)) that estimate the probability of targetlanguage dependencies conditioned on properties of the source-language string. Alignments may be derived in a number of ways; in our method we directly use phrase entries proposed by a phrasebased system. Specifically, for each target word ei in a phrase entry hf1 . . . fn , e1 . . . em i for a training example, we find the smallest5 phrase entry in the same training example that includes ei on the target side, and is a subset of f1 . . . fn on the source side; the word ei is then aligned to the subset of source language words in this “minimal” phrase."
D09-1021,W08-2102,1,0.612555,"s; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information in the target language. As one example, when translating the sentence wir m¨ussen auch diese kritik ernst nehmen from German into English, the following sequence of syntactic phrasal entries might be used (we show each English syntactic fragment above its associated German sub-string): 1 Note that in the above example each English phrase consists of a completely connected syntactic structure; this is not, however, a required constraint, see section 3.2 for discussion. 200 Proceeding"
D09-1021,2003.mtsummit-papers.6,0,0.408892,"Missing"
D09-1021,P01-1017,0,0.357788,"cisms seriously VP must ADVP take ADVP also wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these criticisms and take to be combined to form a new structure with the sub-string take these criticisms. This step in the derivation is neces"
D09-1021,P04-1083,0,0.0266659,"s of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation define"
D09-1021,P08-1023,0,0.0611245,"ous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in the Penn treebank. 201 S es gibt"
D09-1021,2006.amta-papers.15,0,0.018273,"tate (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in th"
D09-1021,P08-1009,0,0.0088223,"t tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in the Penn treeban"
D09-1021,P03-1021,0,0.102403,"ctively. In addition we can define (am , bm ) to be the start and end indices of the words in the foreign string to which the word wm is aligned; this information can be recovered because the s-phrase qm contains alignment information for all target words in the phrase, including wm . Similarly, we can define (ah , bh ) to be alignment information for the head word wh . Finally, we can define ρ to be a binary flag specifying whether or not the adjunction operation involves reordering (in the take criticism example, this flag is set to true, because the order in En7 In practice, MERT training (Och, 2003) will be used to train relative weights for the different model components. 204 i = 1 . . . N from our training data as follows: for each pair of target-language words (wm , wh ) seen in the training data, we can extract associated spines (sm , sh ) from the relevant parse tree, and also extract a label y indicating whether or not a head-modifier dependency is seen between the two words in the parse tree. Given an s-phrase in the training example that includes wm , we can extract alignment information (am , bm ) from the sphrase; we can extract similar information (ah , bh ) for wh . The end r"
D09-1021,P05-1033,0,0.542334,"rect English word order, and is novel in a couple of respects: first, these criticisms is initially seen to the left of take, but after the adjunction this order is reversed; second, and more unusually, the treelet for seriously has been skipped over, with the result that the German words translated at this point (diese, kritik, and nehmen) form a non-contiguous sequence. More generally, we will allow any two 1 Introduction Syntax-based models for statistical machine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented"
D09-1021,P02-1040,0,0.106951,"ce of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM"
D09-1021,P05-1066,1,0.383303,"the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM) Given Qi , define Qi,j for j = 1 . . . n to be the subset of items in Qi which have their j’th bit equal to one (i.e., have the j’th source language word translated). Define Q′i,j to be the N highest scoring elements in Qi,j . Then BEAM(Qi ) = ∪nj=1"
D09-1021,P97-1003,1,0.609084,"rce language) sentence. ADJP DT man sub-strings above their associated sequence of treelets.4 Figure 1: A training example consisting of an English (tarbe VP Figure 2: Example syntactic phrase entries. We show GerNP es gibt keine hierarchie der diskriminierung SG NP PP discrimination VP NP S VP NP hierarchie der ADJP able SG to S NP VP there is To give a more formal description of how syntactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al., 2008). This procedure uses the head finding rules of (Collins, 1997). The resulting derivation consists of a TAG spine for each word seen in the sentence, together with a set of adjunction operations which each involve a modifier spine and a head spine. Given an English string e = e1 . . . en , with an associated parse tree t, the syntactic structure associated with a substring ek . . . el (e.g., there is) is then defined as follows: VP be ⇒ VP respond In this case the treelet for to respond sister-adjoins into the treelet for be able. This operation introduces a bi-lexical dependency between the modifier word to and the head word able. • For each word in the"
D09-1021,P05-1034,0,0.0975814,"the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-"
D09-1021,P95-1021,0,0.361151,"Missing"
D09-1021,P08-1066,0,0.195012,"d AFNLP pora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than stating reordering rules at the level of source or target language parse trees, we capture reordering phenomena using a discriminative dependency model. Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information (although see (Charniak et al., 2003; Shen et al., 2008) for related approaches). tree fragments to be combined during the translation process, irrespective of the reorderings which are introduced, or the non-projectivity of the parsing operations that are required. The use of flexible parsing operations raises two challenges that will be a major focus of this paper. First, these operations will allow the model to capture complex reordering phenomena, but will in addition introduce many spurious possibilities. Inspired by work in discriminative dependency parsing (e.g., (McDonald et al., 2005)), we add probabilistic constraints to the model through"
D09-1021,P03-2041,0,0.186779,"the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction o"
D09-1021,J97-3002,0,0.123383,"are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be lea"
D09-1021,P01-1067,0,0.215065,"and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the"
D09-1021,N03-1017,0,0.541898,"m German to English show improvements over phrase-based systems, both in terms of BLEU scores and in human evaluations. we NP VP these criticisms seriously VP must ADVP take ADVP also wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these cr"
D09-1021,P05-1059,0,0.0157149,"ord and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2"
D09-1021,W04-3250,0,0.0866349,"05). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM) Given Qi , define Qi,j for j = 1 . . . n to be the subset of items in Qi which have their j’th bit equal to one (i.e., have the j’th source language word translated"
D09-1021,W06-3119,0,0.0519657,"to, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of s"
D09-1021,2005.mtsummit-papers.11,0,0.00537231,"ng representations with a record of “pending” treelets which have not yet been included in a derivation. It is also possible to enforce the π-constituent constraint during decoding, as well as a constraint that ensures that reordering operations do not “break apart” English sub-strings within s-phrases that have multiple treelets (for example, for the s-phrase in figure 2, we ensure that there is no remains as a contiguous sequence of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koe"
D09-1021,D08-1076,0,\N,Missing
D09-1058,D07-1101,1,0.74923,"dency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that inThis paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning app"
D09-1058,P99-1065,1,0.699382,"at the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the genera"
D09-1058,C96-1058,0,0.336642,"describes how the parameters θj,a are trained on unlabeled data. Given parameters θj,a , we can simply define the functions q1 . . . qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent feat"
D09-1058,D07-1015,1,0.813809,". qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η −"
D09-1058,P08-1068,1,0.098982,"for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approac"
D09-1058,E06-1011,0,0.589255,".mit.edu Abstract supervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension"
D09-1058,W07-2216,0,0.0658637,"enerative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . (6) This corresponds to a MA"
D09-1058,P05-1012,0,0.451267,"d l is the label of the dependency. We use h = 0 for the root of the sentence. We assume access to a set of labeled training examples, {xi , yi }N i=1 , and in addition a set of unlabeled examples, {x0i }M i=1 . In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: 2.2 X (1) w · f (x, h, m, l) (h,m,l)∈y Here f (x, h, m, l) is a feature vector representing the dependency (h, m, l) in the context of the sentence x (see for example (McDonald et al., 2005a)). In this paper we extend the definition of g(x, y) to include features that are induced from unlabeled data. Specifically, we define g(x, y) = X The Generative Models We now describe how the generative models q1 . . . qk are defined, and how they are induced from unlabeled data. These models make direct use of the feature-vector definition f (x, y) used in the original, fully supervised, dependency parser. The first step is to partition the d features in f (x, y) into k separate feature vectors, r1 (x, y) . . . rk (x, y) (with the result that f is the concatenation of the k feature vectors"
D09-1058,H05-1066,0,0.116818,"Missing"
D09-1058,W06-1615,0,0.174516,"Missing"
D09-1058,J92-4003,0,0.385752,"parameters (w1 , v1 , q1 ). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Second-order Parsing Models Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech 1 We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi , y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 Corp"
D09-1058,W06-2920,0,0.027869,"Missing"
D09-1058,W96-0213,0,0.382393,"ther created through random sampling or by using a predefined subset of document IDs from the labeled training data. mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown"
D09-1058,D07-1070,0,0.0431948,"Missing"
D09-1058,D07-1014,0,0.0889514,"obabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . ("
D09-1058,P08-1076,1,0.72351,"upervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more spe"
D09-1058,P08-1061,0,0.0411284,"Missing"
D09-1058,W03-3023,0,0.20564,"ent, test data (labeled data sets) and unlabeled data used in our experiments parameter-estimation method for the second-order parsing model. In particular, we perform the following optimizations on each update t = 1, ..., T for re-estimating w and v: min ||w(t+1) − w(t) ||+ ||v(t+1) − v(t) || ˆ ) ≥ L(yi , y ˆ) s.t. S(xi , yi ) − S(xi , y ˆ = arg maxy S(xi , y) + L(yi , y), y as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2 , giving a total of 1,796,379 sentences and 43,380,315 tokens. The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,"
D09-1058,J93-2004,0,\N,Missing
D09-1058,D07-1096,0,\N,Missing
D13-1059,P95-1031,0,0.112385,"it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the"
D13-1059,W01-0713,0,0.0496456,"ave also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking to"
D13-1059,N10-1081,0,0.0242134,"ent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algeb"
D13-1059,P12-1024,0,0.038214,"ming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over Σ∗ , the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution over PCFG derivations"
D13-1059,N13-1015,0,0.0635712,"g ∗ , then the algorithm will compute a G that exactly computes g ∗ . In practice, we only have access to empirical estimates of the Hankel matrices. In this case, there exist PAC-style sample complexity bounds that state that gG will be a close approximation to g ∗ (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010). The parameters of the algorithm are the basis and the dimension of the grammar n. One typically employs some validation strategy using held-out data. Empirically, the performance of these methods has been shown to be good, and similar to that of EM (Luque et al., 2012; Cohen et al., 2013). It is also important to mention that in the case that the target g ∗ is a probability distribution, the function gG will be close to g ∗ , but it will only define a distribution in the limit: in practice it will not sum to one, and for some inputs it might return negative values. This is a practical difficulty of spectral methods, for example to apply evaluation metrics like perplexity which are only defined for distributions. 4 Unsupervised Learning of WCFG • • • • • • Characterization of a WCFG Hankel (x) ∈ I ⇒ (x1 , x2 ) ∈ I for x = x1 x2 (x1 , x2 ) ∈ I ⇒ x1 ∈ I, x2 ∈ I hx; zi ∈ O ⇒ hx1 ,"
D13-1059,P13-1044,0,0.113069,"00; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its sy"
D13-1059,P02-1017,0,0.586937,"at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the result"
D13-1059,D07-1072,0,0.0317251,"still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both appr"
D13-1059,E12-1042,1,0.922954,"mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over Σ∗ , the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution"
D13-1059,P92-1017,0,0.39457,"na. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform pafrancolq@famaf.unc.edu.ar rameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical progr"
D13-1059,petrov-etal-2012-universal,0,0.0827828,"ervised one, outperform the WFA in reproducing the target distribution. 5.3 Natural Language Experiments Now we present some preliminar tests using natural language data. For these tests, we used the WSJ10 subset of the Penn Treebank, as Klein and Manning (2002). This dataset consists of the sentences of length ≤ 10 after filtering punctuation and currency. We removed lexical items and mapped the POS tags 1 Given two functions f1 and f2 over strings, the L1 distance is the sum of the absolute difference over all strings in a set: P |f (x) − f2 (x)|. 1 x to the Universal Part-of-Speech Tagset (Petrov et al., 2012), reducing the alphabet to a set of 11 symbols. Table 1 shows the size of the problem for different basis sizes. As described in the previous subsection for the unsupervised case, we obtain the basis by taking the most frequent observed substrings and contexts. We then compute all yields that can be generated with this basis, and close the basis to include all possible insides and outsides with operations completions, such that we create a Hankel as described in Section 4.1. Table 1 shows, for each base, the size of H we induce, the number of observable constraints (i.e. sentences we train fro"
D13-1059,C00-2139,0,0.107305,"Missing"
D13-1059,H92-1024,0,\N,Missing
D14-1049,W05-0620,1,0.767449,"Missing"
D14-1049,J08-2001,1,0.858282,"Missing"
D14-1049,P04-1043,0,0.0488302,"ling Xavier Llu´ıs TALP Research Center Universitat Polit`ecnica de Catalunya Xavier Carreras Xerox Research Centre Europe xavier.carreras@xrce.xerox.com lmarquez@qf.org.qa xlluis@cs.upc.edu Abstract In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role. The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate. We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments. Our ma"
D14-1049,J02-3001,0,0.348468,"Missing"
D14-1049,P02-1031,0,0.0870273,"Missing"
D14-1049,W04-3212,0,0.122752,"Missing"
D14-1049,W09-1209,0,0.0260059,"Missing"
D14-1049,W08-2122,0,0.0707503,"Missing"
D14-1049,W08-2123,0,0.090991,"Missing"
D14-1049,D09-1059,0,0.0880912,"s TALP Research Center Universitat Polit`ecnica de Catalunya Xavier Carreras Xerox Research Centre Europe xavier.carreras@xrce.xerox.com lmarquez@qf.org.qa xlluis@cs.upc.edu Abstract In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role. The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate. We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments. Our main contribution is"
D14-1049,D07-1015,1,0.87509,"Missing"
D14-1049,Q13-1018,1,0.806529,"Missing"
D14-1049,W02-1001,0,\N,Missing
D14-1049,W09-1201,1,\N,Missing
D15-1058,D07-1074,0,0.0761099,"t of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Document-level KB tags We incorporate information from KB tags by building document-sp"
D15-1058,D11-1072,0,0.184778,"Missing"
D15-1058,D07-1073,0,0.662482,"ow how KB tags can be exploited as a useful complement to traditional NER supervision. 2 Background Gazetteers have long been used to augment statistical NER models, adding general evidence of tokens used in names (Nadeau and Sekine, 2007). These are usually drawn from wide-coverage sources like Wikipedia and census lists (Ratinov and Roth, 2009) and can be incorporated into sequence models by designing binary features that indicate whether a token appears in a gazetteer entry. Features can be refined by specifying which part of an entry a token matches using tag encoding schemes such as IOB (Kazama and Torisawa, 2007). Using multiple gazetteers allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has"
D15-1058,W03-1301,0,0.0545499,"allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case o"
D15-1058,P11-1082,0,0.0209573,"technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Docum"
D15-1058,W09-1119,0,0.132725,"tional Linguistics. degrades as we use fewer KB tags, simulating the use-case where a busy knowledge worker spends less time annotating. We find that KB augmentation means we require fewer tags to reach the same performance, which reduces the cost of obtaining KB tags. We show how KB tags can be exploited as a useful complement to traditional NER supervision. 2 Background Gazetteers have long been used to augment statistical NER models, adding general evidence of tokens used in names (Nadeau and Sekine, 2007). These are usually drawn from wide-coverage sources like Wikipedia and census lists (Ratinov and Roth, 2009) and can be incorporated into sequence models by designing binary features that indicate whether a token appears in a gazetteer entry. Features can be refined by specifying which part of an entry a token matches using tag encoding schemes such as IOB (Kazama and Torisawa, 2007). Using multiple gazetteers allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazette"
D15-1058,P11-1138,0,0.0205177,", heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Document-level KB tags We incorporate information from KB tags by building document-specific gazetteers. Figu"
D15-1058,P08-1001,0,0.0644965,"ncrease coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create"
D15-1058,W06-3328,0,0.0314222,"s to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. J"
D15-1058,W03-0419,0,\N,Missing
E03-1038,M95-1012,0,0.010507,"consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikhe"
E03-1038,A97-1029,0,0.13099,"slated into Catalan, including several entities. There is a wide consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–wr"
E03-1038,W02-2002,0,0.0199984,"L&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advant"
E03-1038,M98-1014,0,0.0543784,"performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El president"
E03-1038,W02-2004,1,0.764051,"ng data was left as unlabelled data. As evaluation method we use the common measures for recognition tasks: precision, recall and F1 . Precision is the percentage of NEs predicted by a system which are correct. Recall is the percentage of NEs in the data that a system correctly recognizes. Finally, the F1 measure computes the harmonic mean of precision (p) and recall (r) as 2 p • Op + r). 3 The Spanish NER System The Spanish NER system is based on the best system at CoNLL&apos;02, which makes use of a set of AdaBoost–based binary classifiers for recognizing the Named Entities in running text. See (Carreras et al., 2002) for details. The NE recognition task is performed as a sequence tagging problem through the well–known BIO labelling scheme. Here, the input sentence is treated as a word sequence and the output tagging codifies the NEs in the sentence. In particular, each word is tagged as either the beginning of a NE (B tag), a word inside a NE (I tag), or a word outside a NE (0 tag). In our case, a NER model is composed by: (a) a representation function, which maps a word and its context into a set of features, and (b) three binary classifiers (one corresponding to each tag) which, operating on the feature"
E03-1038,W99-0613,0,0.379481,"Missing"
E03-1038,M98-1015,0,0.0203914,"ssifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El pr"
E03-1038,W02-2019,0,0.0233796,"ly, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advantage of the facts that Spanish and Catalan are two Romance languages with similar syntact"
E03-1038,W02-2020,0,0.0246992,"s between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our"
E03-1038,M98-1021,0,0.0255916,"1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER="
E03-1038,W02-2024,0,0.0701502,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2025,0,0.0420667,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2031,0,0.0471146,"Missing"
E03-1038,M95-1006,0,0.149935,"etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]"
E03-1038,M98-1016,0,0.0312668,"on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justic"
E12-1042,D07-1101,1,0.293774,"cing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what"
E12-1042,P04-1014,0,0.0119406,"his problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. 413 Let (si , sj ) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si , sj ) given a sentence s0:N is defined as X P(y) . µi,j = P((si , sj ) |s0:N ) = y∈Y(s0:N ) : (si ,sj )∈y To compute marginals, the sum over derivations can be decomposed into a product of inside and outsi"
E12-1042,P99-1059,0,0.847316,"hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable model is both expensive and prone to local minima issues. In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999). In this for409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et"
E12-1042,P96-1024,0,0.523695,"(s0:N ) P(y). This problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. 413 Let (si , sj ) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si , sj ) given a sentence s0:N is defined as X P(y) . µi,j = P((si , sj ) |s0:N ) = y∈Y(s0:N ) : (si ,sj )∈y To compute marginals, the sum over derivations can be decomposed into a pro"
E12-1042,P04-1058,0,0.0631239,"Missing"
E12-1042,P10-1001,0,0.00543786,"ns computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be"
E12-1042,P09-1039,0,0.00945581,"cture from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-"
E12-1042,P05-1010,0,0.541052,"algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such i"
E12-1042,E06-1011,0,0.283657,"ated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tel"
E12-1042,H05-1066,0,0.0344058,"r any y ∈ Y. In this paper we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other: Y P(y) = P(x1:T |h, d) . (1) hh,d,x1:T i∈y In the literature, standard arc-factored models further assume that P(x1:T |h, d) = TY +1 P(xt |h, d, σt ) , (2) t=1 where xT +1 is always a special STOP word, and σt is the state of a deterministic automaton generating x1:T +1 . For example, setting σ1 = FIRST and σt>1 = REST corresponds to first-order models, while setting σ1 = NULL and σt>1 = xt−1 corresponds to sibling models (Eisner, 2000; McDonald et al., 2005; McDonald and Pereira, 2006). 1 Throughout the paper we assume we can distinguish the words in a derivation, irrespective of whether two words at different positions correspond to the same symbol. 410 2.2 Operator Models An operator model A with n states is a tuple > , {A } n×n is an ophα1 , α∞ a a∈X i, where Aa ∈ R erator matrix and α1 , α∞ ∈ Rn are vectors. A computes a function f : X ∗ → R as follows: > f (x1:T ) = α∞ A xT · · · A x1 α 1 . (3) One intuitive way of understanding operator models is to consider the case where f computes a probability distribution over strings. Such a distribu"
E12-1042,P08-2054,0,0.362148,"cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such information into a state of bounded size will ty"
E12-1042,P06-1055,0,0.284055,"ng model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such information into a sta"
E12-1042,W06-1666,0,0.0194655,"P inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. 413 Let (si , sj ) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si , sj ) given a sentence s0:N is defined as X P(y) . µi,j = P((si , sj ) |s0:N ) = y∈Y(s0:N ) : (si ,sj )∈y To compute marginals, the sum over derivations can be decomposed into a product of inside and outside quantities (Baker, 1979)"
E12-1042,W07-2218,0,0.0174563,"earn from an annotated corpus an optimal way to compress derivations into hidden states. For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars. A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable model is both expensive and prone to local minima issues. In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999). In this for409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, c Avignon, France, April 23 - 27 2012. 2012 Association for"
E12-1042,N07-1051,0,\N,Missing
E12-1042,J93-2004,0,\N,Missing
E12-1042,J03-4003,0,\N,Missing
E14-2003,W07-1702,1,0.860388,"Missing"
E14-2003,P05-1045,0,0.00899297,"ambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE recognition), and hun"
E14-2003,Q13-1018,1,0.896749,"Missing"
E14-2003,W14-0150,0,0.021713,"ervices following a lightweigth SOA architecture approach, and they are publically accessible and shared through META-SHARE.1 1 2 Linguistic Analyzers Apart from basic state-of-the-art tokenizers, lemmatizers, PoS/MSD taggers, and NE recognizers, each pipeline requires deeper processors able to build the target language-independent semantic representantion. For that, we rely on three steps: dependency parsing, semantic role labeling and word sense disambiguation. These three processes, combined with multilingual ontological resouces such as different WordNets and PredicateMatrix (L´opez de la Calle et al., 2014), a lexical semantics resource combining WordNet, FrameNet, and VerbNet, are the key to the construction of our semantic representation. Introduction Project XLike2 goal is to develop technology able to gather documents in a variety of languages and genres (news, blogs, tweets, etc.) and to extract language-independent knowledge from them, in order to provide new and better services to publishers, media monitoring, and business intelligence. Thus, project use cases are provided by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners"
E14-2003,P05-1012,0,0.0466071,"d by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners in the project are Joˇzef Stefan Institute (JSI), Karlsruhe Institute of Technology (KIT), Universitat Polit`ecnica de Catalunya (UPC), University of Zagreb (UZG), and Tsinghua University (THU). The Spanish company iSOCO is in charge of integration of all components developed in the project. This paper deals with the language technology developed within the project XLike to convert in2.1 Dependency Parsing We use graph-based methods for dependency parsing, namely, MSTParser3 (McDonald et al., 2005) is used for Chinese and Croatian, and Treeler4 is used for the other languages. Treeler is a library developed by the UPC team that implements several statistical methods for tagging and parsing. We use these tools in order to train dependency parsers for all XLike languages using standard available treebanks. 1 accessible and shared here means that the services are publicly callable, not that the code is open-source. 3 http://www.meta-share.eu 2 http://www.xlike.org 4 http://sourceforge.net/projects/mstparser http://treeler.lsi.upc.edu 9 Proceedings of the Demonstrations at the 14th Conferen"
E14-2003,N03-1033,0,0.0120884,"c, 2012). Word sense disambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE"
E14-2003,C12-2001,0,0.0402396,"Missing"
E14-2003,E09-1005,0,0.0627699,"s performed for all languages with a publicly available WordNet. This includes all languages in the project except Chinese. The goal of WSD is to map specific languages to a common semantic space, in this case, WN synsets. Thanks to existing connections between WN and other resources, SUMO and OpenCYC sense codes are also output when available. Thanks to PredicateMatrix, the obtained concepts can be projected to FrameNet, achieving a normalization of the semantic roles produced by the SRL (which are treebank-dependent, and thus, not the same for all languages). The used WSD engine is the UKB (Agirre and Soroa, 2009) implementation provided by FreeLing (Padr´o and Stanilovsky, 2012). 2.4 3 Cross-lingual Semantic Annotation This step adds further semantic annotations on top of the results obtained by linguistic processing. All XLike languages are covered. The goal is to map word phrases in different languages into the same semantic interlingua, which consists of resources specified in knowledge bases such as Wikipedia and Linked Open Data (LOD) sources. Cross-lingual semantic annotation is performed in two stages: (1) first, candidate concepts in the knowledge base are linked to the linguistic resources ba"
E14-2003,padro-stanilovsky-2012-freeling,1,\N,Missing
J08-2001,W04-2412,1,0.733851,"Missing"
J08-2001,W05-0620,1,0.919702,"Missing"
J08-2001,W04-2415,1,0.828554,"Missing"
J08-2001,W05-0622,0,0.0206004,"Missing"
J08-2001,copestake-flickinger-2000-open,0,0.00738653,"]Pred to [the boy beside her]Recipient Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities participating in an event, and Temporal and Manner for the characterization of other aspects of the event or participant relations. This type of role labeling thus yields a ﬁrstlevel semantic representation of the text that indicates the basic event properties and relations among relevant entities that are expressed in the sentence. Research has proceeded for decades on manually created lexicons, grammars, and other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000) in support of deep semantic analysis of language input, but such approaches have been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe an"
J08-2001,P07-1028,0,0.0144889,"Missing"
J08-2001,S07-1048,0,0.143442,"Missing"
J08-2001,W05-0625,0,0.0189955,"Missing"
J08-2001,W04-0803,0,0.00817493,"Missing"
J08-2001,S07-1005,1,0.808742,"Missing"
J08-2001,W05-0628,1,0.603848,"Missing"
J08-2001,S07-1008,1,0.800462,"Missing"
J08-2001,J01-3003,1,0.757374,"been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 an"
J08-2001,W04-2705,0,0.0796831,"ather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and"
J08-2001,W04-2609,0,0.0198714,"evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among the different components of a text. This special issue of Computational Linguistics presents several articles representing the state-of-the-art in SRL, and this overview is intended to provide a broader c"
J08-2001,N06-2026,0,0.0821305,"Missing"
J08-2001,C04-1100,0,0.0222676,"s et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among th"
J08-2001,W03-0411,0,0.0160444,"Missing"
J08-2001,J05-1004,0,0.471683,"Missing"
J08-2001,W05-0634,0,0.0213716,"Missing"
J08-2001,J06-2001,0,0.0246464,"Missing"
J08-2001,P03-1002,0,0.288914,"ea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for a"
J08-2001,H05-1111,1,0.811574,"Missing"
J08-2001,P05-1073,0,0.0404859,"Missing"
J08-2001,W04-3212,0,0.06057,"Missing"
J08-2001,S07-1077,1,0.814405,"Missing"
J08-2001,N07-1069,0,\N,Missing
J08-2001,W04-3213,1,\N,Missing
J08-2001,boas-2002-bilingual,0,\N,Missing
J08-2001,S07-1018,0,\N,Missing
J08-2001,S07-1016,0,\N,Missing
J08-2001,J13-3006,1,\N,Missing
J08-2001,A97-1052,0,\N,Missing
J08-2001,P04-1055,0,\N,Missing
J08-2001,J02-3001,0,\N,Missing
J08-2001,W06-2303,0,\N,Missing
K15-1029,D14-1082,0,0.0607156,"Missing"
K15-1029,P96-1025,0,0.561927,"from a treebank of constituent trees with head-child annotations in each constituent (Carreras et al., 2008): starting from a token, its spine consists of the non-terminal labels of the constituents whose head is the token; the parent node of the top of the spine gives information about the lexical head (by following the head children of the parent) and the position where the spine attaches to. Given a spinal tree it is trivial to recover the constituent and dependency trees. the following: • We define an arc-eager statistical model for spinal parsing that is based on the triplet relations by Collins (1996). Such relations, in conjunction with the partial spinal structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal structures. • In experiments using beam search we show that our method obtains a good tradeoff between speed and accuracy for both dependency-based attachment scores and constituent measures. 2 2.1 Background Spinal Trees A spinal tree is a generalization of a dependency tree that adds constituent structure to the dependencies in the form of spines"
K15-1029,P97-1003,0,0.593379,"Missing"
K15-1029,P15-1033,1,0.872346,"Missing"
K15-1029,P15-1147,0,0.13411,"Missing"
K15-1029,W08-1007,0,0.011848,"edy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical depend"
K15-1029,C14-1076,1,0.875508,"Missing"
K15-1029,W07-2444,0,0.0216766,"of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do"
K15-1029,D12-1133,0,0.0201753,"and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses"
K15-1029,P10-1110,0,0.0875648,"Missing"
K15-1029,W08-2102,1,0.851515,"ier Carreras3 1 NLP Group, Pompeu Fabra University 2 Carnegie Mellon University 3 Xerox Research Centre Europe miguel.ballesteros@upf.edu xavier.carreras@xrce.xerox.com Abstract constituent-based PCFG that make a central use of lexical dependencies. An alternative approach is to view the combined representation as a dependency structure augmented with constituent information. This approach was first explored by Collins (1996), who defined a dependency-based probabilistic model that associates a triple of constituents with each dependency. In our case, we follow the representations proposed by Carreras et al. (2008), which we call spinal trees. In a spinal tree (see Figure 1 for an example), each token is associated with a spine of constituents, and head-modifier dependencies are attached to nodes in the spine, thus combining the two sources of information in a tight manner. Since spinal trees are inherently dependencybased, it is possible to extend dependency models for such representations, as shown by Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and"
K15-1029,P08-1067,0,0.0676573,"Missing"
K15-1029,J14-2001,0,0.0196901,"configuration. The SHIFT transition removes the first node from the buffer and puts it on the stack. The REDUCE transition removes the top node from the stack. The LEFT- ARCt transition introduces a labeled dependency edge between the first element of the buffer and the top element of the stack with the label t. The top element is removed from the stack (reduce transition). The RIGHT- ARCt transition introduces a labeled dependency edge between the top element of the stack and the first element in the buffer with a label d, and it performs a shift transition. Each action can have constraints (Nivre et al., 2014), Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations S NP . VP DT NN This market VBN . VP has VBN VP been ADVP RB ADV very badly VBN damaged (b) — Spinal Tree S ? This NP VP ? market ? has VP VP ADVP ? been ? very ? badly ? damaged ? . Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. In this paper, we took the already existent implementation of arc-eager from ZPar1 ("
K15-1029,N15-1080,0,0.0315935,"Missing"
K15-1029,W03-3017,0,0.465803,"Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and conjunctions of the two. However, the consequence is that the additional spinal structure greatly increases the number of dependency relations. Even though a graph-based model remains parseable in cubic time, it is impractical unless some pruning strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased represe"
K15-1029,W04-0308,0,0.103844,"uctured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others"
K15-1029,P10-1001,0,0.0888556,"Missing"
K15-1029,N07-1051,0,0.0384221,"Missing"
K15-1029,P08-1068,1,0.863908,"Missing"
K15-1029,D10-1004,0,0.11651,"Missing"
K15-1029,D10-1001,0,0.0280099,"l representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work can be seen as the transition-based counterpart of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to"
K15-1029,N06-1020,0,0.100666,"Missing"
K15-1029,W05-1513,0,0.474993,"stituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very h"
K15-1029,E06-1011,0,0.229238,"Missing"
K15-1029,N06-2033,0,0.0979139,"Missing"
K15-1029,P05-1012,0,0.0689642,"uracy, and yields state of the art performance for both dependency and constituent parsing measures. 1 Introduction There are two main representations of the syntactic structure of sentences, namely constituent and dependency-based structures. In terms of statistical modeling, an advantage of dependency representations is that they are naturally lexicalized, and this allows the statistical model to capture a rich set of lexico-syntactic features. The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing (Collins, 1999; Nivre, 2003; McDonald et al., 2005). Constituent structure, on the other hand, might still provide valuable syntactic information that is not captured by standard dependencies. In this work we investigate transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic 289 Proceedings of the 19th Conference on Computational Language Learn"
K15-1029,D09-1058,1,0.894619,"Missing"
K15-1029,J14-2002,0,0.0473466,"Missing"
K15-1029,I11-1140,0,0.0324855,"Missing"
K15-1029,P15-1032,0,0.031042,"Missing"
K15-1029,W03-3023,0,0.297738,"The assumption is that identical contiguous edges correspond to sibling dependencies that attach to the same node in the spine.2 4.1 2. Merge the left L and right R sequences of edges overlapping them as much as possible, i.e. looking for the shortest spine. We do this in O(nm), where n and m are the lengths of the two sequences. Whenever multiple shortest spines are compatible with the left and right edge sequences, we give preference to the spine that places left edges to the bottom. We use the WSJ portion of the Penn Treebank4 , augmented with head-dependant information using the rules of Yamada and Matsumoto (2003). This results in a total of 974 different constituent triplets, which we use as dependency labels in the spinal arc-eager model. We use predicted part-ofspeech tags5 . 4.2 The result of this process is a spine σi with left and right dependents attached to positions of the spine. Note that this strategy has some limitations: (a) it can not recover non-terminal spinal nodes that do not participate in any triplet; and (b) it flattens spinal structures that involve contiguous identical spinal edges. 3 Results in the Development Set In Table 1 we show the results of our parser for the dependency t"
K15-1029,W09-3825,0,0.344063,", Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations S NP . VP DT NN This market VBN . VP has VBN VP been ADVP RB ADV very badly VBN damaged (b) — Spinal Tree S ? This NP VP ? market ? has VP VP ADVP ? been ? very ? badly ? damaged ? . Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. In this paper, we took the already existent implementation of arc-eager from ZPar1 (Zhang and Clark, 2009) which is a beam-search parser implemented in C++ focused on efficiency. ZPar gives competitive accuracies, yielding state-of-the-art results, and very fast parsing speeds for dependency parsing. In the case of ZPar, the parsing process starts with a root node at the top of the stack (see Figure 3) and the buffer contains the words/tokens to be parsed. 3 tees that the arc-eager derivations we produce correspond to spinal trees. Finally we discuss how to map arc-eager derivations to spinal trees. 3.1 We follow Collins (1996) and define a labeling for dependencies based on constituent triplets."
K15-1029,P11-1069,0,0.244694,"strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and"
K15-1029,J11-1005,0,0.18843,"strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and"
K15-1029,P11-2033,0,0.213215,"at the bottom of the spine is well formed. We enforce no further constraints looking at edges in the middle of the spine. This means that left and right arc operations can add spinal edges in a free manner, without explicitly encoding how these edges relate to each other. In other words, we rely on the statistical model to correctly build a spine by adding left and • Top t: S • Left edges L: VP − S • Right edges R: ?−VP, VP − S 293 with the state-of-the-art. We used the ZPar implementation modified to incorporate the constraints for spinal arc-eager parsing. We used the exact same features as Zhang and Nivre (2011), which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack. Since our dependency labels are constituent triplets, these features encode a mix of constituent and dependency structure. In this case the shortest spine that is consistent with the edges and the top is ?−V P − S. Our method runs in two steps: 1. Collapse. Traverse each sequence of edges and replace any contiguous subsequence of identical edges by a single occurrence. The assumption is that identical contiguous edges correspond to sibling dependencies that attach to"
K15-1029,P15-1117,0,0.0207295,"Missing"
K15-1029,P13-1043,0,0.248264,"arts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size Sagae and Lavie (2005)* Ratnaparkhi (1999) Sagae and Lavie (2006)* Collins (1999) Charniak (2000) Zhang and Clark (2009)* Petrov and Klein (2007) Zhu et al. (2013)-1* Carreras et al. (2008) Zhu et al. (2013)-2†* Huang (2008) Charniak (2000) Huang et al. (2010) McClosky et al. (2006) this work (beam 64)* LR 86.1 86.3 87.8 88.1 89.5 90.0 90.1 90.2 90.7 91.1 91.2 91.2 91.2 91.2 88.7 LP 86.0 87.5 88.1 88.3 89.9 89.9 90.2 90.7 91.4 91.5 91.8 91.8 91.8 91.8 89.2 F1 86.0 86.9 87.9 88.2 89.5 89.9 90.1 90.4 91.1 91.3 91.5 91.5 91.5 91.5 89.0 structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also re"
K15-1029,A00-2018,0,\N,Missing
K15-1029,J03-4003,0,\N,Missing
K15-1029,C12-1052,0,\N,Missing
K15-1029,D10-1002,0,\N,Missing
P08-1068,J92-4003,0,0.69795,"argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). 2.2 Brown clustering algorithm In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest"
P08-1068,W06-2920,0,0.790504,"le two-stage semi-supervised approach. First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner. We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features. To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research. However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure. Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications. In general, semi-supervis"
P08-1068,D07-1101,1,0.741046,"ured classification approach to dependency parsing. For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y ∈ Y(x) decomposes into a set of “parts” r ∈ y. In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model. In higher-order parsing models, the parts can consist of interactions between more than two words. For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio “plays”, “Elianti”, and “.” in Figure 1. The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio “*”, “plays”, and “Haag” in Figure 1. These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve"
P08-1068,P99-1065,1,0.578037,"Missing"
P08-1068,W02-1001,1,0.166689,"he Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures. To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times. To select the number 6 We used Joakim Nivre’s “Penn2Malt” conversion tool (http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Dependency labels were obtained via the “Malt” hard-coded setting. 7 For computational reasons, we removed a single 249-word sentence from Section 0. 8 That is, we tagged each fold with the tagger trained on the other 9 folds. 9 We ensured that the sentences of the Penn Treebank were excluded from the text used for the clustering. 10 Following Collins et al. (1999), we used a coarsened v"
P08-1068,W05-1505,0,0.00418844,"Missing"
P08-1068,H05-1064,1,0.179121,"ithout parts of speech is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and th"
P08-1068,J93-2004,0,0.0496562,"parsing applications. In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled data to facilitate the utilization of the supervised corpus, increasing the performance of the model in absolute terms. Second, given a fixed target performance level, we might wish to use unlabeled data to reduce the amount of annotated data necessary to reach this target. We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3). By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4). The remainder of this paper is divided as follows: 595 Proceedings of ACL-08: HLT, pages 595–603, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics root nmod sbj 0 p obj 000 * Ms. Haag plays Elian"
P08-1068,P05-1010,0,0.186274,"ined by using clusters without parts of speech is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributi"
P08-1068,N06-1020,0,0.813326,"when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of “backed-off” version of the baseline features. However, our work is focused on discriminative learning as opposed to generative models. Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice. However, their approach depends on the usage of a high-quality parse reranker, whereas the method described here simply augments the features of an existing parser. Note that our two approaches are compatible in that we could also design a reranker and apply self-training techniques on top of the clusterbased features. 6 Conclusions In this paper, we have presented a simple but effective semi-supervis"
P08-1068,E06-1011,0,0.83386,"s ideal candidates for the application of coarse word proxies such as word clusters. In this paper, we take a part-factored structured classification approach to dependency parsing. For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y ∈ Y(x) decomposes into a set of “parts” r ∈ y. In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model. In higher-order parsing models, the parts can consist of interactions between more than two words. For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio “plays”, “Elianti”, and “.” in Figure 1. The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio “*”, “plays”, and “Haag” in Figure 1. These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters"
P08-1068,P05-1012,0,0.839755,"ons allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). 2.2 Brown clustering algorithm In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text con"
P08-1068,H05-1066,0,0.714491,"Missing"
P08-1068,N04-1043,0,0.873543,"truct a new cluster-based feature mapping for a discriminative learner. We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features. To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research. However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure. Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications. In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled"
P08-1068,P05-1013,0,0.0486766,"Missing"
P08-1068,P06-1055,0,0.409463,"is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased featur"
P08-1068,W96-0213,0,0.520557,"the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24). The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures. To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/te"
P08-1068,P07-1080,0,0.00712696,"rmance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a"
P08-1068,W05-1516,0,0.0119333,"technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of “backed-off” version of the baseline features. However, our work is focused on discriminative learning as opposed to generative models. Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to"
P08-1068,W03-3023,0,0.953733,"es in a wide range of parsing configurations, including first-order and second-order parsers, and labeled and unlabeled parsers.5 3 As in Brown et al. (1992), we limit the clustering algorithm so that it recovers at most 1,000 distinct bit-strings; thus full bit strings are not equivalent to word forms. 4 We used N = 800 for all experiments in this paper. 5 In an “unlabeled” parser, we simply ignore dependency label information, which is a common simplification. 598 The English experiments were performed on the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24). The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for"
P08-1068,D07-1096,0,\N,Missing
P15-1013,J92-4003,0,0.0939332,"lassification of unseen entities to highlight the ability of the regularizer to generalize over conjunctions that are not observed at training. We simulate minimal supervision using the CoNLL-2003 Shared Task data (Tjong Kim Sang and De Meulder, 2003), and compare the performance to `1 and `2 regularizers. 5.1 5.2 Setting We use the CoNLL-2003 English data, which is annotated with four types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). In addition, the data is tagged with partsof-speech (PoS), and we compute word clusters running the Brown clustering algorithm (Brown et al., 1992) on the words in the training set. We consider annotated entity phrases as candidate entities, and all single nouns that are not part of an entity as candidate non-entities (O). Both candidate entities and non-entities will be referred to as candidates in the remaining of this section. We lowercase all candidates and remove the amMinimal Supervision Task We use a minimal supervision setting where we provide the algorithm a seed of entities for each class, that is, a list of entities that is representative for that class. The assumption is that any mention of an entity in the seed is a positive"
P15-1013,W09-1119,0,0.112889,"Missing"
P15-1013,W99-0613,0,0.288294,"it thresholds the singular values. This means that, for nuclear norm regularization, each iteration requires to decompose W using SVD. See (Madhyastha et al., 2014) for details about this optimization for a related application. 4 Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the c"
P15-1013,N13-1008,0,0.0341645,"ith a nuclear-norm relaxation to obtain a convex learning procedure. In their case they explicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization. (5) Also recently, Yao et al. (2013) have framed entity classification as a low-rank matrix completion problem. The idea is based on the fact that if two entities (in rows) have similar descriptions (in columns) they should have similar classes. The low-rank structure of the matrix defines intrinsic representations of entities and feature descriptions. The same idea was applied to relation extraction (Riedel et al., 2013), using a matrix of entity pairs times descriptions that corresponds to a matricization of an entity-entity-description tensor. Very recently Singh et al. (2015) explored alternative ways of applying low-rank constraints to tensor-based relation extraction. where L(W) is a convex loss function, R(W) is a regularizer, and τ is a constant that trades off error and capacity. In experiments we will compare nuclear norm regularization with `1 and `2 regularizers. In all cases we use the negative log-likelihood as loss function, denoting the training data as D: X L(W) = − log Pr(y |hl, e, ri; W) . ("
P15-1013,N06-1041,0,0.024605,"a related application. 4 Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors. Quattoni et al. (2014) used nuclear-norm regularization to learn latentvariable max-margin sequence taggers. Madhyastha et al. (2014) d"
P15-1013,W15-1519,0,0.155528,"Missing"
P15-1013,P14-1130,0,0.0945513,"i Σi,i where W = UΣV&gt; is the singular value decomposition of W. This norm has been used in several applications in machine learning as a convex surrogate for imposing low rank, e.g. (Srebro et al., 2004). Thus, the nuclear norm is used as a regularizer. With this, we define our objective as follows: argmin L(W) + τ R(W) , W tions parameterized by matrices which result lexical embeddings tailored for a particular linguistic relation. Like in our case, the low-dimensional latent projections in these papers are learned implicitly by imposing low-rank constraints on the predictions of the model. Lei et al. (2014) also use low-rank tensor learning in the context of dependency parsing, where like in our case dependencies are represented by conjunctive feature spaces. While the motivation is similar, their technical solution is different. We use the technique of matricization of a tensor combined with a nuclear-norm relaxation to obtain a convex learning procedure. In their case they explicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization. (5) Also recently, Yao et al. (2013) have framed entity classification as a low-rank matrix completion problem. The i"
P15-1013,P10-2066,0,0.0239406,"al successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors. Quattoni et al. (2014) used nuclear-norm regularization to learn latentvariable max-margin sequence taggers. Madhyastha et al. (2014) defined bilexical distribu4 Another approach to entity recognition that, like in our case, learns projections of contextual features is the method by Ando and Zhang (2005). Also known a"
P15-1013,C14-1017,1,0.914665,"L(W) = − log Pr(y |hl, e, ri; W) . (hl,e,ri,y)∈D (6) To solve the objective in Eq. (5) we use a simple optimization scheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009). In a series of iterations, this algorithm performs a gradient update followed by a proximal projection of the parameters. Such projection depends on the regularizer used: for `1 it thresholds the parameters; for `2 it scales them; and for nuclearnorm regularization it thresholds the singular values. This means that, for nuclear norm regularization, each iteration requires to decompose W using SVD. See (Madhyastha et al., 2014) for details about this optimization for a related application. 4 Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been"
P15-1013,E14-1048,0,0.0298666,"Missing"
P15-1013,W03-0419,0,\N,Missing
P19-1594,N09-1051,0,0.042815,"ved empirical moments, i.e. those substrings collected in the Hankel matrix. To align the loss function with a perplexity measure we propose a simple refinement step, where we use the expected counts computed by the learned PNFA as features of a log-linear model, and learn interpolation weights. In contrast to Equation 2, which uses the longest context x of length n to compute the conditional probability, the interpolated model leverages the ability of the PNFA to model substring expectations of all lengths up to n. This is similar to classic interpolation of language models (Rosenfeld, 1994; Chen, 2009). Given a function f computing substring expectations, the interpolation is:   n−1  X g(x1:n , σ) = exp wσ,j log f (xn−j:n · σ)   j=0 (3) where x1:n is a context of size n, σ is the output symbol, and wσ,j are the interpolation weights, with one parameter per output symbol σ and context length j, with 0 ≤ j &lt; n. As it is standard with interpolation models, we train the weights by maximizing the conditional log-likelihood of the development set. We assume that f is fixed, which results in a convex optimization, and we solve with L-BFGS. 4 Experiments We present experiments in character-ba"
P19-1594,D14-1158,0,0.0206139,"al methods when applied to language modeling is that the loss function that the learning algorithm attempts to minimize is not aligned with the loss function that is used to evaluate model performance. Spectral methods minimize the `2 distance on the prediction of expectations of substrings up to a certain length (see Balle et al. (2012) for a formulation of spectral learning in terms of loss minimization), while language models are usually evaluated using conditional perplexity. There have been some proposals on generalizing the fundamental ideas of spectral learning to other loss functions (Parikh et al., 2014; Quattoni et al., 2014). However, while these approaches are promising they have the downside that they lead to relatively expensive iterative convex optimizations and it is still a challenge to scale them to model long-range dependencies. In this paper we propose a simpler yet effective alternative to the iterative optimization. We use the classical spectral method based on low-rank matrix decomposition to learn a PNFA that computes substring expectations. Then we use these expectations as features in an interpolated ngram model and we learn the weights of the interpolation so as to maximize"
P19-1594,P12-1024,0,0.163779,"luate language models. In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. Our experiments in character-based language modeling show that our method matches the performance of stateof-the-art ngram models, while being very fast to train. 1 Introduction In the recent years we have witnessed the development of spectral methods based on matrix decompositions to learn Probabilistic Non-deterministic Finite Automata (PNFA) and related models (Hsu et al., 2009, 2012; Bailly et al., 2009; Balle et al., 2011; Cohen et al., 2012; Balle et al., 2014). Essentially, PNFA can be regarded as recurrent neural networks where the function that predicts the dynamic state representation from previous states is linear. Despite the expressiveness of PNFA and the strong theoretical properties of spectral learning algorithms, it has been challenging to get competitive results on language modeling tasks. We argue and confirm with our experiments that there are two main reasons why using spectral methods for language modeling is challenging. The first reason is a scalability problem to handle long range dependencies. The spectral me"
padro-etal-2014-language,J93-2004,0,\N,Missing
padro-etal-2014-language,E06-1011,0,\N,Missing
padro-etal-2014-language,W07-1702,1,\N,Missing
padro-etal-2014-language,N03-1033,0,\N,Missing
padro-etal-2014-language,W03-1712,0,\N,Missing
padro-etal-2014-language,W08-2102,1,\N,Missing
padro-etal-2014-language,E09-1005,0,\N,Missing
padro-etal-2014-language,P05-1012,0,\N,Missing
padro-etal-2014-language,P08-1068,1,\N,Missing
padro-etal-2014-language,W09-1201,0,\N,Missing
padro-etal-2014-language,W14-0150,0,\N,Missing
padro-etal-2014-language,Q13-1018,1,\N,Missing
padro-etal-2014-language,taule-etal-2008-ancora,0,\N,Missing
padro-etal-2014-language,D07-1101,1,\N,Missing
padro-etal-2014-language,padro-stanilovsky-2012-freeling,1,\N,Missing
padro-etal-2014-language,P05-1045,0,\N,Missing
Q13-1018,W05-0620,1,0.913586,"Missing"
Q13-1018,D07-1101,1,0.741319,"Missing"
Q13-1018,W02-1001,0,0.0930078,"Missing"
Q13-1018,S12-1029,0,0.0736045,"ergences between the two layers. 219 Transactions of the Association for Computational Linguistics, 1 (2013) 219–230. Action Editor: Brian Roark. c Submitted 1/2013; Revised 3/2013; Published 5/2013. 2013 Association for Computational Linguistics. � SBJ Mary ARG 0 main contributions of this paper are: features in the semantic component (Gildea and JuP OPRD loves ARG 1 IM to OBJ play guitar . ARG 1 ARG 0 Figure 1: A sentence with (top) Figure 1: Ansyntactic exampledependencies ... and semantic dependencies for the predicates “loves” and “play” (bottom). The thick arcs illustrate a structural diDas et al. (2012) ... vergence where the argument “Mary” is linked to “play” with a path involving three syntactic Riedel and McCallum (2011) . dependencies. .. 3 A Syntactic-Semantic Dependency This is clearly seen in dependency-based representaModel tions of syntax and semantic roles (Surdeanu et al., rafsky, 2002;SRL Xueas and Palmer, 2004; Punyakanok • We frame a weighted assignment prob- et al.,lem 2008). However, without further assumptions, in a bipartite graph. Under this framework thiswe property makes the optimization problem can control assignment constraints betweencomputationally One simple roles"
Q13-1018,W09-1205,0,0.139785,"Missing"
Q13-1018,J02-3001,0,0.702903,"which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results. 1 Introduction Semantic role labeling (SRL) is the task of identifying the arguments of lexical predicates in a sentence and labeling them with semantic roles (Gildea and Jurafsky, 2002; M`arquez et al., 2008). SRL is an important shallow semantic task in NLP since predicate-argument relations directly represent semantic properties of the type “who” did “what” to “whom”, “how”, and “why” for events expressed by predicates (typically verbs and nouns). Predicate-argument relations are strongly related to the syntactic structure of the sentence: the majority of predicate arguments correspond to some syntactic constituent, and the syntactic structure that connects an argument with the predicate is a strong indicator of its semantic role. Actually, semantic Consequently, since th"
Q13-1018,P02-1031,0,0.0561981,"syntactic structure that connects an argument with the predicate is a strong indicator of its semantic role. Actually, semantic Consequently, since the first works, SRL systems have assumed access to the syntactic structure of the sentence (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). A simple approach is to obtain the parse trees as a pre-process to the SRL system, which allows the use of unrestricted features of the syntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semanti"
Q13-1018,D09-1059,0,0.42298,"yntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semantic predicate-argument structures of a sentence. In principle, these models can exploit syntactic and semantic features simultaneously, and could potentially improve the accuracy for both syntactic and semantic relations. One difficulty in the design of joint syntacticsemantic parsing models is that there exist important structural divergences between the two layers. 219 Transactions of the Association for Computational Ling"
Q13-1018,D07-1015,1,0.40731,"Missing"
Q13-1018,D10-1125,0,0.425569,"control assignment constraints betweencomputationally One simple roles and hard. arguments. Key to approximation our method, weis to usecan a pipeline model: compute the optimal efficiently searchfirst over a large space of syn-syntactic realizations semanticfor arguments. tactic tree, and then of optimize the best semantic structure given the syntactic tree. In the rest of the • We solve joint inference of syntactic and sepaper we describe a method that searches over synmantic dependencies with a dual decompositactic and semantic dependency structures jointly. tion method, similar to that of Koo et al. (2010). We impose the assumption syntactic Ourfirst system produces consistent that syntactic and features of the semantic component are restricted to the predicate-argument structures while searching syntactic a predicate and an argument, over a path large between space of syntactic configurations. following previous work (Johansson, 2009). ForIn the for experimental we compare jointr we mally, a predicatesection p, argument a and role and pipeline models. The final results of our joint will define a vector of dependency indicators π p,a,r syntactic-semantic system are competitive with the p,a,r si"
Q13-1018,W09-1212,1,0.909451,"Missing"
Q13-1018,J08-2001,1,0.470243,"Missing"
Q13-1018,P09-1039,0,0.0362687,"together with its syntactic path π p,a,r . As in the syntactic component, this function is typically defined as a linear function over a set of features of the semantic dependency and its path. The inference problem of our joint model is: argmax s syn(x, y) + s srl(x, z, π) y,z,π (4) subject to cTree : cRole : y is a valid dependency tree X ∀p, r : zp,a,r ≤ 1 a cArg : cPath : ∀p, a : X r zp,a,r ≤ 1 ∀p, a, r : if zp,a,r = 1 then π p,a,r is a path from p to a, otherwise π p,a,r = 0 cSubtree : ∀p, a, r : π p,a,r is a subtree of y Constraint cTree dictates that y is a valid dependency tree; see (Martins et al., 2009) for a detailed specification. The next two sets of constraints concern the semantic structure only. cRole imposes that each semantic role is realized at most once.2 Conversely, cArg dictates that an argument can realize at most one semantic role in a predicate. The final two sets of constraints model the syntactic-semantic interdependencies. cPath imposes that each π p,a,r represents a syntactic path between p and a whenever there exists a semantic relation. Finally, cSubtree imposes that the paths in π are consistent with the full syntactic structure, i.e. they are subtrees. 1 In this paper"
Q13-1018,P05-1012,0,0.158834,"A joint model for syntactic and semantic dependency parsing could be defined as: argmax s syn(x, y) + s srl(x, z, y) y,z . (1) In the equation, s syn(x, y) gives a score for the syntactic tree y. In the literature, it is standard to use arc-factored models defined as X s syn(x, y) = s syn(x, h, m, l) , (2) yh,m,l =1 where we overload s syn to be a function that computes scores for individual syntactic dependencies. In linear discriminative models one has s syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where fsyn is a feature vector for a syntactic dependency and wsyn is a vector of parameters (McDonald et al., 2005). In Section 6 we describe how we trained score functions with discriminative methods. The other term in Eq. 1, s srl(x, z, y), gives a score for a semantic dependency structure z using features of the syntactic structure y. Previous work has empirically proved the importance of exploiting syntactic features in the semantic component (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Punyakanok et al., 2008). However, without further assumptions, this property makes the optimization problem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal synt"
Q13-1018,W04-2705,0,0.0805238,"Missing"
Q13-1018,P04-1043,0,0.0526438,"lem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal syntactic tree y, and then optimize for the best semantic structure z given y. In the rest of the paper we describe a method that searches over syntactic and semantic dependency structures jointly. We first note that for a fixed semantic dependency, the semantic component will typically restrict the syntactic features representing the dependency to a specific subtree of y. For example, previous work has restricted such features to the syntactic path that links a predicate with an argument (Moschitti, 2004; Johansson, 2009), and in this paper we employ this restriction. Figure 1 gives an example of a subtree, where we highlight the syntactic path that connects the semantic dependency between “play” and “Mary” with role ARG 0. Formally, for a predicate p, argument a and role r we define a local syntactic subtree π p,a,r reprep,a,r sented as a vector: πh,m,l indicates if a dependency 221 hh, m, li is part of the syntactic path that links predicate p with token a and role r.1 Given full syntactic and semantic structures y and z it is trivial to construct a vector π that concatenates vectors π p,a,"
Q13-1018,P05-1013,0,0.095953,"Missing"
Q13-1018,J05-1004,0,0.127514,"ion algorithm seeks agreement at the level of individual dependencies. One dif225 ference is that our semantic process predicts partial syntax (restricted to syntactic paths connecting predicates and arguments), while in their case each of the two processes predicts the full set of dependencies. 6 Experiments We present experiments using our syntacticsemantic parser on the CoNLL-2009 Shared Task English benchmark (Hajiˇc et al., 2009). It consists of the usual WSJ training/development/test sections mapped to dependency trees, augmented with semantic predicate-argument relations from PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) also represented as dependencies. It also contains a PropBanked portion of the Brown corpus as an out-of-domain test set. Our goal was to evaluate the contributions of parsing algorithms in the following configurations: Base Pipeline Runs a syntactic parser and then runs an SRL parser constrained to paths of the best syntactic tree. In the SRL it only enforces constraint cArg, by simply classifying the candidate argument in each path into one of the possible semantic roles or as NULL. Pipeline with Assignment Runs the assignment algorithm for SRL, enforcing c"
Q13-1018,J08-2005,0,0.78721,"structure of the sentence (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). A simple approach is to obtain the parse trees as a pre-process to the SRL system, which allows the use of unrestricted features of the syntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semantic predicate-argument structures of a sentence. In principle, these models can exploit syntactic and semantic features simultaneously, and could potentially improve the accuracy for both syntactic and semantic relat"
Q13-1018,D11-1001,0,0.133702,"0. Action Editor: Brian Roark. c Submitted 1/2013; Revised 3/2013; Published 5/2013. 2013 Association for Computational Linguistics. � SBJ Mary ARG 0 main contributions of this paper are: features in the semantic component (Gildea and JuP OPRD loves ARG 1 IM to OBJ play guitar . ARG 1 ARG 0 Figure 1: A sentence with (top) Figure 1: Ansyntactic exampledependencies ... and semantic dependencies for the predicates “loves” and “play” (bottom). The thick arcs illustrate a structural diDas et al. (2012) ... vergence where the argument “Mary” is linked to “play” with a path involving three syntactic Riedel and McCallum (2011) . dependencies. .. 3 A Syntactic-Semantic Dependency This is clearly seen in dependency-based representaModel tions of syntax and semantic roles (Surdeanu et al., rafsky, 2002;SRL Xueas and Palmer, 2004; Punyakanok • We frame a weighted assignment prob- et al.,lem 2008). However, without further assumptions, in a bipartite graph. Under this framework thiswe property makes the optimization problem can control assignment constraints betweencomputationally One simple roles and hard. arguments. Key to approximation our method, weis to usecan a pipeline model: compute the optimal efficiently searc"
Q13-1018,D10-1001,0,0.0786099,"s srl(x, z, π) (2) yh,m,l =1 cate with the argument. We show how efficient predictions these models be made using aswhere wewith overload s syn can to be a function that signment algorithms in bipartite graphs. Simultacomputes scores for individual labeled syntactic neously, we use In a standard arc-factored dependency dependencies. discriminative models one has model that predicts the full syntactic tree of the sens syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where tence. Finally, we employ dual decomposition techfsyn is a feature vector for the syntactic dependency niques (Koo et al., 2010; Rush et al., 2010; Sontag and wsyn is a vector of parameters (McDonald et al., et al., 2010) to find agreement between the full de2005). pendency tree and the partial syntactic trees linking Thepredicate other term, z, y), gives a scorethe for each withsitssrl(x, arguments. In summary, a semantic dependency structure using the syntactic structure y as features. Previous work has empiri220 cally proved the importance of exploiting syntactic y,z,π predicate token p and argument token a labeled with semantic roletor. We will represent a semantic role subject structurecTree as a vector semantic depen: yzisindexed"
Q13-1018,W08-2121,1,0.616614,"Missing"
Q13-1018,J08-2002,0,0.383434,"Missing"
Q13-1018,W04-3212,0,0.0622858,"ndividual syntactic dependencies. In linear discriminative models one has s syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where fsyn is a feature vector for a syntactic dependency and wsyn is a vector of parameters (McDonald et al., 2005). In Section 6 we describe how we trained score functions with discriminative methods. The other term in Eq. 1, s srl(x, z, y), gives a score for a semantic dependency structure z using features of the syntactic structure y. Previous work has empirically proved the importance of exploiting syntactic features in the semantic component (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Punyakanok et al., 2008). However, without further assumptions, this property makes the optimization problem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal syntactic tree y, and then optimize for the best semantic structure z given y. In the rest of the paper we describe a method that searches over syntactic and semantic dependency structures jointly. We first note that for a fixed semantic dependency, the semantic component will typically restrict the syntactic features representing the dependency to a specific subtree of y. For example,"
Q13-1018,C00-2137,0,0.106408,"Missing"
Q13-1018,N07-1070,0,\N,Missing
Q13-1018,J13-3006,1,\N,Missing
Q13-1018,W09-1201,1,\N,Missing
W02-2004,W01-0726,1,\N,Missing
W03-0421,W01-0726,1,0.870152,"Missing"
W03-0421,W02-2004,1,0.561477,"Missing"
W03-0422,W02-1001,0,0.0206526,"ng strategy works online at sentence level. When visiting a sentence, the functions being learned are first used to recognize the NE phrases, and then updated according to the correctness of their solution. We analyze the dependencies among the involved perceptrons and a global solution in order to design a global update rule based on the recognition of namedentities, which reflects to each individual perceptron its committed errors from a global perspective. The learning approach presented here is closely related to –and inspired by– some recent works in the area of NLP and Machine Learning. Collins (2002) adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback. Crammer and Singer (2003) presented an online topic-ranking algorithm involving several perceptrons and ranking-based update rules for training them. 2 Named-Entity Phrase Chunking In this section we describe our NERC approach as a phrase chunking problem. First we formalize the problem of NERC, then we propose a NE-Chunker. 2.1 Problem Formalization Let x be a sentence belonging to the sentence space X , formed by n words xi with i ranging from 0 to n − 1. Let K be the set of NE categories, which"
W03-1504,W02-2004,1,0.882393,"Missing"
W03-1504,E03-1038,1,0.880696,"Missing"
W03-1504,W99-0613,0,0.348162,"for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– independent NERC systems, which may be trained from small training sets by taking advantage of unlabelled examples (Collins and Singer, 1999; Abney, 2002), and which are easy to adapt to changing domains (being all these aspects closely related). This work focuses on exploring the construction of a low–cost Named Entity classification (NEC) module for Catalan without making use of large/expensive resources of the language. In doing so, the paper first explores the training of classification models by using only Catalan resources and then proposes a training scheme, in which a Catalan/Spanish bilingual classifier is trained directly from a training set including examples of the two languages. In both cases, the bootstrapping of the"
W03-1504,W03-0419,0,0.0206414,"Missing"
W03-1504,W02-2024,0,0.0121427,"ch may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifying those units in a text has kept on growing during the last years. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC competition task. More recent approaches can be found in the proceedings of the shared task at the 2002 and 2003 editions of the Conference on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), where several machine–learning (ML) systems were compared at the NERC task for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– in"
W03-1504,P02-1046,0,\N,Missing
W04-2412,W04-2413,0,0.038009,"Missing"
W04-2412,W04-2415,1,0.477006,"Missing"
W04-2412,W03-1006,0,0.0146244,"raining classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et"
W04-2412,W03-0423,0,0.0978149,"Missing"
W04-2412,Y01-1001,0,0.032398,"Missing"
W04-2412,W03-1007,0,0.00623602,"actic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with sema"
W04-2412,W03-1008,0,0.0372572,"c semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, the"
W04-2412,J02-3001,0,0.495339,"nner, Cause, etc. Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic"
W04-2412,P02-1031,0,0.0266753,"ting systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level"
W04-2412,N03-2009,0,0.0138529,"ls. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentr"
W04-2412,W04-2416,0,0.64655,"e development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL str"
W04-2412,W04-2417,0,0.020755,"ng components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature developme"
W04-2412,W04-2421,0,0.307179,"teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL strategies and into the development of features (see sections 4.2 and 4.3). As an exception, van den Bosch et al. (2004) applied a 3 Arguments in data do not embed, though format allows so. The San Francis"
W04-2412,P03-1002,0,0.882784,"evant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) a"
W04-2412,W04-2418,0,0.0168107,"system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different"
W04-2412,W04-2419,0,0.23673,"the CoNLL-2004 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the te"
W04-2412,W00-0726,0,0.168458,"Missing"
W04-2412,J93-2004,0,0.0426925,"n of the predicate of the proposition. Most of the time, the verb corresponds to the target verb of the proposition, which is provided as input, and only in few cases the verb participant spans more words than the target verb. Except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition overestimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation. 3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). We first describe annotations related to argument structure. Then, we describe the preprocessing of input data. Finally, we describe the format of the data sets. 3.1 PropBank The Proposition Bank (PropBank) (Palmer et al., 2004) annotates the Penn Treebank with verb argument structure. The semantic roles covered by PropBank are the following: • Numbered arguments (A0–A5, AA): Arguments defining verb-specific roles. Their semantics depends o"
W04-2412,W01-0708,0,0.0512608,"Missing"
W04-2412,W04-2420,0,0.0341188,"Missing"
W04-2412,W04-2422,0,0.316071,"nd labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As"
W04-2412,W03-0419,0,\N,Missing
W04-2412,J05-1004,0,\N,Missing
W04-2412,W04-2414,0,\N,Missing
W04-2415,W04-2412,1,0.635591,"Missing"
W04-2415,W02-1001,0,0.108027,"We describe a system for the CoNLL-2004 Shared Task on Semantic Role Labeling (Carreras and M`arquez, 2004a). The system implements a two-layer learning architecture to recognize arguments in a sentence and predict the role they play in the propositions. The exploration strategy visits possible arguments bottom-up, navigating through the clause hierarchy. The learning components in the architecture are implemented as Perceptrons, and are trained simultaneously online, adapting their behavior to the global target of the system. The learning algorithm follows the global strategy introduced in (Collins, 2002) and adapted in (Carreras and M`arquez, 2004b) for partial parsing tasks. 2 Semantic Role Labeling Strategy The strategy for recognizing propositional arguments in sentences is based on two main observations about argument structure in the data. The first observation is the relation of the arguments of a proposition with the chunk and clause hierarchy: a proposition places its arguments in the clause directly containing the verb (local clause), or in one of the ancestor clauses. Given a clause, we define the sequence of top-most syntactic elements as the words, chunks or clauses which are dire"
W05-0620,W04-2412,1,0.350852,"Missing"
W05-0620,A00-2018,0,0.317238,"fers a substantial loss in the Brown test set. Noticeably, the parser of Collins (1999) seems to be the more robust when moving from WSJ to Brown. 4 A Review of Participant Systems Nineteen systems participated in the CoNLL-2005 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques • Full parser of Charniak (2000). Jointly predicts PoS tags and full parses. • Named Entities predicted with the MaximumEntropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. However, we allowed its use because there is no available named entity recognizer developed with WSJ data. The reported performance on the CoNLL-2003 test is F1 = 88.31, with Prec/Rec. at 88.12/88.51. Tables 2 and 3 summarize the performance of the syntactic processors on the development and test sets. The performance of full parsers on t"
W05-0620,W05-0627,0,0.0745744,"ee, using a script that is available at the shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, whic"
W05-0620,W03-0423,0,0.105348,"from WSJ to Brown. 4 A Review of Participant Systems Nineteen systems participated in the CoNLL-2005 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques • Full parser of Charniak (2000). Jointly predicts PoS tags and full parses. • Named Entities predicted with the MaximumEntropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. However, we allowed its use because there is no available named entity recognizer developed with WSJ data. The reported performance on the CoNLL-2003 test is F1 = 88.31, with Prec/Rec. at 88.12/88.51. Tables 2 and 3 summarize the performance of the syntactic processors on the development and test sets. The performance of full parsers on the WSJ test is lower than that reported in the corresponding papers. The reason is that our evaluation figures have been computed i"
W05-0620,W05-0622,0,0.185711,"strict evaluation basis with respect to punctuation. by Ponzetto and Strube (2005), who used C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of increasing the robustness and coverage of the systems, which are quite d"
W05-0620,Y01-1001,0,0.0131567,"Missing"
W05-0620,J02-3001,0,0.591475,"the base SRL models. Actually, Haghighi et al. (2005) performed a double selection step: an inner re-ranking of n-best solutions coming from the base system on a single tree; and an outer selection of the final solution among the candidate solutions coming from n-best parse trees. The reranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. 4.3 Features Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005a; Xue and Palmer, 2004). With no exception, all systems have made intensive use of syntax to extract features. While most systems work only on the output of a parser —Charniak’s being the most preferred— some systems depend on many syntactic parsers. In the latter situation, either a system is a combination of many individual systems (each working with a different parser), or a system extracts features from many different parse trees while exploring the nodes of only one parse tree. Most systems have also considered named entities for extracting fe"
W05-0620,W05-0623,0,0.902367,"that is available at the shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased netw"
W05-0620,W05-0624,0,0.00904432,"test sets. Unlike in full parsing, the figures have been computed on a strict evaluation basis with respect to punctuation. by Ponzetto and Strube (2005), who used C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of incr"
W05-0620,W05-0626,0,0.0430337,"d C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of increasing the robustness and coverage of the systems, which are quite dependent on input parsing errors. The different outputs to combine are obtained by varyi"
W05-0620,W04-0803,0,0.418567,"syntactic information. In (Carreras and M`arquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004. Ten systems contributed to the task, which was evaluated using the PropBank corpus (Palmer et al., 2005). The best results were around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming"
W05-0620,J93-2004,0,0.0477049,"tated sentence, in columns. Input consists of words (1st column), PoS tags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th column marks target verbs, and their propositions are found in remaining columns. According to the PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for intersperse (9th), A0 is the arranger, and A1 the entity interspersed. 2.2 Closed Challenge Setting 3 The organization provided training, development and test sets derived from the standard sections of the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005) corpora. In the closed challenge, systems have to be built strictly with information contained in the training sections of the TreeBank and PropBank. Since this collection contains the gold reference annotations of both syntactic and predicate-argument structures, the closed challenge allows: (1) to make use of any preprocessing system strictly developed within this setting, and (2) to learn from scratch any annotation that is contained in the data. To support the former, the organization provided the output of state-of-theart syntactic preprocessors, descri"
W05-0620,W05-0628,1,0.618126,"Missing"
W05-0620,W05-0629,0,0.0642718,"ists of n-best parsings generated by available tools (“n-cha” by Charniak parser; “nbikel” by Bikel’s implementation of Collins parser). Interestingly, Yi and Palmer (2005) retrained Ratnaparkhi’s parser using the WSJ training sections enriched with semantic information coming from PropBank annotations. These are referred to as AN and AM parses. As it can be seen, Charniak parses were used by most of the systems. Collins parses were used also in some of the best performing systems based on combination. The exceptions to the hierarchical processing are the systems by Pradhan et al. (2005b) and Mitsumori et al. (2005), which perform a chunking-based sequential tokenization. As for the former, the system is the same than the one presented in the 2004 edition. The system by M`arquez et al. (2005) explores hierarchical syntactic structures but selects, in a preprocess, a sequence of tokens to perform a sequential tagging afterwards. punyakanok haghighi marquez pradhan surdeanu tsai che moschitti tjongkimsang yi ozgencil johansson cohn park mitsumori venkatapathy ponzetto lin sutton ML-method SNoW ME AB SVM AB ME,SVM ME SVM ME,SVM,TBL ME SVM RVM T-CRF ME SVM ME DT CPM ME synt n-cha,col n-cha cha,upc cha,col/ch"
W05-0620,W05-0634,0,0.120346,"figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks,"
W05-0620,C04-1197,0,0.342398,"around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions"
W05-0620,W05-0625,0,0.784912,"lly, the alternative outputs to combine can be generated by different input syntactic structures or nbest parse candidates, or by applying different learning algorithms to the same input information. The type of combination is reported in the last column. M`arquez et al. (2005) and Tjong Kim Sang et al. (2005) performed a greedy merging of the arguments of base complete solutions (“s-join”). Yi 159 and Palmer (2005) did also a greedy merging of arguments but taking into account not complete solutions but all candidate arguments labeled by base systems (“ac-join”). In a more sophisticated way, Punyakanok et al. (2005) and Tsai et al. (2005) performed global inference as constraint satisfaction using Integer Linear Programming, also taking into account all candidate arguments (“ac-ILP”). It is worth noting that the generalized inference applied in those papers allows to include, jointly with the combination of outputs, a number of linguisticallymotivated constraints to obtain a coherent solution. Pradhan et al. (2005b) followed a stacking approach by learning a chunk-based SRL system including as features the outputs of two syntax-based systems. Finally, Haghighi et al. (2005) and Sutton and McCallum (2005)"
W05-0620,W05-0635,0,0.119021,".92 78.34 75.78 77.04 78.44 74.83 76.59 80.93 71.69 76.03 79.35 71.17 75.04 81.55 69.37 74.97 79.30 71.08 74.97 75.19 73.45 74.31 77.94 70.44 74.00 76.31 71.10 73.61 73.48 72.70 73.09 74.13 71.50 72.79 74.76 69.17 71.86 73.35 69.37 71.31 72.77 66.37 69.43 72.66 64.21 68.17 74.02 63.12 68.13 70.80 63.09 66.72 67.86 63.63 65.68 52.58 29.69 37.95 Table 6: Overall precision, recall and F1 rates obtained by the 19 participating systems in the CoNLL-2005 shared task on the development and test sets. Systems sorted by F 1 score on the WSJ+Brown test set. best individual system on the task is that of Surdeanu and Turmo (2005), which obtained F1 =75.04 on the combined test set, about 3 points below than the best performing combined system. On the development set, that system achieved a performace of 75.17 (slightly below than the 75.27 reported by Che et al. (2005) on the same dataset). According to the description papers, we find that other individual systems, from which the combined systems are constructed, performed also very well. For instance, Tsai et al. (2005) report F1 =75.76 for a base system on the development set, M`arquez et al. (2005) report F1 =75.75, Punyakanok et al. (2005) report F1 =74.76, and Hag"
W05-0620,P03-1002,0,0.906092,"lly, Haghighi et al. (2005) performed a double selection step: an inner re-ranking of n-best solutions coming from the base system on a single tree; and an outer selection of the final solution among the candidate solutions coming from n-best parse trees. The reranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. 4.3 Features Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005a; Xue and Palmer, 2004). With no exception, all systems have made intensive use of syntax to extract features. While most systems work only on the output of a parser —Charniak’s being the most preferred— some systems depend on many syntactic parsers. In the latter situation, either a system is a combination of many individual systems (each working with a different parser), or a system extracts features from many different parse trees while exploring the nodes of only one parse tree. Most systems have also considered named entities for extracting features. The main types"
W05-0620,W05-0636,0,0.441296,"e would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning ("
W05-0620,W05-0630,0,0.0403621,"redicates including argument boundaries. The second stage, reflected in column “label” of Table 4, is the proper labeling of selected candidates. Most of the systems used a two-step procedure consisting of first identifying arguments (e.g., 158 with a binary “null” vs. “non-null” classifier) and then classifying them. This is referred to as “i+c” in the table. Some systems address this phase in a single classification step by adding a “null” category to the multiclass problem (referred to as “c’). The methods performing a sequential tagging use a BIO tagging scheme (“bio”). As a special case, Moschitti et al. (2005) subdivide the “i+c” strategy into four phases: after identification, heuristics are applied to assure compatibility of identified arguments; and, before classifying arguments into roles, a preclassification into core vs. adjunct arguments is performed. Venkatapathy et al. (2005) use three labels instead of two in the identification phase : “null”, “mandatory”, and “optional”. Since arguments in a solution do not embed and most systems identify arguments as nodes in a hierarchical structure, non-embedding constraints must be resolved in order to generate a coherent argument labeling. The “embe"
W05-0620,W05-0637,0,0.0358171,"Missing"
W05-0620,W05-0631,0,0.0211991,"Missing"
W05-0620,W05-0638,0,0.106509,"igures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also repres"
W05-0620,J05-1004,0,0.932473,"ach target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc. Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information. In (Carreras and M`arquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004. Ten systems contributed to the task, which was evaluated using the PropBank corpus (Palmer et al., 2005). The best results were around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the f"
W05-0620,W05-0632,0,0.0658557,"he shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separa"
W05-0620,W05-0633,0,0.0218973,"Missing"
W05-0620,W05-0621,0,0.0322955,"earning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also represented UPC Chunker UPC Clauser Collins (1999) Char"
W05-0620,W04-3212,0,0.801657,"tly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words,"
W05-0620,W05-0639,0,0.251594,"6 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also represented UPC Chunker UPC"
W05-0620,J03-4003,0,\N,Missing
W05-0620,P04-1043,0,\N,Missing
W05-0620,W03-0419,0,\N,Missing
W06-2925,W06-2920,0,0.0741801,"Eisner (2000). We experiment with a large feature set that models: the tokens involved in dependencies and their immediate context, the surfacetext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information"
W06-2925,W02-1001,0,0.184514,", the scoring function is used to select the dependency label that maximizes the score. We take advantage of this two-step processing to introduce features for the scoring function that represent some of the internal dependencies of the span (see Section 3 for details). It has to be noted that the parsing algorithm we use does not score dependencies on top of every possible internal structure. Thus, by conditioning on features extracted from y we are making the search approximative. 2.3 Perceptron Learning As learning algorithm, we use Perceptron tailored for structured scenarios, proposed by Collins (2002). In recent years, Perceptron has been used in a number of Natural Language Learning works, such as in 182 partial parsing (Carreras et al., 2005) or even dependency parsing (McDonald et al., 2005). Perceptron is an online learning algorithm that learns by correcting mistakes made by the parser when visiting training sentences. The algorithm is extremely simple, and its cost in time and memory is independent from the size of the training corpora. In terms of efficiency, though, the parsing algorithm must be run at every training sentence. Our system uses the regular Perceptron working in prima"
W06-2925,C96-1058,0,0.052651,"ext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no spec"
W06-2925,P05-1012,0,0.831221,"rithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the language. Considering its simplicity, our system Model Let 1, . . . , L be the dependency labels, defined beforehand. Let x be a sentence of n words, x1 . . . xn . Finally, let Y(x) be the space of well-formed dependency trees for x. A dependency tree y ∈ Y(x) is a set of n dependencies of the form ["
W06-2925,W03-2403,0,\N,Missing
W06-2925,J03-4003,0,\N,Missing
W06-2925,dzeroski-etal-2006-towards,0,\N,Missing
W06-2925,W03-2405,0,\N,Missing
W06-2925,afonso-etal-2002-floresta,0,\N,Missing
W08-2102,D07-1101,1,0.845906,"y a combination of the two structures in figure 2. hh,m,li∈D(y) Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine hi, ηi to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = hE, Di is a derivation, then: • S(y) is a set of sibling dependencies. Each sibling dependency is a tuple hh, m, l, si. For each hh, m, l, si ∈ S the tuple hh, m, li is an element of D; there is one member of S for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place). • G(y) is a set of grandparent dependencies of type 1. Each type 1 grandparent dependency is a tuple hh, m, l, gi. There is one member of G for every member of D. The additional information, the inde"
W08-2102,P05-1022,0,0.906598,"th in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigr"
W08-2102,A00-2018,0,0.991402,"hms). (2) Use of a lower-order model for pruning. The O(n4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic struc"
W08-2102,P04-1014,0,0.0340546,"ere is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependenc"
W08-2102,W07-1202,0,0.0117506,"r may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass pa"
W08-2102,P97-1003,1,0.899312,"ould calculate 4.2 Extracting Derivations from Parse Trees In the experiments in this paper, the following three-step process was used: (1) derivations were extracted from a training set drawn from the Penn WSJ treebank, and then used to train a parsing model; (2) the test data was parsed using the resulting model, giving a derivation for each test data sentence; (3) the resulting test-data derivations were mapped back to Penn-treebank style trees, using the method described in section 2.1. To achieve step (1), we first apply a set of headfinding rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract µ(x, h, m, hVP VBD NPi) = µ1 (x, h, m, hVPi) × µ2 (x, h, m, hVBDi) ×µ3 (x, h, m, hNPi) Training the three models, and calculating the marginals, now has a grammar constant equal 14 PPK07 FKM08 CH2000 CO2000 PK07 this paper CJ05 H08 CO2000(s24) this paper (s24) precision – 88.2 89.5 89.9 90.2 91.4 – – 89.6 91.1 recall – 87.8 89.6 89.6 89.9 90.7 – – 88.6 89.9 F1 88.3 88.0 89.6 89.8 90.1 91.1 91.4 91.7 89.1 90.5 α 10−4 10−5 10−6 CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, f"
W08-2102,W02-1001,1,0.264623,"ing such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y ∗ for an input x is y ∗ = arg max w · f (x, y) y∈Y(x) (1) where Y(x) is the set of possible labels for the input x; f (x, y) ∈ Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibi"
W08-2102,P08-1109,0,0.229764,"approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak"
W08-2102,P08-1067,0,0.239331,"(x, h, m, hNPi) Training the three models, and calculating the marginals, now has a grammar constant equal 14 PPK07 FKM08 CH2000 CO2000 PK07 this paper CJ05 H08 CO2000(s24) this paper (s24) precision – 88.2 89.5 89.9 90.2 91.4 – – 89.6 91.1 recall – 87.8 89.6 89.6 89.9 90.7 – – 88.6 89.9 F1 88.3 88.0 89.6 89.8 90.1 91.1 91.4 91.7 89.1 90.5 α 10−4 10−5 10−6 CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 92.0 92.5 93.5 2nd stage oracle F1 speed 97.0 5:15 97.9 11:45 98.5 21:50 F1 91.1 91.6 92.0 Table 3: Effect of the beam size, controlled by α, on the performance of the parser on the development set (1,699 sentences). In each case α refers to the beam size used in both training and testing the model. “active”: percentage of dependencies that remain in the beam out of the total number of labeled dependencies (1,000 triple labels times 1,138,167 unl"
W08-2102,P99-1069,0,0.135563,"surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y ∗ for an input x is y ∗ = arg max w · f (x, y) y∈Y(x) (1) where Y(x) is the set of possible labels for the input x; f (x, y) ∈ Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is t"
W08-2102,P08-1068,1,0.868119,"parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use o"
W08-2102,E06-1011,0,0.855269,"g and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar"
W08-2102,P05-1012,0,0.897191,"table”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 2.1 10 The TAG-Based Parsing Model Derivations (a) (b) S clear from context, we will use ηi to refer to the spine in E corresponding to the i’th word. • D is a set of n dependencies. Each dependency is a tuple hh, m, li. Here h is the index of the head-word of the dependency, corresponding to the spine ηh which contains a node that is being adjoined into. m is the index of the modifier-word of the dependency, corresponding to the spine ηm which is being adjoine"
W08-2102,N07-1051,0,0.440491,"model for pruning. The O(n4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail i"
W08-2102,P07-1079,0,0.0224719,"ent parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 2.1 10 The TAG-Based Parsing Model Derivations (a) (b) S clear from context, we will use ηi to refer to the spine in E corresponding to the i’th word. • D is a set of n dependencies. Each dependency is a tuple hh, m, li. Here h is the index of the head-word of the dependency, corresponding to the spine ηh which contains a node that is being adjoined into. m is the index of the modifier-word of the dependency, corresponding to the spine ηm which is being adjoined into ηh . l is a label. The l"
W08-2102,H05-1102,0,0.049514,"rrors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon2 This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however t"
W08-2102,W04-3201,0,0.53722,"3 G) time, where n is the length of the sentence, and G is a grammar constant. The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model. The formalism that we describe"
W08-2102,W03-3023,0,0.573252,"paper 1st stage active coverage 0.07 97.7 0.16 98.5 0.34 99.0 s24 91.0 91.7 92.5 dictionary listing the spines that have been seen with this POS tag in training data; during parsing we only allow spines that are compatible with this dictionary. (For test or development data, we used the part-of-speech tags generated by the parser of (Collins, 1997). Future work should consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.) Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. 5 Experiments Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set. Sections 23 and 24 were used as test sets. The model was trained for 20 epochs with the averaged perceptron algorithm, w"
W08-2102,P00-1058,0,\N,Missing
W17-6305,P12-1092,0,0.0550078,"f the modes, where each projection matrix is a mapping from the original input vector space to a low-dimensional one, i.e. an embedding of the feature of the corresponding mode. One advantage of this approach is that there is no need to choose an unfolding. However, the optimization is non-convex. Beyond applications to PP attachment, word embeddings have been used for a number of prediction tasks. In most cases, embeddings of two or more words are composed by concatenation – see (Turian et al., 2010; Chen and Manning, 2014; Dyer et al., 2015) to name a few, or averaging (Socher et al., 2011; Huang et al., 2012). Compositions based on product of embeddings have been explored in tensor models, which we discuss next. 5.2 6 Conclusion We have described a simple PP attachment model based on tensor products of the word vectors in a PP attachment decision. We have established that the product of vectors improves over more simple compositions (based on sum or concatenation), while it remains computationally manageable due to the compact nature of word embeddings. In experiments on standard PP attachment datasets, our tensor models perform better than other methods using lexical information only, and are clo"
W17-6305,P08-1037,0,0.0217156,"he main sources of errors for syntactic parsers (Kummerfeld et al., 2012). Consider the examples in Figure 1. For the first case, the correct attachment is the prepositional phrase attaching to the restaurant, the noun. Whereas, in the second case the attachment site is the verb went. While the attachments are ambiguous, the ambiguity is more severe when unseen or infrequent words like Hudson are encountered. Classical approaches for the task exploit a wide range of lexical, syntactic, and semantic features and make use of knowledge resources like WordNet and VerbNet (Stetina and Nagao, 1997; Agirre et al., 2008; Zhao and Lin, 2004). We conduct experiments on several datasets and settings and show that this relatively simple multi-linear model can give performances comparable (and in some cases, even superior) than more complex neural network models that use the same information. Our results suggest that for the ∗ This work was carried out when the author was a PhD student at the Universitat Polit`ecnica de Catalunya 32 Proceedings of the 15th International Conference on Parsing Technologies, pages 32–43, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics 3 PP attach"
W17-6305,D12-1096,0,0.0128967,"ttachment. Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP attachment decision. In particular, our model will define parameters over the tensor product of these embeddings. We control the capacity of the model by imposing low-rank constraints on the corresponding tensor which we formulate as a convex loss minimization. Introduction The Prepositional Phrase (PP) attachment problem (Ratnaparkhi et al., 1994) is a classic ambiguity problem and is one of the main sources of errors for syntactic parsers (Kummerfeld et al., 2012). Consider the examples in Figure 1. For the first case, the correct attachment is the prepositional phrase attaching to the restaurant, the noun. Whereas, in the second case the attachment site is the verb went. While the attachments are ambiguous, the ambiguity is more severe when unseen or infrequent words like Hudson are encountered. Classical approaches for the task exploit a wide range of lexical, syntactic, and semantic features and make use of knowledge resources like WordNet and VerbNet (Stetina and Nagao, 1997; Agirre et al., 2008; Zhao and Lin, 2004). We conduct experiments on sever"
W17-6305,P14-2131,0,0.148318,"ord embeddings and a bias term. 4.3 This section presents a series of experiments using the classic binary setting by Ratnaparkhi et al. (1994). Word Embeddings As our models exploit pre-trained word embeddings, we perform experiments with a variety of types of word embeddings. We use two word embedding methods and estimate vectors using different data sources. The methods are: (a) Skipgram (Mikolov et al., 2013): We use the Skipgram model from word2vec, and induce embeddings of different dimensionalities: 50, 100 and 300. In all cases we use a window of size 5 during training.3 (b) Skip-dep (Bansal et al., 2014): This is essentially a Skip-gram model that uses dependency trees to define the context words during training, thus it captures syntactic correlations. We trained 50, 100 and 300 dimensional dependency-based embeddings, using the setting described in Bansal et al. (2014) however we made use of TurboParser (Martins et al., 2013) to obtain dependency trees from the source data 4 . For evaluations on English, we use the following data sources to train word embeddings: (a) BLLIP (Charniak et al., 2000), with ∼1.8 million sentences and ∼43 million tokens of Wall Street Journal text (and excludes P"
W17-6305,Q14-1043,0,0.0723671,"oordinate of a times the j-th coordinate of b results in the (i − 1) ∗ n1 + j coordinate of a ⊗ b. The tensor product model for PP attachment is as follows (see also Figure 2): f (h, p, m) = v &gt; h W [v p ⊗ v m ] , (2) 2 Ratnaparkhi et al. (1994) first proposed a formulation of PP attachment as a binary prediction problem. The task is as follows: we are given a fourway tuple hv, o, p, mi where v is a verb, o is a noun object, p is a preposition, and m is a modifier noun; the goal is to decide whether the prepositional phrase hp, mi attaches to the verb v or to the noun object o. More recently, Belinkov et al. (2014) proposed a generalization of PP attachment that considers multiple attachment candidates. Formally, we are given a tuple hH, p, mi, where H is a set of candidate attachment tokens, and the goal is to decide what is the correct attachment for the hp, mi prepositional phrase. The binary case corresponds to H = {v, o}. In this paper we use the generalized definition. Given a tuple hH, p, mi, the models we present in this paper compute the following prediction: h∈H , where W ∈ Rn×n is a matrix of parameters, taking the embedding of the attachment candidate h on the left, and the product of embedd"
W17-6305,P14-1130,0,0.0252001,"valuation and performs competively on out-of-domain dependency parsing datasets. 1 prep? I went to the restaurant by the Hudson by bike prep? prep? I went to the restaurant Figure 1: PP Attachment Ambiguity In recent years, word embeddings have become a very popular representation for lexical items (Mikolov et al., 2013; Pennington et al., 2014). The idea is that the dimensions of a word embedding capture lexical, syntactic, and semantic features of words –in essence, the type of information that is exploited in PP attachment systems. Recent work in dependency parsing (Chen and Manning, 2014; Lei et al., 2014) suggests that these embeddings can also be useful to resolve PP attachment ambiguities. We follow this last line of research and investigate the use of word embeddings for PP attachment. Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP attachment decision. In particular, our model will define parameters over the tensor product of these embeddings. We control the capacity of the model by imposing low-rank constraints on the corresponding tensor which we formulate as a convex loss minimization. Introdu"
W17-6305,C14-1017,1,0.9175,"owever, for PP attachment, it is common to have parameters for each preposition, and we can easily model this. Let P be the set of prepositions, and let ip ∈ R|P| be an indicator vector for preposition p. We can then set β(p, m) = ip ⊗ v m . Our model is now equivalent to writing: f (h, p, m) = v &gt; h W pvm . (5) 3.2 Low-rank Matrix Learning To learn the parameters we optimize the logistic loss with nuclear norm regularization (`∗ ), an objective that favors matrices W that have lowrank (Srebro et al., 2004). This regularized objective has been used in previous work to learn low-rank matrices (Madhyastha et al., 2014), and has been shown to be very effective for feature spaces that are highly conjunctive (Primadhanty (4) where we have one separate parameter matrix W p ∈ Rn×n per preposition p. This is the model proposed by Madhyastha et al. (2014). 34 et al., 2015), such as those that result from tensor products of word embeddings. In our basic model, the number of parameters is n3 (where n is the size of the individual embeddings). If W has rank k, then we can rewrite 2 W = U V &gt; where U ∈ Rn×k and V ∈ Rn ×k . Thus the score function can we rewritten as a kdimensional inner product between the left and ri"
W17-6305,D14-1082,0,0.620049,"ce on an out-of-domain evaluation and performs competively on out-of-domain dependency parsing datasets. 1 prep? I went to the restaurant by the Hudson by bike prep? prep? I went to the restaurant Figure 1: PP Attachment Ambiguity In recent years, word embeddings have become a very popular representation for lexical items (Mikolov et al., 2013; Pennington et al., 2014). The idea is that the dimensions of a word embedding capture lexical, syntactic, and semantic features of words –in essence, the type of information that is exploited in PP attachment systems. Recent work in dependency parsing (Chen and Manning, 2014; Lei et al., 2014) suggests that these embeddings can also be useful to resolve PP attachment ambiguities. We follow this last line of research and investigate the use of word embeddings for PP attachment. Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP attachment decision. In particular, our model will define parameters over the tensor product of these embeddings. We control the capacity of the model by imposing low-rank constraints on the corresponding tensor which we formulate as a convex loss mi"
W17-6305,P13-2109,0,0.133288,"tors using different data sources. The methods are: (a) Skipgram (Mikolov et al., 2013): We use the Skipgram model from word2vec, and induce embeddings of different dimensionalities: 50, 100 and 300. In all cases we use a window of size 5 during training.3 (b) Skip-dep (Bansal et al., 2014): This is essentially a Skip-gram model that uses dependency trees to define the context words during training, thus it captures syntactic correlations. We trained 50, 100 and 300 dimensional dependency-based embeddings, using the setting described in Bansal et al. (2014) however we made use of TurboParser (Martins et al., 2013) to obtain dependency trees from the source data 4 . For evaluations on English, we use the following data sources to train word embeddings: (a) BLLIP (Charniak et al., 2000), with ∼1.8 million sentences and ∼43 million tokens of Wall Street Journal text (and excludes PTB evaluation sets); (b) English Wikipedia5 , with ∼13.1 million sentences and ∼129 million tokens; (c) The New York Times portion of the GigaWord corpus, with ∼52 million sentences and ∼1, 253 million tokens. For Arabic, we used pre-trained 100dimensional word embeddings from the arTenTen corpus that are distributed with the da"
W17-6305,P15-1033,0,0.0194447,"in the form of lexical, syntactic and semantic features 40 for each of the modes, where each projection matrix is a mapping from the original input vector space to a low-dimensional one, i.e. an embedding of the feature of the corresponding mode. One advantage of this approach is that there is no need to choose an unfolding. However, the optimization is non-convex. Beyond applications to PP attachment, word embeddings have been used for a number of prediction tasks. In most cases, embeddings of two or more words are composed by concatenation – see (Turian et al., 2010; Chen and Manning, 2014; Dyer et al., 2015) to name a few, or averaging (Socher et al., 2011; Huang et al., 2012). Compositions based on product of embeddings have been explored in tensor models, which we discuss next. 5.2 6 Conclusion We have described a simple PP attachment model based on tensor products of the word vectors in a PP attachment decision. We have established that the product of vectors improves over more simple compositions (based on sum or concatenation), while it remains computationally manageable due to the compact nature of word embeddings. In experiments on standard PP attachment datasets, our tensor models perform"
W17-6305,P08-1028,0,0.0662498,"mation (Stetina and Nagao, 1997; Zhao and Lin, 2004; Nakashole and Mitchell, 2015). In our paper, we use word embeddings as the only source of lexical information. Previous work has explored word representations as extra features (Zhao and Lin, 2004). In our case, we define a model that exploits all conjunctions of the word vectors in an attachment decision. Our model is in fact a generalization of that of Madhyastha et al. (2014), as described in section 3.1. From that work, our application to PP attachment differs in using compact word embeddings as opposed to sparse distributional vectors. Mitchell and Lapata (2008) compared a variety of composition operations, including the tensor product, in the context of distributional lexical semantics. Closely related to our work is the approach by Belinkov et al. (2014), who use neural networks that compose the embeddings of the words in the PP attachment structure. Their model composes word embeddings by first concatenating vectors and then projecting to a low-dimensional vector using a non-linear hidden layer. This basic composition block is used to define several compositional models for PP attachment. One difference is that we represent tensor products of embe"
W17-6305,P15-2120,0,0.0521197,"Missing"
W17-6305,P15-1036,0,0.10452,"to as RRR dataset), which is extracted from the Penn TreeBank (PTB). The dataset contains 20,801 training samples of PP attachment tuples hv, o, p, mi. We preprocess the data as in previous work (Collins and Brooks, 1999): we lowercase all tokens, map numbers to a special token NUM and symbols to SYM. We use the development set from PTB, with 4,039 samples, to compare various configurations of our model. For testing, we consider several test sets proposed in the literature: a) The test set from the RRR dataset, with 3,097 samples from the PTB. b) The New York Times test set (NYT) released by (Nakashole and Mitchell, 2015). It contains 293 test samples. c) Wikipedia test set (WIKI) by (Nakashole and Mitchell, 2015). It contains 381 test samples from Wikipedia. Because the texts are not news articles, this is an out-of-domain test. Experiments Belinkov et al. (2014) Datasets. We use the datasets released by Belinkov et al. (2014) for This section presents experiments using tensor models for PP attachment. Our interest is to eval35 English and Arabic.2 These datasets follow the generalized version of PP attachment, and each sample consists of a preposition p, the noun below the preposition m, and a list of possib"
W17-6305,J93-1005,0,0.290194,"One of their templates is the conjunction of the head, preposition and modifier (and word embeddings of these), which is the focus case of our paper. While there are differences in the way we learn a low-rank tensor (see below), they show superior performance, probably due to the combination of different features. Our experiments, in contrast, offer a controlled study over different aspects of word embeddings and their product. Related Work Resolving PP Attachment Ambiguity Several approaches have been proposed for solving the PP attachment problem, including maximum likelihood with back-off (Hindle and Rooth, 1993; Collins and Brooks, 1999), and discriminative training (Ratnaparkhi et al., 1994; Olteanu and Moldovan, 2005), among others. A key part of such systems is the representation they use, in the form of lexical, syntactic and semantic features 40 for each of the modes, where each projection matrix is a mapping from the original input vector space to a low-dimensional one, i.e. an embedding of the feature of the corresponding mode. One advantage of this approach is that there is no need to choose an unfolding. However, the optimization is non-convex. Beyond applications to PP attachment, word emb"
W17-6305,W97-0109,0,0.510236,"y problem and is one of the main sources of errors for syntactic parsers (Kummerfeld et al., 2012). Consider the examples in Figure 1. For the first case, the correct attachment is the prepositional phrase attaching to the restaurant, the noun. Whereas, in the second case the attachment site is the verb went. While the attachments are ambiguous, the ambiguity is more severe when unseen or infrequent words like Hudson are encountered. Classical approaches for the task exploit a wide range of lexical, syntactic, and semantic features and make use of knowledge resources like WordNet and VerbNet (Stetina and Nagao, 1997; Agirre et al., 2008; Zhao and Lin, 2004). We conduct experiments on several datasets and settings and show that this relatively simple multi-linear model can give performances comparable (and in some cases, even superior) than more complex neural network models that use the same information. Our results suggest that for the ∗ This work was carried out when the author was a PhD student at the Universitat Polit`ecnica de Catalunya 32 Proceedings of the 15th International Conference on Parsing Technologies, pages 32–43, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Li"
W17-6305,H05-1035,0,0.0287266,"ese), which is the focus case of our paper. While there are differences in the way we learn a low-rank tensor (see below), they show superior performance, probably due to the combination of different features. Our experiments, in contrast, offer a controlled study over different aspects of word embeddings and their product. Related Work Resolving PP Attachment Ambiguity Several approaches have been proposed for solving the PP attachment problem, including maximum likelihood with back-off (Hindle and Rooth, 1993; Collins and Brooks, 1999), and discriminative training (Ratnaparkhi et al., 1994; Olteanu and Moldovan, 2005), among others. A key part of such systems is the representation they use, in the form of lexical, syntactic and semantic features 40 for each of the modes, where each projection matrix is a mapping from the original input vector space to a low-dimensional one, i.e. an embedding of the feature of the corresponding mode. One advantage of this approach is that there is no need to choose an unfolding. However, the optimization is non-convex. Beyond applications to PP attachment, word embeddings have been used for a number of prediction tasks. In most cases, embeddings of two or more words are com"
W17-6305,P10-1040,0,0.0692926,"such systems is the representation they use, in the form of lexical, syntactic and semantic features 40 for each of the modes, where each projection matrix is a mapping from the original input vector space to a low-dimensional one, i.e. an embedding of the feature of the corresponding mode. One advantage of this approach is that there is no need to choose an unfolding. However, the optimization is non-convex. Beyond applications to PP attachment, word embeddings have been used for a number of prediction tasks. In most cases, embeddings of two or more words are composed by concatenation – see (Turian et al., 2010; Chen and Manning, 2014; Dyer et al., 2015) to name a few, or averaging (Socher et al., 2011; Huang et al., 2012). Compositions based on product of embeddings have been explored in tensor models, which we discuss next. 5.2 6 Conclusion We have described a simple PP attachment model based on tensor products of the word vectors in a PP attachment decision. We have established that the product of vectors improves over more simple compositions (based on sum or concatenation), while it remains computationally manageable due to the compact nature of word embeddings. In experiments on standard PP at"
W17-6305,D14-1162,0,0.0821663,"ation and that a relatively simple multi-linear model that uses only word embeddings of lexical features can outperform more complex non-linear architectures that exploit the same information. Our proposed model gives the current best reported performance on an out-of-domain evaluation and performs competively on out-of-domain dependency parsing datasets. 1 prep? I went to the restaurant by the Hudson by bike prep? prep? I went to the restaurant Figure 1: PP Attachment Ambiguity In recent years, word embeddings have become a very popular representation for lexical items (Mikolov et al., 2013; Pennington et al., 2014). The idea is that the dimensions of a word embedding capture lexical, syntactic, and semantic features of words –in essence, the type of information that is exploited in PP attachment systems. Recent work in dependency parsing (Chen and Manning, 2014; Lei et al., 2014) suggests that these embeddings can also be useful to resolve PP attachment ambiguities. We follow this last line of research and investigate the use of word embeddings for PP attachment. Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP"
W17-6305,N16-1117,0,0.0193664,"ts of embeddings, which result in projected hidden conjunctions when the tensor has low rank. In contrast, projecting concatenated embeddings results in hidden disjunctions of the input coefficients. ... (a) The modifier and correct head are unseen in training. ... the return address for the letters to the Senators ... (b) The correct head is ambiguous. Figure 4: Examples from the Web Treebank development set, with the attachments predicted by the tensor product (solid green arc), the Stanford neural parser (dashed red arc) and the 3rd order TurboParser (dotted blue arc). 5 5.1 More recently, Yu et al. (2016) have also explored tensor models for PP attachment. Their focus is on representing standard feature templates (which are conjunctions of features of a variety of sources) as tensors, and on using low-rank constraints to favor parameter sharing among templates. One of their templates is the conjunction of the head, preposition and modifier (and word embeddings of these), which is the focus case of our paper. While there are differences in the way we learn a low-rank tensor (see below), they show superior performance, probably due to the combination of different features. Our experiments, in co"
W17-6305,P15-1013,1,0.847874,"que definition. A natural and simple way to impose low-rank constraints on a tensor is by first unfolding the tensor into a matrix, and let the rank of the tensor be the rank the unfolded matrix. With this one can apply low-rank constraints by regularization, using the nuclear norm (which is a convex relaxation for low-rank regularization). In practice, this leads to a simple convex optimization that uses an SVD routine to solve the core part of the problem. This technique has been used recently for several problems (Balle and Mohri, 2012; Quattoni et al., 2014; Madhyastha et al., 2014, 2015; Primadhanty et al., 2015). There are 2d ways to unfold a tensor of d modes. In our case, we have made the choice based on the application: we have grouped the preposition and modifier together. This choice has a clear computational advantage for the task: at prediction time, we can first project the prepositional phrase (which is fixed) to its low-dimensional representation, and then do the inner product with the projection of each head candidate. In general, one could try different unfoldings, or use multiple of them in a combination. Another popular approach to low-rank tensor learning is directly optimizing over a"
W17-6305,H94-1048,0,0.348308,"solve PP attachment ambiguities. We follow this last line of research and investigate the use of word embeddings for PP attachment. Different from previous works, we consider several types of compositions for the vector embeddings corresponding to the words involved in a PP attachment decision. In particular, our model will define parameters over the tensor product of these embeddings. We control the capacity of the model by imposing low-rank constraints on the corresponding tensor which we formulate as a convex loss minimization. Introduction The Prepositional Phrase (PP) attachment problem (Ratnaparkhi et al., 1994) is a classic ambiguity problem and is one of the main sources of errors for syntactic parsers (Kummerfeld et al., 2012). Consider the examples in Figure 1. For the first case, the correct attachment is the prepositional phrase attaching to the restaurant, the noun. Whereas, in the second case the attachment site is the verb went. While the attachments are ambiguous, the ambiguity is more severe when unseen or infrequent words like Hudson are encountered. Classical approaches for the task exploit a wide range of lexical, syntactic, and semantic features and make use of knowledge resources like"
W17-6305,W14-6111,0,0.0304938,"Missing"
W17-6305,W95-0103,0,\N,Missing
W17-6316,W08-2102,1,0.853407,"Missing"
W17-6316,P16-1231,0,0.0177774,"nal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tr"
W17-6316,P16-2006,0,0.231347,"Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inherent in head-driven model"
W17-6316,K15-1029,1,0.924659,"d parameter matrix, s represents the token in the stack (and its partial spine, if non-terminals have been added to it) and n represents the non-terminal symbol that we are adding to s; b is a bias term. As shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models. 5 Related Work Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to co"
W17-6316,D16-1001,0,0.112913,"Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inherent in head-driven model"
W17-6316,J17-2002,1,0.819626,"ed on the same architecture, with the addition of the node(n) action. The state of our algorithm presented in Section 3 is represented by the contents of the STACK, the BUFFER and a list with the history of actions with Stack-LSTMs. This state representation is then used to predict the next action to take. Composition: when the parser predicts a left-arc() or right-arc(), we compose the vector representation of the head and dependent elements; this is equivalent to what it is presented by Dyer et al. (2015). The 1 Set to 10 in our experiments We refer interested readers to (Dyer et al., 2015; Ballesteros et al., 2017). 2 117 Leftmost heads Leftmost h., no n-comp Rightmost heads Rightmost h., no n-comp SD heads SD heads, no n-comp SD heads, dummy spines YM heads LR 91.18 90.20 91.03 90.64 90.75 90.38 90.82 LP 90.93 90.76 91.20 91.24 91.11 90.58 90.84 F1 UAS (SD) 91.05 90.48 91.11 90.04 90.93 93.49 90.48 93.16 93.30 90.83 - identities (right or left) work better than those using linguistic ones. This suggests that the StackLSTM model already finds useful head-child relations in a constituent by parsing from the left (or right) even if there are non-local interactions. In this case, head rules are not useful."
W17-6316,de-marneffe-etal-2006-generating,0,0.114196,"Missing"
W17-6316,D16-1211,1,0.853748,"3 present results on the test, for constituent and dependency parsing respectively. As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al. (2016) and Kuncoro et al. (2017) are better than the rest, but compared to the rest our parser is at the same level or better. The most similar system is by Ballesteros and Carreras (2015) and our parser significantly improves the performance. Considering dependency parsing, our model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al. (2016), but it slightly improves the parser by Dyer et al. (2015) with static training. The systems that calculate dependencies by transforming phrase-structures with conversion rules and that use generative training are ahead compared to the rest. Table 1: Development results for spinal models, in terms of labeled precision (LP), recall (LR) and F1 for constituents, and unlabeled attachment score (UAS) against Stanford dependencies. Spinal models are trained using different head annotations (see text). Models labeled with “no ncomp” do not use node compositions. The model labeled with “dummy spines"
W17-6316,P04-1013,0,0.050495,"eads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together"
W17-6316,N16-1024,1,0.927541,"relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inhere"
W17-6316,E17-1117,1,0.889306,"Missing"
W17-6316,P15-1147,0,0.0424669,"Missing"
W17-6316,D16-1180,1,0.888238,"Missing"
W17-6316,Q17-1004,0,0.0362201,"Missing"
W17-6316,J93-2004,0,0.0607348,"Missing"
W17-6316,P16-2016,0,0.0155036,"; b is a bias term. As shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models. 5 Related Work Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LS"
W17-6316,W04-0308,0,0.0385643,"built. Spinal nodes are noted nji , where n is the non-terminal, i is the position of the head token, and j is the node level in the spine. s PP NP s Buffer β [And, their, . . . ] [their, suspicions, . . . ] [suspicions, of, . . . ] [of, each, . . . ] [of, each, . . . ] [of, each, . . . ] [of, each, . . . ] [each, other, . . . ] [each, other, . . . ] [other, run, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] Arc-Standard Spinal Parsing We use the transition system by Cross and Huang (2016a), which extends the arc-standard system by Nivre (2004) for constituency parsing in a headdriven way, i.e. spinal parsing. We describe it here for completeness. The parsing state is a tuple hβ, σ, δi, where β is a buffer of input tokens to be processed; σ is a stack of tokens with partial spines; and δ is a set of spinal dependencies. The operations are the following: • right-arc : hβ, σ:s+n:t, δi → hβ, σ:s+n, δ ∪(n, t)i This operation is symmetric to left-arc, it adds a spinal dependency from the top node n of the second spine in the stack to the top element t, which is reduced from the stack and becomes the rightmost child of n. At a high level,"
W17-6316,J08-4003,0,0.233926,"g a node above, or by reducing the spine with an arc operation with this spine as dependent). By this 116 Figure 2 shows an example of a derivation. The process is initialized with all sentence tokens in the buffer, an empty stack, and an empty set of dependencies. Termination is always attainable and occurs when the buffer is empty and there is a single element in the stack, namely the spine of the full sentence head. This transition system is correct and sound with respect to the class of projective spinal trees, in the same way as the arc-standard system is for projective dependency trees (Nivre, 2008). A derivation has 2n + m steps, where n is the sentence length and m is the number of constituents in the derivation. We note that the system naturally handles constituents of arbitrary arity. In particular, unary productions add one node in the spine without any children. In practice we put a hard bound on the number of consecutive unary productions in a spine1 , to ensure that in the early training steps the model does not generate unreasonably long spines. We also note there is a certain degree of non-determinism: left and right arcs (steps 3 and 4) can be mixed as long as the children of"
W17-6316,N03-1014,0,0.0397213,"reras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems th"
W17-6316,W05-1513,0,0.0473611,"8) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embeddi"
W17-6316,W07-2218,0,0.0303993,"thod is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system"
W17-6316,P15-1113,0,0.0785455,"using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This repre"
W17-6316,W03-3023,0,0.456891,"tituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves. 1 In experiments on the Penn Treebank, we look at how sensitive our method is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operati"
W17-6316,P13-1043,0,0.0217284,"r-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embedding for each elemen"
