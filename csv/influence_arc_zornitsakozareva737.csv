2004.jeptalnrecital-poster.21,da-silva-etal-2004-cluster,1,0.555605,"Missing"
2004.jeptalnrecital-poster.21,W02-2020,0,0.0642333,"Missing"
2004.jeptalnrecital-poster.21,E03-1015,0,0.0201045,"Missing"
2021.eacl-main.246,N18-2118,0,0.177052,"Missing"
2021.eacl-main.246,C16-1189,0,0.0603429,"Missing"
2021.eacl-main.246,D19-1402,1,0.598697,"s like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long text due to significant information loss in the projection operation. On another side, recurrent architectures represent long sentences well, but the sequential nature of the computations increases latency requirements and makes it difficult to launch on-device. Recently, self-attention based architectures like BERT (Devlin et al., 2018) have demonstrated remarkable success in capturing long term dependencies in the input text via purely attention mechanisms. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the or"
2021.eacl-main.246,N16-1062,0,0.0488543,"Missing"
2021.eacl-main.246,P19-1441,0,0.0209122,"approaches like BERT and even outperforms small-sized BERT variants with significant resource savings – reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16× less computation overhead, which is very impressive making it the fastest and smallest on-device model. 1 Introduction Transformers (Vaswani et al., 2017) based architectures like BERT (Devlin et al., 2018), XL-net ∗ Work done during internship at Google Work done while at Google AI † Zornitsa Kozareva Google Mountain View, CA, USA zornitsa@kozareva.com (Yang et al., 2019), GPT-2 (Radford et al., 2019), MT-DNN (Liu et al., 2019a), RoBERTA (Liu et al., 2019b) reached state-of-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva"
2021.eacl-main.246,2021.ccl-1.108,0,0.0599243,"Missing"
2021.eacl-main.246,W17-5530,0,0.041673,"Missing"
2021.eacl-main.246,D18-1092,1,0.912621,"Missing"
2021.eacl-main.246,P19-1368,1,0.790728,"et al., 2019b) reached state-of-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long tex"
2021.eacl-main.246,N19-1339,1,0.743191,"f-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long text due to significant infor"
2021.eacl-main.246,W04-2319,0,0.166867,"Missing"
2021.eacl-main.246,W18-5446,0,0.0742515,"Missing"
2021.eacl-main.250,P16-1046,0,0.0493217,"Missing"
2021.eacl-main.250,N19-1326,0,0.0354635,"Missing"
2021.eacl-main.250,D19-1402,1,0.811554,"to store any embedding matrices, since the projections are dynamically computed. This further enables user privacy by performing inference directly on device without sending user data (e.g., personal information) to the server. The embedding memory size is reduced from O(V ) to O(K), where V is the token vocabulary size and K &lt;&lt; V , is the binary LSH projection size. The projection representations can operate on either word or character level, and can be used to represent a sentence or a word depending on the NLP application. For instance, recently the Projection Sequence Networks (ProSeqo) (Kozareva and Ravi, 2019) used BiLSTMs over word-level projection representations to represent long sentences and achieved close to state-ofthe-art results in both short and long text classification tasks with varying amounts of supervision and vocabulary sizes. 2871 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2871–2876 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 2: Memory for V look-up vectors for each token vs storing K(&lt;&lt; V ) vectors and linearly combining them for token representation. We consider K = 1120 follow"
2021.eacl-main.250,P18-1157,0,0.0232683,"Missing"
2021.eacl-main.250,D14-1162,0,0.0940837,"requently colliding with the projection representations of other valid words. Overall, our studies showcase the robustness of LSH projection representations and resistance to misspellings. Due to their effectiveness, we believe that in the future, text representations using LSH projections can go beyond memory constrained settings and even be exploited in large scale models like Transformers (Vaswani et al., 2017). One way to remove the dependency on the vocabulary size is to learn a smaller matrix, WK ∈ Rd×K (K &lt;&lt; V ), as shown in Figure 2. For instance, 300-dimensional Glove embeddings, WV (Pennington et al., 2014b) with 400k vocabulary size occupies > 1 GB while the WK occupies only ≈ 1.2 MB for K = 1000 yielding a 1000× reduction in size. Instead of learning a unique vector for each token in the vocabulary, we can think of the columns of this WK matrix as a set of basis vectors and each token can be represented as a linear combination of basis vectors in WK as in Figure 1. We select the basis vectors from WK for each token with a fixed K-bit binary vector instead of a V -bit one-hot vector. The LSH Projection function, P (Figure 3)(Ravi, 2017, 2019) used in SGNN (Ravi and Kozareva, 2018) and ProSeqo"
2021.eacl-main.250,N18-1202,0,0.132859,"Missing"
2021.eacl-main.250,P19-1561,0,0.0217079,"et al., 2018) and train two-layer BiLSTMs (with both word-only and word-piece tokenization) for comparable accuracies with respect to the projection based models for a fair comparison. By word-only tokenization, we mean that models encode input words using a lookup table for each word. In our setup, we test the robustness of the neural classifiers by subjecting the corresponding test sets to common misspellings and omissions. We consider the following perturbation operations: randomly dropping, inserting, and swapping internal characters within words of the input sentences (Gao et al., 2018; Pruthi et al., 2019) 1 . We decide to perturb each word in a sentence with a fixed probability, Pperturb . Following (Ravi and Kozareva, 2018), we fix the projection dimension to K = 1120. 3.1 Datasets For evaluation purposes, we use the following text classification datasets for dialog act classification MRDA (Shriberg et al., 2004) and SWDA (Godfrey et al., 1992; Jurafsky et al., 1997), for intent prediction ATIS (T¨ur et al., 2010) and long text classification Amazon Reviews (Zhang et al., 2015) and Yahoo! Answers (Zhang et al., 2015). Table 1 shows the characteristics of each dataset. Tasks ATIS (Dialog act)"
2021.eacl-main.250,D18-1092,1,0.769169,"Missing"
2021.eacl-main.250,P19-1368,1,0.737077,"for text representations The dependency on vocabulary size V , is one of the primary reasons for the huge memory footprint of embedding matrices. It is common to represent a token, x by one-hot representation, Y(x) ∈ [0, 1]V and a distributed representation of the token is obtained by multiplying the one-hot representation with the embedding matrix, WV ∈ Rd×V as in UV (x) = WV ∗ Y(x)> ∈ Rd 1. Classification with perturbed inputs, where we show that Projection based networks 1) Projection Sequence Networks (ProSeqo) (Kozareva and Ravi, 2019) and 2) SelfGoverning Neural Networks (SGNN) models (Ravi and Kozareva, 2019) evaluated with perturbed LSH projections are robust to misspellings and transformation attacks, while we observe significant drop in performance for BiLSTMs and fine-tuned BERT classifiers. 2. Perturbation Analysis, where we test the robustness of the projection approach by directly analyzing the changes in representations when the input words are subject to the char misspellings. The purpose of this study is to examine if the words or sentences with misspelling are nearby in the projection space instead of frequently colliding with the projection representations of other valid words. Overall"
2021.eacl-main.250,N19-1339,1,0.845721,"re pre-trained word embeddings like Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014a) and ELMo (Peters et al., 2018). They help initialize the neural models, lead to faster convergence and have improved performance for numerous application such as Question Answering (Liu et al., 2018), Summarization (Cheng and Lapata, 2016), Sentiment Analysis (Yu et al., 2017). While word embeddings are powerful in unlimited constraints such as computation power ∗ Work done during internship at Google Work done while at Google AI † This led to interesting research by (Ravi and Kozareva, 2018; Sankar et al., 2019), who showed that word embeddings can be replaced with lightweight binary Locality-Sensitive Hashing (LSH) based projections learned on-the-fly. The projection approach surmounts the need to store any embedding matrices, since the projections are dynamically computed. This further enables user privacy by performing inference directly on device without sending user data (e.g., personal information) to the server. The embedding memory size is reduced from O(V ) to O(K), where V is the token vocabulary size and K &lt;&lt; V , is the binary LSH projection size. The projection representations can operate"
2021.eacl-main.250,W04-2319,0,0.0258796,"Missing"
2021.eacl-main.250,D13-1170,0,0.00703734,"Missing"
2021.eacl-main.250,D17-1056,0,0.0585607,"Missing"
2021.sigdial-1.7,2020.nlp4convai-1.6,0,0.453246,"ucted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watches while maintaining high performance. 2 Figure 1: Model architecture for SoDA On-device Sequence Labeling Neural Network. SoDa: On-device Sequence Labeling In this section, we describe the components of our SoDA architecture as shown in Figure 1. 2.1 O(V · d) which is infeasible for on-device applications where storage is limited. Here"
2021.sigdial-1.7,D16-1053,0,0.025745,"Missing"
2021.sigdial-1.7,Q16-1026,0,0.0288392,"rojections: We could use the dynamically constructed projection vector P(x) directly instead of embeddings to build the rest of our model. But to prevent the models from depending on static projection representations too strongly, we further condition or fine-tune the projections on specific sequence tagging task during training to learn better task-specific representations E(x). As noted, both projection conditioning operators result in a tiny number of additional model parameters M  V · d that are tuned during training. 2.1.2 Extending Character-level Representation using CNN Earlier work (Chiu and Nichols, 2016; Ma and Hovy, 2016) showed that CNNs can be effective to model morphological information within words and encode it within neural networks using character-level embeddings. However, these approaches typically compute both word-level (from pre-trained tables) and character-level embeddings (to model long sequence contexts) and combine them to construct word vector representations in their neural network architectures. However as we noted, word embedding lookup tables incur significant memory that are not suitable for on-device usecases. Previous results on sequence labeling (Ma and Hovy, 2016)"
2021.sigdial-1.7,D19-1402,1,0.894181,"Missing"
2021.sigdial-1.7,N19-1423,0,0.0637434,"Missing"
2021.sigdial-1.7,P15-1033,0,0.0239163,"pass through to the next time step. We use the following implementation in SoDA For an input sentence X = (x1 , x2 , ..., xn ) and corresponding sequence of projected embeddings E(X), where each et = [ePt · eCN Nt ] is a ddimensional vector, the LSTM layer in SoDA uses input, forget and output gates to compute a new state ht at time step t. For sequence tagging tasks, both left and right contexts are useful to represent information at any time step. Standard LSTM as well as other sequence models only account for previous history and know nothing about the future. We use a bi-directional LSTM (Dyer et al., 2015) to efficiently model both past and future information in our SoDA model. The only change required is 2.4 CRF Tagging Model For structured prediction tasks like sequence tagging, it is useful to model the dependencies between neighboring labels (Ling et al., 2015) and perform joint decoding of the label sequence for a given input sentence. For example, in sequence labeling tasks with BIO tagging scheme I-LOC label cannot follow B-PER. So, instead of decoding labels at every position separately, similarly to prior work, we perform joint decoding in our model using a condition random field (CRF)"
2021.sigdial-1.7,P19-1544,0,0.0384954,"Missing"
2021.sigdial-1.7,D15-1176,0,0.0872156,"Missing"
2021.sigdial-1.7,N18-2118,0,0.398401,"ngs of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watches while maintaining high performance. 2 Figure 1: Model architecture for SoDA On-device Sequence Labeling Neural Network. SoDa: On-device Sequence Labeling In t"
2021.sigdial-1.7,P16-1101,0,0.0277261,"the dynamically constructed projection vector P(x) directly instead of embeddings to build the rest of our model. But to prevent the models from depending on static projection representations too strongly, we further condition or fine-tune the projections on specific sequence tagging task during training to learn better task-specific representations E(x). As noted, both projection conditioning operators result in a tiny number of additional model parameters M  V · d that are tuned during training. 2.1.2 Extending Character-level Representation using CNN Earlier work (Chiu and Nichols, 2016; Ma and Hovy, 2016) showed that CNNs can be effective to model morphological information within words and encode it within neural networks using character-level embeddings. However, these approaches typically compute both word-level (from pre-trained tables) and character-level embeddings (to model long sequence contexts) and combine them to construct word vector representations in their neural network architectures. However as we noted, word embedding lookup tables incur significant memory that are not suitable for on-device usecases. Previous results on sequence labeling (Ma and Hovy, 2016) show that 58 charac"
2021.sigdial-1.7,D14-1162,0,0.0858935,"ng a sequence of words (x1 , x2 , ..., xn ), where xi refers to i-th word in the sentence, we first construct a sequence of vectors E(X) = (e1 , e2 , ..., en ) where ei denotes a vector representation for word xi . 2.1.1 Word Embedding via Projection Learning good representations for word types from the limited training data (as in slot extraction) is challenging since there are many parameters to estimate. Most neural network approaches for NLP tasks rely on word embedding matrices to overcome this issue. Almost every recent neural network model uses pre-trained word embeddings (e.g., Glove (Pennington et al., 2014), word2vec (Mikolov et al., 2013)) learned from a large corpus that are then plugged into the model and looked up to construct vector representations of individual words and optionally fine-tuned for the specific task. However, these embedding matrices are often huge and require lot of memory F(x) = {hf1 , w1 i, ..., hfK , wK i} 57 (1) where, fk represents each feature id (Fingerprint of the raw character skipgram) and wk its corresponding weight (observed count in the specific input x). We use locality-sensitive projections (Ravi, 2017) to dynamically transform the intermediate feature vector"
2021.sigdial-1.7,P19-1519,0,0.245982,"lf-attention and CRF layer. The resulting network is compact, does not require storing any pre-trained word embedding tables or huge parameters, and is suitable for on-device applications. 56 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devic"
2021.sigdial-1.7,D19-1214,0,0.200425,"ng network is compact, does not require storing any pre-trained word embedding tables or huge parameters, and is suitable for on-device applications. 56 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watch"
2021.sigdial-1.7,D18-1092,1,0.867006,"Missing"
2021.sigdial-1.7,P19-1368,1,0.839552,"mation needed for language understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embeddi"
2021.sigdial-1.7,2021.eacl-main.250,1,0.765897,"e understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embedding-free projections a"
2021.sigdial-1.7,2021.eacl-main.246,1,0.806502,"e understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embedding-free projections a"
C12-2059,agirre-de-lacalle-2004-publicly,0,0.0242015,"sed relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargra"
C12-2059,P07-1073,0,0.138603,"rb relations. KEYWORDS: verb harvesting, relation learning, information extraction, knowledge acquisition. Proceedings of COLING 2012: Posters, pages 599–610, COLING 2012, Mumbai, December 2012. 599 1 Introduction To be able to answer the questions “What causes ebola?”, “What are the duties of a medical doctor?”, “What are the differences between a terrorist and a victim?”, “Which are the animals that have wings but cannot fly?” one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Desp"
C12-2059,W09-2201,0,0.305652,"ome (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargraves, 2007), selectional preferences (Resnik, 1996; Ritter et al., 2010), question answering ("
C12-2059,W08-2207,0,0.0299909,"Missing"
C12-2059,D11-1142,0,0.590962,"of COLING 2012: Posters, pages 599–610, COLING 2012, Mumbai, December 2012. 599 1 Introduction To be able to answer the questions “What causes ebola?”, “What are the duties of a medical doctor?”, “What are the differences between a terrorist and a victim?”, “Which are the animals that have wings but cannot fly?” one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it"
C12-2059,D11-1133,0,0.0140113,"ecision and recall. To the extend to which it is possible, we conduct a human-based evaluation and we compare results to knowledge bases that have been extracted in a similar way (i.e., through pattern application over unstructured text). 5.1 Human-based Evaluation Among the most common approaches on evaluating the correctness of the harvested information is by using human annotators (Pantel and Pennacchiotti, 2006; Navigli et al., 2011). Conducting such evaluations is very important, because the harvested information is often used by QA, machine reading and IE systems (Ferrucci et al., 2010; Freedman et al., 2011). Since the evaluation of all 1, 067, 329 harvested terms is time consuming and costly, we decided to annotate for each term 100 verb relations and argument fillers. We conducted two separate annotations for the verbs and arguments, which resulted in 7200 annotations. We used two annotators who were instructed to mark as incorrect verbs (and argument fillers) that do not correspond to the term. For instance, “drugs affect” is marked as correct, while “drugs discuss” is marked as incorrect. We compute Accuracy as the number of Correct terms, divided by the total number of terms used in the anno"
C12-2059,N03-1011,0,0.0469778,"acts more verb-based relations. • We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations. The rest of the paper is organized as follows. Next, we present related work. Section 3 outlines the verb-based relation learner. Section 4 describes the data collection process. Section 5 reports on the experimental results. Finally, we conclude in Section 6. 2 Related Work Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008). Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specifi"
C12-2059,S07-1003,0,0.0596297,"Missing"
C12-2059,C92-2082,0,0.30749,"ccurately extracts more verb-based relations. • We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations. The rest of the paper is organized as follows. Next, we present related work. Section 3 outlines the verb-based relation learner. Section 4 describes the data collection process. Section 5 reports on the experimental results. Finally, we conclude in Section 6. 2 Related Work Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008). Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relat"
C12-2059,W09-1703,0,0.0231273,"Missing"
C12-2059,C10-1057,0,0.0247267,"Missing"
C12-2059,P10-1150,1,0.89517,"Missing"
C12-2059,P08-1119,1,0.925327,"erb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations. The rest of the paper is organized as follows. Next, we present related work. Section 3 outlines the verb-based relation learner. Section 4 describes the data collection process. Section 5 reports on the experimental results. Finally, we conclude in Section 6. 2 Related Work Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008). Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (Fader et al., 2011). 600 However, recently"
C12-2059,C00-1072,0,0.0589042,"Missing"
C12-2059,C02-1144,0,0.0457755,"Missing"
C12-2059,S07-1005,0,0.186615,"e and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargraves, 2007), selectional preferences (Resnik, 1996; Ritter et al., 2010), question answering (Ferrucci et al., 2010) and textual entailment (Szpektor et al., 2004). The question we address in this paper is: Is it possible to create a procedure which will go beyond existing techniques and learn in a semi-supervised manner for a given term all verb relations associated with it? The main contributions of the paper are: • We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term. • We establish the effectiveness of our approach through"
C12-2059,P06-1015,0,0.172229,"by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations. The rest of the paper is organized as follows. Next, we present related work. Section 3 outlines the verb-based relation learner. Section 4 describes the data collection process. Section 5 reports on the experimental results. Finally, we conclude in Section 6. 2 Related Work Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008). Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (Fader et al., 2011). 600 However, recently developed OpenIE systems like Te"
C12-2059,P02-1006,0,0.487518,"roach yields 12 times more verb relations. KEYWORDS: verb harvesting, relation learning, information extraction, knowledge acquisition. Proceedings of COLING 2012: Posters, pages 599–610, COLING 2012, Mumbai, December 2012. 599 1 Introduction To be able to answer the questions “What causes ebola?”, “What are the duties of a medical doctor?”, “What are the differences between a terrorist and a victim?”, “Which are the animals that have wings but cannot fly?” one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb"
C12-2059,P10-1044,0,0.0239724,"et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargraves, 2007), selectional preferences (Resnik, 1996; Ritter et al., 2010), question answering (Ferrucci et al., 2010) and textual entailment (Szpektor et al., 2004). The question we address in this paper is: Is it possible to create a procedure which will go beyond existing techniques and learn in a semi-supervised manner for a given term all verb relations associated with it? The main contributions of the paper are: • We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term. • We establish the effectiveness of our approach through human-based evaluation. • We conduct a comparative study wit"
C12-2059,P06-2094,0,0.060566,"Missing"
C12-2059,P06-1101,0,0.0782893,"Missing"
C12-2059,W04-3206,0,0.0341371,"any efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargraves, 2007), selectional preferences (Resnik, 1996; Ritter et al., 2010), question answering (Ferrucci et al., 2010) and textual entailment (Szpektor et al., 2004). The question we address in this paper is: Is it possible to create a procedure which will go beyond existing techniques and learn in a semi-supervised manner for a given term all verb relations associated with it? The main contributions of the paper are: • We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term. • We establish the effectiveness of our approach through human-based evaluation. • We conduct a comparative study with the verb-based relation extraction system ReVerb (Fader et al., 2011) and show that our a"
C12-2059,D08-1061,0,0.0571145,"Missing"
C12-2059,N03-1036,0,0.101952,"Missing"
C12-2059,P10-1013,0,0.0273081,"ition. Proceedings of COLING 2012: Posters, pages 599–610, COLING 2012, Mumbai, December 2012. 599 1 Introduction To be able to answer the questions “What causes ebola?”, “What are the duties of a medical doctor?”, “What are the differences between a terrorist and a victim?”, “Which are the animals that have wings but cannot fly?” one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004), ConceptNet (Liu and Singh, 2004), Yago (Suchanek et al., 2007), NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011). Despite the many efforts to date, yet there is no universal repository (or even a system), which"
D09-1099,P99-1008,0,0.0373775,"ut considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for"
D09-1099,P99-1016,0,0.035074,"performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retriev"
D09-1099,C02-1130,1,0.200375,"passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the cont"
D09-1099,N03-1011,0,0.526242,"hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for Animals and 1.5 GB f"
D09-1099,C92-2082,0,0.155172,"l the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym pattern"
D09-1099,P08-1119,1,0.498746,"xample of (when the first argument is an instance/example of the second). Section 2 describes our method for harvesting; Section 3 discusses related work; and Section 4 describes the experiments and the results. 2 Term and Relation Extraction using the Doubly-Anchored Pattern Our goal is to develop a technique that automatically ‘fills in’ the concept space in the taxonomy below any root concept, by harvesting terms through repeated web queries. We perform this in two alternating stages. 949 Stage 1: Basic-level/Instance concept collection: We use the Doubly-Anchored Pattern DAP developed in (Kozareva et al., 2008): DAP: [SeedTerm1] such as [SeedTerm2] and <X> which learns a list of basic-level concepts or instances (depending on whether SeedTerm2 expresses a basic-level concept or an instance).1 DAP is very reliable because it is instantiated with examples at both ‘ends’ of the space to be filled (the higher-level (root) concept SeedTerm1 and a basiclevel term or instance (SeedTerm2)), which mutually disambiguate each other. For example, “presidents” for SeedTerm1 can refer to the leader of a country, corporation, or university, and “Ford” for SeedTerm2 can refer to a car company, an automobile pioneer"
D09-1099,W02-1111,0,0.0154932,"g the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query,"
D09-1099,I08-2112,0,0.018117,"Missing"
D09-1099,N04-1041,0,0.165862,"ncept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus."
D09-1099,W97-0313,1,0.420074,"ept such as Concept and <X> If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent y"
D09-1099,P98-2182,0,0.111571,"If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers h"
D09-1099,W02-1028,1,0.342189,"seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learn"
D09-1099,C02-1114,0,0.0176554,"sion the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for"
D09-1099,C98-2177,0,\N,Missing
D10-1108,C02-1144,0,\N,Missing
D10-1108,C92-2082,0,\N,Missing
D10-1108,P06-1100,0,\N,Missing
D10-1108,P07-1030,0,\N,Missing
D10-1108,P06-1038,0,\N,Missing
D10-1108,P06-1015,0,\N,Missing
D10-1108,N03-1036,0,\N,Missing
D10-1108,P06-1101,0,\N,Missing
D10-1108,P08-1119,1,\N,Missing
D10-1108,D09-1099,1,\N,Missing
D10-1108,P09-1031,0,\N,Missing
D10-1108,P98-2127,0,\N,Missing
D10-1108,C98-2122,0,\N,Missing
D10-1108,N03-1011,0,\N,Missing
D10-1108,P10-1044,0,\N,Missing
D10-1108,P08-1078,0,\N,Missing
D11-1011,P08-1027,0,0.0858512,"f knowledge and are tedious to maintain over time, researchers have developed algorithms for automatic knowledge extraction from structured and unstructured texts. There is a substantial body of work on extracting is-a relations (Etzioni et al., 2005; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Lin and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have focused on the development of"
D11-1011,D09-1028,0,0.0185394,"e conclude in Section 6. 2 Related Work In the past decade, we have reached a good understanding on the knowledge harvesting technology from structured (Suchanek et al., 2007) and unstructured text. Researchers have harvested with varying success semantic lexicons (Riloff and Shepherd, 1997) and concept lists (Katz et al., 2003). Many efforts have also focused on the extraction of is-a relations (Hearst, 1992; Pas¸ca, 2004; Etzioni et al., 2005; Pas¸ca, 2007; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Etzioni et al., 2005; Davidov and Rappoport, 2009; Jain and Pantel, 2010). Various approaches have been proposed following the patterns of (Hearst, 1992) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). A substantial body of work has explored issues such as reranking the harvested knowledge using mutual information (Etzioni et al., 2005) and graph algorithms (Hovy et al., 2009), estimating the goodness of textmining seeds (Vyas et al., 2009), organizing the extracted information (Cafarella et al., 2007a; Cafarella et al., 2007b) and inducing term taxonomies with WordNet (Snow et al., 2006) or starting from scratch (Kozarev"
D11-1011,N03-1011,0,0.0847597,"eng University of Southern California 941 Bloom Walk, SAL 300 Los Angeles, CA 90089 shanghua@usc.edu Introduction Many natural language processing applications use and rely on semantic knowledge resources. Since manually built lexical repositories such as WordNet (Fellbaum, 1998) cover a limited amount of knowledge and are tedious to maintain over time, researchers have developed algorithms for automatic knowledge extraction from structured and unstructured texts. There is a substantial body of work on extracting is-a relations (Etzioni et al., 2005; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Lin and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the"
D11-1011,C92-2082,0,0.288979,"nce manually built lexical repositories such as WordNet (Fellbaum, 1998) cover a limited amount of knowledge and are tedious to maintain over time, researchers have developed algorithms for automatic knowledge extraction from structured and unstructured texts. There is a substantial body of work on extracting is-a relations (Etzioni et al., 2005; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Lin and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of exis"
D11-1011,D09-1099,1,0.925353,"Missing"
D11-1011,C10-1057,0,0.0107708,"lated Work In the past decade, we have reached a good understanding on the knowledge harvesting technology from structured (Suchanek et al., 2007) and unstructured text. Researchers have harvested with varying success semantic lexicons (Riloff and Shepherd, 1997) and concept lists (Katz et al., 2003). Many efforts have also focused on the extraction of is-a relations (Hearst, 1992; Pas¸ca, 2004; Etzioni et al., 2005; Pas¸ca, 2007; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Etzioni et al., 2005; Davidov and Rappoport, 2009; Jain and Pantel, 2010). Various approaches have been proposed following the patterns of (Hearst, 1992) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). A substantial body of work has explored issues such as reranking the harvested knowledge using mutual information (Etzioni et al., 2005) and graph algorithms (Hovy et al., 2009), estimating the goodness of textmining seeds (Vyas et al., 2009), organizing the extracted information (Cafarella et al., 2007a; Cafarella et al., 2007b) and inducing term taxonomies with WordNet (Snow et al., 2006) or starting from scratch (Kozareva and Hovy, 2010). Since"
D11-1011,D10-1108,1,0.702982,"; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have focused on the development of methods that can automatically augment the initially extracted class-instance pairs. (Pennacchiotti and Pantel, 2009) fused information from pattern-based and distributional systems using an ensemble method and a rich set of features derived from query logs, web-crawl and Wikipedia. (Talukdar et al., 2008) improved class-instance extractions exploring the relationships between t"
D11-1011,P08-1119,1,0.594371,"Missing"
D11-1011,C02-1144,0,0.0560977,"ver a limited amount of knowledge and are tedious to maintain over time, researchers have developed algorithms for automatic knowledge extraction from structured and unstructured texts. There is a substantial body of work on extracting is-a relations (Etzioni et al., 2005; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Lin and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have"
D11-1011,P06-1015,0,0.443838,"uthern California 941 Bloom Walk, SAL 300 Los Angeles, CA 90089 shanghua@usc.edu Introduction Many natural language processing applications use and rely on semantic knowledge resources. Since manually built lexical repositories such as WordNet (Fellbaum, 1998) cover a limited amount of knowledge and are tedious to maintain over time, researchers have developed algorithms for automatic knowledge extraction from structured and unstructured texts. There is a substantial body of work on extracting is-a relations (Etzioni et al., 2005; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Lin and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from re"
D11-1011,D09-1025,0,0.643841,"text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have focused on the development of methods that can automatically augment the initially extracted class-instance pairs. (Pennacchiotti and Pantel, 2009) fused information from pattern-based and distributional systems using an ensemble method and a rich set of features derived from query logs, web-crawl and Wikipedia. (Talukdar et al., 2008) improved class-instance extractions exploring the relationships between the classes and the instances to propagate the initial class-labels to the remaining unlabeled instances. Later on (Talukdar and Pereira, 2010) showed that class-instance extraction with label propagation can be further improved by adding semantic information 118 Proceedings of the 2011 Conference on Empirical Methods in Natural Langua"
D11-1011,W97-0313,0,0.0611353,". Section 4 describes the two graphtheoretic methods for class label propagation using an instance-instance network. Section 5 shows a comparative study between the proposed graph algorithms and different baselines. We also show a comparison between class-instance and instanceinstance graphs used in the label propagation. Finally, we conclude in Section 6. 2 Related Work In the past decade, we have reached a good understanding on the knowledge harvesting technology from structured (Suchanek et al., 2007) and unstructured text. Researchers have harvested with varying success semantic lexicons (Riloff and Shepherd, 1997) and concept lists (Katz et al., 2003). Many efforts have also focused on the extraction of is-a relations (Hearst, 1992; Pas¸ca, 2004; Etzioni et al., 2005; Pas¸ca, 2007; Kozareva et al., 2008), part-of relations (Girju et al., 2003; Pantel and Pennacchiotti, 2006) and general facts (Etzioni et al., 2005; Davidov and Rappoport, 2009; Jain and Pantel, 2010). Various approaches have been proposed following the patterns of (Hearst, 1992) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). A substantial body of work has explored issues such as reranking the harvested knowledge usi"
D11-1011,P06-1101,0,0.125726,"in and Pantel, 2001; Davidov and Rappoport, Among the most common knowledge acquisition approaches are those based on lexical patterns (Hearst, 1992; Etzioni et al., 2005; Kozareva et al., 2008) and clustering (Lin and Pantel, 2002; Davidov and Rappoport, 2008). While clustering can find instances and classes that are not explicitly expressed in text, they often may not generate the granularity needed by the users. In contrast, pattern-based approaches generate highly accurate lists, but they are constraint to the information matched by the pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have focused on the development of methods that can automatically augment the initially extracted class-instance pairs. (Pennacchiotti and Pantel, 2009) fused information from pattern-based and distributional systems using an ensemble method and a rich set of features derived from query logs, web-crawl and Wikipedia. (Talukdar et al., 2008) improved class-instance extractions exploring t"
D11-1011,P10-1149,0,0.225113,"Missing"
D11-1011,D08-1061,0,0.186671,"pattern and often suffer from recall. (Pas¸ca, 2004; Snow et al., 2006; Kozareva and Hovy, 2010) have shown that complete lists of semantic classes and instances are valuable for the enrichment of existing resources like WordNet and for taxonomy induction. Therefore, researchers have focused on the development of methods that can automatically augment the initially extracted class-instance pairs. (Pennacchiotti and Pantel, 2009) fused information from pattern-based and distributional systems using an ensemble method and a rich set of features derived from query logs, web-crawl and Wikipedia. (Talukdar et al., 2008) improved class-instance extractions exploring the relationships between the classes and the instances to propagate the initial class-labels to the remaining unlabeled instances. Later on (Talukdar and Pereira, 2010) showed that class-instance extraction with label propagation can be further improved by adding semantic information 118 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 118–128, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics in the form of instance-attribute edges derived from independently d"
D11-1011,P06-1107,0,0.0540823,"Missing"
D18-1092,C16-1189,0,0.563028,"of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has becom"
D18-1092,N16-1062,0,0.555306,"ut dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep ne"
D18-1092,I17-4004,0,0.10651,"Missing"
D18-1092,W17-5530,0,0.605857,"-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the biggest obstacles to deploy"
D18-1092,N06-1036,0,0.0153702,"lex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017). We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. 4.3 Acc. 33.7 47.3 71.0 77.0 74.0 69.9 73.1 74.2 73.8 80.1 83.1 Table 2: SwDA Dataset Results Method Majority Class (baseline)(Ortega and Vu, 2017) Naive Bayes (baseline) (Khanpour et al., 2016) Graphical Model (Ji and Bilmes, 2006) CNN (Lee and Dernoncourt, 2016) RNN+Attention(Ortega and Vu, 2017) RNN (Khanpour et al., 2016) SGNN: Self-Governing Neural Network (ours) Acc. 59.1 74.6 81.3 84.6 84.3 86.8 86.7 Table 3: MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). 5 Conclusion We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and"
D18-1092,D12-1136,1,0.724513,"cation models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and wo"
D18-1092,P13-1036,1,0.784609,"nt over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy. 1 There are multiple strategies to build lightweight text classification models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of u"
D18-1092,W04-2319,0,0.498594,"in standard neural networks for feasible training. The binary representation is significant since this results in a significantly compact representation for the Computing Projections. We employ an efficient randomized projection method for the projection step. We use locality sensitive hashing (LSH) (Charikar, 2002) to model the underlying projection operations in SGNN. LSH is typically used as a dimensionality reduction technique for clustering (Manning et al., 2008). LSH allows us to project similar inputs ~xi or interme806 • MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. projection network parameters that in turn considerably reduces the model size. SGNN Parameters. In practice, we employ T different projection functions Pj=1...T , each resulting in d-bit vector that is concatenated to form the projected vector ip in Equation 5. T and d vary depending on the projection network parameter configuration specified for P and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a single projection matrix of size T · d or T separate matrices o"
D18-1092,J00-3003,0,0.228023,"Missing"
D18-1092,P17-2083,0,0.0405934,"s to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the bigges"
D18-1105,C16-1189,0,0.226412,"of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has becom"
D18-1105,N16-1062,0,0.163316,"ut dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep ne"
D18-1105,I17-4004,0,0.0477525,"Missing"
D18-1105,W17-5530,0,0.143652,"-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the biggest obstacles to deploy"
D18-1105,N06-1036,0,0.0172243,"lex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017). We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. 4.3 Acc. 33.7 47.3 71.0 77.0 74.0 69.9 73.1 74.2 73.8 80.1 83.1 Table 2: SwDA Dataset Results Method Majority Class (baseline)(Ortega and Vu, 2017) Naive Bayes (baseline) (Khanpour et al., 2016) Graphical Model (Ji and Bilmes, 2006) CNN (Lee and Dernoncourt, 2016) RNN+Attention(Ortega and Vu, 2017) RNN (Khanpour et al., 2016) SGNN: Self-Governing Neural Network (ours) Acc. 59.1 74.6 81.3 84.6 84.3 86.8 86.7 Table 3: MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). 5 Conclusion We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and"
D18-1105,D12-1136,1,0.723046,"cation models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and wo"
D18-1105,P13-1036,1,0.777188,"nt over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy. 1 There are multiple strategies to build lightweight text classification models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of u"
D18-1105,W04-2319,0,0.05681,"in standard neural networks for feasible training. The binary representation is significant since this results in a significantly compact representation for the Computing Projections. We employ an efficient randomized projection method for the projection step. We use locality sensitive hashing (LSH) (Charikar, 2002) to model the underlying projection operations in SGNN. LSH is typically used as a dimensionality reduction technique for clustering (Manning et al., 2008). LSH allows us to project similar inputs ~xi or interme889 • MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. projection network parameters that in turn considerably reduces the model size. SGNN Parameters. In practice, we employ T different projection functions Pj=1...T , each resulting in d-bit vector that is concatenated to form the projected vector ip in Equation 5. T and d vary depending on the projection network parameter configuration specified for P and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a single projection matrix of size T · d or T separate matrices o"
D18-1105,J00-3003,0,0.0908289,"Missing"
D18-1105,P17-2083,0,0.0138891,"s to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the bigges"
D19-1402,D14-1179,0,0.0181301,"Missing"
D19-1402,N18-2118,0,0.283504,"in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent repository combined with the larger vocabulary. • AG: AG News corpus is a collection of news articles on the web, where each document has a title and a description field. We used the dataset from (Zhang et al., 2015). • Y!A: Yahoo! Answers is a text classification task with 10 diverse clas"
D19-1402,C16-1189,0,0.0739429,"thods, we do not apply any vocabulary pruning or pre-processing at all to the input sentences and documents, except for splitting tokens by space. We use a 2-layer ProSeqo neural network with recurrent projections of size T = 60, d = 14 and 256 hidden dimensions to represent state in each bidirectional LSTM-projection layer. We use character 7-grams (with 1-skip) and context size of 1 to model the projector described in Section 2.2. ProSeqo network is trained with SGD and Adam 3898 SWDA 88.3 83.1 80.1 73.8 73.1 - Model ProSeqo (our on-device model) SGNN(Ravi and Kozareva, 2018)(on-device) RNN(Khanpour et al., 2016) RNN+Attention(Ortega and Vu, 2017) CNN(Lee and Dernoncourt, 2016) GatedIntentAtten.(Goo et al., 2018) GatedFullAtten.(Goo et al., 2018) JointBiLSTM(Hakkani-Tur et al., 2016) Atten.RNN(Liu and Lane, 2016) MRDA 90.1 86.7 86.8 84.3 84.6 - ATIS 97.8 88.9 94.1 93.6 92.6 91.1 SNIPS 97.9 93.4 96.8 97.0 96.9 96.7 Table 2: Short Text Classification On-device Results & Comparisons to Prior Work optimizer (Kingma and Ba, 2014) over shuffled mini-batches of size 100. We did not do any additional dataset-specific tuning or processing. 4 STC: Short Text Classification Results This section focuses on the mu"
D19-1402,N15-1147,1,0.900338,"Missing"
D19-1402,W04-2319,0,0.225551,"919,336 Avg. Length 7 8 11 10 38 108 92 Train 193,000 78,000 4,478 13,084 120,000 1,400,000 3,000,000 Test 5,000 15,000 893 700 7,600 60,000 650,000 Table 1: Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al."
D19-1402,N16-1062,0,0.270212,"600 60,000 650,000 Table 1: Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent reposito"
D19-1402,W17-5530,0,0.348994,"Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent repository combined with the"
D19-1402,N16-1174,0,0.367912,"he larger vocabulary. • AG: AG News corpus is a collection of news articles on the web, where each document has a title and a description field. We used the dataset from (Zhang et al., 2015). • Y!A: Yahoo! Answers is a text classification task with 10 diverse classes: Society & Culture, Science & Mathematics, Health, Education & Reference, Computers & Internet, Sports, Business & Finance, Entertainment & Music, Family & Relationships and Politics & Government. Each document contains a question title, question context and best answers. We obtained the data from (Zhang et al., 2015). Note that (Yang et al., 2016) used a smaller test sample for evaluation. To present fair results, we will not compare to (Yang et al., 2016). • AMZN: Amazon review dataset is obtained from (Zhang et al., 2015). Resolving this task can be very helpful for product categorization (Kozareva, 2015). The corpora has reviews with ratings ranging from 1 to 5. Following prior work, we use 3,000,000 reviews for training and 650,000 reviews for testing. 3.2 Dataset Characteristics Table 1 shows the datasets characteristics such as the type of the task, number of classes, vocabulary size, average text length, train and test sizes. As"
D19-1402,D18-1092,1,0.753826,"Missing"
D19-1402,P19-1368,1,0.484541,"lop and deploy text and speech models that run inference entirely on-device and return accurate predictions in real-time. To make this work, the on-device models have to be very small in size (few kilobytes or megabytes) to fit on-devices like mobile phone, watch and IoT; to have low latency and be as accurate as server side models. These on-device challenges opened up new active area of research, which recently has shown promising results for speech (Lin et al., 2018), wake word detection (He et al., 2017), dialog act (Ravi and Kozareva, 2018) and intent prediction short text classification (Ravi and Kozareva, 2019). Previous attempts for on-device text classification used hashed &lt;input text, output class> pairs. Unfortunately, such models cannot handle examples that are not part of the lookup table and cannot generalize to more complex tasks like the ones we solve in this paper – long text classification, conversational intent prediction. (Bui et al., 2018) combined graphs with neural networks to improve the robustness of the model, but this resulted in large model sizes that cannot fit on-device. The most successful work is the self-governing neural network (SGNN) from (Ravi and Kozareva, 2018) and (SG"
D19-1402,N19-1339,1,0.547028,"e character-level, and (c) model longer context across words and sentences that is suited for both short and long-text classification tasks. The key differences to prior work (e.g., RNNs, CNNs) are that we use context information to dynamically compute text representations and learn compact neural networks for text classification that are suited for on-device applications. Our work departs significantly from recent on-device work (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) which leverage projection operations to learn efficient networks that can even be transferred to other tasks (Sankar et al., 2019b). However, they are limited to short-text classification tasks. In contrast, our model effectively uses contextual information and combines recurrent and projection operations to achieve efficiency and enable learning more powerful neural networks that generalize well and can solve more complex language classification tasks. As a result, our projection sequence network is able to dynamically project and learn efficient neural classifier models that are competitive with state-of-the-art RNNs and CNNs without the need to store or lookup any pre-trained embeddings. 2.1 The overall architecture"
D19-1506,N15-1011,0,0.0304001,"oves over prior CNN and LSTM models. • Applicability of PRADO for transfer learning, showed its robustness and ability to further improve performance in limited data scenarios. 2 Related Work Early work on text classification relied on sparse lexical features such as n-grams and linear classifiers (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). But with the raise of deep learning, various CNN and LSTM approaches lead to significant improvement in performance and reaching state-of-the-art results. (Kim, 2014) used CNN architecture from computer vision for text classification. (Johnson and Zhang, 2015), used high-dimensional one hot vector and later introduced character-level CNN that achieved even more competitive results. (Tai et al., 2015) used tree structured LSTM for classification, while (Tang et al., 2015) use CNN or LSTM to capture sentence vector followed by bidirectional gated recurrent network which composes the vectors to get a document vector. Recently, (Yang et al., 2016) introduced hierarchical attention neural networks, which captures document representation by incorporating knowledge of the document structure into the model. This approach reaches the best performance on lar"
D19-1506,C16-1189,0,0.110749,"reserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classification tasks such as dialog act and user intent understanding and outperformed prior RNN work (Khanpour et al., 2016; Ortega and Vu, 2017). In this work, we take one step further by proposing a novel projection attention neural network called PRADO . Unlike SGNN which has static projections, PRADO combines trainable projections with attention and convolutions allowing it to capture long range dependencies and making it a powerful and flexible approach for long text classification. We study the impact of different hyperparameters on accuracy vs model size. We also address the problem of producing compact architectures by develop a quantized version of PRADO . In a series of experimental evaluations on multip"
D19-1506,D14-1181,0,0.0193047,"as it has wide applications in spam detection (Jindal and Liu, 2007), product categorization (Kozareva, 2015), sentiment classification (Pang and Lee, 2008) and it also plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 20"
D19-1506,N15-1147,1,0.845654,"Missing"
D19-1506,W17-5530,0,0.0674511,"nable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classification tasks such as dialog act and user intent understanding and outperformed prior RNN work (Khanpour et al., 2016; Ortega and Vu, 2017). In this work, we take one step further by proposing a novel projection attention neural network called PRADO . Unlike SGNN which has static projections, PRADO combines trainable projections with attention and convolutions allowing it to capture long range dependencies and making it a powerful and flexible approach for long text classification. We study the impact of different hyperparameters on accuracy vs model size. We also address the problem of producing compact architectures by develop a quantized version of PRADO . In a series of experimental evaluations on multiple long text classific"
D19-1506,D15-1167,0,0.373154,"tion relied on sparse lexical features such as n-grams and linear classifiers (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). But with the raise of deep learning, various CNN and LSTM approaches lead to significant improvement in performance and reaching state-of-the-art results. (Kim, 2014) used CNN architecture from computer vision for text classification. (Johnson and Zhang, 2015), used high-dimensional one hot vector and later introduced character-level CNN that achieved even more competitive results. (Tai et al., 2015) used tree structured LSTM for classification, while (Tang et al., 2015) use CNN or LSTM to capture sentence vector followed by bidirectional gated recurrent network which composes the vectors to get a document vector. Recently, (Yang et al., 2016) introduced hierarchical attention neural networks, which captures document representation by incorporating knowledge of the document structure into the model. This approach reaches the best performance on large set of text classification tasks. The aforementioned prior work mostly focuses on building the best neural network model independent of any model size or memory constrains. However, recent work by (Ravi and Kozar"
D19-1506,D12-1136,1,0.702567,"ependent of any model size or memory constrains. However, recent work by (Ravi and Kozareva, 2018, 2019) show the importance of building ondevice text classification models that can preserve user privacy, provide consistent user experience and most importantly are compact in size, while yet achieving state-of-art results. Previously, to build lightweight text classification approaches (Ravi, 2013) proposed fast sampling techniques, while (Bui et al., 2018) incorporated deep neural networks with graph learning. While successful, such approaches resulted in large models for response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). To address the challenge of fitting huge deep neural network on-device, (Ravi and Kozareva, 2018) developed a novel self-governing neural networks (SGNNs) that learns projections on the fly leading to small models. SGNN was applied on short text classification tasks such as dialog act and user intent understanding and showed significant improvement over state-of-the-art RNN (Khanpour et al., 2016) and RNN with attention (Ortega and Vu, 2017) approaches. In this work, we take one step further by developing trainable projection network with attention mecha"
D19-1506,N16-1174,0,0.287816,"uct categorization (Kozareva, 2015), sentiment classification (Pang and Lee, 2008) and it also plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on"
D19-1506,P13-1036,1,0.820707,"document structure into the model. This approach reaches the best performance on large set of text classification tasks. The aforementioned prior work mostly focuses on building the best neural network model independent of any model size or memory constrains. However, recent work by (Ravi and Kozareva, 2018, 2019) show the importance of building ondevice text classification models that can preserve user privacy, provide consistent user experience and most importantly are compact in size, while yet achieving state-of-art results. Previously, to build lightweight text classification approaches (Ravi, 2013) proposed fast sampling techniques, while (Bui et al., 2018) incorporated deep neural networks with graph learning. While successful, such approaches resulted in large models for response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). To address the challenge of fitting huge deep neural network on-device, (Ravi and Kozareva, 2018) developed a novel self-governing neural networks (SGNNs) that learns projections on the fly leading to small models. SGNN was applied on short text classification tasks such as dialog act and user intent understanding and showed significant i"
D19-1506,D18-1092,1,0.88315,"Missing"
D19-1506,P19-1368,1,0.847919,"plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classi"
D19-1506,N19-1339,1,0.820034,"lication, in reality it is a look-up of the corresponding row in the embedding matrix as δi is modeled using the Dirac delta function. Embeddings via Trainable Projections: Our approach PRADO replaces this embedding with a projection approach to build the word encoder. Instead of mapping wi to δi , we map it to fi using a projection operator P. Recent work (Ravi, 2017; Ravi and Kozareva, 2018; Ravi, 2019) has shown that projection-based neural approaches can help train compact neural networks that achieve good performance on certain language tasks. These networks learn robust representations (Sankar et al., 2019a) that can be also transferred to other tasks (Sankar et al., 2019b). We follow a similar strategy but unlike the static projections used in these works, we propose a new type of projection that decomposes the operation and makes the projection trainable, leading to more powerful encoders capable of capturing contextual information for long-text classification while maintaining a very low memory footprint. Our method does not rely on a fixed vocabulary. The projection operator we use in this work first fingerprints the words and extracts B bit features from the fingerprint. The word vectors e"
D19-1506,P15-1150,0,0.0950526,"Missing"
da-silva-etal-2004-cluster,W02-2020,0,\N,Missing
da-silva-etal-2004-cluster,E03-1015,0,\N,Missing
da-silva-etal-2004-cluster,E03-1082,0,\N,Missing
E06-3004,M98-1014,0,0.00885853,"er domain. This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data. 1 Introduction Automatic information extraction and information retrieval concerning particular person, location, organization, title of movie or book, juxtaposes to the Named Entity Recognition (NER) task. NER consists in detecting the most silent and informative elements in a text such as names of people, company names, location, monetary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1 . (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 20"
E06-3004,W02-2004,0,0.0139429,"Missing"
E06-3004,E03-1038,0,0.0521334,"Missing"
E06-3004,W03-0429,0,0.0199605,". Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1 . (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1 For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a)"
E06-3004,P04-1075,0,0.0161507,"agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities. We decided to work with available NE annotated corpora in order to conduct an exhaustive and comparative NER study when labeled and unlabeld data is present. For our bootstrapping experiment, we simply ignored the presence"
E06-3004,W99-0613,0,0.598474,"Sang, 2002b), have been used1 . (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1 For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) toward the usage of semi-supervised techniques. These techniques are needed, as we live in a multilingual society and access to information from various language sources is reality. The development of NER systems for languages other than English commenced. This paper presents the development of a Spanish Named Recognition system based on machine learning approach. For it no morphologic or syntactic information was used. However, we propose and incorporate a very simple method for automatic gazetteer2 construction. Such method can be easily ad"
E06-3004,W03-0433,0,0.0141488,"Missing"
E06-3004,W03-0428,0,0.0236204,"tary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1 . (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1 For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003),"
E06-3004,P95-1026,0,0.0833724,"eme we follow is described below. 1. for iteration = 0 . . . n do 2. pool 1000 examples from unlabeled data; 3. annotate all 1000 examples with classifier C1 and C2 ; 4. for each of the 1000 examples compare classes of C1 and C2 ; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities"
E06-3004,W02-2025,0,\N,Missing
E06-3004,W03-0419,0,\N,Missing
E06-3004,P02-1046,0,\N,Missing
E06-3004,W02-2024,0,\N,Missing
E06-3004,M95-1006,0,\N,Missing
E06-3004,M95-1011,0,\N,Missing
E06-3004,A97-1029,0,\N,Missing
N10-1087,P07-1030,0,0.0186868,"Missing"
N10-1087,C96-1079,0,0.01598,"Missing"
N10-1087,C92-2082,0,0.177164,"Missing"
N10-1087,W09-1703,0,0.064246,"Missing"
N10-1087,P08-1119,1,0.423713,"Missing"
N10-1087,P08-1003,0,0.0363013,"Missing"
N10-1087,D08-1061,0,0.0387437,"Missing"
N10-1087,D09-1098,0,\N,Missing
N10-1087,P08-1000,0,\N,Missing
N19-1339,N09-1003,0,0.0691329,"ification tasks. Similarity tests the ability to capture words, while language modeling and classification warrant the ability to transfer the neural projections. 4.2.1 Similarity Task We evaluate our NP-SG word representations on 4 different widely used benchmark datasets for measuring similarities. Dataset: MTurk-287 (Radinsky et al., 2011) has 287 pairs of words and was constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. WS353 (Finkelstein et al., 2001) has 353 pairs of similar English words rated by humans and is further split into WS353-SIM. WS353-REL (Agirre et al., 2009) captures different types of similarities and relatedness. RWSTANFORD (Luong et al., 2013) has 2034 rare word pairs sampled from different frequency bins. Evaluation: For all the datasets, we compute the Spearmans rank correlation coefficient between the rankings computed by skip-gram models (baseline SG and NP-SG) and the human rankings. We use cosine similarity metric to measure word similarity. Results: Table 1 shows that NP-SG, with significantly smaller number of parameters comes close to the skip-gram model (SG) and even outperforms it with 2.5x-10x compression. NP-SG gets better represe"
N19-1339,W13-3512,0,0.135344,"Missing"
N19-1339,D14-1162,0,0.0823304,"ferred to other NLP tasks. For qualitative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work. 1 Introduction Pre-trained word representations are at the core of many neural language understanding models. Among the most popular and widely used word embeddings are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018). The biggest challenge with word embedding is that they require lookup and a large memory footprint, as we have to store one entry (d-dim vector) per word and it blows up. In parallel, the tremendous success of deep learning models and the explosion of mobile, IoT de∗ Work done during internship at Google. vices coupled together with the growing user privacy concerns have led to the need for deploying deep learning models on-device for inference. This has led to new research in compressing large and complex deep learning models for low power ondevice deployment."
N19-1339,N18-1202,0,0.0489516,"itative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work. 1 Introduction Pre-trained word representations are at the core of many neural language understanding models. Among the most popular and widely used word embeddings are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018). The biggest challenge with word embedding is that they require lookup and a large memory footprint, as we have to store one entry (d-dim vector) per word and it blows up. In parallel, the tremendous success of deep learning models and the explosion of mobile, IoT de∗ Work done during internship at Google. vices coupled together with the growing user privacy concerns have led to the need for deploying deep learning models on-device for inference. This has led to new research in compressing large and complex deep learning models for low power ondevice deployment. Recently, (Ravi and Kozareva,"
N19-1339,D18-1092,1,0.377902,"Missing"
P08-1119,P99-1008,0,0.881859,"Missing"
P08-1119,P99-1016,0,0.197953,"ar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic c"
P08-1119,P06-1038,0,0.114438,"d outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly sup"
P08-1119,C02-1130,1,0.642619,"ses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The"
P08-1119,N03-1011,0,0.750933,"c class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local an"
P08-1119,C92-2082,0,0.727441,"formation (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The work most closely related to ours is Hearst’s early work on hyponym learning (Hearst, 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer t"
P08-1119,C02-1144,0,0.168229,"ing high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH"
P08-1119,P98-2127,0,0.33935,"ses, achieving high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic clas"
P08-1119,W02-1111,0,0.0186934,"ogy learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <"
P08-1119,N04-1041,0,0.490325,"create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups"
P08-1119,W02-1017,1,0.837817,"ed on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around"
P08-1119,W97-0313,1,0.630135,"sses desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web"
P08-1119,P98-2182,0,0.755248,"th just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. 1 Introduction Knowing the semantic classes of words (e.g., “trout” is a kind of FISH) can be extremely valuable for many natural language processing tasks. Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology). (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet. Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains. A variety of methods have been developed for automatic semantic class identification, under the rubrics of lexical acquisition, hyponym acquisition, semantic lexicon induction, semantic class learning, and web-based information extraction. Many of these approaches employ surface-level patterns to identify words and their"
P08-1119,E06-1003,0,0.031708,"nstruction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic cl"
P08-1119,W02-1028,1,0.802786,"nd Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web"
P08-1119,C02-1114,0,0.733507,", 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer to these as hyponym patterns. Pasca’s previously mentioned system (Pas¸ca, 2004) applies hyponym patterns to the web and acquires contexts around them. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the web and then evaluates them further by computing mutual information scores based on web queries. The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. However, their graph is based entirely on syntactic relations between words, while our graph captures the ability of instances to find each other in a hyponym pattern based on web querying, without any part-ofspeech tagging or parsing. 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on web pages, but used a precompiled corpus of downloaded web pages. 3 Semantic Class Learning with Hyponym Pattern Linkage Graphs 3.1 A Doubly-Anchored Hyponym Pattern Our work was motivated by"
P08-1119,C98-2177,0,\N,Missing
P08-1119,C98-2122,0,\N,Missing
P10-1150,P08-1004,0,0.0481153,"actic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) introduce the so-called doublyanchored pattern (DAP) that has two anchor seed positions “htypei such as hseedi and *”, plus one open position for the terms to be learned. Learned terms can then be replaced into the seed position automatically, creating a recursive procedure that is reportedly much more accurate and has much higher final yield. (Kozareva et al., 2008; Hovy et al., 200"
P10-1150,P08-1119,1,0.663847,"he given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focus"
P10-1150,C02-1144,0,0.0192029,"ty and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff a"
P10-1150,P07-1030,0,0.00963391,"r) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new lexicosyntactic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) introduce the so-called doublyanchored pattern (DAP) that"
P10-1150,N04-1041,0,0.05365,"icons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing"
P10-1150,C02-1130,1,0.661444,"(Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing errors. Over time this becomes problematic for the bootstrapping process and leads"
P10-1150,N03-1011,0,0.476956,"duction Building and maintaining knowledge-rich resources is of great importance to information extraction, question answering, and textual entailment. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely, the types of arguments that may fill a given"
P10-1150,C92-2082,0,0.524635,"sity of the harvested knowledge. 1 Introduction Building and maintaining knowledge-rich resources is of great importance to information extraction, question answering, and textual entailment. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely"
P10-1150,P06-1100,0,0.370756,"domain these approaches are both very promising, but when tackling an unbounded number of relations they are unrealistic. The quality of clustering decreases as the domain becomes more continuously varied and diverse, and it has proven difficult to create collections of effective patterns and high-yield seeds manually. In addition, the output of most harvesting systems is a flat list of lexical semantic expressions such as “New York is-a city” and “virus causes flu”. However, using this knowledge in inference requires it to be formulated appropriately and organized in a semantic repository. (Pennacchiotti and Pantel, 2006) proposed an algorithm for automatically ontologizing semantic relations into WordNet. However, despite its high precision entries, WordNet’s limited coverage makes it impossible for relations whose arguments are not present in WordNet to be incorporated. One would like a procedure that dynamically organizes and extends 1482 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482–1491, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics its semantic repository in order to be able to accommodate all newly-harvested infor"
P10-1150,D09-1099,1,0.741071,"l, 2006) made an attempt to ontologize the harvested arguments of is-a, part-of, and cause relations. They mapped each argument of the relation into WordNet and identified the senses for which the relation holds. Unfortunately, despite its very high precision entries, WordNet is known to have limited coverage, which makes it impossible for algorithms to map the content of a relation whose arguments are not present in WordNet. To surmount this limitation, we do not use WordNet, but employ a different method of obtaining superclasses of a filler term: the inverse doubly-anchored patterns DAP−1 (Hovy et al., 2009), which, given two arguments, harvests its supertypes from the source corpus. (Hovy et al., 2009) show that DAP−1 is reliable and it enriches WordNet with additional hyponyms and hypernyms. 3 Recursive Patterns A singly-anchored pattern contains one example of the seed term (the anchor) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new l"
P10-1150,W09-1703,0,0.744377,"ng (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing errors. Over time this becomes problematic for the bootstrapping process and leads to significant deterioration in performance. (Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. How1483 ever, if we have at our disposal a repository of semantic relations with their selectional restrictions, the problem addressed in (Igo and Ri"
P10-1150,W97-0313,0,0.0634208,"are supertypes of the first argument and city and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), a"
P10-1150,P98-2182,0,0.014557,"ts of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn und"
P10-1150,D08-1061,0,0.0219601,"attern contains one example of the seed term (the anchor) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new lexicosyntactic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) intro"
P10-1150,W02-1028,0,0.530436,"on and the pattern X flies to Y, automatically determining that John, Prague) and (John, conference) are two valid filler instance pairs, that (RyanAir, Prague) is another, as well as that person and airline are supertypes of the first argument and city and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et"
P10-1150,N07-4013,0,0.553265,"ctional restrictions of semantic relations be learned automatically from the Web with minimal effort using lexico-syntactic recursive patterns? The contributions of the paper are as follows: • A novel representation of semantic relations using recursive lexico-syntactic patterns. • An automatic procedure to learn the selectional restrictions (arguments and supertypes) of semantic relations from Web data. has proven a difficult task to manually find effectively different variations and alternative patterns for each relation. In contrast, when research focuses on any relation, as in TextRunner (Yates et al., 2007), there is no standardized manner for re-using the pattern learned. TextRunner scans sentences to obtain relation-independent lexico-syntactic patterns to extract triples of the form (John, fly to, Prague). The middle string denotes some (unspecified) semantic relation while the first and third denote the learned arguments of this relation. But TextRunner does not seek specific semantic relations, and does not re-use the patterns it harvests with different arguments in order to extend their yields. • An exhaustive human-based evaluation of the harvested knowledge. • A comparison of the results"
P10-1150,P06-1107,0,0.0242775,"text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely, the types of arguments that may fill a given relation, such as person live-in city and airline fly-to location). Selectional restrictions constrain the possible fillers of a relation, and hence the possible contexts in which the patterns expressing that relation can participate in, thereby enabling sense disambiguation of both the fil"
P10-1150,C98-2177,0,\N,Missing
P10-1150,D09-1098,0,\N,Missing
P11-1162,N03-1011,0,0.268649,"cially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and G"
P11-1162,C92-2082,0,0.0658644,"illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, th"
P11-1162,P10-1150,1,0.863755,"ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem"
P11-1162,N10-1087,1,0.935523,"ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem"
P11-1162,P08-1119,1,0.884429,"Missing"
P11-1162,C02-1144,0,0.0450675,"inking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff an"
P11-1162,P06-1015,0,0.0244736,"eb, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 200"
P11-1162,P02-1006,1,0.626575,"l and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1 . Text mining is a process of network traversal, and faces the standard problems of handling"
P11-1162,W97-0313,0,0.256838,"el, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different"
P11-1162,P06-1107,0,0.0139726,"describe the general harvesting procedure, and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web doc"
P11-1162,P10-1149,0,\N,Missing
P13-1067,D09-1062,0,0.0196962,"007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Choi and Cardie, 2009) even documents (Pang and Lee, 2008). Multiple techniques have been employed, from various machine learning classifiers, to clustering and topic models. Various domains and textual sources have been analyzed such as Twitter, Blogs, Web documents, movie and product reviews (Turney, 2002; Kennedy and Inkpen, 2005; Niu et al., 2005; Pang and Lee, 2008), but yet what is missing is affect analyzer for metaphor-rich texts. While the affect of metaphors is well studied from its linguistic and psychological aspects (Blanchette et al., 2001; Tomlinson and Love, 2006; Crawdord, 2009), to our knowledge t"
P13-1067,esuli-sebastiani-2006-sentiwordnet,0,0.014194,"affect (sentiment analysis) of texts (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Choi and Cardie, 2009) even documents (Pang and Lee, 2008). Multiple techniques have been employed, from various machine learning classifiers, to clustering and topic models. Various domains and textual sources have been analyzed such as Twitter, Blogs, Web documents, movie and product reviews (Turney, 2002; Kennedy and Inkpen, 2005; Niu et al., 2005; Pang and Lee, 2008), but yet what is missing is affect analyzer for metaphor-rich texts. While the affect of metaphors is well studied from its linguistic and psychological aspects (Blanch"
P13-1067,P11-2102,0,0.0306979,"czik and Pennebaker, 2010), which has 64 word categories corresponding to different classes like emotional states, psychological processes, personal concerns among other. Each category contains a list of words characterizing it. For instance, the LIWC category discrepancy contains words like should, could among others, while the LIWC category inhibition contains words like block, stop, constrain. Previously LIWC was successfully used to analyze the emotional state of bloggers and tweeters (Quercia et al., 2011) and to identify deception and sarcasm in texts (Ott et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011). When LIWC analyzes texts it generates statistics like number of words found in category Ci divided by the total number of words in the text. For our metaphor polarity task, we use LIWC’s statistics of all 64 categories and feed this information as features for the machine learning classifiers. LIWC repository contains conceptual categories (dictionaries) both for the English and Spanish languages. LIWC Evaluation and Results: In our experiments LIWC is applied to English and Spanish metaphor-rich texts since the LIWC category dictionaries are available for both languages. Table 3 shows the o"
P13-1067,shutova-teufel-2010-metaphor,0,0.198489,"a verbal battle and the structure of an argument (attack, defense) reflects this (Lakoff and Johnson, 1980). As we mentioned before, there has been a lot of work on the automatic identification of metaphors (Wilks, 2007; Shutova et al., 2010) and their mapping into conceptual space (Shutova, 2010a; Shutova, 2010b), however these are beyond the scope of this paper. Instead we focus on an equally interesting, challenging and important problem, which concerns the automatic identification of affect carried by metaphors. To conduct our study, we use human annotators to collect metaphor-rich texts (Shutova and Teufel, 2010) and tag each metaphor with its corresponding polarity (Positive/Negative) and valence [−3, +3] scores. The next sections describe the affect polarity and valence tasks we have defined, the collected and annotated metaphor-rich data for each one of the English, Spanish, Russian and Farsi languages, the conducted experiments and obtained results. 683 4 Task A: Polarity Classification 4.1 Problem Formulation Task Definition: Given metaphor-rich texts annotated with Positive and Negative polarity labels, the goal is to build an automated computational affect model, which can assign to previously"
P13-1067,C04-1200,0,0.2246,"taphors Although there are different views on metaphor in linguistics and philosophy (Black, 1962; Lakoff and Johnson, 1980; Gentner, 1983; Wilks, 2007), the common among all approaches is the idea of an interconceptual mapping that underlies the production of metaphorical expressions. There are two concepts or conceptual domains: the target (also called topic in the linguistics literature) and the source (or vehicle), and the existence of a link between them gives rise to metaphors. Related Work A substantial body of work has been done on determining the affect (sentiment analysis) of texts (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wils"
P13-1067,C10-1113,0,0.0190031,"l models for polarity and valence identification in metaphor-rich texts is still The texts “Your claims are indefensible.” and “He attacked every weak point in my argument.” do not directly talk about argument as a war, however the winning or losing of arguments, the attack or defense of positions are structured by the concept of war. There is no physical battle, but there is a verbal battle and the structure of an argument (attack, defense) reflects this (Lakoff and Johnson, 1980). As we mentioned before, there has been a lot of work on the automatic identification of metaphors (Wilks, 2007; Shutova et al., 2010) and their mapping into conceptual space (Shutova, 2010a; Shutova, 2010b), however these are beyond the scope of this paper. Instead we focus on an equally interesting, challenging and important problem, which concerns the automatic identification of affect carried by metaphors. To conduct our study, we use human annotators to collect metaphor-rich texts (Shutova and Teufel, 2010) and tag each metaphor with its corresponding polarity (Positive/Negative) and valence [−3, +3] scores. The next sections describe the affect polarity and valence tasks we have defined, the collected and annotated met"
P13-1067,C88-1081,0,0.601467,"Farsi that contain annotations of the Positive and Negative polarity and the valence (from −3 to +3 scale) corresponding to the intensity of the affect conveyed in the metaphor. Introduction • We have proposed and developed automated methods for solving the polarity and valence tasks for all four languages. We model the polarity task as a classification problem, while the valence task as a regression problem. Metaphor is a figure of speech in which a word or phrase that ordinarily designates one thing is used to designate another, thus making an implicit comparison (Lakoff and Johnson, 1980; Martin, 1988; Wilks, 2007). For instance, in “My lawyer is a shark” • We have studied the influence of different information sources like the metaphor itself, the context in which it resides, the source and the speaker may want to communicate that his/her lawyer is strong and aggressive, and that he will 682 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682–691, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics a novel task (Smith et al., 2007; Veale, 2012; Veale and Li, 2012; Reyes and Rosso, 2012; Reyes et al., 2013). Litt"
P13-1067,W13-0904,0,0.0261355,"fication algorithms, we calculate the f-score and accuracy on 10-fold cross validation. 4.3 tation toolkit specifically for the task of metaphor detection, interpretation and affect assignment. They hired annotators to collect and annotate data for the English, Spanish, Russian and Farsi languages. The domain for which the metaphors were collected was Governance. It encompasses electoral politics, the setting of economic policy, the creation, application and enforcement of rules and laws. The metaphors were collected from political speeches, political websites, online newspapers among others (Mohler et al., 2013). Table 1 shows the positive and negative class http://www.languagecomputer.com/ 684 distribution for each one of the four languages. ENGLISH SPANISH RUSSIAN FARSI Negative 2086 196 468 384 Table 2 shows the influence of the information sources for Spanish, Russian and Farsi with the ngram features. The best f-scores for each language are shown in bold. For Farsi and Russian high performances are obtained both with the context and with the combination of the context, source and target information. While for Spanish they reach similar performance. Positive 1443 434 418 252 Table 1: Polarity Cla"
P13-1067,P10-1071,0,0.0606717,"ch texts is still The texts “Your claims are indefensible.” and “He attacked every weak point in my argument.” do not directly talk about argument as a war, however the winning or losing of arguments, the attack or defense of positions are structured by the concept of war. There is no physical battle, but there is a verbal battle and the structure of an argument (attack, defense) reflects this (Lakoff and Johnson, 1980). As we mentioned before, there has been a lot of work on the automatic identification of metaphors (Wilks, 2007; Shutova et al., 2010) and their mapping into conceptual space (Shutova, 2010a; Shutova, 2010b), however these are beyond the scope of this paper. Instead we focus on an equally interesting, challenging and important problem, which concerns the automatic identification of affect carried by metaphors. To conduct our study, we use human annotators to collect metaphor-rich texts (Shutova and Teufel, 2010) and tag each metaphor with its corresponding polarity (Positive/Negative) and valence [−3, +3] scores. The next sections describe the affect polarity and valence tasks we have defined, the collected and annotated metaphor-rich data for each one of the English, Spanish, R"
P13-1067,P07-2010,0,0.0290281,"used to designate another, thus making an implicit comparison (Lakoff and Johnson, 1980; Martin, 1988; Wilks, 2007). For instance, in “My lawyer is a shark” • We have studied the influence of different information sources like the metaphor itself, the context in which it resides, the source and the speaker may want to communicate that his/her lawyer is strong and aggressive, and that he will 682 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682–691, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics a novel task (Smith et al., 2007; Veale, 2012; Veale and Li, 2012; Reyes and Rosso, 2012; Reyes et al., 2013). Little (almost no) effort has been put into multilingual computational affect models of metaphor-rich texts. Our research specifically targets the resolution of these problems and shows that it is possible to build such computational models. The experimental result provide valuable contributions and fundings, which could be used by the research community to build upon. target domains of the metaphor, in addition to contextual features and trigger word lists developed by psychologists (Tausczik and Pennebaker, 2010)."
P13-1067,P11-1032,0,0.0161858,"Word Count (LIWC) repository (Tausczik and Pennebaker, 2010), which has 64 word categories corresponding to different classes like emotional states, psychological processes, personal concerns among other. Each category contains a list of words characterizing it. For instance, the LIWC category discrepancy contains words like should, could among others, while the LIWC category inhibition contains words like block, stop, constrain. Previously LIWC was successfully used to analyze the emotional state of bloggers and tweeters (Quercia et al., 2011) and to identify deception and sarcasm in texts (Ott et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011). When LIWC analyzes texts it generates statistics like number of words found in category Ci divided by the total number of words in the text. For our metaphor polarity task, we use LIWC’s statistics of all 64 categories and feed this information as features for the machine learning classifiers. LIWC repository contains conceptual categories (dictionaries) both for the English and Spanish languages. LIWC Evaluation and Results: In our experiments LIWC is applied to English and Spanish metaphor-rich texts since the LIWC category dictionaries are available for"
P13-1067,S07-1013,0,0.132435,"re are different views on metaphor in linguistics and philosophy (Black, 1962; Lakoff and Johnson, 1980; Gentner, 1983; Wilks, 2007), the common among all approaches is the idea of an interconceptual mapping that underlies the production of metaphorical expressions. There are two concepts or conceptual domains: the target (also called topic in the linguistics literature) and the source (or vehicle), and the existence of a link between them gives rise to metaphors. Related Work A substantial body of work has been done on determining the affect (sentiment analysis) of texts (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Cho"
P13-1067,P02-1053,0,0.0048342,"Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Choi and Cardie, 2009) even documents (Pang and Lee, 2008). Multiple techniques have been employed, from various machine learning classifiers, to clustering and topic models. Various domains and textual sources have been analyzed such as Twitter, Blogs, Web documents, movie and product reviews (Turney, 2002; Kennedy and Inkpen, 2005; Niu et al., 2005; Pang and Lee, 2008), but yet what is missing is affect analyzer for metaphor-rich texts. While the affect of metaphors is well studied from its linguistic and psychological aspects (Blanchette et al., 2001; Toml"
P13-1067,P12-3002,0,0.120569,"aking an implicit comparison (Lakoff and Johnson, 1980; Martin, 1988; Wilks, 2007). For instance, in “My lawyer is a shark” • We have studied the influence of different information sources like the metaphor itself, the context in which it resides, the source and the speaker may want to communicate that his/her lawyer is strong and aggressive, and that he will 682 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682–691, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics a novel task (Smith et al., 2007; Veale, 2012; Veale and Li, 2012; Reyes and Rosso, 2012; Reyes et al., 2013). Little (almost no) effort has been put into multilingual computational affect models of metaphor-rich texts. Our research specifically targets the resolution of these problems and shows that it is possible to build such computational models. The experimental result provide valuable contributions and fundings, which could be used by the research community to build upon. target domains of the metaphor, in addition to contextual features and trigger word lists developed by psychologists (Tausczik and Pennebaker, 2010). • We have conducted in depth exp"
P13-1067,P12-2015,0,0.0754828,"other, thus making an implicit comparison (Lakoff and Johnson, 1980; Martin, 1988; Wilks, 2007). For instance, in “My lawyer is a shark” • We have studied the influence of different information sources like the metaphor itself, the context in which it resides, the source and the speaker may want to communicate that his/her lawyer is strong and aggressive, and that he will 682 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682–691, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics a novel task (Smith et al., 2007; Veale, 2012; Veale and Li, 2012; Reyes and Rosso, 2012; Reyes et al., 2013). Little (almost no) effort has been put into multilingual computational affect models of metaphor-rich texts. Our research specifically targets the resolution of these problems and shows that it is possible to build such computational models. The experimental result provide valuable contributions and fundings, which could be used by the research community to build upon. target domains of the metaphor, in addition to contextual features and trigger word lists developed by psychologists (Tausczik and Pennebaker, 2010). • We have co"
P13-1067,H05-1044,0,0.0313405,"2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Choi and Cardie, 2009) even documents (Pang and Lee, 2008). Multiple techniques have been employed, from various machine learning classifiers, to clustering and topic models. Various domains and textual sources have been analyzed such as Twitter, Blogs, Web documents, movie and product reviews (Turney, 2002; Kennedy and Inkpen, 2005; Niu et al., 2005; Pang and Lee, 2008), but yet what is missing is affect analyzer for metaphor-rich texts. While the affect of metaphors is well studied from its linguistic and psychological aspects (Blanchette et al., 2001; Tomlinson and Love, 2006;"
P13-1067,D11-1016,0,0.0130083,"ilosophy (Black, 1962; Lakoff and Johnson, 1980; Gentner, 1983; Wilks, 2007), the common among all approaches is the idea of an interconceptual mapping that underlies the production of metaphorical expressions. There are two concepts or conceptual domains: the target (also called topic in the linguistics literature) and the source (or vehicle), and the existence of a link between them gives rise to metaphors. Related Work A substantial body of work has been done on determining the affect (sentiment analysis) of texts (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; Yessenalina and Cardie, 2011; Breck et al., 2007). Various tasks have been solved among which polarity and valence identification are the most common. While polarity identification aims at finding the Positive and Negative affect, valence is more challenging as it has to map the affect on a [−3, +3] scale depending on its intensity (Polanyi and Zaenen, 2004; Strapparava and Mihalcea, 2007). Over the years researchers have developed various approaches to identify polarity of words (Esuli and Sebastiani, 2006), phrases (Turney, 2002; Wilson et al., 2005), sentences (Choi and Cardie, 2009) even documents (Pang and Lee, 2008"
P13-1067,N10-1147,0,\N,Missing
P16-2018,P10-1136,0,0.0205052,"antially increased interest in building semantic taggers that can accurately recognize product, brand, model and product family types in shopping queries to better understand and match the needs of online shoppers. Despite the necessity for semantic understanding, yet most widely used approaches for product retrieval categorize the query and the offer (Kozareva, 2015) into a shopping taxonomy and use the predicted category as a proxy for retrieving the relevant products. Unfortunately, such procedure falls short and leads to inaccurate product retrieval. Recent efforts (Manshadi and Li, 2009; Li, 2010) focused on building CRF taggers that recognize basic entity types in shopping query such as brands, types and models. (Li, 2010) conducted • Building semantic tagging framework for shopping queries. • Leveraging missing knowledge base entries through word embeddings learned on large amount of unlabeled query logs. • Annotating 37, 000 shopping queries with product, brand, model and product family entity types. • Conducting a comparative and efficiency study of multiple structured prediction algorithms and settings. • Showing that long short-term memory networks reaches the best performance of"
P16-2018,W02-2004,0,0.16608,"Missing"
P16-2018,P09-1097,0,0.0341853,"4). This leads to substantially increased interest in building semantic taggers that can accurately recognize product, brand, model and product family types in shopping queries to better understand and match the needs of online shoppers. Despite the necessity for semantic understanding, yet most widely used approaches for product retrieval categorize the query and the offer (Kozareva, 2015) into a shopping taxonomy and use the predicted category as a proxy for retrieving the relevant products. Unfortunately, such procedure falls short and leads to inaccurate product retrieval. Recent efforts (Manshadi and Li, 2009; Li, 2010) focused on building CRF taggers that recognize basic entity types in shopping query such as brands, types and models. (Li, 2010) conducted • Building semantic tagging framework for shopping queries. • Leveraging missing knowledge base entries through word embeddings learned on large amount of unlabeled query logs. • Annotating 37, 000 shopping queries with product, brand, model and product family entity types. • Conducting a comparative and efficiency study of multiple structured prediction algorithms and settings. • Showing that long short-term memory networks reaches the best per"
P16-2018,W02-1001,0,0.212282,"|x) = Z(h) exp{ i (Wyhi hi + Wyti ,yi−1 )}, where y=(y1 , y2 , ..., yN ) (N ≤ M ) are the shopping query segments labeled with their corresponding entity category. Each segment yi corresponds to a triple hbi , ei , ti i indicating the start index bi and end index ei of the sequence followed by the entity category ti ∈ T . When ti = ⊥, the segment contains only one word. Structured Prediction Models To tackle the shopping tagging problem of query logs, we use Conditional Random Fields (Lafferty et al., 2001, CRF)1 , learning to search (Daum´e III et al., 2009, S EARN)2 , structured perceptron (Collins, 2002, S TRUCT P ERCEPTRON) and a long short-term memory networks extended by CRF layer (Hochreiter and Schmidhuber, 1997; Graves, 2012, LSTM-CRF). CRF: is a popular algorithms for sequence tagging tasks (Lafferty et al., 2001). The objective is 1 2 exp{λ · f (y, x)}, where Zλ (x) is the normalization factor, λ is the weight vector and f (y, x) is the extracted feature vector for the observed sequence x. SEARN is a powerful structured prediction algorithm, which formulates the sequence labeling problem as a search process. The objective is to find the label sequence y = (y1 , ..., yM ) that maximiz"
P16-2018,C92-2082,0,0.238822,"Missing"
P16-2018,P08-1003,0,0.0833139,"Missing"
P16-2018,P82-1020,0,0.494469,"Missing"
P16-2018,W14-1609,0,0.0334106,"es that check if w0 , w−1 and w+1 contain all-digits, any-digit, start-with-digit-endin-letter and start-with-letter-end-in-digit. They are designed to capture model names like hero3 and m560. Positional (PSTNL): are discrete features modeling the position of the words in the query. They capture the way people tend to write products and brands in the query. Part-of-Speech (POS): capture nouns and proper names to better recognize products and brands. We use Stanford tagger (Toutanova et al., 2003). Knowledgebase (KB): are powerful semantic features (Tjong Kim Sang, 2002; Carreras et al., 2002; Passos et al., 2014). We automatically collected and manually validated 200K brands, products, models and product families items extracted from Macy’s and Amazon websites. WordEmbeddings (WE): While external knowledge bases are great resource, they are expensive to create and time-consuming to maintain. We use word embeddings (Mikolov et al., 2013) 3 as a cheap low-maintenance alternative for knowledge base construction. We train the embeddings over 2.5M unlabeled shopping queries. For each token in the query, we use as features the 200 dimensional embeddings of the top 5 most similar terms returned by cosine sim"
P16-2018,D11-1144,0,0.289755,"oo! 701 First Avenue Sunnyvale, CA 94089 zornitsa@kozareva.com {lqi|kzhai}@yahoo-inc.com weiwei@cs.columbia.edu Abstract a study over 4000 shopping queries and showed promising results when huge knowledge bases are present. (Pas¸ca and Van Durme, 2008; Kozareva et al., 2008; Kozareva and Hovy, 2010) focused on using Hearst patterns (Hearst, 1992) to learn semantic lexicons. While such methods are promising, they cannot be used to recognize all product entities in a query. In parallel to the semantic query understanding task, there have been semantic tagging efforts on the product offer side. (Putthividhya and Hu, 2011) recognize brand, size and color entities in eBay product offers, while (Kannan et al., 2011) recognized similar fields in Bing product catalogs. Despite these efforts, to date there are three important questions, which have not been answered, but we address in our work. (1) What is an alternative method when product knowledge bases are not present? (2) Is the performance of the semantic taggers agnostic to the query length? (3) Can we minimize manual feature engineering for shopping query log tagging using neural networks? The main contributions of the paper are: Over the past decade, e-Comme"
P16-2018,W02-2024,0,0.436246,"ufacturer (e.g. Calvin Klein). -Product Family is a brand-specific grouping of products sharing the same product (e.g. Samsung Galaxy). -Model is used by manufacturer to distinguish variations (e.g. for the brand Lexus has IS product family, which has model 200t and 300 F Sport). For modeling, we denote with T = {⊥, t1 , t2 , . . . , tK } the whole label space, where ⊥ indicates a word that is not a part of an entity and ti stands for an entity category. The tagging models have to recognize the following types product, brand, model, product family and ⊥ (other) using the BIO schema (Tjong Kim Sang, 2002). We denote as x = (x1 , x2 , . . . , xM ) a shopping query of length M . The objective is to find the ˆ such that: best configuration y where C(•) is a cost sensitive multiclass classifier and y ˆ are the ground-truth labels. S TRUCT P ERCEPTRON is an extension of the standard perceptron. In our setting we model a segment-based search algorithm, where each unit is a segment of x (e.g., hbi , ei i), rather than a single word (e.g., xi ). The objective is to find the label sequence y = (y1 , ..., yM ) that maximizes p(y|x) ∝ w> · f (x, y), where f (x, y) represents the feature vector for instan"
P16-2018,P10-1150,1,0.893119,"Missing"
P16-2018,N03-1033,0,0.0750866,"vious w−1 and next w+1 words, and bigrams w−1 w0 and w0 w+1 . Orthographic (ORTO): are binary mutually nonexclusive features that check if w0 , w−1 and w+1 contain all-digits, any-digit, start-with-digit-endin-letter and start-with-letter-end-in-digit. They are designed to capture model names like hero3 and m560. Positional (PSTNL): are discrete features modeling the position of the words in the query. They capture the way people tend to write products and brands in the query. Part-of-Speech (POS): capture nouns and proper names to better recognize products and brands. We use Stanford tagger (Toutanova et al., 2003). Knowledgebase (KB): are powerful semantic features (Tjong Kim Sang, 2002; Carreras et al., 2002; Passos et al., 2014). We automatically collected and manually validated 200K brands, products, models and product families items extracted from Macy’s and Amazon websites. WordEmbeddings (WE): While external knowledge bases are great resource, they are expensive to create and time-consuming to maintain. We use word embeddings (Mikolov et al., 2013) 3 as a cheap low-maintenance alternative for knowledge base construction. We train the embeddings over 2.5M unlabeled shopping queries. For each token"
P16-2018,P08-1119,1,0.861518,"Missing"
P16-2018,N15-1147,1,0.737611,"ive study over 37, 000 manually annotated queries and report performance of 90.92 F1 independent of the query length. 1 Introduction Recent study shows that yearly e-Commerce sales in the U.S. top 100 Billion (Fulgoni, 2014). This leads to substantially increased interest in building semantic taggers that can accurately recognize product, brand, model and product family types in shopping queries to better understand and match the needs of online shoppers. Despite the necessity for semantic understanding, yet most widely used approaches for product retrieval categorize the query and the offer (Kozareva, 2015) into a shopping taxonomy and use the predicted category as a proxy for retrieving the relevant products. Unfortunately, such procedure falls short and leads to inaccurate product retrieval. Recent efforts (Manshadi and Li, 2009; Li, 2010) focused on building CRF taggers that recognize basic entity types in shopping query such as brands, types and models. (Li, 2010) conducted • Building semantic tagging framework for shopping queries. • Leveraging missing knowledge base entries through word embeddings learned on large amount of unlabeled query logs. • Annotating 37, 000 shopping queries with p"
P19-1368,I17-4010,0,0.0266403,"Missing"
P19-1368,I17-4009,0,0.0215441,"Missing"
P19-1368,N18-2118,0,0.116232,"Missing"
P19-1368,C16-1189,0,0.38796,"Missing"
P19-1368,N16-1062,0,0.170753,"Missing"
P19-1368,P17-2083,0,0.0495047,"Missing"
P19-1368,I17-4004,0,0.0164021,"the language agnostic capabilities of SGNN++ • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). • SwDA: Switchboard Dialog Act is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). • ATIS: Intent Understanding is a widely used corpus in the speech and dialog community (T¨ur et al., 2010) for understanding different intents during flight reservation. • CF: Customer Feedback is a multilingual customer feedback analysis task (Liu et al., 2017) that aims at categorizing customer feedback as “comment, “request, “bug, “complaint, “meaningless, or “undetermined. The data is in English (EN), Japanese (JP), French (FR) and Spanish (SP) languages. Table 1 shows the characteristics of each task: language, number of classes, training and test data. MRDA SwDA ATIS CF-EN CF-JP CF-FR CF-SP Experimental Setup & Parameter Tuning We setup our experiments as given a classification task and a dataset, generate an on-device model. For each task, we report Accuracy on the test set. Lang. EN EN EN EN JP FR SP #Classes 6 42 21 5 5 5 5 Train 78K 193K 4,"
P19-1368,W17-5530,0,0.234682,"Missing"
P19-1368,D18-1092,1,0.809259,"Missing"
P19-1368,N19-1339,1,0.428301,"roject sequence xi to word projection vectors. We use word-level context features (e.g., phrases and word-level skip-grams) extracted from the raw text to compute the intermediate feature vector ~xw = Fw and compute projections. ej (xi ) = projection(x~iw , P ej ) P w w e1...` (xiw ) eipw = P w e1 (xi ), ..., P e` (xiw ) ] = [P w (7) (8) w We reserve ` bits to capture the word projection space computed using a series of ` functions e1 , ..., P e` . The functions project the sentence P w w structure into low-dimensional representation that 3786 captures similarity in the word-projection space (Sankar et al., 2019). 2.3.2 Character Projections Given the input text xi , we can capture morphology (character-level) information in a similar way. We use character-level context features (e.g., character-level skip-grams) again extracted directly from the raw text to compute ~xc = Fc and compute character projections eipc . ej ) ej (xi ) = projection(x~ic , P (9) P c c `+1...T e eipc = P (10) (xic ) c e`+1 (xi ), ..., P eT (xiw ) ] = [P c c The character feature space and hence projections eipc are reserved and computed separately. Note that even though we compute separate projections for character-level conte"
P19-1368,W04-2319,0,0.245116,"ep is O(T · d · C), where C is the number of hidden units in e hp in the multi-layer projection network and typically smaller than T · d. 3 NLP Datasets and Experimental Setup 3.1 Datasets & Tasks We evaluate our on-device SGNN++ model on four NLP tasks and languages such as English, Japanese, Spanish and French. The datasets were selected so we can compare against prior ondevice work (Ravi and Kozareva, 2018) and also test the language agnostic capabilities of SGNN++ • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). • SwDA: Switchboard Dialog Act is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). • ATIS: Intent Understanding is a widely used corpus in the speech and dialog community (T¨ur et al., 2010) for understanding different intents during flight reservation. • CF: Customer Feedback is a multilingual customer feedback analysis task (Liu et al., 2017) that aims at categorizing customer feedback as “comment, “request, “bug, “complaint, “meaningless, or “undetermined. The data is in English (EN), Japanese (JP), French (FR) and"
R11-1045,N04-1016,0,0.0254641,"using weights that are tuned separately for each individual relation. While semantic relations can hold between different parts of speech, e.g., between a verb and a noun, we focus on relations between nominals.2 The most relevant related publication is that of ´ S´eaghdha and Copestake (2009), who combine O attributional and relational features using kernels. However, they are interested in a special kind of relations: between the nouns in a noun-noun compounds like steel knife. Moreover, they use the British National Corpus instead of the Web, which is known to cause data sparseness issues (Lapata and Keller, 2004), they do not focus on linguistically motivated relational features such as verbs and prepositions explicitly, they use co-hyponyms but not hypernyms to generalize the relation arguments, and they give equal weights to the similarities between heads and between modifiers. The remainder of the paper is organized as follows: Section 2 introduces our Web mining methods for argument and relation modeling, Section 3 presents our experimental setup, Section 4 discusses the results, and Section 5 concludes and points to some directions for future work. 2 Method 2.1 Overview As we said above, we combi"
R11-1045,P08-1027,0,0.0188986,"attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit f"
R11-1045,P08-1052,1,0.911974,"chmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit from many preexisting lexical resources"
R11-1045,S07-1003,1,0.900621,"Missing"
R11-1045,E09-1071,0,0.129319,"Missing"
R11-1045,C92-2082,0,0.444973,"example is the task of extracting semantic relations between nominals from text, which has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are mor"
R11-1045,N09-2060,0,0.0412172,"Missing"
R11-1045,S10-1006,1,0.887905,"Missing"
R11-1045,P06-1015,0,0.163697,"has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approach"
R11-1045,D09-1099,1,0.854304,"g verbs and prepositions and the coordinating conjunctions are extracted from the Web, and there is a frequency of extraction associated with each of them, which we incorporate into a relational vector and use to measure relational similarity between training and testing examples. 2.2 Argument Modeling We model the arguments using a distribution over Web-derived hypernyms and co-hyponyms. Multiple knowledge harvesting procedures have been proposed in the literature for the automatic acquisition of hyponyms (Hearst, 1992; Pas¸ca, 2007; Kozareva et al., 2008) and hypernyms (Ritter et al., 2009; Hovy et al., 2009). While we could have used any of them for our experiments, we chose the method of Kozareva et al. (2008), which (i) can extract hypernyms and hyponyms simultaneously, (ii) has been shown to achieve higher accuracy than the methods described in (Pas¸ca, 2007; Ritter et al., 2009), and also (iii) is easy to implement. It uses a doublyanchored pattern (DAP) of the following general form: Hyp./co-hyp. for arg. 1/2 cohyp arg1:tea hyper arg1:beverage hyper arg1:drink hyper arg1:item hyper arg1:product cohyp arg1:chocolate cohyp arg1:cocoa cohyp arg1:soda hyper arg1:crop hyper arg1:food cohyp arg1:s"
R11-1045,P02-1032,0,0.034789,"wo general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit from many preexisting lexical resources. For example, systems using WordNet (Fellbaum, 1998) had sizable performance gains for SemEval-1 Task 4. However, this advantage was mainly due to manually annotated WordNet senses for the relation arguments being provided for this task. There was a restricted track where using them was not allowed"
R11-1045,P06-2064,0,0.0182905,"inals from text, which has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advanta"
R11-1045,N03-1033,0,0.00552802,"“noun2 THAT? * noun1” where noun1 and noun2 are inflected variants of the head nouns in the relation arguments, THAT? stands for that, which, who or the empty string, and * stands for up to eight instances3 of the search engine’s star operator. We instantiate these generalized patterns and we submit them to Google as exact phrase queries. We then collect the snippets for all returned results (up to 1,000). We split the extracted snippets into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We POS tag the sentences using the Stanford POS tagger (Toutanova et al., 2003) and we make sure that the word sequence following the second mentioned target noun is non-empty and contains at least one non-noun, i.e., that the snippet includes the entire noun phrase of the second noun in the pattern instantiation. This is because we want the second noun in the pattern instantiation to be the head of an NP: if the NP in incomplete, the second noun could be a modifier in that partial NP. “sem-class such as term1 and term2 ” where sem-class stands for a semantic class, and term1 and term2 are members of this class. In our experiments, we use the following twoplaceholder for"
R11-1045,P08-1119,1,0.831503,"coffee of the guy who makes coffee. Again, the paraphrasing verbs and prepositions and the coordinating conjunctions are extracted from the Web, and there is a frequency of extraction associated with each of them, which we incorporate into a relational vector and use to measure relational similarity between training and testing examples. 2.2 Argument Modeling We model the arguments using a distribution over Web-derived hypernyms and co-hyponyms. Multiple knowledge harvesting procedures have been proposed in the literature for the automatic acquisition of hyponyms (Hearst, 1992; Pas¸ca, 2007; Kozareva et al., 2008) and hypernyms (Ritter et al., 2009; Hovy et al., 2009). While we could have used any of them for our experiments, we chose the method of Kozareva et al. (2008), which (i) can extract hypernyms and hyponyms simultaneously, (ii) has been shown to achieve higher accuracy than the methods described in (Pas¸ca, 2007; Ritter et al., 2009), and also (iii) is easy to implement. It uses a doublyanchored pattern (DAP) of the following general form: Hyp./co-hyp. for arg. 1/2 cohyp arg1:tea hyper arg1:beverage hyper arg1:drink hyper arg1:item hyper arg1:product cohyp arg1:chocolate cohyp arg1:cocoa cohyp"
R11-1045,J06-3003,0,0.0288624,"of the arguments, which we use in instance-based classifiers. The evaluation on the dataset of SemEval-1 Task 4 shows an improvement over the state-ofthe-art for the case where using manually annotated WordNet senses is not allowed. 1 The task of identifying semantic relations in text is complicated by their heterogeneous nature. Thus, it is often addressed using non-parametric instance-based classifiers like the k nearest neighbors (kNN), which effectively reduce it to measuring the relational similarity between a testing and each of the training examples. The latter is studied in detail by Turney (2006), who distinguishes between attributional similarity or correspondence between attributes, and relational similarity or correspondence between relations. Attributional similarity is interested in the similarity between two words (or nominals, noun phrases), A and B. In contrast, relational similarity focuses on the relationship between two pairs of words (or nominals, noun phrases), i.e., it asks how similar the relations A:B and C:D are. Measuring relational similarity directly is hard, and thus it is rarely done directly. Instead, relational similarity is typically modeled as a function of t"
S07-1072,P97-1023,0,0.0206586,"nce bag of word counts collected from the World Wide Web. Our hypothesis is that words which tend to cooccur across many documents with a given emotion are highly probable to express this emotion. The rest of the paper is organized as follows. In Section 2 we review some of the related work, in Section 3 we describe our web-based emotion classification approach for which we show a walk-through example in Section 4. A discussion of the obtained results can be found in Section 5 and finally we conclude in Section 6. 2 Related work Our approach for emotion classification is based on the idea of (Hatzivassiloglou and McKeown, 1997) and is similar to those of (Turney, 2002) and (Turney and Littman, 2003). According to Hatzivassiloglou and McKeown (1997), adjectives with the same polarity tended to appear together. For example the negative adjectives “corrupt and brutal” co334 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334–337, c Prague, June 2007. 2007 Association for Computational Linguistics occur very often. The idea of tracing polarity through adjective cooccurrence is adopted by Turney (2002) for the binary (positive and negative) classification of text reviews. They"
S07-1072,W06-0301,0,0.00661716,"web as a source of information. For instance, (Taboada et al., 2006) extracted from the web co-occurrences of adverbs, adjectives, nouns and verbs. Gamon and Aue (2005) were looking for adjectives that did not co-occur at sentence level. (Baroni and Vegnaduzzo, 2004) and (Grefenstette et al., 2004) gathered subjective adjectives from the web calculating the Mutual Information score. Other important works on sentiment analysis are those of (Wilson et al., 2005) and (Wiebe et al., 2005; Wilson and Wiebe, 2005), who used linguistic information such as syntax and negations to determine polarity. Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling. 3 Web co-occurrences In order to determine the emotions of a headline, we measure the Pointwise Mutual Information (MI) of ei and cwj as hits(ei ,cwj ) , where ei ∈ M I(ei , cwj ) = log2 hits(ei )hits(cw j) {anger, disgust, f ear, joy, sadness, surprise} and cwj are the content words of the headline j. For each headline, we have six MI scores which indicate the presence of the emotion. MI is used in our experiments because it provides information about the independence of an emotion and a bag of words."
S07-1072,P02-1053,0,0.0620945,"pothesis is that words which tend to cooccur across many documents with a given emotion are highly probable to express this emotion. The rest of the paper is organized as follows. In Section 2 we review some of the related work, in Section 3 we describe our web-based emotion classification approach for which we show a walk-through example in Section 4. A discussion of the obtained results can be found in Section 5 and finally we conclude in Section 6. 2 Related work Our approach for emotion classification is based on the idea of (Hatzivassiloglou and McKeown, 1997) and is similar to those of (Turney, 2002) and (Turney and Littman, 2003). According to Hatzivassiloglou and McKeown (1997), adjectives with the same polarity tended to appear together. For example the negative adjectives “corrupt and brutal” co334 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334–337, c Prague, June 2007. 2007 Association for Computational Linguistics occur very often. The idea of tracing polarity through adjective cooccurrence is adopted by Turney (2002) for the binary (positive and negative) classification of text reviews. They take two adjectives, for instance “excelle"
S07-1072,W05-0408,0,0.0103099,"AR operator and considers only those documents that contain the adjectives within a specific proximity. In our approach, as far as the majority of the query words appear in the documents, the frequency count is considered. • queries: The queries of Turney (2002) are made up of a pair of adjectives, and in our approach the query contains the content words of the headline and an emotion. There are other emotion classification approaches that use the web as a source of information. For instance, (Taboada et al., 2006) extracted from the web co-occurrences of adverbs, adjectives, nouns and verbs. Gamon and Aue (2005) were looking for adjectives that did not co-occur at sentence level. (Baroni and Vegnaduzzo, 2004) and (Grefenstette et al., 2004) gathered subjective adjectives from the web calculating the Mutual Information score. Other important works on sentiment analysis are those of (Wilson et al., 2005) and (Wiebe et al., 2005; Wilson and Wiebe, 2005), who used linguistic information such as syntax and negations to determine polarity. Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling. 3 Web co-occurrences In order to determine the emotions of"
S07-1072,W05-0308,0,0.023899,"nts with a given emotion are highly probable to express the same emotion. 1 Introduction The subjective analysis of a text is becoming important for many Natural Language Processing (NLP) applications such as Question Answering, Information Extraction, Text Categorization among others (Shanahan et al., 2006). The resolution of this problem can lead to a complete, realistic and coherent analysis of the natural language, therefore major attention is drawn to the opinion, sentiment and emotion analysis, and to the identification of beliefs, thoughts, feelings and judgments (Quirk et al., 1985), (Wilson and Wiebe, 2005). The aim of the Affective Text task is to classify a set of news headlines into six types of emotions: “anger”, “disgust”, “fear”, “joy”, “sadness” and “surprise”. In order to be able to conduct such multi-category analysis, we believe that first we need a comprehensive theory of what a human emotion is, and then we need to understand how the emotion is expressed and transmitted within the natural language. These aspects rise the need of syntactic, semantic, textual and pragmatic analysis of a text (Polanyi and Zaenen, 2006). However, some of the major drawbacks in this field are related to t"
S07-1072,H05-1044,0,0.118174,"es, and in our approach the query contains the content words of the headline and an emotion. There are other emotion classification approaches that use the web as a source of information. For instance, (Taboada et al., 2006) extracted from the web co-occurrences of adverbs, adjectives, nouns and verbs. Gamon and Aue (2005) were looking for adjectives that did not co-occur at sentence level. (Baroni and Vegnaduzzo, 2004) and (Grefenstette et al., 2004) gathered subjective adjectives from the web calculating the Mutual Information score. Other important works on sentiment analysis are those of (Wilson et al., 2005) and (Wiebe et al., 2005; Wilson and Wiebe, 2005), who used linguistic information such as syntax and negations to determine polarity. Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling. 3 Web co-occurrences In order to determine the emotions of a headline, we measure the Pointwise Mutual Information (MI) of ei and cwj as hits(ei ,cwj ) , where ei ∈ M I(ei , cwj ) = log2 hits(ei )hits(cw j) {anger, disgust, f ear, joy, sadness, surprise} and cwj are the content words of the headline j. For each headline, we have six MI scores which ind"
S07-1073,S07-1012,0,0.086371,"to among others. The question is which one of these referents we are actually looking for and interested in. Presently, to be able to answer to this question, we have to skim the content of the documents and retrieve the correct answers on our own. To automate this process, the named entities can be disambiguated and the different underlying meanings of the name can be found. On the basis of this information, the web pages can be clustered together and organized in a hierarchical structure which can ease the documents’ browsing. This is also the objective of the Web People Search (WePS) task (Artiles et al., 2007). What makes the WePS task even more challenging is the fact that in contrast to WSD where the number of senses of a word are predefined, in WePS we do not know the exact number of different individuals. For the resolution of the WePS task, we have developed a web page clustering approach using the title and the body content of the web pages. In addition, we group together the documents that share many location, person and organization names, as well as those that point out to the same sub-links. The rest of the paper is organized as follows. In Section 2 we describe various approaches for nam"
S07-1073,P98-1012,0,0.126325,"Missing"
S07-1073,W03-0405,0,0.0526802,"Missing"
S07-1073,N04-3008,0,0.0236952,"anization) are considered. Therefore, in our approach we use person, organization and location names in order to construct a social similarity network between two documents. Another unsupervised clustering technique for name discrimination of web pages is that of Pedersen and Kulkarni (2007). They used contextual vectors derived from bigrams, and measured the impact of several association measures. During the evaluation, some names were easily discriminable compared to others categories for which was even difficult to find and obtain discriminative feature. We worked with their unigram model (Purandare and Pedersen, 2004) to cluster the web pages using the text content between the title tags. 3 Web Person Disambiguation Our web people clustering approach is presented in Figure 1 and consists of the following steps: • name matching: the location, person and organization names in the body texts are identified with the GATE1 system (Cunningham, 2005). Each named entity of a document is matched with its corresponding named entity category from the rest of the web pages. This information is used to calculate the social semantic similarity of the person, the location and the organization names. Our hypothesis is tha"
S07-1073,C98-1012,0,\N,Missing
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S12-1052,S12-1063,0,0.0291232,"f machine learning algorithms and feature encodings. Instead, some creativity and ingenuity was needed to find a suitable source of commonsense causal information, and determine an automated mechanism for applying this information to COPA questions. Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems (Goodwin et al., 2012), which they described to us as follows: UTDHLT Bigram PMI: The team's first approach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic 1 http://www.ict.usc.edu/~gordon/copa.html 396 (Church & Hanks, 1990) over all pairs of bigrams (at the token level) between the candidate alternative and the premise. PMI statistics were collected using 8.4 million documents from the LDC Gigaword corpus (Graff & Cieri, 2003). A window of 100 terms was used for finding pairs of cooccurring bigrams, and a window/slop size of 2 for the bigram itself. UTDHLT SVM Combined: The t"
S12-1052,P99-1047,0,0.0617206,"Missing"
S12-1052,W09-3813,0,0.0218925,"Missing"
S12-1052,J90-1003,0,\N,Missing
S12-1052,W07-1401,0,\N,Missing
S13-2025,W04-0404,0,0.0532624,"Missing"
S13-2025,C08-1011,1,0.662917,"ages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings."
S13-2025,W09-2416,1,0.899683,"Missing"
S13-2025,P07-1072,0,0.203668,"Missing"
S13-2025,P06-2064,0,0.395087,"Missing"
S13-2025,W04-2609,0,0.0835979,"located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE)"
S13-2025,P08-1052,1,0.483189,"uation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r"
S13-2025,E09-1071,1,0.900906,"Missing"
S13-2025,C08-1082,1,0.890902,"Missing"
S13-2025,W03-1803,0,0.161511,"Missing"
S13-2025,P10-1070,0,0.829879,"Missing"
S13-2052,baccianella-etal-2010-sentiwordnet,0,0.68035,",131 471 648 430 57 2,734 1,541 160 1,071 1,104 159 Vocabulary Size 20,012 4,426 11,736 3,562 Table 2: Statistics for Subtask A. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from training and spanned later periods. To identify messages that express sentiment towards these topics, we filtered the tweets using SentiWordNet (Baccianella et al., 2010). We removed messages that contained no sentimentbearing words, keeping only those with at least one word with positive or negative sentiment score that is greater than 0.3 in SentiWordNet for at least one sense of the words. Without filtering, we found class imbalance to be too high.3 Twitter messages are rich in social media features, including out-of-vocabulary (OOV) words, emoticons, and acronyms; see Table 1. A large portion of the OOV words are hashtags (e.g., #sheenroast) and mentions (e.g., @tash jade). Corpus Twitter - Training Twitter - Dev Twitter - Test SMS - Test Filtering based o"
S13-2052,C10-2005,0,0.783288,"sing beyond those encountered when working with more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is c"
S13-2052,W10-2914,0,0.0363732,"ith more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties"
S13-2052,S10-1097,0,0.598998,"wire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties of persuasive language or those associated wit"
S13-2052,D11-1141,1,0.104433,"ity Classification Given a message, decide whether it is of positive, negative, or neutral sentiment. For messages conveying both a positive and a negative sentiment, whichever is the stronger one was to be chosen. http://www.daedalus.es/TASS/corpus.php 313 Dataset Creation In the following sections we describe the collection and annotation of the Twitter and SMS datasets. 3.1 Data Collection Twitter is the most common micro-blogging site on the Web, and we used it to gather tweets that express sentiment about popular topics. We first extracted named entities using a Twitter-tuned NER system (Ritter et al., 2011) from millions of tweets, which we collected over a one-year period spanning from January 2012 to January 2013; we used the public streaming Twitter API to download tweets. Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark the position of its start and end in the text boxes below. The number above each word indicates its position. The word/phrase will be generated in the adjacent textbox so that you can confirm"
W06-2004,N03-2023,0,0.0540143,"Missing"
W06-2004,W97-0322,1,0.687559,"have been specified in a manually created list. Note that with smaller numbers of contexts (usually 200 or fewer), we lower the frequency threshold to two or more. In general PMI is known to have a bias towards pairs of words (bigrams) that occur a small number of times and only with each other. In this work that is a desirable quality, since that will tend to identify pairs of words that are very strongly associated with each other and also provide unique discriminating information. Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schu¨ tze, 1998) and (Pedersen and Bruce, 1997). Our method treats each occurrence of an ambiguous name as a context that is to be clustered with other contexts that also include the same name. In this paper, each context consists of about 50 words, where the ambiguous name is generally in the middle of the context. The goal is to cluster similar contexts together, based on the presumption that the occurrences of a name that appear in similar contexts will refer to the same underlying entity. This approach is motivated by both the distributional hypothesis (Harris, 1968) and the strong contextual hypothesis (Miller and Charles, 1991). 2.1"
W06-2004,E06-2007,1,0.824583,"ntext is now represented by the vector of words with which it occurred in the feature selection data. If a word does not have a corresponding entry in the co–occurrence matrix, then it is simply removed from the context. After all the words in the context are checked, then all of the vectors that are selected are averaged together to create a vector representation of the context. Then these contexts are clustered into a pre– specified number of clusters using the k–means algorithm. Note that we are currently developing methods to automatically select the number of clusters in the data (e.g., (Pedersen and Kulkarni, 2006)), although we have not yet applied them to this particular work. 3 4 Experimental Data We use data in four languages in these experiments, Bulgarian, English, Romanian, and Spanish. The Language Salad 4.1 Raw Corpora In this paper, we explore the creation of a second order representation for a set of evaluation contexts using three different sets of feature selection data. The co–occurrence matrix may be derived from the evaluation contexts themselves, or from a separate set of contexts in a different language, or from the combination of these two (the Salad or Mix). For example, suppose we h"
W06-2004,W04-2406,1,0.773666,"experimentally) reveals that Ronaldo the soccer player tends to occur more so than any other single entity named Ronaldo, so while there is a bit more noise for Ronaldo, there is not really a significant ambiguity. For the Spanish results we only note one pair (George Bush - Tony Blair) where the Mix of English and Spanish results in the best performance. 31 the mix of English and evaluation contexts, in order to perform more accurate name discrimination. contexts based on the number of features they have in common with other evaluation contexts. In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. However, we see examples in these results that suggests this may not always be the case. In the Bulgarian results, the largest number of Bulgarian contexts are for NATO-USA, but the Mix performs quite a bit better than Bulgarian only. In the case of Romanian, again NATO-USA has the largest number of contexts, but the Mix still does better than Romanian only. And in Spanish, Mexico-India has the largest number of contexts and English-only does better. Thus, even in cases where w"
W06-2004,J98-1004,0,\N,Missing
W07-1703,P98-1012,0,0.0460838,"wering system. According to the obtained results, the precision of the ontology is high, 20 but still suffers in coverage. A similar approach for the population of the CyC Knowledge Base (KB) was presented in (Shah et al., 2006). They used information from the Web and other electronically available text corpora to gather facts about particular named entities, to validate and finally to add them to the CyC KB. In this paper, we present a new text semantic similarity approach for fine-grained person name categorization and discrimination which is similar to those of (Pedersen et al., 2005) and (Bagga and Baldwin, 1998), but instead of simple word co-occurrences, we consider the whole text segment and relate the deduced semantic information of Latent Semantic Analysis (LSA) to trace the text cohesion between thousands of sentences containing named entities which belong to different fine-grained categories or individuals. Our method is based on the word sense discrimination hypothesis of Miller and Charles (1991) according to which words with similar meaning are used in similar context, hence in our approach we assume that the same person or the same fine-grained person category appears in the similar context"
W07-1703,C02-1130,0,0.0876544,"Missing"
W07-1703,W02-1111,0,0.0797503,"Missing"
W07-1703,J98-1004,0,0.0891578,"Missing"
W07-1703,E06-1003,0,0.040534,"Missing"
W07-1703,C98-1012,0,\N,Missing
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W11-2213,D09-1056,0,0.161412,"Missing"
W11-2213,P98-1012,0,0.393346,"Missing"
W11-2213,E06-1002,0,0.398963,"Missing"
W11-2213,S07-1024,0,0.0526643,"Missing"
W11-2213,D07-1074,0,0.3922,"Missing"
W11-2213,P06-1039,0,0.0806836,"Missing"
W11-2213,N10-1061,0,0.0420086,"disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a document corpus D associated with a certain ambiguous name, our task is to group the documents into K sets such that each do"
W11-2213,W03-0405,0,0.332327,"pproaches (Artiles et al., 2009b; Chen et al., 2009; Lan et al., 2009) focus on person name disambiguation, while others (Pedersen et al., 2006) also explore ambiguity in organization and location names. In the medical domain, Hatzivassiloglou et al. (2001) and Ginter et al. (2004) tackle the problem of gene and protein name disambiguation. Due to the high interest in this task, researchers have explored a wide range of approaches and features. Among the most common and efficient ones are those based on clustering and bag-of-words representation (Pedersen et al., 2005; Artiles et al., 2009b). Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. Others (Bunescu and Pasca, 2006; Cucerzan, 2007; Nguyen and Cao, 2008) work on Wikipedia articles, using infobox and link information. Pedersen et al. (2006) rely on second order co-occurrence vectors. A few others (Matthias, 2005; Wan et al., 2005; Popescu and Magnini, 2007) identify names of people, locations and organizations and use them as a source of evidence to measure the similarity between documents containing the ambiguous names. The most similar wor"
W11-2213,S07-1041,0,0.293535,"wide range of approaches and features. Among the most common and efficient ones are those based on clustering and bag-of-words representation (Pedersen et al., 2005; Artiles et al., 2009b). Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. Others (Bunescu and Pasca, 2006; Cucerzan, 2007; Nguyen and Cao, 2008) work on Wikipedia articles, using infobox and link information. Pedersen et al. (2006) rely on second order co-occurrence vectors. A few others (Matthias, 2005; Wan et al., 2005; Popescu and Magnini, 2007) identify names of people, locations and organizations and use them as a source of evidence to measure the similarity between documents containing the ambiguous names. The most similar work to ours is that of Song et al. (2007) who use a topic-based modeling approach for name disambiguation. However, their method explicitly tries to model the distribution of latent topics with regard to person names and words appearing within documents whereas in our method, the latent topics represent the underlying entities (name senses) for an ambiguous name. Unlike the previous approaches which were specif"
W11-2213,P09-1070,0,0.0297868,"or a corpus such as Wikipedia or the Web, in this paper we show a novel unsupervised topic modeling approach for name disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a documen"
W11-2213,P10-1044,0,0.0153272,"er we show a novel unsupervised topic modeling approach for name disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a document corpus D associated with a certain ambiguous"
W11-2213,C98-1012,0,\N,Missing
W11-2213,S07-1012,0,\N,Missing
W12-4107,W09-2201,0,0.0224705,"gh pattern application over unstructured text) and we show how the extracted knowledge can be used by NLP applications such as relation identification between nominals. 41 A1 82 69 21 29 3 .99 A2 75 66 24 31 4 .98 Cause Z PhysicalObj NonPhysicalObj Event State Other Acc. A1 15 89 72 50 5 .98 A2 20 91 72 50 4 .98 Table 2: Term Classification. 5.2 Comparison against Existing Resources To compare the performance of our approach with knowledge bases that have been extracted in a similar way (i.e., through pattern application over unstructured text), we consult the freely available resources NELL (Carlson et al., 2009), Yago (Suchanek et al., 2007) and TextRunner (Etzioni et al., 2005). Although these bases contain millions of facts, it turns out that NELL and Yago do not have information for the cause-effect relation. While the online demo of TextRunner has query limitation, which returns only the top 1000 snippets. Since we do not have the complete and ranked output of TextRunner, comparing results in terms of relative recall and precision is impossible and unfair. Therefore, we decided to conduct an application driven evaluation and see whether the extracted knowledge can aid an NLP system. 5.3 Applicati"
W12-4107,W04-3205,0,0.129205,"on. Multiple algorithms have been created to learn relations. Some like TextRunner (Etzioni et al., 2005) rely on labeled data, which is used to train a sequence-labeling graphical model (CRF) and then the system uses the model to extract terms and relations from unlabeled texts. Although very accurate, such methods require labeled data which is difficult, expensive and time consuming to create. Other more simplistic methods that rely on lexico-syntactic patterns (Hearst, 1992; Riloff and Jones, 1999; Pasca, 2004) have shown to be equally successful at learning relations, temporal verb order (Chklovski and Pantel, 2004) and entailment (Zanzotto et al., 2006). Therefore, in this paper, we have incorporated an automated bootstrapping procedure, which given a pattern representing the relation of interest can quickly and easily learn the terms associated with the relation. In our case, the pattern captures the cause-effect relation. After extraction, we apply graph-based metrics to rerank the information and filter out the erroneous terms. The contributions of the paper are: • an automated procedure, which can learn terms expressing cause-effect relation. • an exhaustive human-based evaluation. • a comparison of"
W12-4107,N03-1011,0,0.0908943,"Missing"
W12-4107,S07-1003,0,0.0999096,"Missing"
W12-4107,S12-1052,1,0.786602,"n pairs of nominals. The success of the described framework opens up many challenging directions. We plan to expand the extraction procedure with more lexicosyntactic patterns that express the cause-effect relation4 such as trigger, lead to, result among others and thus enrich the recall of the existing repository. We also want to develop an algorithm for extracting cause-effect terms from non contiguous positions like “stress is another very important cause of diabetes”. We are also interested in studying how the extracted knowledge can aid a commonsense causal reasoner (Gordon et al., 2011; Gordon et al., 2012) in understanding that if a girl wants to wear earrings it is more likely for her to get her ears pierced rather then get a tattoo. This example is taken from the Choice of Plausible Alternatives (COPA) dataset5 , which presents a series of forced-choice questions such that each question provides a premise and two viable cause or effect scenarios. The goal is to choose a correct answer that is the most plausible cause or effect. Similarly, the cause-effect repository can be used to support a variety of applications, including textual entailment, information extraction and question answering Ac"
W12-4107,C92-2082,0,0.392108,"Missing"
W12-4107,D09-1099,1,0.824357,"en the relation cause and the term virus for which we know that it can cause something, we express the statement in a recursive pattern1 “* and virus cause *” and use the pattern to learn new terms that cause or have been caused by something. Following our example, the recursive pattern learns from the Web on the left side terms like {bacteria, worms, germs} and on the right side terms like {diseases, damage, contamination}. 2.2 Knowledge Extraction Procedure For our study, we have used the general Webbased class instance and relation extraction framework introduced by (Kozareva et al., 2008; Hovy et al., 2009). The procedure is minimally supervised and achieves high accuracy of the produced extractions. Term Extraction: To initiate the learning process, the user must provide as input a seed term Y and a recursive pattern “X∗ and Y verb Z∗ ” from which terms on the X ∗ and Z ∗ positions can be learned. The input pattern is submitted to Yahoo!Boss API as a web query and all snippets matching the query are retrieved, part-of-speech tagged and used for term extraction. Only the previously unexplored terms found on X ∗ position are used as seeds in the subsequent iteration, while the rest of the terms2"
W12-4107,P08-1119,1,0.840125,"tion. For instance, given the relation cause and the term virus for which we know that it can cause something, we express the statement in a recursive pattern1 “* and virus cause *” and use the pattern to learn new terms that cause or have been caused by something. Following our example, the recursive pattern learns from the Web on the left side terms like {bacteria, worms, germs} and on the right side terms like {diseases, damage, contamination}. 2.2 Knowledge Extraction Procedure For our study, we have used the general Webbased class instance and relation extraction framework introduced by (Kozareva et al., 2008; Hovy et al., 2009). The procedure is minimally supervised and achieves high accuracy of the produced extractions. Term Extraction: To initiate the learning process, the user must provide as input a seed term Y and a recursive pattern “X∗ and Y verb Z∗ ” from which terms on the X ∗ and Z ∗ positions can be learned. The input pattern is submitted to Yahoo!Boss API as a web query and all snippets matching the query are retrieved, part-of-speech tagged and used for term extraction. Only the previously unexplored terms found on X ∗ position are used as seeds in the subsequent iteration, while the"
W12-4107,P06-1100,0,0.0264358,"st of the paper is organized as follows. The next section describes the term extraction procedure. Section 3 and 4 describe the extracted data 39 Proceedings of the TextGraphs-7 Workshop at ACL, pages 39–43, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics and its characteristics. Section 5 focuses on the evaluation and finally we conclude in Section 6. 2 Cause-Effect Relation Learning 2.1 Problem Formulation The objectives of cause-effect relation learning are similar to those of any general open domain relation extraction problem (Etzioni et al., 2005; Pennacchiotti and Pantel, 2006). The task is formulated as: Task: Given a cause-effect semantic relation expressed through lexico-syntactic pattern and a seed example for which the relation is true, the objective is to learn from large unstructured amount of texts terms associated with the relation. For instance, given the relation cause and the term virus for which we know that it can cause something, we express the statement in a recursive pattern1 “* and virus cause *” and use the pattern to learn new terms that cause or have been caused by something. Following our example, the recursive pattern learns from the Web on th"
W12-4107,W02-1028,0,0.0805711,"Missing"
W12-4107,P06-1107,0,0.0145519,"learn relations. Some like TextRunner (Etzioni et al., 2005) rely on labeled data, which is used to train a sequence-labeling graphical model (CRF) and then the system uses the model to extract terms and relations from unlabeled texts. Although very accurate, such methods require labeled data which is difficult, expensive and time consuming to create. Other more simplistic methods that rely on lexico-syntactic patterns (Hearst, 1992; Riloff and Jones, 1999; Pasca, 2004) have shown to be equally successful at learning relations, temporal verb order (Chklovski and Pantel, 2004) and entailment (Zanzotto et al., 2006). Therefore, in this paper, we have incorporated an automated bootstrapping procedure, which given a pattern representing the relation of interest can quickly and easily learn the terms associated with the relation. In our case, the pattern captures the cause-effect relation. After extraction, we apply graph-based metrics to rerank the information and filter out the erroneous terms. The contributions of the paper are: • an automated procedure, which can learn terms expressing cause-effect relation. • an exhaustive human-based evaluation. • a comparison of the extracted knowledge with the terms"
