2020.acl-main.228,P19-1175,0,0.144531,"editing model. Our final re-ranking step is task specific, but again very simple in every case. Our goal here is not to find the best possible way to do the re-ranking. Instead, we show that gains are possible and that it helps to see what edits are made for multiple candidates before making the final decision, instead of following previous work by trying to select a single candidate before editing. We evaluate performance on the Gigaword summarization dataset (Rush et al., 2015) and on the English to Dutch (EN-NL) and the English to Hungarian (EN-HU) machine translation (MT) tasks, following Bulte and Tezcan (2019). For MT, we experimented with different re-ranking schemes but found that the original model score (log-likelihood) worked best, amounting to extended beam search within the complete retreive-edit-rerank pipeline. We improve performance by 6.5 BLEU points on EN-NL and 7.5 on EN-HU over the state-of-art Neural Fuzzy Repair system (Bulte and Tezcan, 2019). On Gigaword, we simply re-rank by returning the most common output, and we achieve up 2532 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538 c July 5 - 10, 2020. 2020 Association for Comp"
2020.acl-main.228,D18-1340,0,0.0259011,"nt over the comparable Re3 Sum model (Cao et al., 2018). Finally, through qualitative analysis, we find evidence that better post-generation ranking is feasible and can lead to substantial performance improvement, which emphasizes the need for future work in developing new post-generation ranking techniques. 2 Related Work Recent work has developed retrieve-and-edit approaches for many tasks, including dialogue generation (Weston et al., 2018), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018), neural machine translation (NMT) (Gu et al., 2018; Zhang et al., 2018; Cao and Xiong, 2018) and post-editing for NMT (Hokamp, 2017; Dabre et al., 2017). Candidate ranking has served as a core part in some retrieval-based models (Ji et al., 2014; Yan et al., 2016), but these models do not edit the retrieved candidates. For machine translation, Bulte and Tezcan (2019) developed a retrieve-and-edit based LSTM model called Neural Fuzzy Repair (NFR), which they applied on two MT datasets obtained from (Steinberger et al., 2012). Using a keyword based followed by a token edit distance based retrieval method called sss+ed, they showed that concatenating the source and retrieved outputs as"
2020.acl-main.228,P18-1015,0,0.470561,"andidate output selection in future work. 1 Figure 1: Our retrieve-edit-rerank framework, generating candidate outputs with three retrieved outputs, and re-ranking yˆ2 as the best candidate post-generation. Introduction Retrieve-and-edit text generation methods have received significant recent interest; editing humanauthored text can potentially avoid many of the challenges that are seen while generating text from scratch, including the tendency to be overly repetitive or to degrade on longer texts (Holtzman et al., 2018, 2019). Retrieve-and-edit methods have been developed for summarization (Cao et al., 2018), machine translation (Wu et al., 2019), language modeling (Guu et al., 2018), and conversation generation (Weston et al., 2018). These methods first retrieve a single output from the training set, and then use a learned model to edit it into the final output. In this paper, we show that generation performance can be improved with a retrieve-edit-rerank approach that instead retrieves a set of outputs from the training set, edits each independently, and then re-ranks the results to produce the final output. Figure 1 shows an overview of the approach. We use standard keyword-based retrieval and"
2020.acl-main.228,N19-1423,0,0.0249175,"-ranking and Generation Similar to Re3 Sum, we design a model that can jointly learn to produce the edited output yˆ and re-rank the retrieved outputs y 0 , which we refer to as pre-ranking, a common practice to determine which retrieved outputs are worth editing. For editing, we use a Transformer as our seq2seq model. We provide the model a concatenated input x[SEP]y 0 , where [SEP] is a separator token, and we train it to produce the original target y with a standard cross entropy loss. For pre-ranking, we add a [RANK] token to the Transformer’s encoder analogous to the [CLS] token in BERT (Devlin et al., 2019). We train the model to predict the similarity between y 0 and y as the output of the [RANK] token, akin to predicting a token from a different vocabulary (Ghazvininejad et al., 2019). We use a cross entropy loss based on a text similarity metric2 , adding it to the Transformer’s loss function. 3.3 Post-generation Ranking For source x, given a set of N input (x concatenated with N retrieved outputs y 0 ) and generated candidate output pairs: 0 {(x[SEP]y10 ; yˆ1 ), . . . , (x[SEP]yN ; yˆN )} 2 we use BLEU for MT and ROUGE-L for Gigaword. This can be any other text similarity metric. 2533 this m"
2020.acl-main.228,D19-1633,1,0.839872,"s pre-ranking, a common practice to determine which retrieved outputs are worth editing. For editing, we use a Transformer as our seq2seq model. We provide the model a concatenated input x[SEP]y 0 , where [SEP] is a separator token, and we train it to produce the original target y with a standard cross entropy loss. For pre-ranking, we add a [RANK] token to the Transformer’s encoder analogous to the [CLS] token in BERT (Devlin et al., 2019). We train the model to predict the similarity between y 0 and y as the output of the [RANK] token, akin to predicting a token from a different vocabulary (Ghazvininejad et al., 2019). We use a cross entropy loss based on a text similarity metric2 , adding it to the Transformer’s loss function. 3.3 Post-generation Ranking For source x, given a set of N input (x concatenated with N retrieved outputs y 0 ) and generated candidate output pairs: 0 {(x[SEP]y10 ; yˆ1 ), . . . , (x[SEP]yN ; yˆN )} 2 we use BLEU for MT and ROUGE-L for Gigaword. This can be any other text similarity metric. 2533 this module’s objective is to select a high quality candidate output. Ideally, we want to find: yˆ∗ = arg max similarity(ˆ yi , y), 1 ≤ i ≤ N yˆi For post-ranking, we use simple ranking fun"
2020.acl-main.228,Q18-1031,0,0.209017,"k framework, generating candidate outputs with three retrieved outputs, and re-ranking yˆ2 as the best candidate post-generation. Introduction Retrieve-and-edit text generation methods have received significant recent interest; editing humanauthored text can potentially avoid many of the challenges that are seen while generating text from scratch, including the tendency to be overly repetitive or to degrade on longer texts (Holtzman et al., 2018, 2019). Retrieve-and-edit methods have been developed for summarization (Cao et al., 2018), machine translation (Wu et al., 2019), language modeling (Guu et al., 2018), and conversation generation (Weston et al., 2018). These methods first retrieve a single output from the training set, and then use a learned model to edit it into the final output. In this paper, we show that generation performance can be improved with a retrieve-edit-rerank approach that instead retrieves a set of outputs from the training set, edits each independently, and then re-ranks the results to produce the final output. Figure 1 shows an overview of the approach. We use standard keyword-based retrieval and a simple editor, where the retrieved output is concatenated to the original"
2020.acl-main.228,W17-4775,0,0.0224012,"., 2018). Finally, through qualitative analysis, we find evidence that better post-generation ranking is feasible and can lead to substantial performance improvement, which emphasizes the need for future work in developing new post-generation ranking techniques. 2 Related Work Recent work has developed retrieve-and-edit approaches for many tasks, including dialogue generation (Weston et al., 2018), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018), neural machine translation (NMT) (Gu et al., 2018; Zhang et al., 2018; Cao and Xiong, 2018) and post-editing for NMT (Hokamp, 2017; Dabre et al., 2017). Candidate ranking has served as a core part in some retrieval-based models (Ji et al., 2014; Yan et al., 2016), but these models do not edit the retrieved candidates. For machine translation, Bulte and Tezcan (2019) developed a retrieve-and-edit based LSTM model called Neural Fuzzy Repair (NFR), which they applied on two MT datasets obtained from (Steinberger et al., 2012). Using a keyword based followed by a token edit distance based retrieval method called sss+ed, they showed that concatenating the source and retrieved outputs as the input significantly boosts translat"
2020.acl-main.228,P18-1152,0,0.0286058,"here our analyses show that there is significant room for performance improvement with better candidate output selection in future work. 1 Figure 1: Our retrieve-edit-rerank framework, generating candidate outputs with three retrieved outputs, and re-ranking yˆ2 as the best candidate post-generation. Introduction Retrieve-and-edit text generation methods have received significant recent interest; editing humanauthored text can potentially avoid many of the challenges that are seen while generating text from scratch, including the tendency to be overly repetitive or to degrade on longer texts (Holtzman et al., 2018, 2019). Retrieve-and-edit methods have been developed for summarization (Cao et al., 2018), machine translation (Wu et al., 2019), language modeling (Guu et al., 2018), and conversation generation (Weston et al., 2018). These methods first retrieve a single output from the training set, and then use a learned model to edit it into the final output. In this paper, we show that generation performance can be improved with a retrieve-edit-rerank approach that instead retrieves a set of outputs from the training set, edits each independently, and then re-ranks the results to produce the final outp"
2020.acl-main.228,N19-4009,0,0.0519873,"Missing"
2020.acl-main.228,D15-1044,0,0.623445,"d retrieval and a simple editor, where the retrieved output is concatenated to the original input to train a Transformerbased seq2seq editing model. Our final re-ranking step is task specific, but again very simple in every case. Our goal here is not to find the best possible way to do the re-ranking. Instead, we show that gains are possible and that it helps to see what edits are made for multiple candidates before making the final decision, instead of following previous work by trying to select a single candidate before editing. We evaluate performance on the Gigaword summarization dataset (Rush et al., 2015) and on the English to Dutch (EN-NL) and the English to Hungarian (EN-HU) machine translation (MT) tasks, following Bulte and Tezcan (2019). For MT, we experimented with different re-ranking schemes but found that the original model score (log-likelihood) worked best, amounting to extended beam search within the complete retreive-edit-rerank pipeline. We improve performance by 6.5 BLEU points on EN-NL and 7.5 on EN-HU over the state-of-art Neural Fuzzy Repair system (Bulte and Tezcan, 2019). On Gigaword, we simply re-rank by returning the most common output, and we achieve up 2532 Proceedings"
2020.acl-main.228,P16-1162,0,0.0600224,"Missing"
2020.acl-main.228,steinberger-etal-2012-dgt,0,0.0556274,"Missing"
2020.acl-main.228,P19-1207,0,0.0435116,"Missing"
2020.acl-main.228,W18-5713,0,0.341866,"hree retrieved outputs, and re-ranking yˆ2 as the best candidate post-generation. Introduction Retrieve-and-edit text generation methods have received significant recent interest; editing humanauthored text can potentially avoid many of the challenges that are seen while generating text from scratch, including the tendency to be overly repetitive or to degrade on longer texts (Holtzman et al., 2018, 2019). Retrieve-and-edit methods have been developed for summarization (Cao et al., 2018), machine translation (Wu et al., 2019), language modeling (Guu et al., 2018), and conversation generation (Weston et al., 2018). These methods first retrieve a single output from the training set, and then use a learned model to edit it into the final output. In this paper, we show that generation performance can be improved with a retrieve-edit-rerank approach that instead retrieves a set of outputs from the training set, edits each independently, and then re-ranks the results to produce the final output. Figure 1 shows an overview of the approach. We use standard keyword-based retrieval and a simple editor, where the retrieved output is concatenated to the original input to train a Transformerbased seq2seq editing m"
2020.acl-main.228,N19-1120,0,0.0344033,"Missing"
2020.acl-main.228,N18-1120,0,0.015874,"1.2 ROUGE improvement over the comparable Re3 Sum model (Cao et al., 2018). Finally, through qualitative analysis, we find evidence that better post-generation ranking is feasible and can lead to substantial performance improvement, which emphasizes the need for future work in developing new post-generation ranking techniques. 2 Related Work Recent work has developed retrieve-and-edit approaches for many tasks, including dialogue generation (Weston et al., 2018), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018), neural machine translation (NMT) (Gu et al., 2018; Zhang et al., 2018; Cao and Xiong, 2018) and post-editing for NMT (Hokamp, 2017; Dabre et al., 2017). Candidate ranking has served as a core part in some retrieval-based models (Ji et al., 2014; Yan et al., 2016), but these models do not edit the retrieved candidates. For machine translation, Bulte and Tezcan (2019) developed a retrieve-and-edit based LSTM model called Neural Fuzzy Repair (NFR), which they applied on two MT datasets obtained from (Steinberger et al., 2012). Using a keyword based followed by a token edit distance based retrieval method called sss+ed, they showed that concatenating the source and"
2020.acl-main.536,N19-1388,0,0.0212392,"ty of neural network activation between different layers and different models (Laakso and Cottrell, 2000; Li et al., 2016; Raghu et al., 2017; Morcos et al., 2018; Wang et al., 2018). For example, Raghu et al. 6023 (2017) use canonical correlation analysis (CCA) and a new method, singular vector canonical correlation analysis (SVCCA), to show that early layers converge faster than upper layers in convolutional neural networks. Kudugunta et al. (2019) use SVCCA to investigate the multilingual representations obtained by the encoder of a massively multilingual neural machine translation system (Aharoni et al., 2019). Kornblith et al. (2019) argues that CCA fails to measure meaningful similarities between representations that have a higher dimension than the number of data points and introduce the centered kernel alignment (CKA) to solve this problem. They successfully use CKA to identify correspondences between activations in networks trained from different initializations. 3 Cross-lingual Pretraining We study a standard multilingual masked language modeling formulation and evaluate performance on several different cross-lingual transfer tasks, as described in this section. 3.1 Multilingual Masked Langua"
2020.acl-main.536,P17-1042,0,0.0169006,"performance. Alignment of Word Embeddings Researchers working on word embeddings noticed early that embedding spaces tend to be shaped similarly across different languages (Mikolov et al., 2013). This inspired work in aligning monolingual embeddings. The alignment was done by using a bilingual dictionary to project words that have the same meaning close to each other (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follow-up efforts only required a very small seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on"
2020.acl-main.536,Q19-1038,0,0.0706816,"Missing"
2020.acl-main.536,Q17-1010,0,0.0732513,"we first encode 500k sentences in each language. At each layer, and for each word, we collect all contextualized representations of a word in the 500k sentences and average them to get a single embedding. Since BERT operates at the subword level, for one word we consider the average of all its subword embeddings. Eventually, we get one word embedding per layer. We use the MUSE benchmark (Conneau et al., 2017), a bilingual dictionary induction dataset for alignment supervision and evaluate the alignment on word translation retrieval. As a baseline, we use the first 200k embeddings of fastText (Bojanowski et al., 2017) and learn the mapping using the same procedure as §5.1. Note we use a subset of 200k vocabulary of fastText, the same as BERT, to get a comparable number. We retrieve word translation using CSLS (Conneau et al., 2017) with K=10. In Figure 4, we report the alignment results under these two settings. Figure 4a shows that the subword embeddings matrix of BERT, where each subword is a standalone word, can easily be aligned with an orthogonal mapping and obtain slightly better performance than the same subset of fastText. Figure 4b shows embeddings matrix with the average of all contextual embeddi"
2020.acl-main.536,D18-1269,1,0.938862,"formance on zeroshot cross-lingual transfer without using any parallel data during training (Wu and Dredze, 2019; Pires et al., 2019). This shows that multilingual representations can emerge from a shared Transformer with a shared subword vocabulary. Crosslingual language model (XLM) pretraining (Lample and Conneau, 2019) was introduced concurrently to mBERT. On top of multilingual masked language models, they investigate an objective based on parallel sentences as an explicit crosslingual signal. XLM shows that cross-lingual language model pretraining leads to a new state of the art on XNLI (Conneau et al., 2018), supervised and unsupervised machine translation (Lample et al., 2018). Other work has shown that mBERT outperforms word embeddings on token-level NLP tasks (Wu and Dredze, 2019), and that adding character-level information (Mulcaire et al., 2019) and using multi-task learning (Huang et al., 2019) can improve cross-lingual performance. Alignment of Word Embeddings Researchers working on word embeddings noticed early that embedding spaces tend to be shaped similarly across different languages (Mikolov et al., 2013). This inspired work in aligning monolingual embeddings. The alignment was done"
2020.acl-main.536,N19-1423,0,0.287494,"quirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process. 1 Introduction Multilingual language models such as mBERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) enable effective cross-lingual transfer — it is possible to learn a model from supervised data in one language and apply it to another with no additional training. Recent work has shown that transfer is effective for a wide range of tasks (Wu and Dredze, 2019; Pires et al., 2019). These work speculates why multilingual pretraining works (e.g. shared vocabulary), but only experiment with a single reference mBERT and is unable to systematically measure these effects. In this paper, we present the first detailed empirical study of the effects of different maske"
2020.acl-main.536,P81-1022,0,0.406539,"Missing"
2020.acl-main.536,N13-1073,0,0.0334766,"decent quality bilingual dictionary, although underperforming fastText. We notice that using contextual representations from higher layers obtain better results compared to lower layers. BERT models in contextual setting, and evaluate performance on cross-lingual transfer for NER and parsing. We take the Transformer layers of each monolingual model up to layer i, and learn a mapping W from layer i of the target model to layer i of the source model. To create that mapping, we use the same Procrustes approach but use a dictionary of parallel contextual words, obtained by running the fastAlign (Dyer et al., 2013) model on the 10k XNLI parallel sentences. For each downstream task, we learn task-specific layers on top of i-th English layer: four Transformer layers and a task-specific layer. We learn these on the training set, but keep the first i pretrained layers freezed. After training these taskspecific parameters, we encode (say) a Chinese sentence with the first i layers of the target Chinese BERT model, project the contextualized representations back to the English space using the W we learned, and then use the task-specific layers for NER and parsing. In Figure 5, we vary i from the embedding lay"
2020.acl-main.536,P18-1031,0,0.0298594,"uages. • We show monolingual BERTs of different language are similar with each other. Similar to word embeddings (Mikolov et al., 2013), we show monolingual BERT can be easily aligned with linear mapping to produce crosslingual representation space at each level. 2 Background Language Model Pretraining Our work follows in the recent line of language model pretraining. ELMo (Peters et al., 2018) first popularized representation learning from a language model. The representations are used in a transfer learning setup to improve performance on a variety of downstream NLP tasks. Follow-up work by Howard and Ruder (2018); Radford et al. (2018) further improves on this idea by fine-tuning the entire language model. BERT (Devlin et al., 2019) significantly outperforms these methods by introducing a masked-language model and next-sentence prediction objectives combined with a bi-directional transformer model. The multilingual version of BERT (dubbed mBERT) trained on Wikipedia data of over 100 languages obtains strong performance on zeroshot cross-lingual transfer without using any parallel data during training (Wu and Dredze, 2019; Pires et al., 2019). This shows that multilingual representations can emerge fro"
2020.acl-main.536,D19-1252,0,0.0538289,"Missing"
2020.acl-main.536,kamholz-etal-2014-panlex,0,0.0423726,"Missing"
2020.acl-main.536,P07-2045,0,0.0114002,"Missing"
2020.acl-main.536,D19-1167,0,0.0211172,"ilarity is relatively straightforward, the same cannot be said for the deep contextualized BERT models that we study. Recent work introduces ways to measure the similarity of neural network activation between different layers and different models (Laakso and Cottrell, 2000; Li et al., 2016; Raghu et al., 2017; Morcos et al., 2018; Wang et al., 2018). For example, Raghu et al. 6023 (2017) use canonical correlation analysis (CCA) and a new method, singular vector canonical correlation analysis (SVCCA), to show that early layers converge faster than upper layers in convolutional neural networks. Kudugunta et al. (2019) use SVCCA to investigate the multilingual representations obtained by the encoder of a massively multilingual neural machine translation system (Aharoni et al., 2019). Kornblith et al. (2019) argues that CCA fails to measure meaningful similarities between representations that have a higher dimension than the number of data points and introduce the centered kernel alignment (CKA) to solve this problem. They successfully use CKA to identify correspondences between activations in networks trained from different initializations. 3 Cross-lingual Pretraining We study a standard multilingual masked"
2020.acl-main.536,J82-2005,0,0.496087,"Missing"
2020.acl-main.536,W06-0115,0,0.0767866,"Missing"
2020.acl-main.536,N19-1392,0,0.0253633,"ry. Crosslingual language model (XLM) pretraining (Lample and Conneau, 2019) was introduced concurrently to mBERT. On top of multilingual masked language models, they investigate an objective based on parallel sentences as an explicit crosslingual signal. XLM shows that cross-lingual language model pretraining leads to a new state of the art on XNLI (Conneau et al., 2018), supervised and unsupervised machine translation (Lample et al., 2018). Other work has shown that mBERT outperforms word embeddings on token-level NLP tasks (Wu and Dredze, 2019), and that adding character-level information (Mulcaire et al., 2019) and using multi-task learning (Huang et al., 2019) can improve cross-lingual performance. Alignment of Word Embeddings Researchers working on word embeddings noticed early that embedding spaces tend to be shaped similarly across different languages (Mikolov et al., 2013). This inspired work in aligning monolingual embeddings. The alignment was done by using a bilingual dictionary to project words that have the same meaning close to each other (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follo"
2020.acl-main.536,P19-1492,0,0.0159658,"e by using a bilingual dictionary to project words that have the same meaning close to each other (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follow-up efforts only required a very small seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on dependency parsing as well. Neural Network Activation Similarity We hypothesize that similar to word embedding spaces, language-universal structures emerge in pretrained language models. While computing word embedding similarity is relatively straightforward, the same canno"
2020.acl-main.536,P17-1178,0,0.0920848,"Missing"
2020.acl-main.536,P19-1018,0,0.021426,"The alignment was done by using a bilingual dictionary to project words that have the same meaning close to each other (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follow-up efforts only required a very small seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on dependency parsing as well. Neural Network Activation Similarity We hypothesize that similar to word embedding spaces, language-universal structures emerge in pretrained language models. While computing word embedding similarity is relatively straig"
2020.acl-main.536,N18-1202,1,0.529587,"rosslingual representation can still be learned. With bilingual dictionary, we propose a simple technique to create more anchor points by creating synthetic code-switched corpus, benefiting especially distantly-related languages. • We show monolingual BERTs of different language are similar with each other. Similar to word embeddings (Mikolov et al., 2013), we show monolingual BERT can be easily aligned with linear mapping to produce crosslingual representation space at each level. 2 Background Language Model Pretraining Our work follows in the recent line of language model pretraining. ELMo (Peters et al., 2018) first popularized representation learning from a language model. The representations are used in a transfer learning setup to improve performance on a variety of downstream NLP tasks. Follow-up work by Howard and Ruder (2018); Radford et al. (2018) further improves on this idea by fine-tuning the entire language model. BERT (Devlin et al., 2019) significantly outperforms these methods by introducing a masked-language model and next-sentence prediction objectives combined with a bi-directional transformer model. The multilingual version of BERT (dubbed mBERT) trained on Wikipedia data of over"
2020.acl-main.536,P19-1493,0,0.328434,"word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process. 1 Introduction Multilingual language models such as mBERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) enable effective cross-lingual transfer — it is possible to learn a model from supervised data in one language and apply it to another with no additional training. Recent work has shown that transfer is effective for a wide range of tasks (Wu and Dredze, 2019; Pires et al., 2019). These work speculates why multilingual pretraining works (e.g. shared vocabulary), but only experiment with a single reference mBERT and is unable to systematically measure these effects. In this paper, we present the first detailed empirical study of the effects of different masked lan∗ Equal contribution. Work done while Shijie was interning at Facebook AI. guage modeling (MLM) pretraining regimes on cross-lingual transfer. Our first set of experiments is a detailed ablation study on a range of zero-shot cross-lingual transfer tasks. Much to our surprise, we discover that language universa"
2020.acl-main.536,E17-2025,0,0.0707663,"Missing"
2020.acl-main.536,N19-1162,0,0.0316151,"er (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follow-up efforts only required a very small seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on dependency parsing as well. Neural Network Activation Similarity We hypothesize that similar to word embedding spaces, language-universal structures emerge in pretrained language models. While computing word embedding similarity is relatively straightforward, the same cannot be said for the deep contextualized BERT models that we study. Recent work introduces ways t"
2020.acl-main.536,P16-1162,0,0.0900458,"Missing"
2020.acl-main.536,P17-1179,0,0.0305027,"ddings noticed early that embedding spaces tend to be shaped similarly across different languages (Mikolov et al., 2013). This inspired work in aligning monolingual embeddings. The alignment was done by using a bilingual dictionary to project words that have the same meaning close to each other (Mikolov et al., 2013). This projection aligns the words outside of the dictionary as well due to the similar shapes of the word embedding spaces. Follow-up efforts only required a very small seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on dependency parsing as well. Neural Network Activation Similarity We hypo"
2020.acl-main.536,P19-1307,0,0.0845179,"also plot the cross-lingual similarity of neural network activation with center kernel alignment (§5.2) at each layer. We consider five languages: English, French, German, Russian, and Chinese. 5.1 Aligning Monolingual BERTs To measure similarity, we learn an orthogonal mapping using the Procrustes (Smith et al., 2017) approach: 6027 W ? = argmin kW X − Y kF = U V T W ∈Od (R) with U ΣV T = SVD(Y X T ), where X and Y are representation of two monolingual BERT models, sampled at different granularities as described below. We apply iterative normalization on X and Y before learning the mapping (Zhang et al., 2019). 5.1.1 Word-level alignment In this section, we align both the non-contextual word representations from the embedding layers, and the contextual word representations from the hidden states of the Transformer at each layer. For non-contextualized word embeddings, we define X and Y as the word embedding layers of monolingual BERT, which contain a single embedding per word (type). Note that in this case we only keep words containing only one subword. For contextualized word representations, we first encode 500k sentences in each language. At each layer, and for each word, we collect all contextu"
2020.acl-main.536,P18-1072,0,0.0495677,"Missing"
2020.acl-main.536,D19-1575,0,0.0302383,"all seed dictionary (e.g., only numbers (Artetxe et al., 2017)) or even no dictionary at all (Conneau et al., 2017; Zhang et al., 2017). Other work has pointed out that word embeddings may not be as isomorphic as thought (Søgaard et al., 2018) especially for distantly related language pairs (Patra et al., 2019). Ormazabal et al. (2019) show joint training can lead to more isomorphic word embeddings space. Schuster et al. (2019) showed that ELMo embeddings can be aligned by a linear projection as well. They demonstrate a strong zero-shot crosslingual transfer performance on dependency parsing. Wang et al. (2019) align mBERT representations and evaluate on dependency parsing as well. Neural Network Activation Similarity We hypothesize that similar to word embedding spaces, language-universal structures emerge in pretrained language models. While computing word embedding similarity is relatively straightforward, the same cannot be said for the deep contextualized BERT models that we study. Recent work introduces ways to measure the similarity of neural network activation between different layers and different models (Laakso and Cottrell, 2000; Li et al., 2016; Raghu et al., 2017; Morcos et al., 2018; W"
2020.acl-main.536,D19-1077,1,0.928506,"e for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process. 1 Introduction Multilingual language models such as mBERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) enable effective cross-lingual transfer — it is possible to learn a model from supervised data in one language and apply it to another with no additional training. Recent work has shown that transfer is effective for a wide range of tasks (Wu and Dredze, 2019; Pires et al., 2019). These work speculates why multilingual pretraining works (e.g. shared vocabulary), but only experiment with a single reference mBERT and is unable to systematically measure these effects. In this paper, we present the first detailed empirical study of the effects of different masked lan∗ Equal contribution. Work done while Shijie was interning at Facebook AI. guage modeling (MLM) pretraining regimes on cross-lingual transfer. Our first set of experiments is a detailed ablation study on a range of zero-shot cross-lingual transfer tasks. Much to our surprise, we discover t"
2020.acl-main.626,P13-1023,0,0.017291,"annotations, exhibited in the Dense dataset (§2) as well as the output of the Fitzgerald et al. (2018) parser (§5). To that end, we ignore redundant true-positives, and collapse false-positive errors (see Appendix for details). 4 Dataset Quality Analysis Inter-Annotator Agreement (IAA) To estimate dataset consistency across different annotations, we measure F1 using our UA metric. 10 individual worker-vs-worker experiments yield 79.8 F1 agreement over 150 predicates, indicating high consistency across our annotators, in line with agreement rates in other structured semantic annotations, e.g. Abend and Rappoport (2013). Overall consistency of the dataset is assessed by measuring agreement between different consolidated annotations, obtained by disjoint triplets of workers, which achieves F1 of 84.1, averaged over 4 experiments, 35 predicates each. Notably, consolidation boosts agreement, indicating its necessity. For LA agreement, averaged F1 was 67.8; however, it is likely that the drop from UA is mainly due to falsely rejecting semantically equivalent questions under the S TRICT-M ATCH criterion, given that we found equal LA and UA scores in a manual evaluation of our dataset (see Table 4 below). Dataset"
2020.acl-main.626,P98-1013,0,0.0994746,"t an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering que"
2020.acl-main.626,P19-1409,1,0.751829,"t set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model em"
2020.acl-main.626,N18-1076,0,0.0125319,"e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achie"
2020.acl-main.626,I17-1010,0,0.0131356,"nding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implici"
2020.acl-main.626,P18-1191,1,0.943217,"20. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not resign, cut funding to a program she ran. (2) Why was something cut by someone? she did not resign (3) Who cut something? Perry Table 1: QA-SRL examples. The bar (|) separates multiple answers. Implicit arguments are highlighted. Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (Fitzgerald et al., 2018) for scalability. Naturally, employing crowd workers is challenging when annotating fairly demanding structures like SRL. As Fitzgerald et al. (2018) acknowledge, the main shortage of their large-scale dataset is limited recall, which we estimate to be in the lower 70s (see §4). Unfortunately, such low recall in gold standard datasets hinders proper research and evaluation, undermining the current viability of the QA-SRL paradigm. Aiming to enable future QA-SRL research, we present a generic controlled crowdsourcing annotation protocol and apply it to QA-SRL. Our process addresses worker quali"
2020.acl-main.626,D15-1076,1,0.934655,"rd will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering questions about the predicate (“Who did What to Whom, When, Where and How”, with, e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexic"
2020.acl-main.626,J05-1004,0,0.177821,"valuation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception"
2020.acl-main.626,D16-1252,1,0.870589,"L schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not r"
2020.acl-main.626,P15-2115,0,0.0277604,"ehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in t"
2020.acl-main.626,J12-4003,0,0.0287838,"t role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as"
2020.acl-main.626,2020.acl-main.772,0,0.0935617,"stly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could b"
2020.acl-main.626,C98-1013,0,\N,Missing
2020.acl-main.703,I05-5002,0,0.0666519,"Missing"
2020.acl-main.703,N19-1409,0,0.0820284,"Missing"
2020.acl-main.703,P19-1346,0,0.0551333,"Missing"
2020.acl-main.703,2020.tacl-1.5,1,0.898817,"Missing"
2020.acl-main.703,P17-1099,0,0.560122,"Missing"
2020.acl-main.703,W16-2323,0,0.0872041,"Missing"
2020.acl-main.703,D13-1170,0,0.0103825,"Missing"
2020.acl-main.703,D18-1206,0,0.21281,"Missing"
2020.acl-main.703,W18-5446,1,0.889474,"Missing"
2020.acl-main.703,N18-1202,1,0.621418,"Missing"
2020.acl-main.703,N18-1101,0,0.138739,"Missing"
2020.acl-main.703,D16-1264,0,0.368154,"Missing"
2020.acl-main.738,W09-1901,0,0.554675,"ion P (y = ant(i)) over the set of all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Previous approaches for the annotation of coreference resolution have used mostly pairwise selection, where pairs of mentions are shown to a human annotator who marks whether they are co-referring (Gasperin, 2009; Laws et al., 2012; Zhao and Ng, 2014; Sachan et al., 2015). To incorporate these binary annotations into their clustering coreference model, Sachan et al. (2015) introduced the notion of must-link and cannot-link penalties, which we describe and extend in Section 4. 3 Discrete Annotation In discrete annotation, as exemplified in Figure 1, we present the annotator with a document where the least certain span i (“Po-po”, in the example) and i’s model-predicted antecedent, A(i) (“locals”), are highlighted. Similarly to pairwise annotation, annotators are first asked whether i and A(i) are coref"
2020.acl-main.738,P19-1164,1,0.810293,"ocument, a span (yellow), and the span’s predicted antecedent (blue). In case the answer to the coreference question is negative (i.e., the spans are not coreferring), we present a follow-up question (“what is the first appearance of the entity?”), providing additional cost-effective signal. Our annotation interface can be seen in Figure 5 in the Appendix. Introduction Coreference resolution is the task of resolving anaphoric expressions to their antecedents (see Figure 1). It is often required in downstream applications such as question answering (Dasigi et al., 2019) or machine translation (Stanovsky et al., 2019). Exhaustively annotating coreference is an expensive process as it requires tracking coreference chains across long passages of text. In news stories, for example, important entities may be referenced many paragraphs after their introduction. Active learning is a technique which aims to reduce costs by annotating samples which will be most beneficial for the learning process, rather than fully labeling a large fixed training set. Active learning consists of two components: (1) a taskspecific learning algorithm, and (2) an iterative sample selection algorithm, which examines the performance of"
2020.acl-main.738,W14-1104,0,0.181252,"all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Previous approaches for the annotation of coreference resolution have used mostly pairwise selection, where pairs of mentions are shown to a human annotator who marks whether they are co-referring (Gasperin, 2009; Laws et al., 2012; Zhao and Ng, 2014; Sachan et al., 2015). To incorporate these binary annotations into their clustering coreference model, Sachan et al. (2015) introduced the notion of must-link and cannot-link penalties, which we describe and extend in Section 4. 3 Discrete Annotation In discrete annotation, as exemplified in Figure 1, we present the annotator with a document where the least certain span i (“Po-po”, in the example) and i’s model-predicted antecedent, A(i) (“locals”), are highlighted. Similarly to pairwise annotation, annotators are first asked whether i and A(i) are coreferent. If they answer positively, we m"
2020.acl-main.738,W12-2409,0,0.32051,"Missing"
2020.acl-main.738,D17-1018,1,0.952354,"duct experiments across several sample selection algorithms using existing gold data for user labels and show that both of our contributions significantly improve performance on the CoNLL2012 dataset (Pradhan et al., 2012). Overall, our active learning method presents a superior alternative to pairwise annotation for coreference resolution, achieving better performing models for a given annotation budget. 2 Background Our work relies on two main components: a coreference resolution model and a sample selection algorithm. Coreference resolution model We use the span ranking model introduced by Lee et al. (2017), and later implemented in AllenNLP framework (Gardner et al., 2018). This model computes span embeddings for all possible spans i in a document, and uses them to compute a probability distribution P (y = ant(i)) over the set of all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Prev"
2020.acl-main.738,W12-4501,0,0.518253,"del predictions 8320 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8320–8331 c July 5 - 10, 2020. 2020 Association for Computational Linguistics for all antecedents which belong to the same cluster. This avoids repeated labeling that would come with separately verifying every mention pair within the same cluster, as done in previous methods. We conduct experiments across several sample selection algorithms using existing gold data for user labels and show that both of our contributions significantly improve performance on the CoNLL2012 dataset (Pradhan et al., 2012). Overall, our active learning method presents a superior alternative to pairwise annotation for coreference resolution, achieving better performing models for a given annotation budget. 2 Background Our work relies on two main components: a coreference resolution model and a sample selection algorithm. Coreference resolution model We use the span ranking model introduced by Lee et al. (2017), and later implemented in AllenNLP framework (Gardner et al., 2018). This model computes span embeddings for all possible spans i in a document, and uses them to compute a probability distribution P (y ="
2020.acl-main.738,N13-1014,0,\N,Missing
2020.acl-main.738,W18-2501,1,\N,Missing
2020.acl-main.747,C18-1139,0,0.0222089,"the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. Named Entity Recognition. For NER, we consider the CoNLL-2002 (Sang, 2002) and CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). Cross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018). We use BERTLarge and RoBERTa as baselines. 5 An"
2020.acl-main.747,D14-1162,0,0.0865806,"Missing"
2020.acl-main.747,L18-1550,1,0.853318,"ortance of scaling the amount of data and RoBERTa (Liu et al., 2019) shows that training BERT longer on more data leads to significant boost in performance. Inspired by RoBERTa, we show that mBERT and XLM are undertuned, and that simple improvements in the learning procedure of unsupervised MLM leads to much better performance. We train on cleaned CommonCrawls (Wenzek et al., 2019), which increase the amount of data for low-resource languages by two orders of magnitude on average. Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages (Grave et al., 2018). Several efforts have trained massively multilingual machine translation models from large parallel corpora. They uncover the high and low resource trade-off and the problem of capacity dilution (Johnson et al., 2017; Tan et al., 2019). The work most similar to ours is Arivazhagan et al. (2019), which trains a single model in 103 languages on over 25 billion parallel sentences. Siddhant et al. (2019) further analyze the representations obtained by the encoder of a massively multilingual machine translation system and show that it obtains similar results to mBERT on cross-lingual NLI. Our work"
2020.acl-main.747,N18-1202,1,0.596746,"I benchmarks, where XLM-R obtains results competitive with state-of-the-art monolingual models, including RoBERTa (Liu et al., 2019). These results demonstrate, for the first time, that it is possible to have a single large model for all languages, without sacrificing per-language performance. We will make our code, models and data publicly available, with the hope that this will help research in multilingual NLP and low-resource language understanding. 2 Related Work From pretrained word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) to pretrained contextualized representations (Peters et al., 2018; Schuster et al., 2019) and transformer based language models (Radford et al., 2018; Devlin et al., 2018), unsupervised representation learning has significantly improved the state of the art in natural language understanding. Parallel work on cross-lingual understanding (Mikolov et al., 2013a; Schuster et al., 2019; Lample and Conneau, 2019) extends these systems to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages. Most recently, Devlin et al. (2018) and Lample and Conneau (2019) introduced mBERT and XLM - masked langu"
2020.acl-main.747,D19-1252,0,0.171651,"nguage modeling (TLM) as a way to leverage parallel data and obtain a new state of the art on the cross-lingual natural language inference (XNLI) benchmark (Conneau et al., 2018). They further show strong improvements on unsupervised machine translation and pretraining for sequence generation. Wu et al. (2019) shows that monolingual BERT representations are similar across languages, explaining in part the natural emergence of multilinguality in bottleneck architectures. Separately, Pires et al. (2019) demonstrated the effectiveness of multilingual models like mBERT on sequence labeling tasks. Huang et al. (2019) showed gains over XLM using cross-lingual multi-task learning, and Singh et al. (2019) demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI. However, all of this work was at a relatively modest scale, in terms of the amount of training data, as compared to our approach. The benefits of scaling language model pretraining by increasing the size of the model as well as the training data has been extensively studied in the literature. For the monolingual case, Jozefowicz et al. (2016) show how large-scale LSTM models can obtain much stronger performance on language"
2020.acl-main.747,E17-2068,1,0.723734,"hili and Urdu. We chose this set as it covers a suitable range of language families and includes low-resource languages such as Swahili and Urdu. We also consider larger sets of 15, 30, 60 and all 100 languages. When reporting results on high-resource and lowresource, we refer to the average of English and French results, and the average of Swahili and Urdu results respectively. Scaling the Amount of Training Data. Following Wenzek et al. (2019) 2 , we build a clean CommonCrawl Corpus in 100 languages. We use an internal language identification model in combination with the one from fastText (Joulin et al., 2017). We train language models in each language and use it to filter documents as described in Wenzek et al. (2019). We consider one CommonCrawl dump for English and twelve dumps for all other languages, which significantly increases dataset sizes, especially for low-resource languages like Burmese and Swahili. Figure 1 shows the difference in size between the Wikipedia Corpus used by mBERT and XLM100, and the CommonCrawl Corpus we use. As we show in Section 5.3, monolingual Wikipedia corpora are too small to enable unsupervised representation learning. Based on our experiments, we found that a fe"
2020.acl-main.95,C14-1151,0,0.32001,"ing a shared output space (Raganato et al., 2017a). Other neural approaches used semi-supervised learning to augment the learned representations with additional data (Melamud et al., 2016; Yuan et al., 2016). 2.1 Lexical Resources for WSD Definitions of senses, or glosses, have been shown to be a valuable resource for improving WSD. Lesk (1986) used the overlap between the definitions of senses and the context of the target word to predict the target sense. This approach was later extended to incorporate WordNet graph structure (Banerjee and Pedersen, 2003) and to incorporate word embeddings (Basile et al., 2014). More recently, Luo et al. (2018a,b) added sense glosses as additional inputs into their neural WSD system, significantly improving overall performance. Most similar to our work, Kumar et al. (2019) represented senses as continuous representations learned from encoded glosses. However, they took a pipelined approach and supervised the gloss encoder with knowledge graph embeddings; they then froze the sense representations to use them as static supervision for training the WSD system. This approach requires an additional form of supervision (for which they used knowledge graph embeddings), mak"
2020.acl-main.95,D18-1181,0,0.0273355,"froze the sense representations to use them as static supervision for training the WSD system. This approach requires an additional form of supervision (for which they used knowledge graph embeddings), making it more difficult to generalize to new data without that source of supervision. In comparison, our model is trained in an end-to-end manner and learns to embed gloss text without additional supervision. Other work has shown that neural models capture useful semantic information about words from their definitions, and has used them to encode lexical representations (Bahdanau et al., 2017; Bosc and Vincent, 2018). While they focused on representing words, rather than specific senses, their modeling approaches could be extended to sense representations. 2.2 Pretrained NLP Models for WSD Pretrained models have been shown to capture a surprising amount of word sense information from their pretraining objectives alone (Peters et al., 2018; Stanovsky and Hopkins, 2018; Coenen et al., 2019), allowing the frozen pretrained representations to compete with previous state-of-the-art WSD systems (Hadiwinoto et al., 2019). Building on these findings, Vial et al. (2019) incorporates pretrained BERT representations"
2020.acl-main.95,D15-1084,0,0.0533907,"The source code and trained models for our WSD bi-encoders can be found at https://github. com/facebookresearch/wsd-biencoders. 2 Background and Related Work Word Sense Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better featu"
2020.acl-main.95,W06-1670,0,0.0599332,"es well from limited data. The source code and trained models for our WSD bi-encoders can be found at https://github. com/facebookresearch/wsd-biencoders. 2 Background and Related Work Word Sense Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encod"
2020.acl-main.95,N19-1423,0,0.69032,"g performance on MFS examples. Previous work has found that incorporating lexical information such as sense definitions, or glosses, into WSD systems improves performance (Luo et al., 2018a,b).1 Glosses have also been found to improve LFS performance; however, absolute performance on rare senses is still low, with models showing a 62.3 F1 performance drop between the MFS examples and the LFS ones (Kumar et al., 2019). In this paper, we show that this gap can be significantly reduced by jointly fine-tuning multiple pretrained encoders on WSD. We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding space by using a context encoder to represent the target word and surrounding context, and a gloss encoder to represent the sense definitions. However, our two encoders are jointly learned from the WSD objective alone and trained in an end-to-end fashion. This approach allows our model to outperform prior work on the English all-words WSD task introduced in Raganato et al. (2017b). Analysis of our model shows that these gains come"
2020.acl-main.95,D19-1533,0,0.628519,"heir definitions, and has used them to encode lexical representations (Bahdanau et al., 2017; Bosc and Vincent, 2018). While they focused on representing words, rather than specific senses, their modeling approaches could be extended to sense representations. 2.2 Pretrained NLP Models for WSD Pretrained models have been shown to capture a surprising amount of word sense information from their pretraining objectives alone (Peters et al., 2018; Stanovsky and Hopkins, 2018; Coenen et al., 2019), allowing the frozen pretrained representations to compete with previous state-of-the-art WSD systems (Hadiwinoto et al., 2019). Building on these findings, Vial et al. (2019) incorporates pretrained BERT representations as inputs into their WSD system, and Loureiro and Jorge 1007 Scores Sense Embeddings Context Encoder Gloss Encoder The Transformer Transformer plant [CLS] sprouted a living ... ... Figure 1: Architecture of our bi-encoder model for WSD. The context sentence and sense gloss text are input into the context and gloss encoders, respectively; each encoder is initialized with BERT. We take the ith output of the context encoder as the representation for the target word wi ; the first output of the gloss enco"
2020.acl-main.95,D19-1355,0,0.638361,"ith a dot product, and the sen se with the highest similarity to wi is assigned as the predicted label. (2019) uses BERT’s contextualized outputs to create sense embeddings for each sense in WordNet. Another approach to using pretrained models for WSD is to formulate the task as a sentencepair classification problem, in which (context sentence, gloss) pairs are concatenated and crossencoded with the pretrained model. This reduces the WSD task to a binary classification problem where the model is trained to predict whether the gloss matches the sense of the target word in the context sentence (Huang et al., 2019). Given that transformer compute scales polynomially in the input length, our approach of independently encoding the contexts and sense glosses is more computationally efficient, and we also show that it performs better on the all-words WSD task (Section 5.1). 3 Methodology In this section, we present an approach for WSD that is designed to more accurately model less frequent senses by better leveraging the glosses that define them. The overall model architecture is shown in Figure 1. Our bi-encoder model (BEM) consists of two independent encoders: (1) a context encoder, which represents the t"
2020.acl-main.95,P16-1085,0,0.0660994,"Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled each word using a shared output space (Raganato et al., 2017a). Other neural approaches used semi-supervised learning to augment the learned representations with additional data (Melamud et al., 2016; Yuan et al., 2016). 2.1 Lexical Resources for WSD Definitions of senses, or glosses, have been shown to be a valuable resource for improving WSD. Lesk (1986) used the"
2020.acl-main.95,W16-5307,0,0.266633,"Missing"
2020.acl-main.95,P19-1568,0,0.717347,"ounding context (Postma et al., 2016). A successful WSD system should be able to overcome this bias and correctly disambiguate cases where a word takes a less frequent sense (LFS), without sacrificing performance on MFS examples. Previous work has found that incorporating lexical information such as sense definitions, or glosses, into WSD systems improves performance (Luo et al., 2018a,b).1 Glosses have also been found to improve LFS performance; however, absolute performance on rare senses is still low, with models showing a 62.3 F1 performance drop between the MFS examples and the LFS ones (Kumar et al., 2019). In this paper, we show that this gap can be significantly reduced by jointly fine-tuning multiple pretrained encoders on WSD. We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding space by using a context encoder to represent the target word and surrounding context, and a gloss encoder to represent the sense definitions. However, our two encoders are jointly learned from the WSD objective alone and trained in a"
2020.acl-main.95,2021.ccl-1.108,0,0.088368,"Missing"
2020.acl-main.95,P19-1569,0,0.487791,"Missing"
2020.acl-main.95,D18-1170,0,0.704607,"half of the ten senses of plant occur in the dataset at all (Miller et al., 1993). Due to this data imbalance, many WSD systems show a strong bias towards predicting the most frequent sense (MFS) of a word regardless of the surrounding context (Postma et al., 2016). A successful WSD system should be able to overcome this bias and correctly disambiguate cases where a word takes a less frequent sense (LFS), without sacrificing performance on MFS examples. Previous work has found that incorporating lexical information such as sense definitions, or glosses, into WSD systems improves performance (Luo et al., 2018a,b).1 Glosses have also been found to improve LFS performance; however, absolute performance on rare senses is still low, with models showing a 62.3 F1 performance drop between the MFS examples and the LFS ones (Kumar et al., 2019). In this paper, we show that this gap can be significantly reduced by jointly fine-tuning multiple pretrained encoders on WSD. We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding sp"
2020.acl-main.95,P18-1230,0,0.689678,"half of the ten senses of plant occur in the dataset at all (Miller et al., 1993). Due to this data imbalance, many WSD systems show a strong bias towards predicting the most frequent sense (MFS) of a word regardless of the surrounding context (Postma et al., 2016). A successful WSD system should be able to overcome this bias and correctly disambiguate cases where a word takes a less frequent sense (LFS), without sacrificing performance on MFS examples. Previous work has found that incorporating lexical information such as sense definitions, or glosses, into WSD systems improves performance (Luo et al., 2018a,b).1 Glosses have also been found to improve LFS performance; however, absolute performance on rare senses is still low, with models showing a 62.3 F1 performance drop between the MFS examples and the LFS ones (Kumar et al., 2019). In this paper, we show that this gap can be significantly reduced by jointly fine-tuning multiple pretrained encoders on WSD. We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding sp"
2020.acl-main.95,K16-1006,0,0.0812174,"iguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled each word using a shared output space (Raganato et al., 2017a). Other neural approaches used semi-supervised learning to augment the learned representations with additional data (Melamud et al., 2016; Yuan et al., 2016). 2.1 Lexical Resources for WSD Definitions of senses, or glosses, have been shown to be a valuable resource for improving WSD. Lesk (1986) used the overlap between the definitions of senses and the context of the target word to predict the target sense. This approach was later extended to incorporate WordNet graph structure (Banerjee and Pedersen, 2003) and to incorporate word embeddings (Basile et al., 2014). More recently, Luo et al. (2018a,b) added sense glosses as additional inputs into their neural WSD system, significantly improving overall performance. Most similar"
2020.acl-main.95,H93-1061,0,0.242222,"tion rsi has the highest dot product score with rw . We use a cross-entropy loss on the scores for the candidate senses of the target word w to train our bi-encoder model; the loss function of our system given a (word, sense) pair (w, si ) is |Sw | L(w, si ) = −φ(w, si ) + log X exp(φ(w, sj )) j=0 4 4.1 Experimental Setup WSD Task and Datasets We evaluate our BEM system with the WSD framework established in Raganato et al. (2017b). We train our model on SemCor, a large dataset manually annotated with senses from WordNet that contains 226,036 annotated examples covering 33,362 separate senses (Miller et al., 1993). We use the SemEval-2007 (SE07) dataset as our development set (Pradhan et al., 2007); we hold out Senseval2 (SE2; Palmer et al. (2001)), Senseval-3 (SE3; Snyder and Palmer (2004)), SemEval-2013 (SE13; Navigli et al. (2013)), and SemEval-2015 (SE15; Moro and Navigli (2015)) as evaluation sets, following standard practice. All sense glosses used in our system are retrieved from WordNet 3.0 (Miller, 1995). 4.2 Baselines We compare the BEM against a number of baseline systems. We first consider two knowledge-based baselines: WordNet S1, which labels each example with its first (most common) sens"
2020.acl-main.95,S15-2049,0,0.823987,"Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled each word using a shared"
2020.acl-main.95,S13-2040,0,0.530365,"e) pair (w, si ) is |Sw | L(w, si ) = −φ(w, si ) + log X exp(φ(w, sj )) j=0 4 4.1 Experimental Setup WSD Task and Datasets We evaluate our BEM system with the WSD framework established in Raganato et al. (2017b). We train our model on SemCor, a large dataset manually annotated with senses from WordNet that contains 226,036 annotated examples covering 33,362 separate senses (Miller et al., 1993). We use the SemEval-2007 (SE07) dataset as our development set (Pradhan et al., 2007); we hold out Senseval2 (SE2; Palmer et al. (2001)), Senseval-3 (SE3; Snyder and Palmer (2004)), SemEval-2013 (SE13; Navigli et al. (2013)), and SemEval-2015 (SE15; Moro and Navigli (2015)) as evaluation sets, following standard practice. All sense glosses used in our system are retrieved from WordNet 3.0 (Miller, 1995). 4.2 Baselines We compare the BEM against a number of baseline systems. We first consider two knowledge-based baselines: WordNet S1, which labels each example with its first (most common) sense as specified in WordNet, and most frequent sense (MFS), which assigns each word the most frequent sense it occurs with in the training data. We also compare against the pretrained model used to initialize our BEM system, B"
2020.acl-main.95,S01-1005,0,0.365059,"elated Work Word Sense Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled"
2020.acl-main.95,N18-1202,1,0.607644,"nd-to-end manner and learns to embed gloss text without additional supervision. Other work has shown that neural models capture useful semantic information about words from their definitions, and has used them to encode lexical representations (Bahdanau et al., 2017; Bosc and Vincent, 2018). While they focused on representing words, rather than specific senses, their modeling approaches could be extended to sense representations. 2.2 Pretrained NLP Models for WSD Pretrained models have been shown to capture a surprising amount of word sense information from their pretraining objectives alone (Peters et al., 2018; Stanovsky and Hopkins, 2018; Coenen et al., 2019), allowing the frozen pretrained representations to compete with previous state-of-the-art WSD systems (Hadiwinoto et al., 2019). Building on these findings, Vial et al. (2019) incorporates pretrained BERT representations as inputs into their WSD system, and Loureiro and Jorge 1007 Scores Sense Embeddings Context Encoder Gloss Encoder The Transformer Transformer plant [CLS] sprouted a living ... ... Figure 1: Architecture of our bi-encoder model for WSD. The context sentence and sense gloss text are input into the context and gloss encoders, r"
2020.acl-main.95,C16-1330,0,0.0449344,"regated ALL evaluation set. Table 2 shows the performance of the balanced BEM. By breaking down the balanced model performance, the balanced BEM outperforms the standard BEM on LFS examples, but suffers from worse performance on the more common MFS examples. We also find that this balancing during training slightly improves performance on both zero-shot words and senses. These findings show that while weighting the data gives better signal for less common senses, it comes at the cost of the (sometimes helpful) data bias towards more frequent sense. This finding holds with similar results from Postma et al. (2016), although their experiments focused on altering the composition of the training data, rather than modifying the loss. One possible direction for future work is a more thorough investigation of methods for obtain a stronger training signal from less frequent senses, while still taking the MFS bias into account. 6.2 6.3 Balancing the Senses Despite the improvement on less common senses over baselines (Section 5.2), we still see a large performance gap between the MFS and LFS subsets. One possible explanation is data imbalance, since the MFS subset contains many more training examples. To contro"
2020.acl-main.95,S07-1016,0,0.119996,"he scores for the candidate senses of the target word w to train our bi-encoder model; the loss function of our system given a (word, sense) pair (w, si ) is |Sw | L(w, si ) = −φ(w, si ) + log X exp(φ(w, sj )) j=0 4 4.1 Experimental Setup WSD Task and Datasets We evaluate our BEM system with the WSD framework established in Raganato et al. (2017b). We train our model on SemCor, a large dataset manually annotated with senses from WordNet that contains 226,036 annotated examples covering 33,362 separate senses (Miller et al., 1993). We use the SemEval-2007 (SE07) dataset as our development set (Pradhan et al., 2007); we hold out Senseval2 (SE2; Palmer et al. (2001)), Senseval-3 (SE3; Snyder and Palmer (2004)), SemEval-2013 (SE13; Navigli et al. (2013)), and SemEval-2015 (SE15; Moro and Navigli (2015)) as evaluation sets, following standard practice. All sense glosses used in our system are retrieved from WordNet 3.0 (Miller, 1995). 4.2 Baselines We compare the BEM against a number of baseline systems. We first consider two knowledge-based baselines: WordNet S1, which labels each example with its first (most common) sense as specified in WordNet, and most frequent sense (MFS), which assigns each word the"
2020.acl-main.95,D17-1120,0,0.732873,". We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding space by using a context encoder to represent the target word and surrounding context, and a gloss encoder to represent the sense definitions. However, our two encoders are jointly learned from the WSD objective alone and trained in an end-to-end fashion. This approach allows our model to outperform prior work on the English all-words WSD task introduced in Raganato et al. (2017b). Analysis of our model shows that these gains come almost entirely from better performance on the less frequent senses, with an 15.6 absolute improvement in F1 performance over the closest performing system; our model also improves on prior work in the zeroshot setting, where we evaluate performance on words and senses not seen during training. Finally, we train our model in a few-shot setting in order to investigate how well the bi-encoder system learns on a limited set of training examples per sense. The bi-encoder architecture is able to generalize better from the limited number of exam1"
2020.acl-main.95,E17-1010,0,0.698331,". We present a bi-encoder model built on top of BERT (Devlin et al., 2019) that is designed to improve performance on rare and zero-shot senses. Similar to prior work, our system represents the target words and senses in the same embedding space by using a context encoder to represent the target word and surrounding context, and a gloss encoder to represent the sense definitions. However, our two encoders are jointly learned from the WSD objective alone and trained in an end-to-end fashion. This approach allows our model to outperform prior work on the English all-words WSD task introduced in Raganato et al. (2017b). Analysis of our model shows that these gains come almost entirely from better performance on the less frequent senses, with an 15.6 absolute improvement in F1 performance over the closest performing system; our model also improves on prior work in the zeroshot setting, where we evaluate performance on words and senses not seen during training. Finally, we train our model in a few-shot setting in order to investigate how well the bi-encoder system learns on a limited set of training examples per sense. The bi-encoder architecture is able to generalize better from the limited number of exam1"
2020.acl-main.95,W17-4702,0,0.0364265,"ning setting demonstrating that the bi-encoder generalizes well from limited data. The source code and trained models for our WSD bi-encoders can be found at https://github. com/facebookresearch/wsd-biencoders. 2 Background and Related Work Word Sense Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neu"
2020.acl-main.95,P15-1173,0,0.0410977,"Missing"
2020.acl-main.95,S13-1003,0,0.070038,"P tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled each word using a shared output space (Raganato et al., 2017a). Other neural approaches used semi-supervised learning to augment the learned representations with additional data (Melamud et al., 2016; Yuan et al., 2016). 2.1 Lexica"
2020.acl-main.95,W04-0811,0,0.145838,"loss function of our system given a (word, sense) pair (w, si ) is |Sw | L(w, si ) = −φ(w, si ) + log X exp(φ(w, sj )) j=0 4 4.1 Experimental Setup WSD Task and Datasets We evaluate our BEM system with the WSD framework established in Raganato et al. (2017b). We train our model on SemCor, a large dataset manually annotated with senses from WordNet that contains 226,036 annotated examples covering 33,362 separate senses (Miller et al., 1993). We use the SemEval-2007 (SE07) dataset as our development set (Pradhan et al., 2007); we hold out Senseval2 (SE2; Palmer et al. (2001)), Senseval-3 (SE3; Snyder and Palmer (2004)), SemEval-2013 (SE13; Navigli et al. (2013)), and SemEval-2015 (SE15; Moro and Navigli (2015)) as evaluation sets, following standard practice. All sense glosses used in our system are retrieved from WordNet 3.0 (Miller, 1995). 4.2 Baselines We compare the BEM against a number of baseline systems. We first consider two knowledge-based baselines: WordNet S1, which labels each example with its first (most common) sense as specified in WordNet, and most frequent sense (MFS), which assigns each word the most frequent sense it occurs with in the training data. We also compare against the pretraine"
2020.acl-main.95,D18-1182,0,0.0256481,"learns to embed gloss text without additional supervision. Other work has shown that neural models capture useful semantic information about words from their definitions, and has used them to encode lexical representations (Bahdanau et al., 2017; Bosc and Vincent, 2018). While they focused on representing words, rather than specific senses, their modeling approaches could be extended to sense representations. 2.2 Pretrained NLP Models for WSD Pretrained models have been shown to capture a surprising amount of word sense information from their pretraining objectives alone (Peters et al., 2018; Stanovsky and Hopkins, 2018; Coenen et al., 2019), allowing the frozen pretrained representations to compete with previous state-of-the-art WSD systems (Hadiwinoto et al., 2019). Building on these findings, Vial et al. (2019) incorporates pretrained BERT representations as inputs into their WSD system, and Loureiro and Jorge 1007 Scores Sense Embeddings Context Encoder Gloss Encoder The Transformer Transformer plant [CLS] sprouted a living ... ... Figure 1: Architecture of our bi-encoder model for WSD. The context sentence and sense gloss text are input into the context and gloss encoders, respectively; each encoder is"
2020.acl-main.95,2019.gwc-1.14,0,0.629727,"representations (Bahdanau et al., 2017; Bosc and Vincent, 2018). While they focused on representing words, rather than specific senses, their modeling approaches could be extended to sense representations. 2.2 Pretrained NLP Models for WSD Pretrained models have been shown to capture a surprising amount of word sense information from their pretraining objectives alone (Peters et al., 2018; Stanovsky and Hopkins, 2018; Coenen et al., 2019), allowing the frozen pretrained representations to compete with previous state-of-the-art WSD systems (Hadiwinoto et al., 2019). Building on these findings, Vial et al. (2019) incorporates pretrained BERT representations as inputs into their WSD system, and Loureiro and Jorge 1007 Scores Sense Embeddings Context Encoder Gloss Encoder The Transformer Transformer plant [CLS] sprouted a living ... ... Figure 1: Architecture of our bi-encoder model for WSD. The context sentence and sense gloss text are input into the context and gloss encoders, respectively; each encoder is initialized with BERT. We take the ith output of the context encoder as the representation for the target word wi ; the first output of the gloss encoder, which corresponds to the BERT-specific star"
2020.acl-main.95,H05-1097,0,0.0524296,"ing an evaluation of the BEM in a few-shot learning setting demonstrating that the bi-encoder generalizes well from limited data. The source code and trained models for our WSD bi-encoders can be found at https://github. com/facebookresearch/wsd-biencoders. 2 Background and Related Work Word Sense Disambiguation (WSD) is the task of predicting the particular sense, or meaning, of a word when it occurs in a specific context (Navigli, 2009). Understanding what a word means in context is critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe"
2020.acl-main.95,P10-4014,0,0.255562,"critical to many NLP tasks, and WSD has been shown to help downstream tasks such as machine translation (MT) (Vickrey et al., 2005; Neale et al., 2016; Rios Gonzales et al., 2017) and information extraction (IE) (Ciaramita and Altun, 2006; Bovi et al., 2015). The formulation of WSD that we address is allwords WSD, where the model disambiguates every ambiguous word in the data (e.g., Palmer et al. (2001); Moro and Navigli (2015)). Many WSD systems approached this task with manually engineered features that were used to learn an independent classifier, or word expert, for each ambiguous lemma (Zhong and Ng, 2010; Shen et al., 2013). Later work also integrated word embeddings into this independent classifier approach (Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). Neural models for WSD built on this approach by training encoders for better feature extraction; they then either still learned independent classifiers on top of the encoded features (K˚ageb¨ack and Salomonsson, 2016), or labeled each word using a shared output space (Raganato et al., 2017a). Other neural approaches used semi-supervised learning to augment the learned representations with additional data (Melamud et al., 2016; Yuan et al"
2020.acl-main.95,L16-1441,0,\N,Missing
2020.coling-main.274,J12-4003,0,0.0254671,"ing the intuitive nature of QA-SRL, Fitzgerald et al. (2018) crowdsourced a large scale QA-SRL corpus via Amazon Mechanical Turk, and released the first QA-SRL parser. QA-SRL is appealing not just for its scalability, but also for its content. By relying on natural comprehension of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 202"
2020.coling-main.274,N03-1013,0,0.0817758,"ks — detect predicates in text, extract their arguments, and label each with a semantic role. While verb predicates are relatively easy to detect, requiring no more than a POS tagger, this phase is not trivial for nominal SRL. For QANom, the predicate detection task is coupled with finding a corresponding verb whose meaning is aligned to the nominal predicate. In this way, the QA-SRL questions centered by this verb could naturally capture arguments of the noun. Thus, our data collection setting involves leveraging lexical resources to find candidate nouns having a related verb. We use CatVar (Habash and Dorr, 2003), WordNet (Miller, 1995) and an in-house suffix-driven heuristic to identify those noun candidates, along with their corresponding verb. More details about our lexical candidate extraction procedure are in Appendix 8.2. During annotation, workers first determine whether a candidate noun mention carries verbal meaning in the context of the sentence (IS V ERBAL). We instructed the workers to consider the automatically extracted related verb, and to judge whether it will be natural to ask questions about the target noun instance using this verb. For example, given the noun phrase “the organizatio"
2020.coling-main.274,D15-1076,1,0.375208,"uestion-Answer driven SRL for Nominalizations Ayal Klein1 Jonathan Mamou2 Valentina Pyatkin1 Daniela Brook Weiss1 Hangfeng He3 Dan Roth3 Luke Zettlemoyer4 Ido Dagan1 1 Computer Science Department, Bar Ilan University 2 Intel Labs, Israel 3 University of Pennsylvania 4 University of Washington {ayal.s.klein,jonathan.mamou,valpyatkin, daniela.stepanov}@gmail.com {hangfeng,danroth}@seas.upenn.edu lsz@cs.washington.edu dagan@cs.biu.ac.il Abstract We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction S"
2020.coling-main.274,P18-2058,1,0.840711,"the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-d"
2020.coling-main.274,2020.acl-main.772,1,0.904092,"it arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal predicates. We identify verbal nouns that are eventive in nature along with their correspo"
2020.coling-main.274,W06-1617,0,0.0756852,"rhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalization"
2020.coling-main.274,kingsbury-palmer-2002-treebank,0,0.789046,"nnotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument str"
2020.coling-main.274,P13-1008,0,0.0813397,"Missing"
2020.coling-main.274,meyers-etal-2004-annotating,0,0.129033,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,W04-2705,0,0.84605,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,N18-2089,1,0.772819,"ts not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). A related QA-driven semantic annotation dataset is QA-driven Meaning Represen"
2020.coling-main.274,W18-3601,0,0.0213093,"Missing"
2020.coling-main.274,S15-2153,0,0.0987273,"Missing"
2020.coling-main.274,C08-1084,0,0.111033,"Missing"
2020.coling-main.274,D14-1162,0,0.0861543,"Missing"
2020.coling-main.274,W13-3516,0,0.0484846,"Missing"
2020.coling-main.274,P18-2124,0,0.0583201,"in focus, to investigate the interaction between the semantic space of the pre-training task and of the target task. See Appendix 8.5 for further experimental details. Our findings are shown in Table 6. Generally, results re-establish the findings of He et al. (2020), that further pre-training on QA annotations improve downstream performance over the BERT encoder, especially in low resource settings. This effect is more profound for semantically-oriented QA annotation (QAMR, QA-SRL and QANom), tackling semantic relations within a sentence, than for simple questionanswering data, i.e., SQuAD (Rajpurkar et al., 2018). As a trend, results also indicate that the performance gain is larger the more relevant the semantic space of the pre-training task is for the target task. Specifically, QAMR provides the best “generalpurpose” semantic signal, outperforming all the other models on most heterogeneous tasks (i.e. tasks with mixed types of “targets”). This is inline with its “whole-sentence” semantic nature, capturing relations for all word types. On the other hand, QANom’s semantic signal is improving BERT’s performance particularly for noun-targeting tasks, performing comparably to QAMR, and the same is true"
2020.coling-main.274,2020.acl-main.626,1,0.402678,"”. Thomas ARG0 Who has proved something? NomBank ARG1 QANom What has someone proved? the existence of God How did someone prove something? the Ontological argument Table 1: An illustration of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as t"
2020.coling-main.274,D16-1252,1,0.933373,"of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal pre"
2020.coling-main.274,N18-1081,1,0.845623,"sion of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et"
2020.coling-main.274,D18-1548,0,0.0201207,"notating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file"
2020.coling-main.274,W11-2822,0,0.0544657,"Missing"
2020.coling-main.274,P15-2115,0,0.140863,"and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research"
2020.emnlp-main.153,P04-1035,0,\N,Missing
2020.emnlp-main.153,N16-3020,0,\N,Missing
2020.emnlp-main.153,N18-1023,0,\N,Missing
2020.emnlp-main.153,P19-1487,0,\N,Missing
2020.emnlp-main.153,D17-1042,0,\N,Missing
2020.emnlp-main.153,N19-1423,0,\N,Missing
2020.emnlp-main.153,D19-1420,0,\N,Missing
2020.emnlp-main.153,D19-1276,0,\N,Missing
2020.emnlp-main.413,2020.emnlp-main.408,1,0.858005,"Missing"
2020.emnlp-main.413,N19-1423,0,0.178039,"ic parsers for low-resource domains (e.g. 25 training samples per intent or slot label), and propose a solution that is competitive against supervised models trained with 10x more data. We identify two key factors for successfully adapting task-oriented semantic parsers to new domains: better representation learning and better training techniques. We first show that pre-trained language representations are critical in the low-resource setting for the model to quickly generalize to new intents and slots. Furthermore, most pre-trained language representations used in previous work such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) are encoder-only models, and are hence not ideal for a compositional parser with an encoder-decoder (seq2seq) architecture. We therefore propose to use BART (Lewis et al., 2020), a pre-trained seq2seq model that can be used to initialize both the encoder and decoder of our semantic parser, which significantly outperforms other pre-trained representations such as RoBERTa. More importantly, these large pre-trained models are sometimes known to pose challenges to fine-tuning with very few training samples. In order to better adapt the semantic parser to lowresource"
2020.emnlp-main.413,W17-2607,0,0.0215538,"tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approaches for handling the more complex compositional queries using neural shift-reduce parsers (Gupta et al., 2018; Einolghozati et al., 2018) or seq2seq models (Jia and Liang, 2016; Rongali et al., 2020). On the other hand, there have been research efforts on scaling task-oriented parsers to new domains with less training data (Jaech et al., 2016; Bapna et al., 2017; Fan et al., 2017; Goyal et al., 2018; Lee and Jha, 2019). These methods, however, only focus on the simpler flat queries. Our proposed method, in contrast, can effectively parse both flat and compositional queries for low5097 resource target domains. Meta-Learning (Lake et al., 2015), or learning to learn, aims to learn a model that can quickly adapt to new tasks with a small amount of training data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute se"
2020.emnlp-main.413,N18-3018,0,0.0590947,"Missing"
2020.emnlp-main.413,D18-1398,0,0.0405991,"quickly adapt to new tasks with a small amount of training data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute second derivatives, which are computation and memory intensive, there have been studies to use either first-order approximation such as firstorder MAML and Reptile (Nichol et al., 2018), or implicit differentiation (Rajeswaran et al., 2019). Furthermore, meta-learning has also been applied to a number of NLP tasks lately (Gu et al., 2018; Dou et al., 2019; Mi et al., 2019; Qian and Yu, 2019; Sun et al., 2019). 7 Conclusion In this work, we study the low-resource domain scaling problem for task-oriented semantic parsing. In particular, we focus on the 25 SPIS setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data. Our approach distinguishes itself from previous methods on two fronts. First of all, we argue the encoder-only pre-trained representations used in existing work are not ideal for the seq2seq model employed in task-oriented semantic parsing, and instead"
2020.emnlp-main.413,D18-1300,1,0.834194,"hen Yashar Mehdad Asish Ghoshal Luke Zettlemoyer Sonal Gupta Facebook Inc. {xilun,aghoshal,mehdad,lsz,sonalgupta}@fb.com Utterance: Driving directions to the Eagles game Abstract Semantic Parse: [IN:GET_DIRECTIONS Driving directions to [SL:DESTINATION [IN:GET_EVENT the [SL:NAME_EVENT Eagles ] [SL:CAT_EVENT game ] ] ] ] Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al., 2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). Tree Representation: Driving directions to the In this paper, we focus on adapting taskoriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2020) to ini"
2020.emnlp-main.413,P16-1002,0,0.0202161,"since 1990s with the advent of the ATIS dataset (Price, 1990). Traditionally, the task is formulated as a joint text classification (intent prediction) and sequence tagging (slot filling) problem, and can be tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approaches for handling the more complex compositional queries using neural shift-reduce parsers (Gupta et al., 2018; Einolghozati et al., 2018) or seq2seq models (Jia and Liang, 2016; Rongali et al., 2020). On the other hand, there have been research efforts on scaling task-oriented parsers to new domains with less training data (Jaech et al., 2016; Bapna et al., 2017; Fan et al., 2017; Goyal et al., 2018; Lee and Jha, 2019). These methods, however, only focus on the simpler flat queries. Our proposed method, in contrast, can effectively parse both flat and compositional queries for low5097 resource target domains. Meta-Learning (Lake et al., 2015), or learning to learn, aims to learn a model that can quickly adapt to new tasks with a small amount of training data. In par"
2020.emnlp-main.413,2020.acl-main.703,1,0.908657,"ueries (Gupta et al., 2018; Rongali et al., 2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). Tree Representation: Driving directions to the In this paper, we focus on adapting taskoriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2020) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to lowresource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain taskoriented semantic parsing dataset (TOPv21 ). 1 Eagles game Figure 1: An compositional query from TOP dataset. emerged to tackle such task-oriented semantic parsing task, for both simple and more complex queries. Introduction Virtual Assistants n"
2020.emnlp-main.413,2021.ccl-1.108,0,0.130635,"Missing"
2020.emnlp-main.413,N19-4009,0,0.0238804,"batch size of 32 for Reptile, with both inner (η) and outer (α) learning rates being 5 × 10−5 . All models are trained for 100 epochs on the source domains with a batch size of 128 (except Reptile), using early stopping if the validation accuracy does not improve in the last 10 epochs. Finetuning is done for 2000 epochs, with a batch size of either 64 (LSTM and RoBERTa) or 32 (BART and meta-learning). Model validation is performed once every 10 epochs during fine-tuning, and stops early after 20 consecutive validations with no improvements. Our model is implemented with the fairseq framework (Ott et al., 2019) and trained on a Nvidia Telsa P100 GPU with 16GB memory. 6 Related Work Task-Oriented Semantic Parsing has attracted attention from the research community since 1990s with the advent of the ATIS dataset (Price, 1990). Traditionally, the task is formulated as a joint text classification (intent prediction) and sequence tagging (slot filling) problem, and can be tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approache"
2020.emnlp-main.413,H90-1020,0,0.321196,"r, Reptile behaves differently and performs similar updates compared to MAML as shown by Nichol et al. (2018) using Taylor Series analysis. 5 Experiments In this section, we first introduce TOPv2, a multidomain task-oriented semantic parsing dataset we are releasing to the community. It is an extension to the TOP dataset with 6 additional domains and 137k new samples. We then outline the setup of our low-resource domain scaling experiments in §5.2, and present the experimental results in §5.3. 5.1 The TOPv2 Dataset While multiple datasets exist for task-oriented semantic parsing such as ATIS (Price, 1990) or SNIPS (Coucke et al., 2018), the TOP dataset (Gupta et al., 2018) is unique in that it contains compositional queries with complex and hierarchical structures (Figure 1). On the other hand, the queries from the TOP dataset are limited to only two domains, namely navigation and event, making it unsuited for domain scaling experiments. To this end, we extend the TOP dataset with 6 additional domains: alarm, messaging, music, reminder, timer, and weather, with a good mixture of simple (flat) and complex (compositional) domains. Table 1 shows some basic statistics of the TOPv2 dataset. We foll"
2020.emnlp-main.413,P19-1253,0,0.0224571,"raining data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute second derivatives, which are computation and memory intensive, there have been studies to use either first-order approximation such as firstorder MAML and Reptile (Nichol et al., 2018), or implicit differentiation (Rajeswaran et al., 2019). Furthermore, meta-learning has also been applied to a number of NLP tasks lately (Gu et al., 2018; Dou et al., 2019; Mi et al., 2019; Qian and Yu, 2019; Sun et al., 2019). 7 Conclusion In this work, we study the low-resource domain scaling problem for task-oriented semantic parsing. In particular, we focus on the 25 SPIS setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data. Our approach distinguishes itself from previous methods on two fronts. First of all, we argue the encoder-only pre-trained representations used in existing work are not ideal for the seq2seq model employed in task-oriented semantic parsing, and instead propose to use BART, a pre-trained model with an encod"
2020.emnlp-main.413,P17-1099,0,0.0195561,"copy probability and the generation probability. The generation probability gt is produced by the decoder by mapping the decoder state onto the output vocabulary, which only includes ontology tokens but not utterance tokens. gt = sof tmax(OutputEmbed(dt )) Base Model In this section, we present our core model architecture. Our meta-learning technique will be introduced in Section 4. We follow recent state-of-theart approaches (Rongali et al., 2020; Aghajanyan et al., 2020) and adopt a seq2seq model as our base architecture (S EQ 2S EQ -C OPY P TR), derived from the Pointer Generator Network (See et al., 2017). The base architecture is shown in Figure 2. For an input sequence S = [w1 , w2 , . . . , wn ], the encoder first encodes it into a series of hidden vectors (encoder states) [e1 , e2 , . . . , en ]. The encoder states are then passed to an decoder that autoregressively produces target tokens ot for each timestamp t. Specifically, the decoder first outputs a hidden decoder state dt based on the decoder states from previous timestamps as well as all enThe copy probability ct , on the other hand, indicates whether to copy one of the source tokens as the decoder output for timestamp t, and is pre"
2020.emnlp-main.413,P16-1162,0,0.026352,"On the other hand, supervised models are trained with 500 or 1000 SPIS to assess the performance of our low-resource domain scaling model. For the source domains, all available training data is utilized. Validation Set To perform model selection and early stopping, a validation set is adopted, which is also set to 25 SPIS for simplicity. In contrast, the supervised models utilize the entire validation set as shown in Table 2. Data Preprocessing We first perform standard preprocessing such as lower-casing and tokenization. For models initialized with pre-trained language representations, BPE (Sennrich et al., 2016) tokenization is done to match that used by the pretrained model. We do not tokenize ontology tokens (intents and slot labels) into BPE, but instead treat them as atomic tokens which are appended to the BPE vocabulary. We then perform two additional preprocessing (canonicalization) steps, consistent across all models. First of all, note that certain utterance tokens do not contribute to the semantics of the query. For instance, in Figure 1, the phrase Driving directions to under IN:GET_DIRECTIONS and the under IN:GET_EVENT can be omitted as their semantics are already captured by the intent la"
2020.emnlp-main.466,D13-1160,0,0.0755468,"a dataset with 14,042 annotations on NQ- OPEN questions containing diverse types of ambiguity. 3. We introduce the first baseline models that produce multiple answers to open-domain questions, with experiments showing their effectiveness in learning from our data while highlighting avenues for future work. 2 Related Work Open-domain Question Answering requires a system to answer any factoid question based on evidence provided by a large corpus such as Wikipedia (Voorhees et al., 1999; Chen et al., 2017). Existing benchmarks use questions of various types, from open-ended information-seeking (Berant et al., 2013; Kwiatkowski et al., 2019; Clark et al., 2019) to more specialized trivia/quiz (Joshi et al., 2017; Dunn et al., 2017). To the best of our knowledge, all existing formulations assume each question has a single clear answer. Our work is built upon an open-domain version of NATURAL Q UESTIONS (Kwiatkowski et al., 2019), denoted NQ- OPEN, composed of questions posed by real users of Google search, each with an answer drawn from Wikipedia. NQ- OPEN has promoted several recent advances in open5784 domain question answering (Lee et al., 2019; Asai et al., 2020; Min et al., 2019a,b; Guu et al., 2020"
2020.emnlp-main.466,P17-1171,0,0.0606172,"an open-domain question, along with disambiguated questions to differentiate them. 2. We construct A MBIG NQ, a dataset with 14,042 annotations on NQ- OPEN questions containing diverse types of ambiguity. 3. We introduce the first baseline models that produce multiple answers to open-domain questions, with experiments showing their effectiveness in learning from our data while highlighting avenues for future work. 2 Related Work Open-domain Question Answering requires a system to answer any factoid question based on evidence provided by a large corpus such as Wikipedia (Voorhees et al., 1999; Chen et al., 2017). Existing benchmarks use questions of various types, from open-ended information-seeking (Berant et al., 2013; Kwiatkowski et al., 2019; Clark et al., 2019) to more specialized trivia/quiz (Joshi et al., 2017; Dunn et al., 2017). To the best of our knowledge, all existing formulations assume each question has a single clear answer. Our work is built upon an open-domain version of NATURAL Q UESTIONS (Kwiatkowski et al., 2019), denoted NQ- OPEN, composed of questions posed by real users of Google search, each with an answer drawn from Wikipedia. NQ- OPEN has promoted several recent advances in"
2020.emnlp-main.466,P17-1147,1,0.891536,"Missing"
2020.emnlp-main.466,2020.emnlp-main.550,1,0.929349,"found every possible interpretation of a question. Nonetheless, we are able to collect high quality data covering high levels of ambiguity (2.1 distinct answers per question on average) with high estimated agreement (89.0 F1) on valid answers. The types of ambiguity are diverse and sometimes subtle (Table 1), including ambiguous entity or event references, or ambiguity over the answer type; many are only apparent after examining one or more Wikipedia pages. To establish initial performance levels on this data, we present a set of strong baseline methods. We extend a state-of-the-art QA model (Karpukhin et al., 2020) with three new components: (1) set-based question answering with a sequence-tosequence model, (2) a question disambiguation model, and (3) a modification to democratic cotraining (Zhou and Goldman, 2004) which leverages the partial supervision available in the full NQ- OPEN dataset. We also do an ablation study and qualitative analysis, which suggest there is significant room for future work on this task. To summarize, our contributions are threefold. 1. We introduce A MBIG QA, a new task which requires identifying all plausible answers to an open-domain question, along with disambiguated que"
2020.emnlp-main.466,Q19-1026,0,0.17048,"Missing"
2020.emnlp-main.466,P19-1612,0,0.407958,"Missing"
2020.emnlp-main.466,2020.acl-main.703,1,0.719676,"Missing"
2020.emnlp-main.466,N18-2089,1,0.866416,"Missing"
2020.emnlp-main.466,N19-4009,0,0.0508657,"Missing"
2020.emnlp-main.466,P02-1040,0,0.106889,"imilarity function f valued in [0, 1]. ci = max I[yi ∈ Y¯j ]f (xi , x ¯j ). 1≤j≤n Intuitively, ci considers (1) the correctness of the answer and (2) the similarity f (xi , x ¯j ) between the predicted and reference question. We calculate F1 treating the ci as measures of correctness: P P ci i ci precf = , recf = i , m n 2 × precf × recf F1f = . precf + recf We consider three choices of Ff . F1ans is the F1 score on answers only, where f always yields 1. This may be used without the question disambiguation step. F1BLEU accounts for string similarity between questions, calculating f with BLEU (Papineni et al., 2002). F1EDIT-F1 uses E DIT-F1 as f , where E DIT-F1 is a new measure that represents each disambiguated question by its added and 5785 deleted unigrams compared to the prompt question, and computes the F1 score between them. For example, consider the prompt question “Who made the play the crucible?”, the reference “Who wrote the play the crucible?” and the prediction “Who made the play the crucible in 2012?”. The gold edits3 here are * -made , +wrote + while the predicted edits are * +in , +2012 +. Their E DIT-F1 is thus zero, even though the questions are similar. Unlike BLEU which we use to dire"
2020.emnlp-main.466,P18-1255,0,0.029512,"Missing"
2020.emnlp-main.466,N19-1013,0,0.058877,"Missing"
2020.emnlp-main.466,D16-1158,0,0.0163108,"xamples with multiple question-answer pairs (multi) are lower, indicating that predicting all plausible answers is more challenging than predicting a single answer, as expected. S PAN S EQ G EN also obtains the best performance in F1BLEU and F1EDIT-F1 , although their absolute values are low in general; we discuss this in our question disambiguation ablations below. There is a substantial difference in performance between development and test overall, likely due to distributional differences in the original questions 5 This problem has also been reported in other conditional generation tasks (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019); we leave it for future work. in NQ- OPEN; detailed discussion is in Appendix B. Effect of co-training. The last two rows of Table 3 reports the effect of our co-training method. As co-training requires multiple trained models, we compare with a naive ensemble. While we see gains from ensembling alone, an ensemble trained with the co-training method achieves the best performance on all metrics. This result demonstrates the potential of jointly using A MBIG NQ and partial supervision from NQ- OPEN. Ablations on question disambiguation. Table 4 reports results of an"
2020.emnlp-main.466,D19-1331,0,0.0139938,"n-answer pairs (multi) are lower, indicating that predicting all plausible answers is more challenging than predicting a single answer, as expected. S PAN S EQ G EN also obtains the best performance in F1BLEU and F1EDIT-F1 , although their absolute values are low in general; we discuss this in our question disambiguation ablations below. There is a substantial difference in performance between development and test overall, likely due to distributional differences in the original questions 5 This problem has also been reported in other conditional generation tasks (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019); we leave it for future work. in NQ- OPEN; detailed discussion is in Appendix B. Effect of co-training. The last two rows of Table 3 reports the effect of our co-training method. As co-training requires multiple trained models, we compare with a naive ensemble. While we see gains from ensembling alone, an ensemble trained with the co-training method achieves the best performance on all metrics. This result demonstrates the potential of jointly using A MBIG NQ and partial supervision from NQ- OPEN. Ablations on question disambiguation. Table 4 reports results of an ablation experiment on quest"
2020.emnlp-main.466,D19-1284,1,0.86334,"Missing"
2020.emnlp-main.466,N19-1300,0,\N,Missing
2020.emnlp-main.466,N19-1423,0,\N,Missing
2020.emnlp-main.466,D19-1172,0,\N,Missing
2020.emnlp-main.519,C18-1057,0,0.0112273,"aining on source and target domain. Average performance across a set of worlds is computed by macroaveraging. We also report cross-encoder performance on the same retrieval method (BM25) used by Logeswaran et al. (2019) in Table 3, where the performance is evaluated on the subset of test instances for which the gold entity is among the top 64 candidates retrieved by BM25. We observe that our cross-encoder obtains slightly better results than reported by Logeswaran et al. (2019), likely due to implementation and hyper-parameter details. 5.2.2 TACKBP-2010 Following prior work (Sun et al., 2015; Cao et al., 2018; Gillick et al., 2019; Onoe and Durrett, 2019), we pre-train our models on Wikipedia6 data. Data and model training details can be found in Appendix A.1. 6 https://pytorch.org 6401 https://www.wikipedia.org/ Method Valid TF-IDF† Test Ganea and Hofmann Gupta et al. (2017)† Logeswaran et al. (2019) 26.06 26.96 27.03 76.06 75.06 Ours (base) 78.24 76.58 (2017)† Method Table 3: Normalized accuracy on validation and test set on Zero-shot EL, where the performance is evaluated on the subset of test instances for which the gold entity is among the top-k candidates retrieved during candidate generatio"
2020.emnlp-main.519,N19-1423,0,0.0627289,"ng the bi-encoder for candidate generation, we train our cross-encoder (initialized with pre-trained BERT) on the top 64 retrieved candidates from bi-encoder for each sample on the train50 100 150 200 k: number of retrieved entities Figure 2: Top-k entity retrieval recall on validation dataset of Zero-shot EL dataset ing set, and evaluate the cross-encoder on the test dataset. Overall, we are able to obtain a much better end-to-end accuracy, as shown in Table 2, largely due to the improvement on the retrieval stage. Evaluation Setup and Results We experiment with both BERT-base and BERTlarge (Devlin et al., 2019) for our bi-encoders and cross-encoders. The details of training infrastructure and hyperparameters can be found in Appendix A. All models are implemented in PyTorch5 and optimizied with Adam (Kingma and Ba, 2014). We use (base) and (large) to indicate the version of our model where the underlying pretrained transformer model is BERT-base and BERT-large, respectively. 5 1 Method U.Acc. Logeswaran et al. (2019) Logeswaran et al. (2019)(domain)† 55.08 56.58 Ours (base) Ours (large) 61.34 63.03 Table 2: Performance on test domains on the Zero-shot EL dataset. U.Acc. represents the unnormalized ac"
2020.emnlp-main.519,K17-1008,0,0.0784644,"r can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as"
2020.emnlp-main.519,D17-1277,0,0.135051,"Missing"
2020.emnlp-main.519,K19-1049,0,0.797778,"curacy gain from the more expensive crossencoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github. com/facebookresearch/BLINK. 1 Introduction Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables (Ganea and Hofmann, 2017), incoming Wikipedia link popularity (Yamada et al., 2016), and gold Wikipedia entity categories (Gillick et al., 2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels ∗ Work done during internship with Facebook. for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed tradeoff inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy. More specifically, we introduce a two stage approach for zero-shot linking (see Figur"
2020.emnlp-main.519,P16-1059,0,0.0516805,"Missing"
2020.emnlp-main.519,D17-1284,0,0.0501717,"Missing"
2020.emnlp-main.519,P13-2006,0,0.199127,"nds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidat"
2020.emnlp-main.519,P84-1044,0,0.249739,"Missing"
2020.emnlp-main.519,K18-1050,0,0.177648,"nsferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP. Gillick et al. (201"
2020.emnlp-main.519,P19-1335,0,0.318892,"on and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP. Gillick et al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are"
2020.emnlp-main.519,K16-1025,0,0.239437,"ain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on larg"
2020.emnlp-main.558,P19-1007,0,0.0799169,"Missing"
2020.emnlp-main.558,N19-1423,0,0.0247058,"ecoder that selects among syntax candidates cs (e.g. SQL keywords) and environment candidate ce (e.g. table columns). Then, we fill in slots in qˆ with a separate decoder that selects among cu in the utterance to form q. Note that logical form template qˆ is distinct from coarse templates z described in sampling (Section 2.1). Figure 2 describes the forward semantic parser. Let u denote words in the utterance, and di denote words in the ith document in the environment description. Let [a; b] denote the concatenation of a and b. First, we cross-encode the utterance and the document using BERT (Devlin et al., 2019), which has led to improvements on a number of NLP tasks. ! B i = BERT! ([u; di ]) Next, we extract environment candidates in document i using self-attention. Let s, e denote the start and end positions of the jth environment candidate in the ith document. We compute an intermediate representation xij for each environment candidate: ! ! a = softmax(W [ B is ; ... B ie ] + b) (2) e X ! xij = ak B ik (3) k=s For ease of exposition, we abbreviate the above ! self-attention function as xij = selfattn( B i [s : e]) Because xij do not model dependencies between different documents, we further proces"
2020.emnlp-main.558,D17-1091,0,0.0201303,"parsing, a variety of methods have been proposed to generalize to new databases by selecting from table schemas in the new database (Zhang et al., 2019; Guo et al., 2019). Our method is complementary to these work — the synthesis, cycle-consistency, and adaptation steps in GAZP can be applied to any parser, so long as we can learn a backward utterance generator and evaluate logical-form equivalence. Data augmentation. Data augmentation transforms original training data to synthesize artificial training data. Krizhevsky et al. (2017) crop and rotate input images to improve object recognition. Dong et al. (2017) and Yu et al. (2018a) respectively paraphrase and back-translate (Sennrich et al., 2016; Edunov et al., 2018) questions and documents to improve question-answering. Jia 6876 and Liang (2016) perform data-recombination in the training domain to improve semantic parsing. Hannun et al. (2014) superimpose noisy background tracks with input tracks to improve speech recognition. Our method is distinct from dataaugmentation in the following ways. First, we synthesize data on logical forms sampled from the new environment instead of the original environment, which allows for adaptation to the new env"
2020.emnlp-main.558,D18-1045,0,0.0223374,"emas in the new database (Zhang et al., 2019; Guo et al., 2019). Our method is complementary to these work — the synthesis, cycle-consistency, and adaptation steps in GAZP can be applied to any parser, so long as we can learn a backward utterance generator and evaluate logical-form equivalence. Data augmentation. Data augmentation transforms original training data to synthesize artificial training data. Krizhevsky et al. (2017) crop and rotate input images to improve object recognition. Dong et al. (2017) and Yu et al. (2018a) respectively paraphrase and back-translate (Sennrich et al., 2016; Edunov et al., 2018) questions and documents to improve question-answering. Jia 6876 and Liang (2016) perform data-recombination in the training domain to improve semantic parsing. Hannun et al. (2014) superimpose noisy background tracks with input tracks to improve speech recognition. Our method is distinct from dataaugmentation in the following ways. First, we synthesize data on logical forms sampled from the new environment instead of the original environment, which allows for adaptation to the new environments. Second, we propose cycle-consistency to prune low-quality data and keep high-quality data for adapt"
2020.emnlp-main.558,P19-1444,0,0.120219,"sing. Semantic parsers parse natural language utterances into executable logical forms with respect to an environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). In zero-shot semantic parsing, the model is required to generalize to environments (e.g. new domains, new database schemas) not seen during training (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018b). For languageto-SQL zero-shot semantic parsing, a variety of methods have been proposed to generalize to new databases by selecting from table schemas in the new database (Zhang et al., 2019; Guo et al., 2019). Our method is complementary to these work — the synthesis, cycle-consistency, and adaptation steps in GAZP can be applied to any parser, so long as we can learn a backward utterance generator and evaluate logical-form equivalence. Data augmentation. Data augmentation transforms original training data to synthesize artificial training data. Krizhevsky et al. (2017) crop and rotate input images to improve object recognition. Dong et al. (2017) and Yu et al. (2018a) respectively paraphrase and back-translate (Sennrich et al., 2016; Edunov et al., 2018) questions and documents to improve questio"
2020.emnlp-main.558,P15-1142,0,0.0370517,"pment set). thesized data used for adaptation. Figure 5 shows that that adaptation performance generally increases with the amount of synthesized data in the inference environment, with diminishing return after 30-40k examples. 4 Related work Semantic parsing. Semantic parsers parse natural language utterances into executable logical forms with respect to an environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). In zero-shot semantic parsing, the model is required to generalize to environments (e.g. new domains, new database schemas) not seen during training (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018b). For languageto-SQL zero-shot semantic parsing, a variety of methods have been proposed to generalize to new databases by selecting from table schemas in the new database (Zhang et al., 2019; Guo et al., 2019). Our method is complementary to these work — the synthesis, cycle-consistency, and adaptation steps in GAZP can be applied to any parser, so long as we can learn a backward utterance generator and evaluate logical-form equivalence. Data augmentation. Data augmentation transforms original training data to synthesize artificial training data. Krizhev"
2020.emnlp-main.558,P16-1009,0,0.0437742,"electing from table schemas in the new database (Zhang et al., 2019; Guo et al., 2019). Our method is complementary to these work — the synthesis, cycle-consistency, and adaptation steps in GAZP can be applied to any parser, so long as we can learn a backward utterance generator and evaluate logical-form equivalence. Data augmentation. Data augmentation transforms original training data to synthesize artificial training data. Krizhevsky et al. (2017) crop and rotate input images to improve object recognition. Dong et al. (2017) and Yu et al. (2018a) respectively paraphrase and back-translate (Sennrich et al., 2016; Edunov et al., 2018) questions and documents to improve question-answering. Jia 6876 and Liang (2016) perform data-recombination in the training domain to improve semantic parsing. Hannun et al. (2014) superimpose noisy background tracks with input tracks to improve speech recognition. Our method is distinct from dataaugmentation in the following ways. First, we synthesize data on logical forms sampled from the new environment instead of the original environment, which allows for adaptation to the new environments. Second, we propose cycle-consistency to prune low-quality data and keep high-"
2020.emnlp-main.558,P14-2105,0,0.0245958,"vironment whose inputoutput consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms dataaugmentation in the training environment, performance increases with the amount of GAZPsynthesized data, and cycle-consistency is central to successful adaptation. 1 Introduction Semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) build executable meaning representations for a range of tasks such as question-answering (Yih et al., 2014), robotic control (Matuszek et al., 2013), and intelligent tutoring systems (Graesser et al., 2005). However, they are usually engineered for each application environment. For example, a languageto-SQL parser trained on an university management database struggles when deployed to a sales database. How do we adapt a semantic parser to new environments where no training data exists? We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing, which adapts existing semantic parsers to new environments by synthesizing new, cycle-consistent data. In the previous example, GAZP synthesiz"
2020.findings-emnlp.272,D15-1075,0,0.0397343,"dividually optimal predictions, and we train the ensemble with this constraint using methods from bi-level optimization (Colson et al., 2007). We evaluate our method by training models on datasets with known biases, and then testing them on out-of-domain datasets built to penalize models that learn to use those biases. First, we construct a series of synthetic datasets to show our method can adapt to multiple kinds of biases. Then, we consider three datasets from prior work that test against question-type biases for visual question answering (Goyal et al., 2018) and hypothesis keyword biases (Bowman et al., 2015; Gururangan et al., 2018) or lexical overlap biases (McCoy et al., 2019) for sentence entailment. Finally, we construct an image recognition dataset using Imagenet (Deng et al., 2009) that includes a test set of examples with misleading backgrounds (e.g., a fish photographed on dry land) to test our method on background-class biases. We show improved performance in all settings, in some cases nearly matching the results that can be achieved with an upper-bound that does use knowledge of the bias being targeted. We release our datasets and code to facilitate future work.1 2 Mixed Capacity Ense"
2020.findings-emnlp.272,W19-1801,0,0.0439868,"nd about 1.3x the time for the larger BERT and LXMERT ensembles (performance during evaluation is identical). Our implementation performs the optimization on the CPU; a GPU based optimizer might reduce this overhead. 5 Related Work Prior work has shown that, given precise knowledge of a dataset bias, it is possible to train a debaised model. For example, if it is known particular intermediate representations in a network could only contain features that are shallowly indicative of the class due to bias (e.g., a local patch in an image), adversarial networks can be used (Belinkov et al., 2019; Grand and Belinkov, 2019; Wang et al., 2019; Cadene et al., 2019). Alternatively, given a model that is so tightly constrained it can only utilize dataset bias (e.g., a question only model for VQA), REBI (Bahng et al., 2020) employs a conditional independence penalty between that model and a debaised model, HEX (Wang et al., 2019) constructs a feature space that is orthogonal to the one learned by the bias model, and Clark et al. (2019) and He et al. (2019) pre-train the bias model and then train a debiased model in an ensemble with it. Our approach also makes use of ensembling, and the idea of requiring conditional"
2020.findings-emnlp.272,D19-1418,1,0.635937,"les include textual entailment models learning the word “not” always implies contradiction (Gururangan et al., 2018), visual question answering (VQA) models learning “2” is almost always the answer to “How many” questions (Jabri et al., 2016), and question answering models selecting entities that occur near question words irrespective of context (Jia and Liang, 2017). We call these kinds of dataset-specific correlations dataset bias. Models that exploit dataset bias can perform well on in-domain data, but will be brittle and perform poorly on out-of-domain or adversarial examples. Prior work (Clark et al., 2019; He et al., 2019; Wang et al., 2019; Bahng et al., 2020) has shown it is possible to prevent models from adopting biased methods, but require the bias to be known and carefully modeled in advance, e.g., by assuming access to a pre-specified classifier that uses only the bias to make predictions. In this paper, we present a debiasing method that can achieve similar results, but that automatically learns the bias, removing the need for such datasetspecific knowledge. To make this possible, we observe that many known examples of dataset bias involve models learning overly simple patterns (Min et"
2020.findings-emnlp.272,N18-2017,0,0.370008,"n machine learning algorithms have been able to achieve impressive results on complex tasks such as language comprehension or image understanding. However, recent work has cautioned that this success is often partially due to exploiting incidental correlations that were introduced during dataset creation, and are not fundamental to the tar∗ Work completed at the University of Washington Work completed at the Allen Institute for AI † Luke Zettlemoyer University of Washington lsz@cs.uw.edu get task. Examples include textual entailment models learning the word “not” always implies contradiction (Gururangan et al., 2018), visual question answering (VQA) models learning “2” is almost always the answer to “How many” questions (Jabri et al., 2016), and question answering models selecting entities that occur near question words irrespective of context (Jia and Liang, 2017). We call these kinds of dataset-specific correlations dataset bias. Models that exploit dataset bias can perform well on in-domain data, but will be brittle and perform poorly on out-of-domain or adversarial examples. Prior work (Clark et al., 2019; He et al., 2019; Wang et al., 2019; Bahng et al., 2020) has shown it is possible to prevent mode"
2020.findings-emnlp.272,D17-1215,0,0.0450309,"ons that were introduced during dataset creation, and are not fundamental to the tar∗ Work completed at the University of Washington Work completed at the Allen Institute for AI † Luke Zettlemoyer University of Washington lsz@cs.uw.edu get task. Examples include textual entailment models learning the word “not” always implies contradiction (Gururangan et al., 2018), visual question answering (VQA) models learning “2” is almost always the answer to “How many” questions (Jabri et al., 2016), and question answering models selecting entities that occur near question words irrespective of context (Jia and Liang, 2017). We call these kinds of dataset-specific correlations dataset bias. Models that exploit dataset bias can perform well on in-domain data, but will be brittle and perform poorly on out-of-domain or adversarial examples. Prior work (Clark et al., 2019; He et al., 2019; Wang et al., 2019; Bahng et al., 2020) has shown it is possible to prevent models from adopting biased methods, but require the bias to be known and carefully modeled in advance, e.g., by assuming access to a pre-specified classifier that uses only the bias to make predictions. In this paper, we present a debiasing method that can"
2020.findings-emnlp.272,P19-1334,0,0.0694429,"es use of ensembling, and the idea of requiring conditional independence between the models. However, our method identifies biased strategies during training instead of requiring them to be pre-specified. Additional work has used multi-label annotations (Singh et al., 2020), pixel-level annotations (Hendricks et al., 2018), or other annotated features (Kim et al., 2019) to help train debiased models, although these kinds of annotations will not always be available. There are also debiasing strategies that identify hard examples in the dataset and re-train the model to focus on those examples (Yaghoobzadeh et al., 2019; Li and Vasconcelos, 2019; Le Bras et al., 2020). This approach reflects a similar intuition that simplicity is connected to dataset bias, although our method is able to explicitly model the bias and does not assume a pool of bias-free examples exist within the training data. Another debiasing approach is to use pretraining (Lewis and Fan, 201; Carlucci et al., 2019) or carefully designed model architectures (Agrawal et al., 2018; Zhang et al., 2016; Carlucci et al., 2019) to make models more prone to focus on semantic content. We expect these methods to be complementary to our work; for inst"
2020.findings-emnlp.272,P19-1416,1,0.826927,", 2019; He et al., 2019; Wang et al., 2019; Bahng et al., 2020) has shown it is possible to prevent models from adopting biased methods, but require the bias to be known and carefully modeled in advance, e.g., by assuming access to a pre-specified classifier that uses only the bias to make predictions. In this paper, we present a debiasing method that can achieve similar results, but that automatically learns the bias, removing the need for such datasetspecific knowledge. To make this possible, we observe that many known examples of dataset bias involve models learning overly simple patterns (Min et al., 2019; McCoy et al., 2019; Anand et al., 2018). This leads us to propose that many dataset biases will be shallower and easier to model than more generalizable patterns. This reflects the intuition that high-quality models for tasks like language comprehension or image understanding will require some minimum amount of complexity (e.g., a visual question answering model should at least consider the question, image, and ways they might correspond), and therefore shallower approaches are likely to be modelling dataset bias. Our method, called Mixed Capacity Ensembling (MCE), follows prior work (Clark"
2020.tacl-1.47,N19-1388,0,0.0243667,"Conneau et al., 2019) to exploit shared representations across languages. Table 9: BT vs. language transfer for unsupervised MT for X-En translations. For language transfer, we present the best transferring scores together with the language transferred from. Multilinguality in NLP tasks This work is also related to the continual trend of multilingual language learning, including aligning multilingual word embeddings (Mikolov et al., 2013; Chen and Cardie, 2018; Lample et al., 2018b) into 737 For MT, the most relevant field is multilingual translation (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019) where the ultimate goal is to jointly train one translation model that translates multiple language directions at the same time, and shares representations to improve the translation performance on low-resource languages (Gu et al., 2018). In this paper, we focus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bilingual scenario. 7 Conclusion Compared with multilingual translation, we do not require parallel data across multiple languages but the targeted direction, which improves the scalability to low-resource languages"
2020.tacl-1.47,D18-1024,0,0.0312434,"i ) 22.1 Zh 11.3 9.2 ( Ko ) 15.0 Nl 28.5 34.1 ( It ) 35.4 universal space, and learning crosslingual models (Wada and Iwata, 2018; Lample and Conneau, 2019; Conneau et al., 2019) to exploit shared representations across languages. Table 9: BT vs. language transfer for unsupervised MT for X-En translations. For language transfer, we present the best transferring scores together with the language transferred from. Multilinguality in NLP tasks This work is also related to the continual trend of multilingual language learning, including aligning multilingual word embeddings (Mikolov et al., 2013; Chen and Cardie, 2018; Lample et al., 2018b) into 737 For MT, the most relevant field is multilingual translation (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019) where the ultimate goal is to jointly train one translation model that translates multiple language directions at the same time, and shares representations to improve the translation performance on low-resource languages (Gu et al., 2018). In this paper, we focus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bilingual scenario. 7 Conclusion Compared with multilingual"
2020.tacl-1.47,N19-1409,1,0.894747,"Missing"
2020.tacl-1.47,P19-1121,1,0.800381,"BLEU d-BLEU 34.5 35.9 × 7.7 (b) Document-level BLEU scores on Zh-En mBART25 s-BLEU d-BLEU 36.4 38.0 37.1 38.5 Model Sent-MT Doc-MT Random d-BLEU 22.0 3.2 mBART25 d-BLEU 28.4 29.6 HAN (2018) d-BLEU − 24.0 Table 6: Document-level machine translation on En-De and Zh-En. (×) The randomly initialized DocMT model cannot produce translations aligned to the original sentences, so only document evaluation is possible. 4.2 Main Results 2. No bi-text for the target pair, but both languages appear in bi-text corpora with other pairs. This setup is common for multilingual MT systems (Johnson et al., 2017; Gu et al., 2019). In this paper, we limit our focus to building models for single language pairs, and leave discussions for multilingual MT to future work. Table 6 shows the main results for both En-De and Zh-En at both sentence-level and document-level. Random vs. Pre-trained The MT models initialized with pre-trained weights outperform randomly initialized models by large margins, for both sentence-level and document-level training. Our mBART25 models (both Sent-MT and DocMT) also outperform HAN (Miculicich et al., 2018),6 despite the fact that they are not customized for document-level MT. 3. No bi-text fo"
2020.tacl-1.47,D18-1549,0,0.0935274,"Ko ) 15.0 Nl 28.5 34.1 ( It ) 35.4 universal space, and learning crosslingual models (Wada and Iwata, 2018; Lample and Conneau, 2019; Conneau et al., 2019) to exploit shared representations across languages. Table 9: BT vs. language transfer for unsupervised MT for X-En translations. For language transfer, we present the best transferring scores together with the language transferred from. Multilinguality in NLP tasks This work is also related to the continual trend of multilingual language learning, including aligning multilingual word embeddings (Mikolov et al., 2013; Chen and Cardie, 2018; Lample et al., 2018b) into 737 For MT, the most relevant field is multilingual translation (Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019) where the ultimate goal is to jointly train one translation model that translates multiple language directions at the same time, and shares representations to improve the translation performance on low-resource languages (Gu et al., 2018). In this paper, we focus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bilingual scenario. 7 Conclusion Compared with multilingual translation, we do n"
2020.tacl-1.47,Q17-1024,0,0.0611793,"Missing"
2020.tacl-1.47,K15-1031,0,0.0561952,"Missing"
2020.tacl-1.47,D19-1387,0,0.0269562,"rent languages. First, for 736 Self-supervised Learning for Text Generation This work inherits from the recent success brought by pre-training for NLP applications (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019), especially for text generation (Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Raffel et al., 2019; Lewis et al., 2019). The pre-trained models are usually used as the initialization for finetuning downstream tasks such as controllable language modeling (Shirish Keskar et al., 2019), summarization (Song et al., 2019; Liu and Lapata, 2019) and dialogue generation (Zhang et al., 2019). Specifically for machine translation, unsupervised pre-training methods were also explored to improve the performance. Qi et al. (2018) investigated the application of pre-trained word embeddings for MT; Ramachandran et al. (2017) proposed to pre-train the encoder-decoder modules as two separate language models. Yang et al. (2019a); Zhu et al. (2020) explored fusion approaches to incorporate the pre-trained BERT weights to improve NMT training. In contrast to most prior work, we focus on pre-training one denoising autoencoder, and adapt the weight"
2020.tacl-1.47,D18-1325,0,0.0658857,"Missing"
2020.tacl-1.47,N19-4009,1,0.801156,"e. Pretraining at ‘‘multi sentence’’ level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer (ǫ = 1e−6, β2 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). Seq2Seq setup, which is more natural to adapt to machine translation applications. Similar to mBART, MASS (Song et al., 2019) is also a Seq2Seq-based pre-training technique with ‘‘word-masking’’. However, the decoder of MASS only predicted tokens that was masked in the encoder, whereas mBART reconstructs the full target sequence which allows to apply not only ‘‘masking’’ but any possible noise functions. Furthermore, both XLM and MASS did not show evidence of the pre-trained models improving translation performance over two languages. 2.3 Pre-trained Models To better measure the effects of d"
2020.tacl-1.47,P19-1493,0,0.0658958,"Missing"
2020.tacl-1.47,P19-1293,1,0.81281,", Artetxe et al. (2017) and Lample et al. (2018a) proposed to jointly learn denoising auto-encoder and back-translation from both directions, which, however, required good initialization and only worked well on similar language pairs. Wu et al. (2019) solve the problem by mining sentences from Wikipedia and using them as weakly supervised translation pairs. Similar to Lample and Conneau (2019) and Song et al. (2019), we follow the first approach and treat our pre-trained model as the initialization step. We also investigate unsupervised translation using language transfer, which is similar to Pourdamghani et al. (2019), where the authors generate translationese of the source language and train a system on high-resource languages to correct these intermediate utterances. It is also closely related to Conneau et al. (2018) and Artetxe et al. (2019) for cross-lingual representation learning where we also show representation learned by mBART can be easily transferred between language without supervised data. We demonstrate that multilingual de-noising pretraining is able to significantly improve both supervised and unsupervised machine translation at both the sentence level and document level. We analyze when a"
2020.tacl-1.47,N18-2084,0,0.0441869,"adford et al., 2018; Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019), especially for text generation (Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Raffel et al., 2019; Lewis et al., 2019). The pre-trained models are usually used as the initialization for finetuning downstream tasks such as controllable language modeling (Shirish Keskar et al., 2019), summarization (Song et al., 2019; Liu and Lapata, 2019) and dialogue generation (Zhang et al., 2019). Specifically for machine translation, unsupervised pre-training methods were also explored to improve the performance. Qi et al. (2018) investigated the application of pre-trained word embeddings for MT; Ramachandran et al. (2017) proposed to pre-train the encoder-decoder modules as two separate language models. Yang et al. (2019a); Zhu et al. (2020) explored fusion approaches to incorporate the pre-trained BERT weights to improve NMT training. In contrast to most prior work, we focus on pre-training one denoising autoencoder, and adapt the weights of the entire model for various MT applications. Testing Languages Domain ZH JA KO CS RO NL IT AR HI NE SI GU Zh News 23.7 9.9 5.8 9.3 16.2 14.4 16.9 5.8 3.2 2.1 5.0 8.2 Ja TED 8.8"
2020.tacl-1.47,W17-4811,0,0.0808628,"Missing"
2020.tacl-1.47,Q18-1029,0,0.0291547,"cus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bilingual scenario. 7 Conclusion Compared with multilingual translation, we do not require parallel data across multiple languages but the targeted direction, which improves the scalability to low-resource languages and specific domains. Document Translation As one of the key applications, our work is also related to previous efforts for incorporating document-level context into neural machine translation (Wang et al., 2017; Jean et al., 2017; Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Tu et al., 2018). Li et al. (2019) is the most relevant work that also utilized pre-trained encoder (BERT) for handling longer context. However, the focus has been on designing new task-specific techniques, and doing sentence-level translation with a wider input context. To the best of our knowledge, our multilingual pre-trained model is the first that shows improved results on document-level translation with standard Seq2Seq models. Unsupervised Translation This work also summarizes the previous efforts of learning to translate between languages without a direct parallel corpus. When no parallel data of any"
2020.tacl-1.47,Q19-1000,0,0.270655,"Missing"
2020.tacl-1.47,P02-1040,0,\N,Missing
2020.tacl-1.47,D18-2012,0,\N,Missing
2020.tacl-1.47,D18-1269,0,\N,Missing
2020.tacl-1.47,2015.iwslt-evaluation.1,0,\N,Missing
2020.tacl-1.47,D19-1632,0,\N,Missing
2020.tacl-1.47,P16-1009,0,\N,Missing
2020.tacl-1.47,N19-1423,0,\N,Missing
2020.tacl-1.47,2012.eamt-1.60,0,\N,Missing
2020.tacl-1.5,N19-1423,0,0.486582,"n boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across diff"
2020.tacl-1.5,D19-5801,1,0.908682,"Missing"
2020.tacl-1.5,S17-2001,0,0.0656926,"Missing"
2020.tacl-1.5,W07-1401,0,0.151879,"ks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018),7 we exclude WNLI from the results to enable a fair comparison. Although recent work Liu et al. (2019a) h"
2020.tacl-1.5,P18-2058,1,0.849186,"19), but deviates from its bi-text classification framework in three ways. First, we use a different random process to mask spans of tokens, rather than individual ones. We also introduce a novel auxiliary objective—the SBO—which tries to predict the entire masked span using only the representations of the tokens at the span’s boundary. Finally, SpanBERT samples a single contiguous segment of text for each training example (instead of two), and thus does not use BERT’s next sentence prediction objective, which we omit. 3.2 Span Boundary Objective Span selection models (Lee et al., 2016, 2017; He et al., 2018) typically create a fixed-length representation of a span using its boundary tokens (start and end). To support such models, we would ideally like the representations for the end of the span to summarize as much of the internal span content as possible. We do so by introducing a span boundary objective that involves predicting each token of a masked span using only the representations of the observed tokens at the boundaries (Figure 1). Formally, we denote the output of the transformer encoder for each token in the sequence by x1 , . . . , xn . Given a masked span of tokens (xs , . . . , xe )"
2020.tacl-1.5,P19-1285,0,0.030626,"ping (Song et al., 2019; Chan et al., 2019) multiple words from the input—particularly as pretraining for lan72 (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. substantially better performance on span selection tasks in particular. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1–5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Appendices A Pre-training Procedure We describe our pre-training procedure as follows: 1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre"
2020.tacl-1.5,P18-1031,0,0.0617737,"Missing"
2020.tacl-1.5,N19-1362,1,0.921184,"at, e.g., all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept. 67 answer span to be the special token [CLS] for both training and testing. was born in [OBJ-LOC] , Michigan, . . . ’’, and finally add a linear classifier on top of the [CLS] token to predict the relation type. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coreference model (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018)"
2020.tacl-1.5,P17-1147,1,0.858746,"MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 ,"
2020.tacl-1.5,D19-1588,1,0.819519,"at, e.g., all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept. 67 answer span to be the special token [CLS] for both training and testing. was born in [OBJ-LOC] , Michigan, . . . ’’, and finally add a linear classifier on top of the [CLS] token to predict the relation type. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coreference model (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018)"
2020.tacl-1.5,P19-1441,0,0.137858,"iccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018),7 we exclude WNLI from the results to enable a fair comparison. Although recent work Liu et al. (2019a) has applied several task-specific strategies to increase performance on the individual GLUE tasks, we follow BERT’s single-task setting and only add a linear classifier on top of the [CLS] token for these classification tasks. Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and φ(x, y ) represents the handengineered features (e.g., speake"
2020.tacl-1.5,Q19-1026,0,0.0481222,"and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 , q2 , . . . , ql ) into a single sequence X = [CLS]p1 p2 . . . pl [SEP] q1 q2 ."
2020.tacl-1.5,K16-1006,0,0.0787294,"Missing"
2020.tacl-1.5,D17-1018,1,0.83938,"Missing"
2020.tacl-1.5,N19-4009,0,0.0302846,"re information). A more detailed description of the model can be found in Joshi et al. (2019b). Relation Extraction TACRED (Zhang et al., 2017) is a challenging relation extraction dataset. Given one sentence and two spans within it— subject and object—the task is to predict the relation between the spans from 42 pre-defined relation types, including no relation. We follow the entity masking schema from Zhang et al. (2017) and replace the subject and object entities by their NER tags such as ‘‘[CLS] [SUBJ-PER] 4.2 Implementation We reimplemented BERT’s model and pretraining method in fairseq (Ott et al., 2019). 6 https://data.quora.com/First-QuoraDataset-Release-Question-Pairs. 7 Previous work has excluded WNLI on account of construction issues outlined on the GLUE website – https:// gluebenchmark.com/faq. The length was chosen from {128, 256, 384, 512}. See more details in Appendix B. 5 68 We used the model configuration of BERTlarge as in Devlin et al. (2019) and also pre-trained all our models on the same corpus: BooksCorpus and English Wikipedia using cased Wordpiece tokens. Compared with the original BERT implementation, the main differences in our implementation include: (a) We use different"
2020.tacl-1.5,N18-1202,1,0.849243,"Missing"
2020.tacl-1.5,W12-4501,0,0.659074,"Missing"
2020.tacl-1.5,E17-2025,0,0.0297017,"t al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. h0 = [xs−1 ; xe+1 ; pi−s+1 ] h1 = LayerNorm (GeLU(W1 h0 )) yi = LayerNorm (GeLU(W2 h1 )) We then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA ("
2020.tacl-1.5,W17-2623,0,0.0605729,"ut embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert t"
2020.tacl-1.5,P18-2124,0,0.0900822,"Missing"
2020.tacl-1.5,D16-1264,0,0.546497,"presentation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain"
2020.tacl-1.5,P19-1279,0,0.0469656,"Missing"
2020.tacl-1.5,N18-1101,0,0.0381691,"lity and SST-2 (Socher et al., 2013) for sentiment classification. • Three sentence-pair similarity tasks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018)"
2020.tacl-1.5,D13-1170,0,0.00994137,"odel (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018) for evaluating linguistic acceptability and SST-2 (Socher et al., 2013) for sentiment classification. • Three sentence-pair similarity tasks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpur"
2020.tacl-1.5,2020.emnlp-demos.6,0,0.0633789,"Missing"
2020.tacl-1.5,D18-1259,0,0.0368485,"on Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 , q2 , . . . , ql ) into a sin"
2020.tacl-1.5,P19-1139,0,0.0641703,"Missing"
2021.acl-long.35,I17-2071,0,0.0444436,"rge 416 Multi-document summarization aims to condense a cluster of thematically-related documents into a short and informative summary. A wide range of multi-document summarization datasets have been built for the Document Understanding and Text Analysis Conferences (Over and Yen, 2004; Owczarzak and Dang, 2011), news (Fabbri et al., 2019), events (Gholipour Ghalandari et al., 2020) and Wikipedia summaries (Liu et al., 2018). Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summarization (Banerjee et al., 2015; Chali et al., 2017; Nayeem et al., 2018). However, existing datasets typically are not entity focused and assume the input documents are at least loosely centered around a coherent topic or event. Sources We paired entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To capture the challenge of emerging entities, we retrieve source documents that are not in Wikipedia using Wikilinks and RealNews. We also include specialized entities in Fandom that do not have Wikipedia pages. For quality control, we filter out entities for which the unigram rec"
2021.acl-long.35,D19-1040,0,0.02012,"n ROUGE-L) between state-of-the-art models and human performance, suggesting that the data will support significant future work.1 Table 1: An example from D ESC G EN exhibiting the diversity of source documents and the abstractive nature of the entity description summaries. Entity knowledge has been shown to play an important role in various applications including language modeling (Peters et al., 2019), open-domain question answering (Xu et al., 2016), and dialogue generation (Qin et al., 2019). Recent studies suggest that such entity knowledge can be provided by simple textual descriptions (Chen et al., 2019), which can be incorporated to improve downstream task performance (Nie et al., 2018; Logeswaran et al., 2019). However, manually curating entity descriptions is labor-intensive and it is challenging to keep pace with the ever growing emergence of new entities. In this paper, we present a new dataset D ESC G EN for automatically generating entity descriptions from relevant documents and mentions, which provides high quality supervision for a highly abstractive version of this task that targets early description of new entities as they emerge. For example, in Table 13, machines are required to"
2021.acl-long.35,2020.emnlp-main.90,0,0.0291572,"Output size Human-authored descriptions Wikipedia 26,585 177,454 11,568 53 598 Fandom 11,366 170,204 1,872 32 403 Table 2: Basic statistics for D ESC G EN. Input size and output size refer to the average number of words in the description and source documents respectively. amounts of text and abstractive generation from it, particularly for emerging entities. • We present a two-stage method and benchmark various models on our dataset, aiming to facilitate future work on this dataset. 2 Related work Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descriptions. However, knowledge graphs, often mined from text corpora, are overwhelmingly incomplete on real-world entities and may not be updated in real-time (Dong et al., 2014). Therefore, we focus on generating descriptions from natural language sources such as web texts and news because they are often primary sources for entities and have better coverage of entities across multiple domains. D ESC G EN is most related to WikiSum, a recent dataset for generating Wikipedia summarie"
2021.acl-long.35,W14-3348,0,0.0179492,"15 26 R-L 35.8 44.5 Table 5: Rouge results on human reference against Wikipedia/Fandom descriptions. 4.2 Table 6: Number of times a human-authored description is classified into error categories with Wikipedia/Fandom descriptions as reference. The sample size is 40. Inter-annotator agreement Each entity in the verified subset has two descriptions written by two annotators. Following previous work (Chen et al., 2015), we quantify interannotator agreement on descriptions by treating one of the descriptions as the prediction and the other as the reference to compute ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). Table 4 shows high inter-annotator agreement of 47.7 in terms of ROUGE-L. We additionally measure the agreement on content selection using sentences marked by annotators. In particular, agreement is achieved when both annotators selected the exact same sentences in all source documents for an entity. Cohen’s Kappa is 0.38, which indicates high agreement (Brennan and Prediger, 1981) considering the strict criterion of reaching agreement. 4.3 Comparison between human-authored and Wikipedia/Fandom descriptions To understand how human-authored descriptions differ with Wikipedia and Fandom descri"
2021.acl-long.35,N19-1423,0,0.0201466,"nts but almost no long phrases. Density(A, S) = 1 X 2 |f | |S| f ∈F We compare our dataset with several multidocument summarization datasets, including CNN / Daily Mail, Multi-News (Fabbri et al., 2019) and WikiSum (Liu et al., 2018). Figure 2 presents the density and coverage distribution. The density of Multi-News, CNN / Daily Mail and WikiSum are high, showing that there is much copying of long sequences with respect to source documents. D E SC G EN shows high coverage but low density, suggesting it is not common to copy long sequences and the data overall is much more abstractive. 5 BERT (Devlin et al., 2019) with a classifier uses a linear layer stacked on top of the BERT outputs and predict whether a sentence should be selected. The model is trained on our training dataset in which sentences are labeled by the cheating method. 5.2 Abstractive stage We compare three pre-trained language generation models, including BART (Lewis et al., 2020b), T5 (Raffel et al., 2019) and MARGE (Lewis et al., 2020a) to generate abstractive entity descriptions. We fine-tuned these models on our training dataset in a sequence-to-sequence fashion. Baselines In this section, we introduce several new baseline methods,"
2021.acl-long.35,P19-1102,0,0.13039,"rvised test data. However, our experiments demonstrate that manually annotated data allows for much better evaluation of model performance (Table 7). • We conduct an extensive analysis of properties of the dataset and identify its challenges— extractive content selection from large 416 Multi-document summarization aims to condense a cluster of thematically-related documents into a short and informative summary. A wide range of multi-document summarization datasets have been built for the Document Understanding and Text Analysis Conferences (Over and Yen, 2004; Owczarzak and Dang, 2011), news (Fabbri et al., 2019), events (Gholipour Ghalandari et al., 2020) and Wikipedia summaries (Liu et al., 2018). Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summarization (Banerjee et al., 2015; Chali et al., 2017; Nayeem et al., 2018). However, existing datasets typically are not entity focused and assume the input documents are at least loosely centered around a coherent topic or event. Sources We paired entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To ca"
2021.acl-long.35,N18-1065,0,0.0648584,"Missing"
2021.acl-long.35,D17-1284,0,0.0286143,"ns are not available. 417 Real News To expand the collection of source documents, we extract entity mentions in RealNews (Zellers et al., 2019), a large corpus of news articles from Common Crawl. We first conduct a longest prefix match between the entity surface form and text tokens via trie, a prefix tree structure that supports efficient string searching. More specifically, we build a trie of entity names where each node is a word and its children indicate all possible continuations from the prefix. After retriving candidates for entity mentions, we use an offthe-shelf entity linking model (Gupta et al., 2017) to rank the candidates and add the corresponding news articles as source documents of the rank-1 candidate. Fandom Fandom3 is a collection of encyclopedias, centered around particular subjects and themes such as movies, TV shows, and games. It contains specialized entities that require domain experts with background knowledge to make edits. Entities and their source documents can be automatically extracted by internal links. We filter out entities and only keep those without Wikipedia pages, which can be viewed as new or emerging entities. The description of the entity is extracted 3 https://"
2021.acl-long.35,2020.acl-main.703,1,0.70378,"WikiSum are high, showing that there is much copying of long sequences with respect to source documents. D E SC G EN shows high coverage but low density, suggesting it is not common to copy long sequences and the data overall is much more abstractive. 5 BERT (Devlin et al., 2019) with a classifier uses a linear layer stacked on top of the BERT outputs and predict whether a sentence should be selected. The model is trained on our training dataset in which sentences are labeled by the cheating method. 5.2 Abstractive stage We compare three pre-trained language generation models, including BART (Lewis et al., 2020b), T5 (Raffel et al., 2019) and MARGE (Lewis et al., 2020a) to generate abstractive entity descriptions. We fine-tuned these models on our training dataset in a sequence-to-sequence fashion. Baselines In this section, we introduce several new baseline methods, building on state-of-the-art pre-trained models. The input documents can be long (Section 8), making it computationally infeasible to train end-to-end models. We instead introduce a pipelined approach to generate an entity description in two stages. In the first extractive stage, a selector is used to identify representative sentences r"
2021.acl-long.35,P19-1335,0,0.0194365,"significant future work.1 Table 1: An example from D ESC G EN exhibiting the diversity of source documents and the abstractive nature of the entity description summaries. Entity knowledge has been shown to play an important role in various applications including language modeling (Peters et al., 2019), open-domain question answering (Xu et al., 2016), and dialogue generation (Qin et al., 2019). Recent studies suggest that such entity knowledge can be provided by simple textual descriptions (Chen et al., 2019), which can be incorporated to improve downstream task performance (Nie et al., 2018; Logeswaran et al., 2019). However, manually curating entity descriptions is labor-intensive and it is challenging to keep pace with the ever growing emergence of new entities. In this paper, we present a new dataset D ESC G EN for automatically generating entity descriptions from relevant documents and mentions, which provides high quality supervision for a highly abstractive version of this task that targets early description of new entities as they emerge. For example, in Table 13, machines are required to generate a description of Carl Menger, given multiple documents mentioning him. D ESC G EN contains 37K entity"
2021.acl-long.35,C18-1102,0,0.0208578,"nt summarization aims to condense a cluster of thematically-related documents into a short and informative summary. A wide range of multi-document summarization datasets have been built for the Document Understanding and Text Analysis Conferences (Over and Yen, 2004; Owczarzak and Dang, 2011), news (Fabbri et al., 2019), events (Gholipour Ghalandari et al., 2020) and Wikipedia summaries (Liu et al., 2018). Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summarization (Banerjee et al., 2015; Chali et al., 2017; Nayeem et al., 2018). However, existing datasets typically are not entity focused and assume the input documents are at least loosely centered around a coherent topic or event. Sources We paired entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To capture the challenge of emerging entities, we retrieve source documents that are not in Wikipedia using Wikilinks and RealNews. We also include specialized entities in Fandom that do not have Wikipedia pages. For quality control, we filter out entities for which the unigram recall of the entity desc"
2021.acl-long.35,W17-5525,0,0.0262474,"es Documents Input size Output size Human-authored descriptions Wikipedia 26,585 177,454 11,568 53 598 Fandom 11,366 170,204 1,872 32 403 Table 2: Basic statistics for D ESC G EN. Input size and output size refer to the average number of words in the description and source documents respectively. amounts of text and abstractive generation from it, particularly for emerging entities. • We present a two-stage method and benchmark various models on our dataset, aiming to facilitate future work on this dataset. 2 Related work Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descriptions. However, knowledge graphs, often mined from text corpora, are overwhelmingly incomplete on real-world entities and may not be updated in real-time (Dong et al., 2014). Therefore, we focus on generating descriptions from natural language sources such as web texts and news because they are often primary sources for entities and have better coverage of entities across multiple domains. D ESC G EN is most related to WikiSum, a recent dataset for generatin"
2021.acl-long.35,D18-1179,1,0.806726,"using source documents from Table 13. BART, one of the best performing baselines, generates a description that has highest overlap with the Wikipedia description, but it still misses some important facts. T5-Base and MARGE confuse Carl Menger and his son, and incorrectly include information that does not describe the target entity. 7.2 Entity knowledge in pre-trained models BART, T5, and MARGE are language models pretrained on text corpora including Wikipedia and Common Crawl. The parameters of the models appear to contain substantial linguistic and factual information (Petroni et al., 2019; Peters et al., 2018). In particular, we wonder if entity-related knowledge is captured in the pretraining stage and investigate the following questions: (a) Can the model memorize entity descriptions in pretraining stage? (b) Does the memorized knowledge improve model performance on generating entity descriptions? To investigate the questions, we test the model’s ability to write a description given only the entity name instead of source documents. We train the model on our training dataset to adapt to the style of Wikipedia in a similar way. The results are shown in Table 11. Considering the name-only baselines,"
2021.acl-long.35,D19-1005,0,0.0526816,"Missing"
2021.acl-long.35,D19-1250,0,0.0241631,"ptions for Carl Menger using source documents from Table 13. BART, one of the best performing baselines, generates a description that has highest overlap with the Wikipedia description, but it still misses some important facts. T5-Base and MARGE confuse Carl Menger and his son, and incorrectly include information that does not describe the target entity. 7.2 Entity knowledge in pre-trained models BART, T5, and MARGE are language models pretrained on text corpora including Wikipedia and Common Crawl. The parameters of the models appear to contain substantial linguistic and factual information (Petroni et al., 2019; Peters et al., 2018). In particular, we wonder if entity-related knowledge is captured in the pretraining stage and investigate the following questions: (a) Can the model memorize entity descriptions in pretraining stage? (b) Does the memorized knowledge improve model performance on generating entity descriptions? To investigate the questions, we test the model’s ability to write a description given only the entity name instead of source documents. We train the model on our training dataset to adapt to the style of Wikipedia in a similar way. The results are shown in Table 11. Considering th"
2021.acl-long.35,D19-1013,0,0.0410977,"Missing"
2021.acl-long.35,P09-1024,0,0.106292,"entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To capture the challenge of emerging entities, we retrieve source documents that are not in Wikipedia using Wikilinks and RealNews. We also include specialized entities in Fandom that do not have Wikipedia pages. For quality control, we filter out entities for which the unigram recall of the entity description against its concatenated source documents is lower than 0.6. Wikipedia generation Our work is also related to research on generating Wikipedia articles. For instance, Sauper and Barzilay (2009) learn to build content templates using an integer linear program to generate full articles. Similarly, Banerjee and Mitra (2016) generate Wikipedia pages by building a topic classifier to assign web retrieved contents into relevant sections. We focus on a different task – generating a short text description that can identify and best summarize an entity. 3.1 3 Dataset Collection Distantly supervised data collection Wikilinks Wikilinks (Singh et al., 2012) is a large dataset designed for cross-document coreference. It consists of non-Wikipedia web pages (discovered using the Google search inde"
2021.acl-long.35,2020.emnlp-demos.6,0,0.0709208,"Missing"
2021.acl-long.35,P16-1220,0,0.0350087,"Missing"
2021.acl-long.35,K17-1045,0,0.0284521,"• We conduct an extensive analysis of properties of the dataset and identify its challenges— extractive content selection from large 416 Multi-document summarization aims to condense a cluster of thematically-related documents into a short and informative summary. A wide range of multi-document summarization datasets have been built for the Document Understanding and Text Analysis Conferences (Over and Yen, 2004; Owczarzak and Dang, 2011), news (Fabbri et al., 2019), events (Gholipour Ghalandari et al., 2020) and Wikipedia summaries (Liu et al., 2018). Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summarization (Banerjee et al., 2015; Chali et al., 2017; Nayeem et al., 2018). However, existing datasets typically are not entity focused and assume the input documents are at least loosely centered around a coherent topic or event. Sources We paired entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To capture the challenge of emerging entities, we retrieve source documents that are not in Wikipedia using Wikilinks and RealNews. We also include speciali"
2021.acl-long.35,C16-1145,0,0.0604766,"Missing"
2021.acl-long.568,W19-4828,0,0.431745,"rained representations (Aghajanyan et al., 2020, 2021). A holistic explanatory picture of the successes of fine-tuning has not yet been painted. A clear understanding of the underlying mechanisms which lead to the incredible generalization of fine-tuned pre-trained representations is currently missing. Moreover, we still do not understand why various pre-training methodology manifests in universally useful representations, although recent line of works have attempted to cover this gap by looking at loss landscapes, and the learned linguistic properties of pre-trained models (Hao et al., 2019; Clark et al., 2019a). 3 Intrinsic Dimensionality of Finetuning Background The intrinsic dimension of an objective function measures the minimum number of parameters needed to reach satisfactory solutions to the respective objective (Li et al., 2018). Alternatively, the intrinsic dimension represents the lowest dimensional subspace in which one can optimize the original function to within a certain level of approximation error. Computing the exact intrinsic dimensional of the objective function is computation intractable; therefore, we resort to heuristic methods to calculate an upper bound. Let θD = [θ0 , θ1 ,"
2021.acl-long.568,P19-4007,0,0.046912,"Missing"
2021.acl-long.568,D19-6117,0,0.0213621,"arning was first proposed by Li et al. (2018). They analyzed the impact of various architectures on the intrinsic dimensionality of their objective. Our work is a direct extension of this approach, focusing on analyzing pre-trained representations instead. There is a large collection of literature analyzing pre-trained models from the perspective of capacity. For example, a recent line of work has shown that pre-trained models such as BERT are redundant in their capacity, allowing for significant sparsification without much degradation in end metrics (Chen et al., 2020; Prasanna et al., 2020; Desai et al., 2019). Houlsby et al. (2019) showed that finetuning top layers of pre-trained models is not effective and that alternate methods allow fine-tuning effectively with a couple of percent of the parameters. Furthermore, we can view computing the intrinsic dimensionality as a continuous relaxation of the sparsification problem. There also exist connections between intrinsic dimensionality, knowledge distillation, and other model compression methods. Fundamentally intrinsic dimensionality attempts to find the smallest set of parameters needed to tune to reach satisfactory solutions, which can be thought"
2021.acl-long.568,I05-5002,0,0.0667995,"Lastly, we see that adding a notion of structure in the computation of intrinsic dimension is beneficial with the SAID method consistently improving over the structure unaware DID method. DID MRPC QQP MRPC QQP 1608 1037 8030 1200 1861 2493 9295 1389 896 207 896 774 1000 322 1389 774 Table 1: Estimated d90 intrinsic dimension computed with SAID and DID for a set of sentence prediction tasks and common pre-trained models. (Wang et al., 2018). We focus on analyzing BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) at both the base and large model sizes. We chose to experiment with MRPC (Dolan and Brockett, 2005) and QQP (Iyer et al., 2017) as reference examples of small and large tuning datasets. MRPC is a binary classification task for predicting semantic equivalency for two paraphrases with roughly 3700 training samples, while QQP is a binary classification task for predicting semantic equality of two questions, with roughly 363k samples. For every dataset and every model, we run 100 subspace trainings with d ranging from 10 to 10000 on a log scale. For every training run, we do a small hyperparameter search across four learning rates. We initialize every θd to the zero vector to allow for our star"
2021.acl-long.568,D19-1424,0,0.0143601,"alization of pre-trained representations (Aghajanyan et al., 2020, 2021). A holistic explanatory picture of the successes of fine-tuning has not yet been painted. A clear understanding of the underlying mechanisms which lead to the incredible generalization of fine-tuned pre-trained representations is currently missing. Moreover, we still do not understand why various pre-training methodology manifests in universally useful representations, although recent line of works have attempted to cover this gap by looking at loss landscapes, and the learned linguistic properties of pre-trained models (Hao et al., 2019; Clark et al., 2019a). 3 Intrinsic Dimensionality of Finetuning Background The intrinsic dimension of an objective function measures the minimum number of parameters needed to reach satisfactory solutions to the respective objective (Li et al., 2018). Alternatively, the intrinsic dimension represents the lowest dimensional subspace in which one can optimize the original function to within a certain level of approximation error. Computing the exact intrinsic dimensional of the objective function is computation intractable; therefore, we resort to heuristic methods to calculate an upper bound."
2021.acl-long.568,P84-1044,0,0.345472,"Missing"
2021.acl-long.568,2021.ccl-1.108,0,0.0738288,"Missing"
2021.acl-long.568,2020.emnlp-main.259,0,0.0136692,"the context of deep-learning was first proposed by Li et al. (2018). They analyzed the impact of various architectures on the intrinsic dimensionality of their objective. Our work is a direct extension of this approach, focusing on analyzing pre-trained representations instead. There is a large collection of literature analyzing pre-trained models from the perspective of capacity. For example, a recent line of work has shown that pre-trained models such as BERT are redundant in their capacity, allowing for significant sparsification without much degradation in end metrics (Chen et al., 2020; Prasanna et al., 2020; Desai et al., 2019). Houlsby et al. (2019) showed that finetuning top layers of pre-trained models is not effective and that alternate methods allow fine-tuning effectively with a couple of percent of the parameters. Furthermore, we can view computing the intrinsic dimensionality as a continuous relaxation of the sparsification problem. There also exist connections between intrinsic dimensionality, knowledge distillation, and other model compression methods. Fundamentally intrinsic dimensionality attempts to find the smallest set of parameters needed to tune to reach satisfactory solutions,"
2021.acl-long.568,D13-1170,0,0.00420221,"from only training for a total of 200k steps (instead of 500k) with half the batch size (1k). To calculate the intrinsic dimension more efficiently, we reuse the best learning rates discovered in Section 4 for d &lt; 10000 and use a fixed learning rate for anything else. To find d90 we do a binary search across d per each checkpoint, with a minimum d of 100 and a maximum of 4 million. The “full solution” that we use when deciding d90 cutoff is computed by fine-tuning the checkpointed model in the standard way. We compute SAID on six datasets; MRPC, QQP, Yelp Polarity (Zhang et al., 2015), SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018) and ANLI using all rounds of data (Nie et al., 2019). Although we focus on bench-marking sentence classification tasks the selected set of tasks contains variety, from sentiment classification (Yelp Polarity, SST-2) to Natural Language Inference (MNLI, ANLI) to question similarity (QQP). We present our results in Figure 2. The intrinsic dimensionality of RoBERTa-Base monotonically decreases as we continue pre-training. We do not explicitly optimize for intrinsic dimensionality, specifically during pre-training (the language model does not have access to downstrea"
2021.acl-long.568,W18-5446,0,0.05141,"Missing"
2021.acl-long.568,N18-1101,0,0.0308562,"al of 200k steps (instead of 500k) with half the batch size (1k). To calculate the intrinsic dimension more efficiently, we reuse the best learning rates discovered in Section 4 for d &lt; 10000 and use a fixed learning rate for anything else. To find d90 we do a binary search across d per each checkpoint, with a minimum d of 100 and a maximum of 4 million. The “full solution” that we use when deciding d90 cutoff is computed by fine-tuning the checkpointed model in the standard way. We compute SAID on six datasets; MRPC, QQP, Yelp Polarity (Zhang et al., 2015), SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018) and ANLI using all rounds of data (Nie et al., 2019). Although we focus on bench-marking sentence classification tasks the selected set of tasks contains variety, from sentiment classification (Yelp Polarity, SST-2) to Natural Language Inference (MNLI, ANLI) to question similarity (QQP). We present our results in Figure 2. The intrinsic dimensionality of RoBERTa-Base monotonically decreases as we continue pre-training. We do not explicitly optimize for intrinsic dimensionality, specifically during pre-training (the language model does not have access to downstream datasets!), but none-the-les"
2021.acl-long.568,2020.acl-main.703,1,0.795247,"Missing"
2021.acl-long.67,Q17-1010,0,0.0262883,"= ReLU W1 xhs ,t i + b1 h i j  ˆ + b2 g = W2 · h exp (gy ) , PΦ (y |si , tj , S, T ) = P y 0 exp gy 0 where Φ = {W1 W2 , b1 , b2 }. On the training stage, we maximize the log-likelihood of groundtruth labels: Φ∗ = arg max Φ X X X log PΦ (yhsi ,tj i |si , tj , S, T ). hS,T i∈B si ∈S tj ∈T On the inference stage, we keep all word token pairs hsi , tj i that have X EP [y] := y · P (y |si , tj , S, T ) &gt; 1 V EC M AP. Popular and robust method for aligning monolingual word embeddings via a linear projection and extracting lexicons. Here, we use the standard implementation9 with FastText vectors (Bojanowski et al., 2017)10 trained on the union of Wikipedia and Common Crawl corpus for each language.11 We include both supervised and unsupervised versions. WikiMatrix (Schwenk et al., 2019)12 is a dataset of mined bitext. The mining method LASER (Artetxe and Schwenk, 2019b) is trained on real bitext and then used to mine more bitext from the Wikipedia corpora to get the WikiMatrix dataset. We test our lexicon induction method with WikiMatrix bitext as the input and compare to our methods that do not use bitext supervision. WM . 7 y BLI Results and Analysis as the prediction. 7.1 6 We evaluate bidirectional transl"
2021.acl-long.67,J93-2003,0,0.218899,"Missing"
2021.acl-long.67,N13-1073,0,0.101584,"y a statistical heuristics (Hoshen and Wolf, 2018; Artetxe et al., 2018). Artetxe et al. (2019) show strong gains over previous works by word aligning bitext generated with unsupervised machine translation. We show that retrieval-based bitext mining and contextual word alignment achieves even better performance. Word alignment. Word alignment is a fundamental problem in statistical machine translation, of which the goal is to align words that are translations of each in within parallel sentences (Brown et al., 1993). Most methods assume parallel sentences for training data (Och and Ney, 2003; Dyer et al., 2013; Peter et al., 2017, inter alia). In contrast, Sabet et al. (2020) propose SimAlign, which does not train on parallel sentences but instead aligns words that have the most similar preBitext mining/parallel corpus mining. Bitext mining has been a long studied task (Resnik, 1999; Shi et al., 2006; Abdul-Rauf and Schwenk, 2009, inter alia). Most methods train neural multilingual encoders on bitext, which are then used with efficent nearest neighbor search to expand the training set (Espana-Bonet et al., 2017; Schwenk, 2018; Guo et al., 2018; Artetxe and Schwenk, 2019a, inter alia). Recent work h"
2021.acl-long.67,D19-1453,0,0.111322,"ts for random 20 error cases made by GEN - RTV (left) and V EC M AP (right). 3 and 7 denote acceptable and unacceptable translation respectively. ? denotes word pairs that may be acceptable in rare or specific contexts. Data Source de-en http://www-i6.informatik.rwth-aachen. de/goldAlignment (de-en); https://web.eecs. following Sabet et al. (2020). We investigate four language pairs: German–English (de-en), English–French (en-fr), English–Hindi (en-hi) and Romanian–English (ro-en). We find that the CRISS-based SimAlign already achieves competitive performance with the state-of-the-art method (Garg et al., 2019) which requires real bitext for training. By ensembling the argmax and itermax CRISS-based SimAlign results (Section 5), we set the new state of the art of word alignment without using any bitext supervision. However, by substituting the CRISS-based SimAlign in the BLI pipeline with our aligner, we obtain an average F1 score of 73.0 for GEN RTV , which does not improve over the result of 73.3 achieved by CRISS-based SimAlign (Table 1), indicating that further effort is required to take the advantage of the improved word aligner. 9 Discussion We present a direct and effective framework for BLI"
2021.acl-long.67,W18-6317,0,0.0191061,"rallel sentences for training data (Och and Ney, 2003; Dyer et al., 2013; Peter et al., 2017, inter alia). In contrast, Sabet et al. (2020) propose SimAlign, which does not train on parallel sentences but instead aligns words that have the most similar preBitext mining/parallel corpus mining. Bitext mining has been a long studied task (Resnik, 1999; Shi et al., 2006; Abdul-Rauf and Schwenk, 2009, inter alia). Most methods train neural multilingual encoders on bitext, which are then used with efficent nearest neighbor search to expand the training set (Espana-Bonet et al., 2017; Schwenk, 2018; Guo et al., 2018; Artetxe and Schwenk, 2019a, inter alia). Recent work has also shown that unsupervised mining is possible (Tran et al., 2020; Keung et al., 2020). We use CRISS (Tran et al., 2020)2 as one of our component models. 3 Baseline Components We build on unsupervised methods for word alignment and bitext construction, as reviewed below. 3.1 Unsupervised Word Alignment SimAlign (Sabet et al., 2020) is an unsupervised word aligner based on the similarity of contextualized token embeddings. Given a pair of parallel sentences, SimAlign computes embeddings using pretrained multilingual language models suc"
2021.acl-long.67,D18-1043,0,0.0261912,"ilingual lexicon (i.e., word translation) from comparable monolingual corpora (e.g., Wikipedia in different languages). Following Mikolov et al. (2013), most methods train a linear projection to align two monolingual embedding spaces. For supervised BLI, a seed lexicon is used to learn the projection matrix (Artetxe et al., 2016; Smith et al., 2017; Joulin et al., 2018). For unsupervised BLI, the projection matrix is typically found by an iterative procedure such as adversarial learning (Lample et al., 2018; Zhang et al., 2017), or iterative refinement initialized by a statistical heuristics (Hoshen and Wolf, 2018; Artetxe et al., 2018). Artetxe et al. (2019) show strong gains over previous works by word aligning bitext generated with unsupervised machine translation. We show that retrieval-based bitext mining and contextual word alignment achieves even better performance. Word alignment. Word alignment is a fundamental problem in statistical machine translation, of which the goal is to align words that are translations of each in within parallel sentences (Brown et al., 1993). Most methods assume parallel sentences for training data (Och and Ney, 2003; Dyer et al., 2013; Peter et al., 2017, inter alia"
2021.acl-long.67,D18-1330,0,0.079028,"sentences, and provides one of the baseline components for our work. We also present a simple yet effective method to improve performance over SimAlign (Section 5). Related Work Bilingual lexicon induction (BLI). The task of BLI aims to induce a bilingual lexicon (i.e., word translation) from comparable monolingual corpora (e.g., Wikipedia in different languages). Following Mikolov et al. (2013), most methods train a linear projection to align two monolingual embedding spaces. For supervised BLI, a seed lexicon is used to learn the projection matrix (Artetxe et al., 2016; Smith et al., 2017; Joulin et al., 2018). For unsupervised BLI, the projection matrix is typically found by an iterative procedure such as adversarial learning (Lample et al., 2018; Zhang et al., 2017), or iterative refinement initialized by a statistical heuristics (Hoshen and Wolf, 2018; Artetxe et al., 2018). Artetxe et al. (2019) show strong gains over previous works by word aligning bitext generated with unsupervised machine translation. We show that retrieval-based bitext mining and contextual word alignment achieves even better performance. Word alignment. Word alignment is a fundamental problem in statistical machine transla"
2021.acl-long.67,D19-1328,0,0.0115899,"reases as more lower frequency words are included seems true for all language pairs (Appendix A). On average and for the majority of language pairs, both methods do better on low-frequency source words than high-frequency ones (Figure 3a), which is consistent with the findings by BUCC 2020 participants (Rapp et al., 2020). V EC M AP. While BLI through bitext construction and word alignment clearly achieves superior performance than that through vector rotation (Table 1), we further show that the gap is larger on low-frequency words (Figure 3). 7.3 Ground-truth Analysis Following the advice of Kementchedjhieva et al. (2019) that some care is needed due to the incompleteness and biases of the evaluation, we perform manual analysis of selected results. For Chinese–English translations, we uniformly sample 20 wrong lexicon entries according to the evaluation for both GEN - RTV and weakly-supervised V EC M AP. Our judgments of these samples are shown in Table 4. For GEN - RTV, 18/20 of these sampled errors are actually acceptable translations, whereas for V EC M AP, only 11/20 are acceptable. This indicates that the improvement in quality may be partly limited by the incompleteness of the reference lexicon and the g"
2021.acl-long.67,2020.bucc-1.8,0,0.0639748,"Missing"
2021.acl-long.67,P06-1062,0,0.0744321,"ter performance. Word alignment. Word alignment is a fundamental problem in statistical machine translation, of which the goal is to align words that are translations of each in within parallel sentences (Brown et al., 1993). Most methods assume parallel sentences for training data (Och and Ney, 2003; Dyer et al., 2013; Peter et al., 2017, inter alia). In contrast, Sabet et al. (2020) propose SimAlign, which does not train on parallel sentences but instead aligns words that have the most similar preBitext mining/parallel corpus mining. Bitext mining has been a long studied task (Resnik, 1999; Shi et al., 2006; Abdul-Rauf and Schwenk, 2009, inter alia). Most methods train neural multilingual encoders on bitext, which are then used with efficent nearest neighbor search to expand the training set (Espana-Bonet et al., 2017; Schwenk, 2018; Guo et al., 2018; Artetxe and Schwenk, 2019a, inter alia). Recent work has also shown that unsupervised mining is possible (Tran et al., 2020; Keung et al., 2020). We use CRISS (Tran et al., 2020)2 as one of our component models. 3 Baseline Components We build on unsupervised methods for word alignment and bitext construction, as reviewed below. 3.1 Unsupervised Wor"
2021.eacl-main.36,C14-1151,0,0.305495,"e: our analysis contains tags such as ethnic slurs, offensive, vulgar, and derogatory that correspond to examples of toxic language. For many of these examples, the meaning of the target word is only toxic due to the context in which it appears. These examples provide an opportunity for improving models for hate speech detection and related tasks, but we leave this exploration to future work. 4 in the set compared against the context – since these example sentences are used as the contexts in FEWS. Lesk+emb This baseline is an extension of the above approach that incorporates word embeddings (Basile et al., 2014). A vector representation is built for the context around an ambiguous word (vc ) and the glosses of each sense of that word (vg ), where vc and vg are the element-wise sums of the word vectors for words in the context and gloss, respectively. The sense that corresponds to the vg with the highest cosine similarity to vc is chosen as the label for the target word. We use Glove embeddings (Pennington et al., 2014) for our implementation of this baseline. 4.2 ProbeBERT This baseline is a linear classifier trained on contextualized representations output by the final layer of a frozen pretrained m"
2021.eacl-main.36,2020.acl-main.255,0,0.414812,"Missing"
2021.eacl-main.36,2020.acl-main.95,1,0.932874,"softmax over all of the senses in the Wiktionary sense inventory and mask out any senses not relevant to the target word. Baselines for FEWS We run a series of baseline approaches on FEWS to demonstrate how current methods for WSD perform on this dataset. We consider a number of knowledge-based approaches (Lesk, Lesk+emb, and MFS) and two neural models that build on pretrained encoders (ProbeBERT and BEMBERT ). We also ascertain how well humans perform on FEWS as a potential upper bound for model performance. 4.1 BEM Our other neural baseline is the biencoder model (BEM) for WSD introduced by Blevins and Zettlemoyer (2020). The BEM has two independent encoders, a context encoder that processes the context sentence (including the target word) and a gloss encoder that encodes the glosses of senses into a sense representation. The BEM takes the dot product of the target word representation from the context encoder and sense representations from the gloss encoder, and it labels the target word with the sense that has the highest dot product score. We train BEMBERT by initializing each encoder with BERT and training on the FEWS train set. Knowledge-based Baselines Most Frequent Sense (MFS) The MFS baseline assigns e"
2021.eacl-main.36,N19-1423,0,0.0241851,"ntext around an ambiguous word (vc ) and the glosses of each sense of that word (vg ), where vc and vg are the element-wise sums of the word vectors for words in the context and gloss, respectively. The sense that corresponds to the vg with the highest cosine similarity to vc is chosen as the label for the target word. We use Glove embeddings (Pennington et al., 2014) for our implementation of this baseline. 4.2 ProbeBERT This baseline is a linear classifier trained on contextualized representations output by the final layer of a frozen pretrained model; we use BERT as our pretrained encoder (Devlin et al., 2019). We train this classifier by performing a softmax over all of the senses in the Wiktionary sense inventory and mask out any senses not relevant to the target word. Baselines for FEWS We run a series of baseline approaches on FEWS to demonstrate how current methods for WSD perform on this dataset. We consider a number of knowledge-based approaches (Lesk, Lesk+emb, and MFS) and two neural models that build on pretrained encoders (ProbeBERT and BEMBERT ). We also ascertain how well humans perform on FEWS as a potential upper bound for model performance. 4.1 BEM Our other neural baseline is the b"
2021.eacl-main.36,E12-1039,0,0.0331939,"al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to our work, Henrich et al. (2012) and Segonne et al. (2019) mine WSD examples from Wiktionary to augment labeled WSD data for non-English languages. However, FEWS is the first dataset specifically designed to evaluate zero and few-shot learning with the balanced dictionary sense distribution. 456 3 FEWS: Low-shot Learning for WSD FEWS (Few-shot Examples of Word Senses) is a new dataset for learning to do low-shot WSD.1 It is created with example contexts drawn from Wiktionary, an online collaborative dictionary.2 Since 1 We use the term low-shot as an umbrella term for fewand zero-shot learning. 2 https://en.wiktionary.org/ F"
2021.eacl-main.36,D19-1355,0,0.103599,"Missing"
2021.eacl-main.36,P19-1568,0,0.185254,"d Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to our work, Henrich et"
2021.eacl-main.36,2020.emnlp-main.283,0,0.0281975,"Missing"
2021.eacl-main.36,Q13-1013,0,0.0272209,"garriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to our work, Henrich et al. (2012) and Segonne et al. (2019) mine WSD examples from Wiktionary to augment labeled WSD data for non-English languages. However, FEWS is the first dataset specifically designed to evaluate zero and few-shot learning with the balanced dictionary sense distribution. 456 3 FEWS: Low-shot Learning for WSD FEWS (Few-shot Examples of Word Senses) is a new dataset for learning to do low-shot WSD.1 It is created with example contexts dr"
2021.eacl-main.36,I11-1099,0,0.0290644,"bution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to our work, Henrich et al. (2012) and Segonne et al. (2019) mine WSD examples from Wiktionary to augment labeled WSD data for non-English languages. However, FEWS is the first dataset specifically designed to evaluate zero and few-shot learning with the balanced dictionary sense distribution. 456 3 FEWS: Low-shot Learning for WSD FEWS (Few-shot Examples of Word Senses) is a new dataset for learning to do low-shot WSD.1 It is c"
2021.eacl-main.36,N07-1025,0,0.0497429,"ding to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to our work, Henrich et al. (2012) and Segonne et al. (2019) mine WSD examples from Wiktionary to augment labeled WSD data for non-English languages. However, FEWS is the first dataset specifically designed to evaluate zero and few-shot learning with the balanced dictionary sense distribution. 456 3 FEWS: Low-shot Learning for WSD FEWS (Few-shot Examples of Word Senses) is a new dataset for learning to do low-shot WSD.1 It is created with example contexts drawn from Wiktionary, an online collaborative dictionary.2 Since 1 We use the term"
2021.eacl-main.36,H93-1061,0,0.613929,"2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more"
2021.eacl-main.36,S15-2049,0,0.164436,"e WSD Evaluation Framework (Raganato et al., 2017); this suggests that future improvements on FEWS could generalize other WSD benchmarks. FEWS is available at https://nlp.cs.washington.edu/fews. 2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 20"
2021.eacl-main.36,S13-2040,0,0.214145,"ent senses (MFS) in the WSD Evaluation Framework (Raganato et al., 2017); this suggests that future improvements on FEWS could generalize other WSD benchmarks. FEWS is available at https://nlp.cs.washington.edu/fews. 2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al"
2021.eacl-main.36,S01-1005,0,0.559923,"improves performance on all but the most frequent senses (MFS) in the WSD Evaluation Framework (Raganato et al., 2017); this suggests that future improvements on FEWS could generalize other WSD benchmarks. FEWS is available at https://nlp.cs.washington.edu/fews. 2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse perfor"
2021.eacl-main.36,D14-1162,0,0.0913741,"mpared against the context – since these example sentences are used as the contexts in FEWS. Lesk+emb This baseline is an extension of the above approach that incorporates word embeddings (Basile et al., 2014). A vector representation is built for the context around an ambiguous word (vc ) and the glosses of each sense of that word (vg ), where vc and vg are the element-wise sums of the word vectors for words in the context and gloss, respectively. The sense that corresponds to the vg with the highest cosine similarity to vc is chosen as the label for the target word. We use Glove embeddings (Pennington et al., 2014) for our implementation of this baseline. 4.2 ProbeBERT This baseline is a linear classifier trained on contextualized representations output by the final layer of a frozen pretrained model; we use BERT as our pretrained encoder (Devlin et al., 2019). We train this classifier by performing a softmax over all of the senses in the Wiktionary sense inventory and mask out any senses not relevant to the target word. Baselines for FEWS We run a series of baseline approaches on FEWS to demonstrate how current methods for WSD perform on this dataset. We consider a number of knowledge-based approaches"
2021.eacl-main.36,C16-1330,0,0.0223261,"et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less common senses (Postma et al., 2016; Kumar et al., 2019). In contrast, we use examples sentences from Wiktionary as an alternative source of text for WSD data with FEWS. This means that FEWS has a more uniform sense distribution, providing more balanced coverage across different senses of words. Wiktionary has previously been used as a resource for WSD research. Most work has investigated mapping Wiktionary senses onto WordNet synsets (Meyer and Gurevych, 2011; Matuschek and Gurevych, 2013); other work has learned similar mappings for Wikipedia articles (Mihalcea (2007); Navigli and Ponzetto (2012); inter alia). More similar to"
2021.eacl-main.36,S07-1016,0,0.815225,"ditional training data improves performance on all but the most frequent senses (MFS) in the WSD Evaluation Framework (Raganato et al., 2017); this suggests that future improvements on FEWS could generalize other WSD benchmarks. FEWS is available at https://nlp.cs.washington.edu/fews. 2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, l"
2021.eacl-main.36,E17-1010,0,0.390168,"993). We see that while SemCor tends to have more examples for these common words, most examples correspond to a single sense of the word. However, the FEWS train set covers many more senses per word, albeit with fewer total examples. This high coverage of senses is particularly important for the evaluation sets provided in FEWS (Table 2). Each evaluation set (development and test) covers 10,000 different senses; half of these examples are few-shot and occur in the training set, and the other half of the evaluation senses are zero-shot. In comparison, the current benchmark for WSD evaluation (Raganato et al., 2017) only contains 796 unique zero-shot and 761 unique fewshot senses (where the sense is seen three or fewer times in the SemCor (Miller et al., 1993) training set) across development and test evaluation sets. This much larger sample of few- and zero-shot evaluation examples means that FEWS provides a 1 2 3 4 525 148 68 20 5 6 7 8 9 9 Num. Occurences of Sense 2 0 4 10 11 12 Figure 3: Histogram of sense frequencies in the FEWS training data. robust setting to evaluate model performance on less common senses. Low-Shot Learning Because the data in FEWS come from example sentences for definitions in"
2021.eacl-main.36,W19-0422,0,0.0354247,"Missing"
2021.eacl-main.36,W04-0811,0,0.0610452,"on all but the most frequent senses (MFS) in the WSD Evaluation Framework (Raganato et al., 2017); this suggests that future improvements on FEWS could generalize other WSD benchmarks. FEWS is available at https://nlp.cs.washington.edu/fews. 2 Related Work WSD is a long-standing NLP task and is the focus of many datasets. The current de facto benchmark for modeling English WSD is the WSD Evaluation Framework (Raganato et al., 2017), which includes the SemCor dataset (Miller et al., 1993) as training data and consolidates a number of evaluation sets (Pradhan et al., 2007a; Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2013; Moro and Navigli, 2015) into a standardized evaluation suite. These datasets are annotated with the senses (known as synsets) from Wordnet, a manually constructed ontology of semantic relations (Miller et al., 1993). Most existing datasets for WSD, including those in the WSD Evaluation framework and others like Pradhan et al. (2007b), are annotated on natural language documents that contain a Zipfian distribution of word senses (Kilgarriff, 2004). This data source causes these datasets to have low coverage of rare senses, leading to worse performance on these less commo"
2021.emnlp-main.468,P17-1147,1,0.888825,"Missing"
2021.emnlp-main.544,2021.naacl-main.195,1,0.701399,"token to text Multi-modal Video-Text Pre-training. Multilabel similarity with rejection) (see §4). modal models have also adopted the preOur experiments reveal that VideoCLIP has training+fine-tuning paradigm. One line of work strong performance, even compared to supervised adopts multiple unimodal encoders for retrieval approaches which use human-annotated labels on tasks. For example, (Miech et al., 2019, 2020; the downstream tasks. For example, in text-video Ging et al., 2020; Gabeur et al., 2020; Alayrac et al., retrieval on Youcook2 (Zhou et al., 2017), Video- 2020; Patrick et al., 2021; Huang et al., 2021) adopt CLIP outperforms all existing zero-shot methods contrastive learning for pre-training and shows the and even outperforms fully supervised pre-training possibility of zero-shot transfer to text-video re+ fine-tuning methods, but without using any labels. trieval tasks. CBT (Sun et al., 2019a), HERO (Li In summary, the main contributions of this paper et al., 2020b), VideoAsMT (Korbar et al., 2020) include: (i) we propose to pre-train a unified model and UniVL (Luo et al., 2020) adopt multi-task that is capable of zero-shot transfer to multiple end learning (MTL) for pre-training on retri"
2021.emnlp-main.544,2020.emnlp-main.550,0,0.0156363,"(Zhou et al., 2018), ActBERT (Zhu and Yang, 2020) and VLM (Xu et al., 2021). Although this approach is intuitive, it limits the capability of zero-shot transfer. For example, it is non-trivial to perform retrieval tasks on a single encoder as feeding vision and text in a pairwise manner is not flexible and data efficient (Luo et al., 2020). Retrieval Augmented Training. Augmenting traditional training with a non-parametric retrieval component has recently shown impressive results in pre-training (Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020a) and QA (Izacard and Grave, 2020; Karpukhin et al., 2020). We find that contrastive learning and retrieval augmented training can have good synergy because the former aims to discriminate examples and the latter aims to find harder examples for discrimination. To the best of our knowledge, there is no existing work of retrieval augmented training for video, perhaps because videos exhibit unique challenges for dataefficient training (see §3.4). video features, extracted by a convolutional neural network (CNN), are first projected to video tokens before fed into our video transformer, as described next. Video and Text Transformers. Let cv be a video c"
2021.emnlp-main.544,N19-1423,0,0.60959,"xt in a transformer using a contrastive objective with two key novelties: (1) for positive pairs, we use video and text clips that are loosely temporarily overlapping instead of enforcing strict start/end timestamp overlap; (2) for negative pairs, we employ a retrieval based sampling technique that uses video clusters to form batches with mutually harder videos. Introduction pre-training dataset, HowTo100M (Miech et al., 2019), for zero-shot video understanding. We show The popular “pre-training + fine-tuning” paradigm that the resulting pre-trained model can be either has revolutionized NLP (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Lewis et al., 2020b) directly applied to, or fine-tuned on, a series of video-text tasks at both the global sequence and and CV (Chen et al., 2020a; He et al., 2020) over local clip/token level. the last few years. Although models trained this way can achieve impressive performance, they still We find that straightforward objectives (Chen require task-specific annotated data and fine-tuning et al., 2020a) lead to poor results, and hypothefor each end task. Recent work adopt pre-training size that learning fine-grained associations between for zero-shot t"
2021.emnlp-main.544,2020.acl-main.703,1,0.936892,"key novelties: (1) for positive pairs, we use video and text clips that are loosely temporarily overlapping instead of enforcing strict start/end timestamp overlap; (2) for negative pairs, we employ a retrieval based sampling technique that uses video clusters to form batches with mutually harder videos. Introduction pre-training dataset, HowTo100M (Miech et al., 2019), for zero-shot video understanding. We show The popular “pre-training + fine-tuning” paradigm that the resulting pre-trained model can be either has revolutionized NLP (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Lewis et al., 2020b) directly applied to, or fine-tuned on, a series of video-text tasks at both the global sequence and and CV (Chen et al., 2020a; He et al., 2020) over local clip/token level. the last few years. Although models trained this way can achieve impressive performance, they still We find that straightforward objectives (Chen require task-specific annotated data and fine-tuning et al., 2020a) lead to poor results, and hypothefor each end task. Recent work adopt pre-training size that learning fine-grained associations between for zero-shot transfer to end tasks without fine- video and text is cruci"
2021.emnlp-main.544,2020.emnlp-main.161,0,0.0237074,"all existing zero-shot methods contrastive learning for pre-training and shows the and even outperforms fully supervised pre-training possibility of zero-shot transfer to text-video re+ fine-tuning methods, but without using any labels. trieval tasks. CBT (Sun et al., 2019a), HERO (Li In summary, the main contributions of this paper et al., 2020b), VideoAsMT (Korbar et al., 2020) include: (i) we propose to pre-train a unified model and UniVL (Luo et al., 2020) adopt multi-task that is capable of zero-shot transfer to multiple end learning (MTL) for pre-training on retrieval tasks. 6788 HERO (Li et al., 2020b) and UniVL (Luo et al., 2020) further adopt a cross-encoder to further learn the fusion of different modalities. The other line of work adopts a single crossmodal encoder and concatenates the vision and text sequences as inputs, including VideoBERT (Sun et al., 2019b), Unicoder-VL (Li et al., 2020a), VL-BERT (Su et al., 2020), UNITER (Chen et al., 2020b), VLP (Zhou et al., 2018), ActBERT (Zhu and Yang, 2020) and VLM (Xu et al., 2021). Although this approach is intuitive, it limits the capability of zero-shot transfer. For example, it is non-trivial to perform retrieval tasks on a single enco"
2021.emnlp-main.544,2021.ccl-1.108,0,0.0273567,"Missing"
2021.emnlp-main.544,N18-1202,1,0.471595,"-shot Transfer. Recently, text clips may have better alignment with a video the paradigm of pre-training has made impressive clip (Miech et al., 2020) and many clips may not have any corresponding captions (see a detailed dis- progress with the scale of training data and compucussion of issues in §3.3). To address these issues, tational power. For example, in NLP, the paradigm has shifted from learning word embeddings for we pre-train with temporally overlapped pairs of task-specific architecture (Mikolov et al., 2013; Bovideo and text clips (of varying length), thereby janowski et al., 2017; Peters et al., 2018), to pregreatly increasing the quality and quantity of the training+fine-tuning (Devlin et al., 2019; Liu et al., video-text alignment. We show in experiments that this simple and general approach significantly im- 2019b; Lewis et al., 2020b) and few-shot/zero-shot transfer (Radford et al., 2018, 2019; Brown et al., proves performance. 2020; Alayrac et al., 2020; Ramesh et al., 2021) Second, we learn fine-grained video-text simithat have task-agnostic architecture. One line of larity from a contrastive loss with a new method pre-training for zero-shot transfer focuses on genfor gathering (impl"
2021.emnlp-main.544,2021.findings-acl.370,1,0.367702,"nd UniVL (Luo et al., 2020) adopt multi-task that is capable of zero-shot transfer to multiple end learning (MTL) for pre-training on retrieval tasks. 6788 HERO (Li et al., 2020b) and UniVL (Luo et al., 2020) further adopt a cross-encoder to further learn the fusion of different modalities. The other line of work adopts a single crossmodal encoder and concatenates the vision and text sequences as inputs, including VideoBERT (Sun et al., 2019b), Unicoder-VL (Li et al., 2020a), VL-BERT (Su et al., 2020), UNITER (Chen et al., 2020b), VLP (Zhou et al., 2018), ActBERT (Zhu and Yang, 2020) and VLM (Xu et al., 2021). Although this approach is intuitive, it limits the capability of zero-shot transfer. For example, it is non-trivial to perform retrieval tasks on a single encoder as feeding vision and text in a pairwise manner is not flexible and data efficient (Luo et al., 2020). Retrieval Augmented Training. Augmenting traditional training with a non-parametric retrieval component has recently shown impressive results in pre-training (Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020a) and QA (Izacard and Grave, 2020; Karpukhin et al., 2020). We find that contrastive learning and retrieval aug"
2021.emnlp-main.564,P16-1162,0,0.00783626,"ter-level language modeling, and machine translation (Holtzman et al., 2020; Al- as PMIDC when surface form competition is eliminated. In future work we would like to explore Rfou et al., 2019; Peters et al., 2019). how surface form competition affects generation, Another (not mutually exclusive) argument is that length normalization may account for uncon- as we hypothesize that it may be the cause of overly ditional probability in a similar way to PMIDC . generic outputs under high model uncertainty. Length normalization is often measured over Byte Acknowledgments Pair Encoding (BPE) tokens (Sennrich et al., 2016) and BPE tends to produce vocabularies where most This work was supported in part by the tokens are equally frequent (Wang et al., 2020). ARO (AROW911NF-16-1-0121), the NSF (IISRecent evidence suggests that language is approxi- 1562364), DARPA under the MCS program mately uniformly information dense (Levy, 2018; through NIWC Pacific (N66001-19-2-4031) and Levy and Jaeger, 2007; Jaeger, 2006). As such, the Allen Institute for AI (AI2). We thank Mitchell length in BPE tokens may correspond roughly to Wortsman, Gabriel Ilharco, Tim Dettmers, and a unigram estimate of log-probability, supposing Ri"
2021.emnlp-main.564,2020.emnlp-main.346,0,0.245715,"has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to"
2021.emnlp-main.564,D13-1170,0,0.0108623,"erent to the domain or task, e.g. completions of the domain premise that are just inherently unlikely will be upweighted more. This allows us to directly measure how much an answer tells us about the question and vice versa (mutual information is symmetric, see §3). Valid hypotheses no longer need to compete with each other: both “Whirlpool bath” and “Bathtub ” will be considered reasonable answers to the question, and so both will attain a high score. 2 Background and Related Work Zero-shot vs. Few-Shot Zero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot"
2021.emnlp-main.564,N19-1421,0,0.164119,"mpensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. * 1 Figure 1: While humans select from given options, language models implicitly assign probability to every possible string. This creates surface form competition between different strings that represent the same concept. Example from CommonsenseQA (Talmor et al., 2019). Introduction Domain Conditional Pointwise Mutual Information (PMIDC ), which reweighs scores by how much more likely a hypothesis (answer) becomes given a premise (question) within the specific task domain. Specifically, consider the example question (shown in Figure 1): “A human wants to submerge himself in water, what should he use?” with multiple choice options “Coffee cup”, “Whirlpool bath”, “Cup”, and “Puddle.” From the given options, “Whirlpool bath” is the only one that makes sense. Yet, other answers are valid and easier for a language model to generate, e.g., “Bathtub” and “A * Code"
2021.emnlp-main.564,N18-2049,0,0.0242966,"n §5. Prompt Sensitivity Recent work highlights LM sensitivity to inputs, and proposes to consider paraphrases of the prompt to overcome this (Davison et al., 2019; Jiang et al., 2020b), as well as noting that certain trigger tokens (Shin et al., 2020) can strongly effect the output of such models. In this work, we focus on the surface form of possible outputs, but do also analyze robustness to different prompts in §4.4. Interpreting Language Models Language models tend to model selectional preferences and thematic fit (Pantel et al., 2007; Erk et al., 2010) rather than semantic plausibility (Wang et al., 2018). Probability, possibility and plausibility are distinct (Van der Helm, 2006), but reporting bias (Gordon and Van Durme, 2013) means that language models only model what people are likely to write (on websites that are easily crawled). PMIDC aims to adjust for these challenges to better measure the underlying agreement between language models and human judgements, but of course is still subject to the limits and biases of the language model used. 3 Zero-shot Scoring Strategies This paper does not define any new modeling or finetuning methods. Rather, we propose the broad use of PMIDC scoring f"
2021.emnlp-main.564,2021.findings-emnlp.244,0,0.0323581,"tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms raw, normalized, and calibrated work in question answering has demonstrated the probability scoring methods on zero-shot multiple importance of considering all multiple choice opchoice for more than a dozen datasets and it does so for every model in the GPT-2 and GPT-3 fami- tions together (Khashabi"
2021.emnlp-main.564,D19-1192,0,0.0261235,"s is not the case for multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In"
2021.emnlp-main.564,D17-1233,0,0.0248982,"or multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In contrast, we repor"
2021.emnlp-main.564,2021.emnlp-main.572,0,0.012031,"to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms r"
2021.emnlp-main.564,P19-1472,1,0.828514,"for examples from each dataset in our templated format. 4.2 Datasets We report results on 16 splits of 13 datasets, and briefly describe each dataset here. Continuation These datasets require the model to select a continuation to previous text, making them a natural way to test language models. Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) asks for cause and effect relationships, as shown in Figure 2. StoryCloze (SC) (Mostafazadeh et al., 2017) gives the model a choice between two † https://beta.openai.com/ alternative endings to 5 sentence stories. Finally, HellaSwag (HS) (Zellers et al., 2019) uses GPT-2 to generate, BERT to filter, and crowd workers to verify possible continuations to a passage. Following previous work (Brown et al., 2020) we report development set numbers for COPA and HS. Question Answering RACE-M & -H (R-M & R-H) (Lai et al., 2017) are both drawn from English exams given in China, the former being given to Middle Schoolers and the latter to High Schoolers. Similarly, ARC Easy & Challenge (ARC-E & ARC-C) (Clark et al., 2018) are standardized tests described as “natural, grade-school science questions,” with the “Easy” split found to be solvable by either a retrie"
2021.findings-acl.120,2020.nlpcovid19-2.5,1,0.623006,"Missing"
2021.findings-acl.120,W19-5435,1,0.897806,"Missing"
2021.findings-acl.120,2020.acl-main.747,1,0.696864,"Missing"
2021.findings-acl.120,N19-1423,0,0.00646772,"on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM loss We also add the masked language model loss (MLM) Lmlm following (Devlin et al., 2019). To learn this loss, we create a different batch from the above by concatenating only the source S and target T as the input, since the hallucinated target T 0 could provide erroneous information for predicting masked words in T . We find that such multi-task learning objective helps learn better representations of the input and further improves performance on predicting hallucination labels. The final loss is L = Lpred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the mo"
2021.findings-acl.120,2020.acl-main.454,1,0.844635,"sed for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the so"
2021.findings-acl.120,D19-1632,1,0.862838,"Missing"
2021.findings-acl.120,D16-1139,0,0.178743,"risingly find our model can generalize well without references, even when they were present during training. To prevent the model from overly relying on the true target T and learning spurious correlations (e.g. the edit distance), we explored two techniques: (1) dropout – randomly drop out tokens in T to force the dependence on the source input; (2) paraphrase – recall that at synthetic data generation time, we generate T 0 from BART conditioned on the noised T . Instead, we can apply noise functions to the paraphrased sentence of T . We create paraphrased targets via knowledge distillation (Kim and Rush, 2016) where we use the output from pretrained Seq2Seq model conditioned on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM los"
2021.findings-acl.120,W19-5404,1,0.902402,"Missing"
2021.findings-acl.120,W17-3204,0,0.022642,"nce summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resource case where a modest amount of out of domain data is available at training time. Data We use a multi-domain Chinese-English (Zh-En) translation dataset (Wang et al., 2020b) which consists of four balanced domains: law, news, patent and subtitles. We create a new training data Dtrain with law (1.46M sentences), news (1.54M), subtitles (1.77M) train data and randomly sample 870 parallel sentences from the patent training"
2021.findings-acl.120,D18-1512,0,0.0477721,"Missing"
2021.findings-acl.120,2020.acl-main.703,1,0.885035,"token by computing the edit distance between T 0 and T . Labels of 1 refer to hallucinated words. a supervised model on this synthetic labeled data set of ((S, T 0 ), LT 0 ). The key challenge is that T 0 should be a fluent sentence that does not differ too much from T . Generation of hallucinated sentences To control this synthetic hallucination process, we build on a pre-trained denoising autoencoder, which maps a corrupted sentence back to the original text it was derived from, learning to reconstruct missing words that have been arbitrarily masked out. Specifically, we use the BART model (Lewis et al., 2020), without providing it any access to the source sentence, thereby encouraging it to insert new content as needed to ensure fluency. As shown in Fig. 2, we first apply a noising function that removes words from the original target sentence T 4 and then use a pretrained BART to generate T 0 conditioned on the noised T with beam search. T Mike T&apos; Jerry 1 goes to the bookstore on happily goes to the bookstore with his friend. 1 0 0 1 1 1 0 0 Thursday. Figure 3: An example of label assignment. Label assignments After obtaining the hallucinated sentence T 0 with BART, we need to assign appropriate l"
2021.findings-acl.120,W04-1013,0,0.0302168,"work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tai"
2021.findings-acl.120,W18-6478,0,0.0259819,"l. (2020), which injects two types of noise into the input sentences: (1) paraphrase noise created by round-trip translations, and (2) random noise from dropping, mask5 We also tried removing hallucinated target words before training. This underperformed, likely because it produces too many ungrammatical target sentences. Case Study II: Improving Corpus Filtering for Low-Resource MT High-quality parallel data is critical for training effective neural MT systems, but acquiring it can be expensive and time-consuming. Many systems instead use mined and filtered parallel data to train NMT models (Junczys-Dowmunt, 2018; Zhang et al., 2020; Koehn et al., 2019). Nonetheless, the selected parallel data can still be noisy, containing misaligned segments. In this section, we demonstrate that token-level hallucination labels can allow us to make better use of noisy data to and improve the overall translation quality. We apply the token loss truncation method proposed in §6 to the filtered parallel data and evaluate it on the WMT2019 low-resource parallel corpus filtering shared task. Experimental Setup The WMT19 shared task focuses on two low-resource languages – Nepali and Sinhala. It released a very noisy 40.6"
2021.findings-acl.120,2020.tacl-1.47,1,0.838219,"ock are our results. Bold indicates best results not using references. the patent test data. In addition, we also test the NMT models on the COVID-19 domain, sampling 100 examples from the dataset of Anastasopoulos et al. (2020). We denote this 250-sentence dataset as Deval and ask human annotators to evaluate the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the"
2021.findings-acl.120,2020.acl-main.66,0,0.0496903,"Missing"
2021.findings-acl.120,2021.ccl-1.108,0,0.0517646,"Missing"
2021.findings-acl.120,P19-3020,0,0.0349607,"Missing"
2021.findings-acl.120,W19-6623,0,0.0233024,"seline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introducti"
2021.findings-acl.120,2020.acl-main.173,0,0.114553,"ements over strong baseline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Mach"
2021.findings-acl.120,D18-1206,0,0.0429095,"hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resour"
2021.findings-acl.120,N19-4009,0,0.0150506,"e the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the noise level when generating the synthetic data. For MT, we set hm and hr to 0.6 and 0.3 respectively. For abstractive summarization, we use 0.4 and 0.2. We use beam search for decoding from BART with beam size of 4 and length penalty of 3. For MT, we first create paraphrased target sentences D0 through knowle"
2021.findings-acl.120,P02-1040,0,0.111696,"s∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez"
2021.findings-acl.120,W18-6319,0,0.0420902,"n Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020"
2021.findings-acl.120,2020.tacl-1.18,0,0.170716,"each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building mode"
2021.findings-acl.120,2020.findings-emnlp.147,0,0.0501994,"Missing"
2021.findings-acl.120,P17-1099,0,0.0248872,"pred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and add"
2021.findings-acl.120,2020.acl-main.704,0,0.0160313,"48 12.55 ST + seq loss truncation ST-R + seq loss truncation 19.91 19.37 -0.048 -0.057 8.26 10.06 ST + token loss truncation ST + decoder HS masking ST-R + token loss truncation ST-R + decoder HS masking 20.32 20.57 21.02 20.64 0.00244 -0.0001 0.043 0.0308 6.37 6.38 7.34 8.70 ing and shuffling input tokens. We also compare with the recently proposed loss truncation method (Kang and Hashimoto, 2020) that adaptively removes entire examples with high log loss, which was shown to reduce hallucinations. Results and Analysis We present the tokenized BLEU score (Papineni et al., 2002), BLEURT score (Sellam et al., 2020) and the percentage of hallucinated tokens predicted by our system in Tab. 4. We can see that ST improves over the baseline by around 3 BLEU and our best result further improves ST by 1.7 BLEU. Compared with strong baseline methods, our method not only achieves the best translation quality measured by BLEU and BLEURT but also the largest hallucination reduction. We also observe that: (1) Our method with ST alone can outperform other baseline methods, when combined with perturbed ST (noise), and using fine-grained control over the target tokens can further improve the results. (2) ST with parap"
2021.findings-acl.120,P16-1162,0,0.0953211,"Missing"
2021.findings-acl.120,2020.wmt-1.79,1,0.831521,"Missing"
2021.findings-acl.120,2011.mtsummit-papers.58,0,0.0597625,"lness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the source input. This task does not use the reference output to assess faithfulness, which offers us the ability to also apply it at run-time. Similar to the spirit of our proposed task, word-level quality estimation (Specia et al., 2018; Fonseca et al., 2019) in the MT community predicts if tokens are correctly translated based on human post-editing. However, these methods generally do not distinguish errors in terms of fluency and adequacy (Specia et al., 2011), with the exception of a subset of the WMT 2020 shared task on quality estimation (Specia et al., 2020), where different types and levels of severity of word-level errors are defined. Our proposed task specifically focuses on hallucination errors, and we define these errors in a simpler way with only binary labels, which we argue makes them simpler to use and more conducive to labeling at large scale. The proposed hallucination detection method (described below) is also applicable to the word-level quality estimation task as demonstrated in §5.4. We measure hallucination for two conditional s"
2021.findings-acl.120,N03-1033,0,0.021265,"odels for Conditional Sequence Generation Recent work (Maynez et al., 2020) has shown that pretrained models are better at generating faithful summaries as evaluated by humans. In Tab. 2, summaries generated from BERTS2S contain significantly fewer hallucinations than other model outputs. We also confirmed this trend in MT that translations from MBART contain less hallucinated content than that from TranS2S. Analysis on Hallucinated Words and their Partof-Speech Tags In Fig. 5, we present the percentage of hallucinated tokens categorized by their part-of-speech tags predicted by a POS tagger (Toutanova et al., 2003). First, we see that for both MT and summarization datasets, nouns are the most hallucinated words. In abstractive summarization, verbs also account for a certain number of hallucinations. Second, our model predicted hallucinated words match well with gold annotations on the distributions of POS tags. We also compare the percentage of hallucinations within each POS tag in Appendix E.2. In addition, we provide more 1398 Normalized Hallucination Ratio Normalized Hallucination Ratio MT 0.6 Gold Our predictions 0.4 0.2 0.0 NN others JJ VB IN POS tag CD RB SYM PRP XSum 0.5 Gold Our predictions 0.4"
2021.findings-acl.120,N03-1000,0,0.110311,"Missing"
2021.findings-acl.120,2020.acl-main.450,0,0.299588,"t standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinate"
2021.findings-acl.120,2020.acl-main.326,0,0.261799,"rce meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational"
2021.findings-acl.120,2020.acl-main.756,0,0.0338433,"Missing"
2021.findings-acl.120,2020.acl-main.552,0,0.0260132,"Missing"
2021.findings-acl.120,N19-1161,1,0.858552,"Missing"
2021.findings-acl.366,2020.acl-main.703,1,0.930521,"the task, and facilitate a novel method to evaluate explanation faithfulness. 1 ii) The geese prefer to nest in the fields rather than the forests because in the predators are more hidden. Contrastive Expl. - Forests are denser than fields Table 1: Examples of Winograd Schema Instances where the correct and incorrect answer choices are highlighted in blue and red respectively. Choices are contrasted along attributes like taste (for i) and density of vegetation (for ii) by humans to explain why they prefer some answer choice. Introduction Pretrained Language Models (PLMs) (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019; Brown et al., 2020) have been shown to encode substantial amounts of knowledge in their parameters (Petroni et al., 2019; Talmor et al., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcom"
2021.findings-acl.366,2020.emnlp-main.153,1,0.893158,"Missing"
2021.findings-acl.366,D19-1250,0,0.0567481,"Missing"
2021.findings-acl.366,S18-2023,0,0.0649282,"Missing"
2021.findings-acl.366,P19-1487,0,0.382699,"l., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcome this limitation, recent work has shown that PLMs can explain themselves by generating free-form natural language explanations of their reasoning patterns (Rajani et al., 2019a; Camburu et al., 2018; Narang et al., 2020). However, the space of possible free-form explanations is incredibly large, inherently ambiguous, and difficult to annotate or evaluate (Wiegreffe et al., 2020; Latcinnik and Berant, 2020). Furthermore, quantifying the model’s dependence on free-form explanations is also challenging (Camburu et al., 2020). We address these challenges by proposing an unsupervised method that uses contrastive prompts, which require the model to explicitly contrast different possible answers in its explanation (Table 1). Our approach is based on a key observation: Man"
2021.findings-acl.366,2020.emnlp-main.437,0,0.0379399,"Missing"
2021.findings-acl.366,2021.findings-acl.336,0,0.0392813,"Missing"
2021.findings-acl.366,N18-2002,0,0.0642609,"Missing"
2021.findings-acl.366,2020.emnlp-main.346,0,0.0324025,"., 2020). Most recently, Latcinnik and Berant (2020) use an unsupervised approach to generate free-form explanations as sequences of tokens that are not well-formed sentences. In contrast, our method uses specialized prompts to generate well-formed human-interpretable explanations without any additional supervision. Specialized prompts have been shown useful for extracting knowledge from PLMs in a targeted manner (Petroni et al., 2020; Richardson and Sabharwal, 2020; Talmor et al., 2020; Donahue et al., 2020; Lin et al., 2019) and improving performance on downstream tasks (Brown et al., 2020; Shin et al., 2020). Most relevant to our work is the self-talk model of Shwartz et al. (2020), an unsupervised approach using a fixed set of clarification questions as prompts to elicit knowledge from PLMs for commonsense reasoning tasks. Our work differs by focusing specifically on contrastive PLM prompts, which we find further improve performance by eliciting explanations which are highly relevant to the classification decision (Section 6). Our approach to contrastive reasoning is also closely related to counterfactuals, which can be used to give contrastive explanations, i.e., answers to “Why P rather than Q"
2021.findings-acl.366,2020.emnlp-main.373,0,0.0483818,"Missing"
2021.findings-acl.366,2020.tacl-1.48,0,0.184828,"edators are more hidden. Contrastive Expl. - Forests are denser than fields Table 1: Examples of Winograd Schema Instances where the correct and incorrect answer choices are highlighted in blue and red respectively. Choices are contrasted along attributes like taste (for i) and density of vegetation (for ii) by humans to explain why they prefer some answer choice. Introduction Pretrained Language Models (PLMs) (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019; Brown et al., 2020) have been shown to encode substantial amounts of knowledge in their parameters (Petroni et al., 2019; Talmor et al., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcome this limitation, recent work has shown that PLMs can explain themselves by generating free-form natural language explanations of their reasoning patterns (Rajani e"
2021.findings-acl.366,N19-1421,0,0.0228381,"rmation required for commonsense tasks. Even if the full model does not always use the explanations, these evaluations show that our contrastive explanations contain rich task-relevant knowledge, and suggest that future work might focus on how to better make use of this signal. 6.4 Generalizability of Prompts The set of contrastive prompts used in our framework are curated from an in-house analysis of training instances from Winogrande and PIQA datasets. To determine the generalizability of these prompts for other commonsense reasoning tasks, we also experiment with the CommonsenseQA dataset (Talmor et al., 2019), which consists of multiple-choice questions created over ConceptNet – “Where on a river can you hold a cup upright to catch water on a sunny day? a) waterfall, b) bridge, c) valley, d) pebble, e) mountain”. Since there are more than two answer choices to contrast, we convert each instance into 10 pairwise (binary) classification instances. Contrastive explanations are generated for each pairwise decision in the zero-shot setting, similar to Winograd and PIQA datasets. To choose the final answer, we consider two inference procedures: (a) Vote: The answer that receives the maximum number of vo"
2021.findings-acl.366,2020.findings-emnlp.90,0,0.051371,"Missing"
2021.findings-acl.366,2020.acl-main.508,0,0.112732,"ess.1 2 Related Work Models that rationalize their decisions by extracting a contiguous subsequence of the input as an explanation (Lei et al., 2016; DeYoung et al., 2020; Paranjape et al., 2020) are inadequate in explaining 1 Code is available at https://github.com/ bhargaviparanjape/RAG-X commonsense reasoning tasks that require knowledge that is implicit in the input. Such tasks necessitate PLMs to rely on embedded parametric knowledge. Recent work use free-form textual explanations to generate explanations for commonsense reasoning tasks like SNLI (Camburu et al., 2018), Winograd Schemas (Zhang et al., 2020) and CommonsenseQA (Rajani et al., 2019b) through explicit human supervision, which are inherently ambiguous, incomplete and consequently, expensive to collect and evaluate on (Camburu et al., 2019b,a; DeYoung et al., 2020). Most recently, Latcinnik and Berant (2020) use an unsupervised approach to generate free-form explanations as sequences of tokens that are not well-formed sentences. In contrast, our method uses specialized prompts to generate well-formed human-interpretable explanations without any additional supervision. Specialized prompts have been shown useful for extracting knowledge"
2021.findings-acl.370,W19-4828,0,0.0187845,"and n are lengths for video and text, respectively) by feeding video/text separately to BERT but we adopt squared masks for simplicity. 4230 Figure 3: Fine-tuning of downstream tasks: we adopt different types of attention masks for BERT to accommodate downstream tasks that require different modalities: in each box, the upper sub-figure indicates a forward computation; the lower sub-figure indicates squared self-attention mask, where tokens from each row have a weighted sum of columns that are not in white colors. be used by video and text, respectively, aiming to learn sequence-level features(Clark et al., 2019). The [CLS] is disabled as no need to learn features across video and text. After forwarding, all hidden states of video and text tokens are average pooled, respectively. Then we use a contrastive loss on text-video similarity to discriminate a ground-truth video clip from other video clips in the same batch for a given text clip. During the evaluation, to ensure video and text are isolated (to avoid leaking ground-truth of a similar pair), we split text and video and forward them separately. We report an ablation study in Section 5 showing that the MMM introduced in the previous section is cr"
2021.findings-acl.370,N19-1423,0,0.049325,"aining to merge two streams of data to cover a wide range of downstream tasks (such as retrieval or text generation). Our video-language model (VLM) (lower figure) uses a single BERT encoder for task-agnostic pre-training (e.g. only masking tokens, no matching or alignment for specific end tasks) in a joint feature space, while still covering a wide range of tasks (see Figure 3). Introduction We study the challenge of achieving task-agnostic pre-training for multimodal video understanding, building on recent unimodal approaches such as pretrained language models for text (Peters et al., 2018; Devlin et al., 2019). Although certain language models are near task-agnostic (Devlin et al., 2019; Lewis et al., 2020) on NLP tasks, being taskagnostic on multi-modal tasks are more challenging due to cross-modal tasks such as text-video retrieval. Existing video-and-language pre-trainings are task-specific, which adopt either (1) a crossmodal single encoder (Sun et al., 2019b,a; Zhu and Yang, 2020) favoring tasks that require cross-modal reasoning (e.g. video captioning), or (2) multiple unimodal encoders/decoders (Miech et al., 2019, 2020; Li et al., 2020b; Luo et al., 2020; Korbar et al., 2020) combining spec"
2021.findings-acl.370,2021.naacl-main.195,1,0.421782,"eo captioning during fine-tuning. (Zhu and Yang, 2020) uses [CLS] token for pairwise metric-learning based retrieval (which is an easier problem but requires a quadratic number of examples and is 50 times slower as reported in (Luo et al., 2020)). Meanwhile, many existing approaches adopt or add task-specific pre-training to accommodate retrieval and video captioning tasks (e.g. twostream encoders (video and text separately) and text decoders). For example, (Miech et al., 2019, 2020; Rouditchenko et al., 2020; Ging et al., 2020; Gabeur et al., 2020; Alayrac et al., 2020; Patrick et al., 2021; Huang et al., 2021) adopts a retrieval task for pre-training. CBT (Sun et al., 2019a), HERO (Li et al., 2020b), VideoAsMT (Korbar et al., 2020) and UniVL (Luo et al., 2020) adopt multi-task learning (MTL) to learn retrieval tasks on video and text encoders. HERO (Li et al., 2020b) and UniVL (Luo et al., 2020) adopts another cross-encoder to further learn the fusion of different modality. UniVL (Luo et al., 2020) and VideoAsMT (Korbar et al., 2020) add another text decoder for video captioning. Compared with the single-stream input in the shared encoder approach, two-stream encoders typically come with a complex"
2021.findings-acl.370,2020.acl-main.703,1,0.924034,"text generation). Our video-language model (VLM) (lower figure) uses a single BERT encoder for task-agnostic pre-training (e.g. only masking tokens, no matching or alignment for specific end tasks) in a joint feature space, while still covering a wide range of tasks (see Figure 3). Introduction We study the challenge of achieving task-agnostic pre-training for multimodal video understanding, building on recent unimodal approaches such as pretrained language models for text (Peters et al., 2018; Devlin et al., 2019). Although certain language models are near task-agnostic (Devlin et al., 2019; Lewis et al., 2020) on NLP tasks, being taskagnostic on multi-modal tasks are more challenging due to cross-modal tasks such as text-video retrieval. Existing video-and-language pre-trainings are task-specific, which adopt either (1) a crossmodal single encoder (Sun et al., 2019b,a; Zhu and Yang, 2020) favoring tasks that require cross-modal reasoning (e.g. video captioning), or (2) multiple unimodal encoders/decoders (Miech et al., 2019, 2020; Li et al., 2020b; Luo et al., 2020; Korbar et al., 2020) combining specific tasks that require separately embedding each modality (e.g. video 1 Code will be released unde"
2021.findings-acl.370,2020.emnlp-main.161,0,0.199619,"ned language models for text (Peters et al., 2018; Devlin et al., 2019). Although certain language models are near task-agnostic (Devlin et al., 2019; Lewis et al., 2020) on NLP tasks, being taskagnostic on multi-modal tasks are more challenging due to cross-modal tasks such as text-video retrieval. Existing video-and-language pre-trainings are task-specific, which adopt either (1) a crossmodal single encoder (Sun et al., 2019b,a; Zhu and Yang, 2020) favoring tasks that require cross-modal reasoning (e.g. video captioning), or (2) multiple unimodal encoders/decoders (Miech et al., 2019, 2020; Li et al., 2020b; Luo et al., 2020; Korbar et al., 2020) combining specific tasks that require separately embedding each modality (e.g. video 1 Code will be released under fairseq. retrieval). We instead show that it is possible to pretrain a task-agnostic model called video-language model (VLM) that can accept text, video, or both as input. As shown in Figure 1, this task-agnostic single encoder approach has several advantages: (1) it reduces the complexity of pre-training with multiple losses and models (e.g. Luo et al. (2020)), and (2) it holds less assumption on being close to end tasks as in retrieval-b"
2021.findings-acl.370,2021.ccl-1.108,0,0.0552081,"Missing"
2021.findings-acl.370,N18-1202,1,0.444827,"iple task-specific training to merge two streams of data to cover a wide range of downstream tasks (such as retrieval or text generation). Our video-language model (VLM) (lower figure) uses a single BERT encoder for task-agnostic pre-training (e.g. only masking tokens, no matching or alignment for specific end tasks) in a joint feature space, while still covering a wide range of tasks (see Figure 3). Introduction We study the challenge of achieving task-agnostic pre-training for multimodal video understanding, building on recent unimodal approaches such as pretrained language models for text (Peters et al., 2018; Devlin et al., 2019). Although certain language models are near task-agnostic (Devlin et al., 2019; Lewis et al., 2020) on NLP tasks, being taskagnostic on multi-modal tasks are more challenging due to cross-modal tasks such as text-video retrieval. Existing video-and-language pre-trainings are task-specific, which adopt either (1) a crossmodal single encoder (Sun et al., 2019b,a; Zhu and Yang, 2020) favoring tasks that require cross-modal reasoning (e.g. video captioning), or (2) multiple unimodal encoders/decoders (Miech et al., 2019, 2020; Li et al., 2020b; Luo et al., 2020; Korbar et al."
2021.findings-acl.370,2020.tacl-1.18,0,0.0349841,"allenge of using a single encoder is how to apply generative tasks (such as video captioning) without pre-training an explicit decoder. We observe that a transformer decoder (Vaswani et al., 2017) has the following major differences from an encoder: (1) an auto-regressive loss that does not allow a text token to see future tokens; (2) a prediction head to generate texts. To resolve (1), one can easily fine-tune the text segment of VLM as auto-regressive loss by passing in shifted tokens and a lower-triangle attention mask to the text segment, as shown in Figure 3. To resolve (2), inspired by (Rothe et al., 2020; Zhou et al., 2020) that uses BERT as a decoder, one can re-use language model heads as prediction heads for generation. Note that this setting has less architecture design than a standard transformer decoder (e.g. no explicit self-attention on text or cross-attention on video). The implicit text decoder inside BERT shares self-attention with the video encoder so to save the total number of parameters. 5 Experiment 5.1 5.1.1 Dataset Pre-training We adopt the Howto100M dataset (Miech et al., 2019) for pre-training, which contains instructional videos originally from YouTube via searching keywo"
2021.findings-acl.370,D19-1514,0,0.0243606,"s on video and text encoders. HERO (Li et al., 2020b) and UniVL (Luo et al., 2020) adopts another cross-encoder to further learn the fusion of different modality. UniVL (Luo et al., 2020) and VideoAsMT (Korbar et al., 2020) add another text decoder for video captioning. Compared with the single-stream input in the shared encoder approach, two-stream encoders typically come with a complex architecture and proxy tasks to cover more end tasks. To the best of our knowledge, none of the existing works target task-agnostic pre-training. 2.1 Image-Text Pre-training ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019) adopt two transformers for image and text encoding separately. VisualBERT (Li et al., 2019), Unicoder-VL (Li et al., 2020a), VL-BERT (Su et al., 2020), UNITER (Chen et al., 2020), Unified VLP (Zhou et al., 2020) use one shared BERT model. These models employ MLM and pairwise image-text matching as pretraining tasks which are effective for downstream multimodal tasks. Our fine-tuning for video captioning is inspired by Unified VLP (Zhou et al., 2020) that adopts attention masks and language model heads of BERT for image-captioning. 2.2 Video-Text Pre-training VideoBERT (Sun et al., 2019b) and"
2021.findings-acl.389,P98-1013,0,0.467539,"cluster the arguments for each predicate into sets corresponding to semantic roles. We may then compare these clusters to gold labels using clustering metrics (Section 4.3). 2 Previous work (Lang and Lapata, 2010) assumes a syntactic dependency tree and marks each argument by its syntactic head, which allows for features based on argument lemmas and dependency paths. We instead assume sets of argument spans, but no syntax tree; this allows for features based on spans (such as QA-SRL questions). Both approaches are ways of featurizing the same gold arguments. 3 Some ontologies, like FrameNet (Baker et al., 1998), define frames that span multiple lemmas (e.g., buy and sell share a Commercial Transaction frame), whereas others like PropBank (Palmer et al., 2005) use frames which are specific to each lemma, denoting something closer to word sense. In our case, assuming a single frame per lemma simplifies modeling and allows us to compare to previous work. However, modeling predicate sense is an important problem for future work, as we will suggest in Section 6.3. 4428 3 Modeling Our model treats each argument x as a set of counts of QA-SRL questions,4 denoted φ(x). We produce these counts from a trained"
2021.findings-acl.389,W13-2322,0,0.0140849,"ething given something? .07 Table 1: Roles for give produced by our final model. Core arguments are captured almost perfectly, exhibiting both passive and dative alternations. Introduction Semantic role labeling (SRL) requires extracting propositional predicate-argument structure from language, i.e., who is doing what to whom. Applications of SRL include information extraction (Christensen et al., 2011), machine reading (Wang et al., 2015), and model analysis (Tenney et al., 2019; Kuznetsov and Gurevych, 2020), and semantic roles form the backbone of many more general meaning representations (Banarescu et al., 2013; Abend and Rappoport, 2013). The primary challenge, and promise, for SRL systems is to distill syntactically variable surface structures into semantic predicate-argument structures from an ontology (Palmer et al., 2005; Baker 1 Labels Code, models, and a web interface to explore the results are available at https://github.com/ julianmichael/qasrl-roles. et al., 1998). However, ontologies and their associated training data require time and expertise to annotate and do not readily generalize to new domains, limiting their broad-coverage applicability. Prior work towards mitigating this problem"
2021.findings-acl.389,W02-1502,0,0.1337,"ms that have separate layers of functional structure, like Combinatory Categorial Grammar (Steedman, 1996, 2000), Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994), or Lexical Functional Grammar (Bresnan et al., 2015) — but without grammar engineering or expert data annotation. One challenge is that QA-SRL is currently only defined for English. Future work may benefit from our lessons about the utility of surrogate syntax when designing similar annotation methodologies for other languages; combining this with insights from existing work on grammar development for diverse languages (Bender et al., 2002) may be key. While formal ontologies of semantic roles and syntax are difficult to formulate and scale, our results show how it may be comparatively feasible to formulate, scale, and build robust models for the phenomena that such ontologies are meant to explain. QA-SRL exhibits enough of these phenomena that a relatively simple model over it (the Hard Unigram Mixture in Section 3) yields stateof-the-art induced semantic roles which are interpretable and linguistically meaningful. This suggests that identifying and gathering supervision for more phenomena (e.g., those related to word sense or"
2021.findings-acl.389,2020.emnlp-main.389,0,0.0260815,"n. QA-SRL exhibits enough of these phenomena that a relatively simple model over it (the Hard Unigram Mixture in Section 3) yields stateof-the-art induced semantic roles which are interpretable and linguistically meaningful. This suggests that identifying and gathering supervision for more phenomena (e.g., those related to word sense or modifier semantics) in a relatively theoryagnostic way, then building models grounded in linguistic theory, may be a promising avenue for future work. This general approach has recently been applied to syntax as well, for example leveraging constituency tests (Cao et al., 2020) and naturallyoccurring bracketings (Shi et al., 2021). The fact that discrete structures can be reliably derived from ontology-free annotation schemes like QA-SRL can potentially inform future efforts to construct large-scale ontologies of semantic structure. QA-SRL has the further benefit over traditional SRL of including a broader scope of implicit arguments than those addressed by supervised systems, as shown by Roit et al. (2020). Taken together, our results suggest that with the right kind of annotation scheme, it should be possible to construct rich semantic ontologies in new domains, w"
2021.findings-acl.389,W12-1914,0,0.0730657,"Missing"
2021.findings-acl.389,P89-1010,0,0.583774,"Missing"
2021.findings-acl.389,N19-1423,0,0.0400764,"one with what or something. Generating Question Counts Let p denote a predicate, s denote a span, and q denote a simplified QA-SRL question. To generate our question count vectors φ, we reproduce the QA-SRL question generator of FitzGerald et al. (2018), which generates a distribution P(q |p, s) over QA-SRL questions conditioned on a predicate p and answer span s in a sentence. This model uses a BiLSTM encoder, concatenating the output representations of span endpoints and feeding them into a custom LSTM decoder which models the QA-SRL slot values in sequence. We modify the model to use BERT (Devlin et al., 2019) features as input embeddings for the BiLSTM (details in Appendix A). Recall from Section 2 that an argument x consists of a set of spans from its sentence. We gen|q| erate question counts φ(x) ∈ R≥0 by taking the mean 1 X φ(x) = P(q |p, s), |x |s∈x where R≥0 denotes the nonnegative real numbers and |q |is the number of possible simplified QA4 As of now, this model only works for English, as QA-SRL is only defined and annotated in English. SRL questions. Since |q |is large, to make this tractable we approximate P(q |p, s) with beam search, using a sparse representation and assigning counts of"
2021.findings-acl.389,P18-1191,1,0.801213,"erage applicability. Prior work towards mitigating this problem includes unsupervised induction of semantic roles from syntactic representations (Lang and Lapata, 2010). However, the need for formal syntactic supervision retains some of the annotation and generalization difficulties of supervised SRL, and it has proven difficult to do much better than a simple syntactic baseline (Lang and Lapata, 2011). An alternative is to use an ontology-free annotation scheme like QA-SRL (He et al., 2015), which represents roles with natural language questions. While QA-SRL can be annotated at large scale (FitzGerald et al., 2018), many different QA-SRL questions may correspond to the same role, making it more 4427 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4427–4442 August 1–6, 2021. ©2021 Association for Computational Linguistics The plane was diverting around weather formations over the Java Sea when contact with air traffic control (ATC) in Jakarta was lost. wh aux What What What What Where was was was was was subj verb something being diverted diverting being diverted lost lost obj prep around obj2 ? Answer ? ? ? ? ? weather formations The plane The plane contact with air tra"
2021.findings-acl.389,P17-1044,1,0.827011,"subject and active object questions. This happens because the Hard Unigram Mixture objective clusters together distributions whose uncertainty is spread over the same set of elements, which here correspond to syntactic alternations. As an example, Table 1 shows the induced clusters for give, which exhibit both passive and dative alternations; give gained 31 F1 on A1 in HUM-QQ. 6.2 tinguished from modifiers, reflecting the difficulty of the argument–adjunct distinction for these arguments, which often have similar meanings to modifiers and form a significant error case for supervised labelers (He et al., 2017). AM-ADV tends to be confused with other modifier roles, which reflects its definition in the PropBank guidelines as a sort of “catch-all” role for meanings not captured in the other modifiers (Babko-Malaya, 2005). Finally, AM-CAU (cause) and AM-PNC (purpose, not cause) tend to be confused with each other, since they both elicit why questions. Mismatched Modifiers HUM-QQ struggles to identify PropBank modifier roles, and it has room for improvement on trailing arguments like A2 and A3. In QA-SRL, the semantics of these roles are primarily expressed by the initial wh-word, such as when, where,"
2021.findings-acl.389,D15-1076,1,0.903363,"ining data require time and expertise to annotate and do not readily generalize to new domains, limiting their broad-coverage applicability. Prior work towards mitigating this problem includes unsupervised induction of semantic roles from syntactic representations (Lang and Lapata, 2010). However, the need for formal syntactic supervision retains some of the annotation and generalization difficulties of supervised SRL, and it has proven difficult to do much better than a simple syntactic baseline (Lang and Lapata, 2011). An alternative is to use an ontology-free annotation scheme like QA-SRL (He et al., 2015), which represents roles with natural language questions. While QA-SRL can be annotated at large scale (FitzGerald et al., 2018), many different QA-SRL questions may correspond to the same role, making it more 4427 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4427–4442 August 1–6, 2021. ©2021 Association for Computational Linguistics The plane was diverting around weather formations over the Java Sea when contact with air traffic control (ATC) in Jakarta was lost. wh aux What What What What Where was was was was was subj verb something being diverted divert"
2021.findings-acl.389,nivre-etal-2006-maltparser,0,0.322089,"Missing"
2021.findings-acl.389,J05-1004,0,0.701484,"ires extracting propositional predicate-argument structure from language, i.e., who is doing what to whom. Applications of SRL include information extraction (Christensen et al., 2011), machine reading (Wang et al., 2015), and model analysis (Tenney et al., 2019; Kuznetsov and Gurevych, 2020), and semantic roles form the backbone of many more general meaning representations (Banarescu et al., 2013; Abend and Rappoport, 2013). The primary challenge, and promise, for SRL systems is to distill syntactically variable surface structures into semantic predicate-argument structures from an ontology (Palmer et al., 2005; Baker 1 Labels Code, models, and a web interface to explore the results are available at https://github.com/ julianmichael/qasrl-roles. et al., 1998). However, ontologies and their associated training data require time and expertise to annotate and do not readily generalize to new domains, limiting their broad-coverage applicability. Prior work towards mitigating this problem includes unsupervised induction of semantic roles from syntactic representations (Lang and Lapata, 2010). However, the need for formal syntactic supervision retains some of the annotation and generalization difficulties"
2021.findings-acl.389,D14-1162,0,0.0856515,"Missing"
2021.findings-acl.389,P93-1024,0,0.577658,"Missing"
2021.findings-acl.389,N18-1202,1,0.706077,"Missing"
2021.findings-acl.389,2020.acl-main.626,1,0.770765,"c theory, may be a promising avenue for future work. This general approach has recently been applied to syntax as well, for example leveraging constituency tests (Cao et al., 2020) and naturallyoccurring bracketings (Shi et al., 2021). The fact that discrete structures can be reliably derived from ontology-free annotation schemes like QA-SRL can potentially inform future efforts to construct large-scale ontologies of semantic structure. QA-SRL has the further benefit over traditional SRL of including a broader scope of implicit arguments than those addressed by supervised systems, as shown by Roit et al. (2020). Taken together, our results suggest that with the right kind of annotation scheme, it should be possible to construct rich semantic ontologies in new domains, without expert curation and in a data-driven, linguistically motivated way. Acknowledgements Thanks to the anonymous reviewers and Victor Zhong for their helpful comments. 4435 References Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228–238, Sofia, Bulgaria. Association for C"
2021.findings-acl.389,D07-1043,0,0.0499766,"ted independently for each verb and averaged, weighing each verb by its number of argument instances. The harmonic mean of the final results is reported as an F1 score. B3 For deeper analysis, we use the B 3 (B-cubed) family of clustering metrics (Bagga and Baldwin, 1998). B 3 precision and recall are the precision and recall of each point’s predicted cluster against its gold cluster, averaging over points. In comparison to purity and collocation, these metrics are tougher and more discriminative between clusterings, respecting important constraints like the cluster completeness constraint of Rosenberg and Hirschberg (2007), among others (Amigó et al., 9 Full lexica for these rules are provided in Appendix C. F1 ∆F1 79.6 82.4 82.2 85.2 83.0 81.8 0.0 +2.8 +2.6 +5.6 +3.4 +2.2 78.8 79.2 -0.8 -0.4 82.1 80.8 87.1 +2.5 +1.2 +7.5 CO Gold Syntax Passive to Active Conversion We also propose a syntactic rule that applies only to S YNT F, where we the transform the dependencies as follows: • Passive voice can be detected when the predicate verb is in past participle form (part-ofspeech tag VBN) and its syntactic parent is a be-verb (part of speech VC, lemma “be”). In these cases, we change the syntactic label of any SBJ de"
2021.findings-acl.389,2021.naacl-main.234,0,0.0855334,"Missing"
D07-1071,C04-1180,0,0.0148097,"ls hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sen"
D07-1071,P98-1025,0,0.0289435,"egrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sentences from childdirected speech, which differ significantly from the natural language interface queries we consider. Finally, there is work on"
D07-1071,J83-3001,0,0.468325,"order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sentences from childdirected speech, which differ significantly from the natural language interface queries we consider. Finally, there is work on manually developing parsing techniques to improve robustness (Carbonell and Hayes, 1983; Seneff, 1992). In contrast, our approach is integrated into a learning framework. 6 Experiments The main focus of our experiments is on the ATIS travel planning domain. For development, we used 4978 sentences, split into a training set of 4500 examples, and a development set of 478 examples. For test, we used the ATIS NOV93 test set which contains 448 examples. To create the annotations, we created a script that maps the original SQL annotations provided with the data to lambda-calculus expressions. He and Young (2006) previously reported results on the ATIS domain, using a learning approach"
D07-1071,W03-1013,0,0.00950692,"emoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Sisk"
D07-1071,W02-1001,1,0.0960636,", Λ)) as its output. The algorithm is online, in that it visits each example in turn, and updates both w and Λ if necessary. In Step 1 on each example, the input xi is parsed. If it is parsed correctly, the algorithm immediately moves to the next example. In Step 2, the algorithm temporarily introduces all lexical entries seen in GENLEX(xi , zi ), and finds the highest scoring parse that leads to the correct semantics zi . A small subset of GENLEX(xi , zi )—namely, only those lexical entries that are contained in the highest scoring parse—are added to Λ. In Step 3, a simple perceptron update (Collins, 2002) is performed. The hypothesis is parsed again with the new lexicon, and an update to the parameters w is made if the resulting parse does not have the correct logical form. This algorithm differs from the approach in ZC05 in a couple of important respects. First, the ZC05 algorithm performed learning of the lexicon Λ at each iteration in a batch method, requiring a pass over the entire training set. The new algorithm is fully online, learning both Λ and w in an example-by-example fashion. This has important consequences for the efficiency of the algorithm. Second, the parameter estimation meth"
D07-1071,H94-1010,0,0.484357,"at it allows a system to handle quite complex semantic effects, such as coordination or scoping phenomena. In particular, it allows us to leverage the considerable body of work on semantics within these formalisms, for example see Carpenter (1997). However, a grammar based on a formalism such as CCG can be somewhat rigid, and this can cause problems when a system is faced with spontaneous, unedited natural language input, as is commonly seen in natural language interface applications. For example, consider the sentences shown in figure 1, which were taken from the ATIS travel-planning domain (Dahl et al., 1994). These sentences exhibit characteristics which present significant challenges to the approach of ZC05. For ex678 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 678–687, Prague, June 2007. 2007 Association for Computational Linguistics ample, the sentences have quite flexible word order, and include telegraphic language where some words are effectively omitted. In this paper we describe a learning algorithm that retains the advantages of using a detailed grammar, but is highly effective in dealing"
D07-1071,P06-2034,0,0.0481911,"ns. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabili"
D07-1071,P99-1069,0,0.0363867,"linguistic phenomena. As we will see, they also turn out to be useful when modeling constructions with relaxed word order, as seen frequently in domains such as ATIS. In addition to the application and composition rules, we will also make use of type raising and coordination combinators. A full description of these combinators goes beyond the scope of this paper. Steedman (1996; 2000) presents a detailed description of CCG. 2.3 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows. Our models are similar to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). We will write x to denote a sentence, and y to denote a CCG parse for a sentence. We use GEN(x; Λ) to refer to all possible CCG parses for x under some CCG lexicon Λ. We will define f(x, y) ∈ Rd to be a d-dimensional feature–vector that represents a parse tree y paired with an input sentence x. In principle, f could include features that are sensitive to arbitrary substructures within the pair (x, y). We will define w ∈ Rd to be a parameter vector. The optimal parse for a sentence x under parameters w and lexicon Λ is then defined a"
D07-1071,P06-1115,0,0.736567,", would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to"
D07-1071,P06-1096,0,0.0126236,"Missing"
D07-1071,P96-1008,0,0.0448016,"emantic representations. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm t"
D07-1071,P06-2080,0,0.0657208,"Missing"
D07-1071,W00-1317,0,0.0402486,"april twenty second dallas to washington the latest nighttime departure one way argmax(λx.f light(x) ∧ f rom(x, dallas)∧ to(x, washington) ∧ month(x, april)∧ day number(x, 22) ∧ during(x, night)∧ one way(x), λy.depart time(y)) Figure 1: Three sentences from the ATIS domain. other derivations. The output from the learning algorithm is a combinatory categorial grammar (CCG), together with parameters that define a log-linear distribution over parses under the grammar. Experiments show that the approach gives high accuracy on two database-query problems, introduced by Zelle and Mooney (1996) and Tang and Mooney (2000). Introduction Recent work (Mooney, 2007; He and Young, 2006; Zettlemoyer and Collins, 2005) has developed learning algorithms for the problem of mapping sentences to underlying semantic representations. In one such approach (Zettlemoyer and Collins, 2005) (ZC05), the input to the learning algorithm is a training set consisting of sentences paired with lambda-calculus expressions. For instance, the training data might contain the following example: Sentence: list flights to boston Logical Form: λx.f light(x) ∧ to(x, boston) In this case the lambda-calculus expression denotes the set of all fli"
D07-1071,W04-3201,0,0.0334154,"be useful when modeling constructions with relaxed word order, as seen frequently in domains such as ATIS. In addition to the application and composition rules, we will also make use of type raising and coordination combinators. A full description of these combinators goes beyond the scope of this paper. Steedman (1996; 2000) presents a detailed description of CCG. 2.3 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows. Our models are similar to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). We will write x to denote a sentence, and y to denote a CCG parse for a sentence. We use GEN(x; Λ) to refer to all possible CCG parses for x under some CCG lexicon Λ. We will define f(x, y) ∈ Rd to be a d-dimensional feature–vector that represents a parse tree y paired with an input sentence x. In principle, f could include features that are sensitive to arbitrary substructures within the pair (x, y). We will define w ∈ Rd to be a parameter vector. The optimal parse for a sentence x under parameters w and lexicon Λ is then defined as y ∗ (x) = arg max w · f(x, y) . y∈GEN(x;Λ) 680 Assuming su"
D07-1071,W99-0909,0,0.0931974,"emoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavi"
D07-1071,N06-1056,0,0.726929,"on learning to map sentences to underlying semantic representations. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He"
D07-1071,C98-1025,0,\N,Missing
D08-1082,J93-2003,0,0.0329288,"d. 2 S TATE : exclude (S TATE S TATE) S TATE : state (all) S TATE : loc 1 (R IVER) R IVER : river (all) Figure 1: An example MR structure Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. S ILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work ha"
D08-1082,J05-1003,0,0.0283535,"erceptron algorithms usually optimize the accuracy measure, we extend it to allow optimization of the F-measure by introducing an explicit separating plane on the feature space that rejects certain predictions even when they score highest. The idea is to find a threshold b after w is learned, such that a prediction with score below b gets rejected. We pick the threshold that leads to the optimal F-measure when applied to the training set. 8.2 Features We list in Table 2 the set of features we used. Examples are given based on the hybrid tree in Figure 789 3. Some of the them are adapted from (Collins and Koo, 2005) for a natural language parsing task. Features 1-5 are indicator functions (i.e., it takes value 1 if a certain combination as the ones listed in Table 2 is present, 0 otherwise), while feature 6 is real valued. Features that do not appear more than once in the training set are discarded. 9 Evaluation Our evaluations were performed on two corpora, G EOQUERY and ROBOCUP. The G EOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in"
D08-1082,W02-1001,0,0.00973314,"el’s joint probability Example : S TATE : loc 1(R IVER ) → have R IVER : S TATE : loc 1(R IVER ) → hhave, R IVER : river(all)i : S TATE : exclude(S TATE S TATE ) → rivers : S TATE : loc 1(R IVER ) → rivers : hR IVER : river(all), S TATE : loc 1(R IVER )i → rivers   log b P(w, m, T ) . f1 f2 f3 f4 f5 Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if the combination is present, and 0 otherwise. Feature 6 takes real values. 8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking. The detailed algorithm can be found in (Collins, 2002). In this section, we extend the conventional averaged perceptron by introducing an explicit separating plane on the feature space. Our reranking approach requires three components during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each training instance; and a feature function Φ that defines a mapping from a hybrid tree to a feature vector."
D08-1082,J03-4003,0,0.0881966,"well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process. This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. This implicit grammar representation leads to flexible learned models that generalize well. In practice, we observe that it can correctly parse a wider range of test examples than previous approaches. The generative model is learned from data that consists of sentences paired with their meaning representations. However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees. This creates a challenging, hidde"
D08-1082,W05-0602,0,0.281678,"e translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR s"
D08-1082,P06-2034,0,0.375014,"ues. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider th"
D08-1082,P06-1115,0,0.830701,"ranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit gr"
D08-1082,N06-1056,0,0.872814,"ith a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not ma"
D08-1082,P07-1121,0,0.687661,"s a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hiera"
D08-1082,P01-1067,0,0.0121173,"accessible. In other words, there is a single deterministic derivation associated with each training instance. Therefore model parameters can be directly estimated from the training corpus by counting. However, in our task, the correct correspondence between NL words and MR structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi , wi i as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv , wv i as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspon"
D08-1082,D07-1071,1,0.4397,"on structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree 784 structure, as shown in Figure 1. Following an"
D08-1082,P02-1062,0,\N,Missing
D10-1119,C04-1180,1,0.305945,"(Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a"
D10-1119,W03-1013,0,0.0248429,"ntries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle & Mooney, 1996"
D10-1119,J07-4004,0,0.145444,"r S|N P/N P λyλx.state(x) ∧ next to(x, y) &gt; S|N P λx.state(x) ∧ next to(x, tex) &gt; texas S|N P/(S|N P ) SN P/N P λf λx.state(x)∧f (x) λyλx.next to(x, y) S λx.state(x) ∧ next to(x, tex) NP tex &gt;B &gt; &gt; Figure 1: Two examples of CCG parses with different logical form representations. defined as: eθ·φ(x,y,z) θ·φ(x,y 0 ,z 0 ) (y 0 ,z 0 ) e P (y, z|x; θ, Λ) = P (1) Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in Λ are used in the parse y. For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below. The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters θ and lexicon Λ are known: f (x) = arg max p(z|x; θ, Λ) z (2) where the probability of the logical form is found by summing over all parses that produce it: X p(z|x; θ, Λ) = p(y, z|x; θ, Λ) (3) y In this approach the distribution over parse trees y is modeled as a hidden variable. The sum over parses in Eq. 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm. To estimate the parameters themselves, we use"
D10-1119,P06-2034,0,0.0924891,"007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representatio"
D10-1119,P06-1115,0,0.935826,"g representations, or both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representat"
D10-1119,D08-1082,1,0.902357,"natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the fact that there is no standard representation for logical f"
D10-1119,P96-1008,0,0.0802861,"en designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) con"
D10-1119,J03-1002,0,0.00259079,"tures: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a),i) for the predicate argument-type relation. Initialization The weights for the semantic features are initialized to zero. The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1. We compute translation scores for (word, constant) pairs that cooccur in examples in the training data. The initial weight for each φL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ΛN P which are set to 10 (equivalent to the highest possible coocurrence score). We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations. These values were selected with cross validation on 1230 the Geo880 development set, describe"
D10-1119,N06-1056,0,0.88739,"both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the"
D10-1119,P07-1121,0,0.766071,"Λ) [φ(xi , y, zi )] −Ep(y,z|xi ;θ,Λ) [φ(xi , y, z)] • Set θ = θ + γ∆ Output: Lexicon Λ and parameters θ. Figure 2: The UBL learning algorithm. WASP system (Wong & Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &"
D10-1119,D07-1071,1,0.85741,"Missing"
D10-1119,P09-1110,1,0.900752,"Missing"
D11-1039,W07-1207,0,0.0105978,"Missing"
D11-1039,P09-1010,1,0.431274,"], CHI) > &lt;Φ> > Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as b"
D11-1039,P10-1129,1,0.830702,"Missing"
D11-1039,J07-4004,0,0.0341192,"or example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector representation and θ ∈ Rd to be a parameter vector. The optimal parse for sentence x is y ∗ (x) = arg max y∈GEN(x"
D11-1039,W10-2903,0,0.472556,"et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, reI want to go from Boston to New York and then to Chicago S/N λf.f (N N )/N P λy.λf.λx.f (x) ∧ f rom(x, y) NP BOS (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP NY C CON J[] (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP CHI (N N ) λf.λx.f ("
D11-1039,P11-1149,0,0.199604,"3; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, reI want to go from Boston to New York and then to Chicago S/N λf.f (N N )/N P λy.λf.λx.f (x) ∧ f rom(x, y) NP BOS (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP NY C CON J[] (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP CHI (N N ) λf.λx.f (x) ∧ f rom(x, BOS) > (N N ) λf.λx.f (x) ∧ to(x, N Y C) (N N ) λf.λx.f (x) ∧ f rom(x, BOS) ∧ to(x, N Y C) > &lt;B (N N ) λf.λx.f (x) ∧ to(x, CHI) (N N"
D11-1039,P99-1069,0,0.010269,"ucted with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector representation and θ ∈ Rd to be a parameter vector. The optimal parse for sentence x is y ∗ (x) = arg max y∈GEN(x;Λ) θ · φ(x, y) and the final output logical form z is the lambdacalculus expression at the root of y ∗ (x). We compute y ∗ (x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we us"
D11-1039,P06-1115,0,0.563022,"here has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Add"
D11-1039,D10-1119,1,0.815384,"previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al."
D11-1039,P09-1011,0,0.0511511,"to(x[1], N Y C) ∧ bef ore(x[1], x[2]) ∧ to(x[2], CHI) > &lt;Φ> > Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weigh"
D11-1039,P11-1060,0,0.352543,"Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has been significant recent work on learning to do other, reI want to go from Boston to New York and then to Chicago S/N λf.f (N N )/N P λy.λf.λx.f (x) ∧ f rom(x, y) NP BOS (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP NY C CON J[] (N N )/N P λy.λf.λx.f (x) ∧ to(x, y) NP CHI (N N ) λf.λx.f (x) ∧ f rom(x, BOS) > (N"
D11-1039,D08-1082,1,0.797493,"focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches fo"
D11-1039,P96-1008,0,0.28956,"om(x[1], BOS) ∧ to(x[1], N Y C) ∧ bef ore(x[1], x[2]) ∧ to(x[2], CHI) S λx[] .f rom(x[1], BOS) ∧ to(x[1], N Y C) ∧ bef ore(x[1], x[2]) ∧ to(x[2], CHI) > &lt;Φ> > Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additional"
D11-1039,P06-2080,0,0.0150522,"nt work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) presents work on unsupervised learning. Our approach provides an alternative method of supervision that could complement these approaches. Additionally, there has b"
D11-1039,P10-1083,0,0.0471922,"example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section."
D11-1039,H01-1015,0,0.214106,"p-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory categorial grammars (CCGs) are a linguistically-motivated model for a wide range of language phenomena (Steedman, 1996; 2000). A"
D11-1039,D09-1001,0,0.0703466,"f a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning representations. 5 Mapping Sentences to Logical Form We will use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 5.1 Combinatory Categorial Grammars Combinatory cate"
D11-1039,P10-1031,0,0.055379,"Missing"
D11-1039,P06-2034,0,0.0501635,"rsational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) desc"
D11-1039,W00-1317,0,0.0125826,"2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on learning to do semantic analysis with alternate forms of supervision. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011)"
D11-1039,W04-3201,0,0.00795453,"he explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector representation and θ ∈ Rd to be a parameter vector. The optimal parse for sentence x is y ∗ (x) = arg max y∈GEN(x;Λ) θ · φ(x, y) and the final output logical form z is the lambdacalculus expression at the root of y ∗ (x). We compute y ∗ (x) with a CKY-style chart parsing algorithm. Since each chart entry contains a full lambda-calculus meaning expression, we use N -best pruning to control the number of options we consid"
D11-1039,N06-1056,0,0.313514,"rn to recover user speech acts or integrate the logical 423 Related Work Most previous work on learning from conversational interactions has focused on the dialog sub-problems of response planning (e.g., Levin et al., 2000; Singh et al., 2002) and natural language generation (e.g., Lemon, 2011). We are not aware of previous work on inducing semantic parsers from conversations. There has been significant work on supervised learning for inducing semantic parsers. Various techniques were applied to the problem including machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006; 2007; Matuszek et al., 2010), higherorder unification (Kwiatkowski et al., 2010), parsing (Ruifang and Mooney, 2006; Lu et al., 2008), inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Tang and Mooney, 2000), probabilistic push-down automata (He and Young, 2005; 2006) and ideas from support vector machines and string kernels (Kate and Mooney, 2006; Nguyen et al., 2006). The algorithms we develop in this paper build on previous work on supervised learning of CCG parsers (Zettlemoyer and Collins, 2005; 2007), as we describe in Section 5.3. There is also work on le"
D11-1039,P07-1121,0,0.789995,"Missing"
D11-1039,D07-1071,1,0.78629,"xtension for modeling arraytyped variables to represent lists of individual entries. These constructions are used, for example, to model sentences describing a sequence of segments while specifying flight preferences. Figure 2 shows how a CCG parse builds a logical form for a complete sentence with an array-typed variable. Each intermediate node in the tree is constructed with one of a small set of CCG combinator rules, see the explanation from Steedman (1996; 2000). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. 5.2 Weighted Linear CCGs A weighted linear CCG (Clark and Curran, 2007) provides a ranking on the space of possible parses under the grammar, which can be used to select the best logical form for a sentence. This type of model is closely related to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector repr"
D11-1039,P09-1110,1,0.856724,"1], N Y C) ∧ bef ore(x[1], x[2]) ∧ to(x[2], CHI) S λx[] .f rom(x[1], BOS) ∧ to(x[1], N Y C) ∧ bef ore(x[1], x[2]) ∧ to(x[2], CHI) > &lt;Φ> > Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[] that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the lower ones create new nonterminals according the CCG combinators (>, &lt;, etc.), see Steedman (2000) for details. lated, natural language semantic analysis tasks from context-dependent database queries (Miller et al., 1996; Zettlemoyer and Collins, 2009), grounded event streams (Chen et al., 2010; Liang et al., 2009), environment interactions (Branavan et al., 2009; 2010; Vogel and Jurafsky, 2010), and even unannotated text (Poon and Domingos, 2009; 2010). Finally, the DARPA Communicator data (Walker et al., 2002) has been previously studied. Walker and Passonneau (2001) introduced a schema of speech acts for evaluation of the DARPA Communicator system performance. Georgila et al. (2009) extended this annotation schema to user utterances using an automatic process. Our speech acts extend this work to additionally include full meaning represen"
D11-1140,C04-1180,1,0.278766,"s learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“Show me flights to Boston” and zi = λ x. f light(x) ∧to(x, bos). Model We will represent"
D11-1140,P10-1129,1,0.585752,"e recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified"
D11-1140,W03-1013,0,0.00562958,"y available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z r"
D11-1140,J07-4004,0,0.026062,"c parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored lexicon. We evaluate the appr"
D11-1140,W10-2903,0,0.551773,"ncluding ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and fro"
D11-1140,P06-2034,0,0.0236428,"guage-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on uns"
D11-1140,P11-1149,0,0.158983,"ls (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a met"
D11-1140,P02-1043,1,0.652263,"obabilistic CCG grammars for semantic parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored l"
D11-1140,D10-1119,1,0.165291,"ction for Semantic Parsing Tom Kwiatkowski∗ Luke Zettlemoyer† Sharon Goldwater∗ Mark Steedman∗ t.m.kwiatkowksi@sms.ed.ac.uk lsz@cs.washington.edu sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk † Computer ∗ School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Science & Engineering University of Washington Seattle, WA 98195 Abstract and the goal is to learn a grammar that can map new, unseen, sentences onto their corresponding meanings, or logical forms. One approach to this problem has developed algorithms for leaning probabilistic CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). These grammars are well-suited to the task of semantic parsing, as they closely link syntax and semantics. They can be used to model a wide range of complex linguistic phenomena and are strongly lexicalized, storing all language-specific grammatical information directly with the words in the lexicon. For example, a typical learned lexicon might include entries such as: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pai"
D11-1140,P09-1011,0,0.0264637,"ations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the"
D11-1140,P11-1060,0,0.573776,"chine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon a"
D11-1140,D08-1082,1,0.938058,"ed CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York ` NP : nyc New York ` NN : λ f λ x. f rom(x, nyc) ∧ f (x) A"
D11-1140,P96-1008,0,0.0661923,"d an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011"
D11-1140,P06-2080,0,0.0157424,"ge independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environme"
D11-1140,J03-1002,0,0.00538065,"elation; and φ(p,Ty(a),i) for the predicate argument-type relation. Boolean operator features look at predicates that occurr together in conjunctions and disjunctions. For each variable vi that fills argument slot i in two conjoined predicates p1 and p2 we introduce a binary indicator feature φcon j(i,p1 ,p2 ) . We introduce similar features φdis j(i,p1 ,p2 ) for variables vi that are shared by predicates in a disjunction. Initialization The weights for lexeme features are initialized according to coocurrance statistics between words and logical constants. These are estimated with the Giza++ (Och and Ney, 2003) implementation of IBM Model 1. The initial weights for templates are set by adding −0.1 for each slash in the syntactic category and −2 if the template contains logical constants. Features on lexeme-template pairs and all parse features are initialized to zero. Systems We compare performance to all recentlypublished, directly-comparable results. For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer System ZC07 UBL FUBL Exact Match Rec. Pre. F1 74.4 87.3 80.4 65.6 67.1 66.3 81.9 82.1 82.0 System ZC07 HY06 UBL FUBL Exact Match Rec. Pre. F1. 84.6 85.8 85.2 71.4 72.1 71.7 82.8 82.8 82.8 Partial"
D11-1140,D09-1001,0,0.0645657,"(2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach."
D11-1140,P10-1031,0,0.0256343,"Missing"
D11-1140,P06-1115,0,0.933771,"odel systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x,"
D11-1140,W00-1317,0,0.0167049,"od, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific i"
D11-1140,P10-1083,0,0.0173826,"d on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the tra"
D11-1140,W99-0909,0,0.0192112,"antly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“"
D11-1140,N06-1056,0,0.926806,"on in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York"
D11-1140,P07-1121,0,0.892548,"rsing datasets: GeoQuery, which is made up of natural language queries to a database of geographical information; and Atis, which contains natural language queries to a flight booking system. The Geo880 dataset has 880 (English-sentence, logicalform) pairs split into a training set of 600 pairs and a test set of 280. The Geo250 data is a subset of the Geo880 sentences that have been translated into Japanese, Spanish and Turkish as well as the original English. We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). The Atis dataset contains 5410 (sentence, logical-form) pairs split into a 4480 example training set, a 480 example development set and a 450 example test set. 1519 Evaluation Metrics We report exact match Recall (percentage of sentences for which the correct logical-form was returned), Precision (percentage of returned logical-forms that are correct) and F1 (harmonic mean of Precision and Recall). For Atis we also report partial match Recall (percentage of correct literals returned), Precision (percentage of returned literals that are correct) and F1, computed as described by Zettlemoyer an"
D11-1140,D07-1071,1,0.893493,"three lexical items separately is inefficient, since each word of this class (such as “fare”) will require three similarly structured lexical entries differing only in predicate name. There may also be systemtatic semantic variation between entries for a certain class of words. For example, in (6) “Boston” is paired with the constant bos that represents its meaning. However, item (7) also adds the predicate from to the logical form. This might be used to analyse somewhat elliptical, unedited sentences such as “Show me flights Boston to New York,” which can be challenging for semantic parsers (Zettlemoyer and Collins, 2007). This paper builds upon the insight that a large proportion of the variation between lexical items for a given class of words is systematic. Therefore it should be represented once and applied to a small set of basic lexical units. 1 We develop a factored lexicon that captures this insight by distinguishing lexemes, which pair words with logical constants, from lexical templates, which map lexemes to full lexical items. As we will see, this can lead to a significantly more compact lexicon that can be learned from less data. Each word or phrase will be associated with a few lexemes that can be"
D11-1140,P09-1110,1,0.855677,"ree of success on all of these datasets. 2 Related work There has been significant previous work on learning semantic parsers from training sentences labelled with logical form meaning representations. We extend a line of research that has addressed this problem by developing CCG grammar induction techniques. Zettlemoyer and Collins (2005, 2007) presented approaches that use hand generated, English-language specific rules to generate lexical items from logical forms as well as English specific type-shifting rules and relaxations of the CCG combinators to model spontaneous, unedited sentences. Zettlemoyer and Collins (2009) extends this work to the case of learning in context dependent environments. Kwiatkowski et al. (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translat"
D13-1029,P12-1041,0,0.0150219,"ow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from"
D13-1029,H05-1013,0,0.0623872,"Missing"
D13-1029,D13-1203,0,0.0561717,"Missing"
D13-1029,P08-2012,0,0.0307614,"C O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example, two mentions of “Rutgers”"
D13-1029,D09-1120,0,0.2063,"es precision for two main reaMethod B3 R MUC R P NEC O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia ent"
D13-1029,N10-1061,0,0.0609941,"the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our m"
D13-1029,P03-1054,0,0.0243169,"l be assigned the same link as “president” but “The governor of Alaska Sarah Palin” would not be assigned an exact link to Sarah Palin. For mentions m0 that do not receive an exact link, we assign a head link h(m0 ) if the head word2 m has been linked, by setting h(m0 ) = l(m). For instance, the head link for the mention “President Clinton” (with “Clinton” as head word) will be the Wikipedia title of Bill Clinton. We use head links for the Relaxed NEL sieve (Sec. 3.6). Next, we define L(m) to be the set con2 A head word is assigned to every mention with the Stanford parser head finding rules (Klein and Manning, 2003). country company place body manager site senator attraction origin capital nominee president state agency market organization prosecutor stadium government plant operation candidate city region power park owner attorney network department airport author film area location unit province trial county building person kingdom period venue Figure 3: The most commonly used fine-grained attributes from Freebase and Wikipedia (out of over 500 total attributes). taining l(m) and l(m0 ) for all sub-phrases m0 of m. We add the sub-phrase links only if their confidence is higher than the confidence for l"
D13-1029,W11-1902,0,0.086838,"Missing"
D13-1029,J13-4004,0,0.472275,"new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NEC O also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: 2.1 Coreference Resolution Coreference resolution is the the task of identifying all text spans (called mentions) that refer to"
D13-1029,P10-1142,0,0.0377113,"rained attributes caused precision errors in cases where many proper noun mentions were potential antecedents for a common noun. Although attributes such as country are useful for resolving a generic “country” mention, this information is insufficient when two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., apposit"
D13-1029,N06-1025,0,0.132958,"hen two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations."
D13-1029,D08-1068,0,0.0651385,"reaMethod B3 R MUC R P NEC O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example,"
D13-1029,W11-1901,0,0.156903,"to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE- NWIRE, CONLL, and ACE- NWIRE - NEL. ACE- NWIRE, the newswire subset of the ACE 2004 corpus (NIST, 2004), includes 128 documents. The CONLL coreference dataset includes text from five different domains: broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and web data (WB) (Pradhan et al., 2011). The broadcast conversation and broadcast news domains consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE- NWIRE - NEL by taking a subset of ACE- NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 lin"
D13-1029,D10-1048,0,0.119626,"stics We present NEC O, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NEC O also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: 2.1 Coreference Resolution Coreference resolution is the the task of identifying all text spans (called ment"
D13-1029,P11-1082,0,0.142131,"; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL"
D13-1029,D12-1113,0,0.750669,"tures. For example, “Michael Eisner” is relatively unambiguous but the isolated mention “Eisner” is more challenging. However, these mentions could be clustered with a coreference model, allowing for improved NEL through link propagation from the easier mentions. 289 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289–299, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics We present NEC O, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent."
D13-1029,P11-1138,0,0.649874,") mj is a linked proper noun, (2) if mi or the title of its linked Wikipedia page is in the list of fine-grained attributes of mj , or (3) if h(mj ) is related to the head link h(mi ) according to Freebase as defined above. Because this sieve has low precision, we only allow merges between mentions that have a maximum distance of three sentences between one another. We add the Relaxed NEL sieve near the end of the pipeline, just before pronoun resolution. 293 4 Experimental Setup Core Components and Baselines The Stanford sieve-based coreference system (Lee et al., 2013), the GLOW NEL system (Ratinov et al., 2011), and WikipediaMiner (Milne and Witten, 2008) provide core functionality for our joint model, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE- NWIRE, CON"
D13-1029,P10-1144,0,0.0134412,"ions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B 3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford Method Stanford Sieves NEC O No NEL Mentions No Mention Pruning No Attributes No Constraints P 39.9 46.8 46.1 43.6 45.9 42.3 MUC R 46.2 52.5 48.3 45.6 47.4 49.3 F1 42.8 49.5 47.2 44.6 46.6 45.5 P 67.9 70.4 71.4 70.5 71.8 68.3 B3 R 71.8 72.6 70.0 69.9 69.7 72.3 F1 69.8 71.5 70.9 70.2 70.7 70.2 P 44.2 51.5 49.7 46.2 48.6 44.2 Pairwise R F1 29.7 35.6 34.6 41.4 30.9 38.1 29.4 35.9 27.0 34.7 28.6 34.7 Table 1: Coreference results on ACE- NWIRE with predicted mentions and automatic linking. coreference software for ACE2004 and using the CoNLL scorer fo"
D13-1029,M95-1005,0,0.548965,"consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE- NWIRE - NEL by taking a subset of ACE- NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B 3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford Method Stanford Sieves NEC O No NEL Mentions No Mention Pruning No Attributes No Constraints P 39.9 46.8 46.1 43.6 45.9 42.3 MUC R 46.2 52.5 48.3 45.6 47.4 49.3 F1 42.8 49.5 47.2 44.6 46.6 45.5 P 67.9 70.4 71.4 70.5 71.8 68."
D13-1029,W13-3517,0,0.406478,"act fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL to improve coreference resolution, but did not consider a joint approach. They extracted attributes from Wikipedia categories and used them as features in a learned mention-pair model, but did not do mention detection. Unfortunately, it is"
D13-1029,H05-1033,0,\N,Missing
D13-1029,W11-1900,0,\N,Missing
D13-1029,W11-0300,0,\N,Missing
D13-1145,E06-1042,0,0.0975959,"diomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-"
D13-1145,J06-1003,0,0.0103457,"Missing"
D13-1145,W07-1106,0,0.0194187,", where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Data Set All Train Unannotated Dev Annotated Dev Unannotated Test Annotated Test Literal 56,037 47,633 2,801 2,212 5,603 4,510 Idiomatic 7,764 6,600 388 958 776 1,834 Total 63,801 54,233 3,189 3,170 6,379 6,344 Figure 1: Number of dictiona"
D13-1145,W09-2903,0,0.0662175,"of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Data Set All Train Unannotated Dev Annotated Dev Unannotated Test Annotated Test Literal 56,037 47,633 2,801 2,212 5,603 4,510 Idiomatic 7,764 6,600 388 958 776 1,834 Total 63,801 54,233 3,189 3,170 6,379 6,344 Figure 1: Number of dictionary entries with each class fo"
D13-1145,E06-1043,0,0.0869801,"10), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Data Set All Train Unannotated Dev Annotated Dev Unannotated Test Annotated Test Literal 56,037 47,633 2,801 2,212 5,603 4,510 Idiomatic 7,764 6,600 388 958 776 1,834 Total 63,801 54,233 3,189 3,170 6,379 6,344 Figure 1"
D13-1145,S12-1017,0,0.0397669,"brecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Data Set All Train Unannotated Dev Annotated Dev Unannotated Test Annotated Test Literal 56,037 47,633 2,801 2,212 5,603 4,510 Idiomatic 7,764 6,600 388 958 776 1,834 Total 63,801 54,233 3,189 3,170 6,379 6,344 Figure 1: Number of dictionary entries with each class for the Wiktionary identification data. Data Set Dev Test Literal 171"
D13-1145,W06-3506,0,0.195525,"many applications, including parsing (Sag et al., 2002) and machine translation (Shutova et al., 2012). We use Wiktionary as a large, but incomplete, reference for idiomatic entries; individual entries can be marked as idiomatic but, in practice, most are Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (sti"
D13-1145,W06-1203,0,0.0965509,"omplete, reference for idiomatic entries; individual entries can be marked as idiomatic but, in practice, most are Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill a"
D13-1145,D09-1033,0,0.456878,"o reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any p"
D13-1145,C10-1113,0,0.0556934,"luding parsing (Sag et al., 2002) and machine translation (Shutova et al., 2012). We use Wiktionary as a large, but incomplete, reference for idiomatic entries; individual entries can be marked as idiomatic but, in practice, most are Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Faz"
D13-1145,zesch-etal-2008-extracting,0,0.019682,"Missing"
D13-1145,W06-1200,0,\N,Missing
D13-1145,J13-2003,0,\N,Missing
D13-1161,D11-1039,1,0.729671,"unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the 1547 UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to sc"
D13-1161,Q13-1005,1,0.80831,"Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the 1547 UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a"
D13-1161,W08-2222,0,0.0204448,"and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), w"
D13-1161,P13-1042,0,0.748448,"uring learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark 1546 datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in Freebase (Bollacker et al., 2008) and O w"
D13-1161,S13-1045,0,0.581632,"uring learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark 1546 datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in Freebase (Bollacker et al., 2008) and O w"
D13-1161,J07-4004,0,0.129919,"target domain. In our example, producing either MR1, MR2 or another more appropriate option, depending on the QA database schema. This two stage approach enables parsing without any domain-dependent lexicon that pairs words with logical constants. Instead, word meaning is filled in on-the-fly through ontology matching, enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains. Figure 1 shows the desired outputs for two example Freebase sentences. The first parsing stage uses a probabilistic combinatory categorial grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) to map sentences to new, underspecified logical-form meaning representations containing generic logical constants that are not tied to any specific ontology. This approach enables us to share grammar structure across domains, instead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the f"
D13-1161,W10-2903,0,0.892207,"ure across domains, instead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the final logical forms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark 1546 datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-auth"
D13-1161,P13-1158,1,0.700977,"tes our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables"
D13-1161,P85-1008,0,0.782027,"Gs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontologica"
D13-1161,P12-1051,0,0.0858607,"tionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning wit"
D13-1161,P06-1115,0,0.359054,"r of new features (Section 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Ze"
D13-1161,Q13-1016,0,0.0644244,"utput logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Q1: Q2: MR1: MR2: Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instrucWhat is the population of Seattle? How many people live in Seattle? λx.population(Seattle, x) count(λx.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases (‘How many,’ ‘people,’ etc.). Unfortunately, few ontologies have sufficient coverage to support both meaning representations, for example many QA databases would only include the population relation required for MR1. Most existing approaches would, given this deficiency, simply aim to produce MR1 for Q2"
D13-1161,D12-1069,0,0.822255,"epresentations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Q1: Q2: MR1: MR2: Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instrucWhat is the population of Seattle? How many people live in Seattle? λx.population(Seattle, x) count(λx.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases (‘How many,’ ‘people,’ etc.). Unfortunately, few ontologies have sufficient coverage to support both meaning representations, for example many QA databases would only include the population relation required for MR1. Most existing ap"
D13-1161,N13-1103,0,0.159646,"Missing"
D13-1161,E12-1024,1,0.856417,"Missing"
D13-1161,D10-1119,1,0.938921,"valuation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstratin"
D13-1161,D11-1140,1,0.890245,"bilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Q1: Q2: MR1: MR2: Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instrucWhat is the population of Seattle? How many people live in Seattle? λx.population(Seattle, x) count(λx.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases (‘How many,’ ‘people,’ etc.). Unfortunately, few ontologies have sufficient coverage to support both meaning representations, for example many QA databases"
D13-1161,P11-1060,0,0.879871,"uistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Q1: Q2: MR1: MR2: Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instrucWhat is the population of Seattle? How many people live in Seattle? λx.population(Seattle, x) count(λx.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases (‘How many,’ ‘people,’ etc.). Unfortunately, few ontologies have sufficient coverage to support both meaning representations, for example many QA databases would only include th"
D13-1161,P07-1121,0,0.784729,"ion 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our w"
D13-1161,D12-1035,0,0.058195,"all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms t"
D13-1161,D07-1071,1,0.771044,"Missing"
D13-1161,P09-1110,1,0.322923,"hing techniques to expand a CCG lexicon learned with the 1547 UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are des"
D13-1197,W08-1107,0,0.0551631,"tively plausible, for example by following Gricean maxims (Grice, 1975). Krahmer and van Deemter (2012) and van Deemter et al. (2012a) survey recent literature on REG. Different approaches have been proposed for generating referring expressions for sets of objects. Van Deemter (2002) extended the Incremental Algorithm to allow disjunction and negation, enabling reference to sets. Further work attempted to resolve the unnaturally long expressions which could be generated by this approach (Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Later, description logic was used to name sets (Areces et al., 2008; Ren et al., 2010). All of these algorithms are manually engineered and deterministic. In practice, human utterances are surprisingly varied, loosely following the Gricean ideals (van Deemter et al., 2012b). Much recent work in REG has identified the importance of modeling the variation observed in human-generated referring expressions (Viethen and Dale, 2010; Viethen et al., 2013; van Deemter et al., 2012b; Mitchell et al., 2013), and some approaches have applied machinelearning techniques to single-object references (Viethen and Dale, 2010; Mitchell et al., 2011a,b). Recently, Mitchell et a"
D13-1197,D11-1039,1,0.416903,"Missing"
D13-1197,Q13-1005,1,0.19665,"(Barzilay and Lapata, 2005; Carenini et al., 2006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set of logical expressions that"
D13-1197,H05-1042,0,0.0185248,"Mitchell et al. (2013) introduced a probabilistic approach for conjunctive descriptions of single objects, which will provide a comparison baseline for experiments in Section 8. To the best of our knowledge, this paper presents the first learned probabilistic model for referring expressions defining sets, and is the first effort to treat REG as a density estimation problem. REG is related to content selection, which has been studied for generating text from databases (Konstas and Lapata, 2012), event streams (Chen et al., 2010), images (Berg et al., 2012; Zitnick and Parikh, 2013), and text (Barzilay and Lapata, 2005; Carenini et al., 2006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 201"
D13-1197,E06-1039,0,0.00832935,"roduced a probabilistic approach for conjunctive descriptions of single objects, which will provide a comparison baseline for experiments in Section 8. To the best of our knowledge, this paper presents the first learned probabilistic model for referring expressions defining sets, and is the first effort to treat REG as a density estimation problem. REG is related to content selection, which has been studied for generating text from databases (Konstas and Lapata, 2012), event streams (Chen et al., 2010), images (Berg et al., 2012; Zitnick and Parikh, 2013), and text (Barzilay and Lapata, 2005; Carenini et al., 2006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Lian"
D13-1197,P02-1013,0,0.0375533,"systems are used in generation pipelines (Dale and Reiter, 2000) and are also commonly designed to be cognitively plausible, for example by following Gricean maxims (Grice, 1975). Krahmer and van Deemter (2012) and van Deemter et al. (2012a) survey recent literature on REG. Different approaches have been proposed for generating referring expressions for sets of objects. Van Deemter (2002) extended the Incremental Algorithm to allow disjunction and negation, enabling reference to sets. Further work attempted to resolve the unnaturally long expressions which could be generated by this approach (Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Later, description logic was used to name sets (Areces et al., 2008; Ren et al., 2010). All of these algorithms are manually engineered and deterministic. In practice, human utterances are surprisingly varied, loosely following the Gricean ideals (van Deemter et al., 2012b). Much recent work in REG has identified the importance of modeling the variation observed in human-generated referring expressions (Viethen and Dale, 2010; Viethen et al., 2013; van Deemter et al., 2012b; Mitchell et al., 2013), and some approaches have applied machinelearning t"
D13-1197,D07-1011,0,0.0462833,"Missing"
D13-1197,W07-2307,0,0.104069,"Missing"
D13-1197,D12-1040,0,0.0181944,"rikh, 2013), and text (Barzilay and Lapata, 2005; Carenini et al., 2006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set"
D13-1197,N12-1093,0,0.00962008,"e applied machinelearning techniques to single-object references (Viethen and Dale, 2010; Mitchell et al., 2011a,b). Recently, Mitchell et al. (2013) introduced a probabilistic approach for conjunctive descriptions of single objects, which will provide a comparison baseline for experiments in Section 8. To the best of our knowledge, this paper presents the first learned probabilistic model for referring expressions defining sets, and is the first effort to treat REG as a density estimation problem. REG is related to content selection, which has been studied for generating text from databases (Konstas and Lapata, 2012), event streams (Chen et al., 2010), images (Berg et al., 2012; Zitnick and Parikh, 2013), and text (Barzilay and Lapata, 2005; Carenini et al., 2006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007),"
D13-1197,J12-1006,0,0.0728329,"Missing"
D13-1197,Q13-1016,0,0.066752,"t full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set of logical expressions that select a target set of objects G in a world state S, as formally defined in Section 5.1. We aim to learn a probability distribution P (z |S"
D13-1197,P09-1011,0,0.0226925,"006). However, most approaches to this problem output bags of concepts, while we construct full logical expressions, 1916 allowing our approach to capture complex relations between attributes. Finally, our approach to modeling meaning using lambda calculus is related to a number of approaches that used similar logical representation in various domains, including database query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set of logical expressions that select a target set of objects G in a"
D13-1197,D11-1149,0,0.122199,"elle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set of logical expressions that select a target set of objects G in a world state S, as formally defined in Section 5.1. We aim to learn a probability distribution P (z |S, G), with z ∈ Z. For example, in the referring expressions domain we work with, the state S = {o1 , . . . , on } is a set of n objects oi . Each oi has three properties: color, shape and type. The target set G ⊆ S is the subset of objects to be described. Figure 1a shows an example scene"
D13-1197,U10-1013,0,0.0259598,"ling reference to sets. Further work attempted to resolve the unnaturally long expressions which could be generated by this approach (Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Later, description logic was used to name sets (Areces et al., 2008; Ren et al., 2010). All of these algorithms are manually engineered and deterministic. In practice, human utterances are surprisingly varied, loosely following the Gricean ideals (van Deemter et al., 2012b). Much recent work in REG has identified the importance of modeling the variation observed in human-generated referring expressions (Viethen and Dale, 2010; Viethen et al., 2013; van Deemter et al., 2012b; Mitchell et al., 2013), and some approaches have applied machinelearning techniques to single-object references (Viethen and Dale, 2010; Mitchell et al., 2011a,b). Recently, Mitchell et al. (2013) introduced a probabilistic approach for conjunctive descriptions of single objects, which will provide a comparison baseline for experiments in Section 8. To the best of our knowledge, this paper presents the first learned probabilistic model for referring expressions defining sets, and is the first effort to treat REG as a density estimation problem"
D13-1197,W13-2108,0,0.033162,"Further work attempted to resolve the unnaturally long expressions which could be generated by this approach (Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Later, description logic was used to name sets (Areces et al., 2008; Ren et al., 2010). All of these algorithms are manually engineered and deterministic. In practice, human utterances are surprisingly varied, loosely following the Gricean ideals (van Deemter et al., 2012b). Much recent work in REG has identified the importance of modeling the variation observed in human-generated referring expressions (Viethen and Dale, 2010; Viethen et al., 2013; van Deemter et al., 2012b; Mitchell et al., 2013), and some approaches have applied machinelearning techniques to single-object references (Viethen and Dale, 2010; Mitchell et al., 2011a,b). Recently, Mitchell et al. (2013) introduced a probabilistic approach for conjunctive descriptions of single objects, which will provide a comparison baseline for experiments in Section 8. To the best of our knowledge, this paper presents the first learned probabilistic model for referring expressions defining sets, and is the first effort to treat REG as a density estimation problem. REG is related to co"
D13-1197,D09-1043,0,0.0727827,"tabase query interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005, 2007), natural language instructions (Chen and Mooney, 2011; Matuszek et al., 2012b; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013b), event streams (Liang et al., 2009; Chen et al., 2010), and visual descriptions (Matuszek et al., 2012a; Krishnamurthy and Kollar, 2013). Our use of logical forms follows this line of work, while extending it to handle plurality and coordination, as described in Section 4.1. In addition, lambda calculus was shown to enable effective natural language generation from logical forms (White and Rajkumar, 2009; Lu and Ng, 2011). If combined with these approaches, our approach would allow the creation of a complete REG pipeline. 3 Technical Overview Task Let Z be a set of logical expressions that select a target set of objects G in a world state S, as formally defined in Section 5.1. We aim to learn a probability distribution P (z |S, G), with z ∈ Z. For example, in the referring expressions domain we work with, the state S = {o1 , . . . , on } is a set of n objects oi . Each oi has three properties: color, shape and type. The target set G ⊆ S is the subset of objects to be described. Figure 1a show"
D13-1197,W11-2808,0,0.114698,"Missing"
D13-1197,H89-1033,0,0.0262041,"Missing"
D13-1197,N13-1137,0,0.215452,"Missing"
D13-1197,W10-4212,0,0.129801,"Missing"
D13-1197,W00-1416,0,0.0628378,"are identified with the entity-type e and pick out individual objects (Carpenter, 1997). This makes it difficult to interpret plural noun-phrases which pick out a set of objects, like “The red cubes”. Previous approaches would map this sentence to the same logical expression as the singular “The red cube”, ignoring the semantic distinction encoded by the plural. Instead, we define the primitive entity e to range over sets of objects. he, ti-type expressions are therefore functions from sets to a truth-value. These are used in two ways, modeling both distributive and collective predicates (cf. Stone, 2000): 1917 1. Distributive predicates are he, ti-type expressions which will return true if every individual in the set has a given property. For example, the expression λx.red(x) will be true for all sets which contain only objects for which the value red is true. 2. Collective predicates are he, ti-type expressions which indicate a property of the set itself. For example, in the phrase “the two cubes”, “two” corresponds to the expression λx.cardinality2 (x) which will return true only for sets which have exactly two members. We define semantic plurality in terms of two special collective predica"
D13-1197,J02-1003,0,0.0866679,"Missing"
D13-1197,pereira-etal-2012-corpus,0,0.0575393,"Missing"
D13-1197,D07-1071,1,0.347815,"Missing"
D14-1135,H94-1010,0,0.611962,"Missing"
D14-1135,P13-2009,0,0.0634919,"sers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2"
D14-1135,D11-1039,1,0.760784,"ss-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998"
D14-1135,Q13-1005,1,0.18856,"analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011"
D14-1135,D13-1160,0,0.0573539,"), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have relatively simple structure and can be interepreted accurately using only factoid lookups with no database joins (Yao and Van Durme, 2014). Our work focuses on learning more syntactically rich models that support compositional reasoning. 3 Background Lambda Calculus We represent the meanings of sentences, words and phrases with 1285 list one S/N λf.f way flights from various cities N/N N P P/N P N P/N N λf λx.oneway(x) ∧ f (x) λx.f light(x) λxλy.f rom(y, x) λf Ax.f (x) λx"
D14-1135,W08-2222,0,0.00960693,"algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have relatively simple structure and can be interepreted accurately using only factoid lookups with no database joins (Yao and Van Durme, 2014). Our work focuses on learning mor"
D14-1135,P13-1042,0,0.0214197,"ng semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as"
D14-1135,S13-1045,0,0.0740604,"ng semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as"
D14-1135,P88-1004,0,0.377091,"Missing"
D14-1135,P88-1012,0,0.1227,"es paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fi"
D14-1135,C10-2051,0,0.0249436,"ision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have relatively simple structure and can be interepreted accurately using only factoid lookups with no database joins (Yao an"
D14-1135,P12-1051,0,0.0166426,"for CCG semantic parsers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical fo"
D14-1135,P06-1115,0,0.0470162,"reas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. T"
D14-1135,Q13-1016,0,0.0178061,"ences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). One common approach is to induce a probabilistic CCG grammar, which defines the meanings of individual words and phrases and how to best combine them to analyze complete sentences. There has been recent work developing learning algorithms for CCG semantic parsers (Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2011) and using them for applications ranging from question answering (Cai and Yates, 2013b; Kwiatkowski et al., 2013) to robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). One key learning challenge for this style of learning is to induce the CCG lexicon, which lists possible meanings for each phrase and defines a set of possible parses for each sentence. Previous approaches have either hand-engineered a small set of lexical templates (Zettlemoyer and Collins, 2005, 2007) or automatically learned such templates (Kwiatkowski et al., 2010, 2011). These methods are designed to learn grammars that overgenerate; they produce spurious parses that can complicate parameter estimation. In this paper, we demonstrate that significant gains can instead be achieved by usin"
D14-1135,W10-2903,0,0.0460273,"each new application domain. 2 Related Work Grammar induction methods for CCG semantic parsers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned f"
D14-1135,D12-1069,0,0.0616968,"oyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidso"
D14-1135,D13-1161,1,0.592872,"Missing"
D14-1135,E12-1024,1,0.480318,"aning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design"
D14-1135,D10-1119,1,0.615148,"Missing"
D14-1135,D11-1140,1,0.930984,"Missing"
D14-1135,P13-1092,0,0.14893,"cable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2"
D14-1135,J91-4003,0,0.118789,"Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have rel"
D14-1135,P07-1121,0,0.0258684,"mmar induction methods for CCG semantic parsers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision,"
D14-1135,Q13-1015,0,0.0151433,"have been developed for each case above, making our techniques applicable. There has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing. Finally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have relatively simple structure and can be interepreted accurately using only factoid lookups with no database joins (Yao and Van Durme, 2014). Our work focuses on learning more syntactically rich models"
D14-1135,P14-1090,0,0.0228656,"Missing"
D14-1135,P11-1060,0,0.114447,"y reduce the amount of data that is needed for each new application domain. 2 Related Work Grammar induction methods for CCG semantic parsers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicabl"
D14-1135,P96-1008,0,0.0460217,"kowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supe"
D14-1135,D07-1071,1,0.88666,"Missing"
D14-1135,P09-1110,1,0.626312,"2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains. Although CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG, the general idea of using linguistic constraints to improve learning is broadly applicable. Semantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitch"
D15-1076,kingsbury-palmer-2002-treebank,0,0.0240059,"ge corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3 Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 2 Questions starting with a wh-word, such as who, what, when, how, etc. 644 Sentence (1) Stock-fund managers , meantime , went into October with less cash on hand than they held earlier this year . (2) Mr. Spielvogel added pointedly : “ The pressure on commissions did n’t begi"
D15-1076,P13-1023,0,0.0119744,"Missing"
D15-1076,P03-1054,0,0.0436704,"ionary Dependency parent and edge label, dependency children and edge label Question role label, Wh-word, Preposition / Syntactic parent and edge label, Left/Right-most syntactic children, / Relative position (left or right), Syntactic relation, Syntactic path Table 10: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser (Klein and Manning, 2003). Classifier Random Newswire 78.7 26.3 Wikipedia 82.3 26.9 schemes that require new lexicons to be created for each language. The biggest challenge in annotating sentences with our scheme is choosing the questions. We introduced a method for generating candidate questions automatically, which has the potential to enable very large-scale annotation by only asking the annotators to provide answers. This will only be possible if performance can be improved to the point where we achieve high recall question with acceptable levels of precision. Finally, future work will also explore applications of"
D15-1076,P98-1013,0,0.103025,"annotators with a short tutorial and a few examples. Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gramIntroduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles 1 The PropBank annotation guide is 89 pages (Bonial et al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the"
D15-1076,P11-1112,0,0.0197522,": Mapping question fields to roles in R. The mapping is based on whether certain question fields are empty and the voice of the verb in the question (active or passive). φ indicates that a field is either an empty string or equals “do/doing”. If a question is in passive voice and contains the preposition “by”, then OBJ2 is tagged with R0 instead, as in What is built by someone? this mapping heuristically with reasonable accuracy. In the future, we might try to induce the set of possible roles given each target verb, following the semantic role induction work of Titov and Klementiev (2012) and Lang and Lapata (2011), or use crowdsourcing to label proto-roles, following Reisinger et al. (2015). R ={R0, R1, R2, R2[p], w, w[p]} w ∈{Where, When, Why, How, HowMuch} p ∈Prepositions Predicting Question Roles Given this space of possible roles, our first step in generation is to determine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions. We formulate this task as a supervised multi-label learning problem. We define the set of possible labels L by combining the roles in R with different pronoun values: We then normalize the annotated que"
D15-1076,W13-2322,0,0.0778655,"Missing"
D15-1076,W05-0636,0,0.130719,"Missing"
D15-1076,basile-etal-2012-developing,0,0.0251502,"Missing"
D15-1076,C10-1081,0,0.0200224,"al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gildea, 2010) and coreference resolution (Ponzetto and Strube, 2006). The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches. For example, in “It was his mother’s birthday, so he was going to play her favorite tune”, annotators created the QA pair “When would someone play something? His mother’s birthday” which describes an implicit temporal relation. Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natur"
D15-1076,W07-1431,0,0.0103357,"Missing"
D15-1076,S10-1059,0,0.0534013,"k (Marcus et al., 1993) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3 Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 2 Questions starting with a wh-word, such as who, what, when, how, etc. 644 Sentence (1) Stock-fund managers , meantime , went into October with less cash on hand than they held"
D15-1076,J93-2004,0,0.0593642,"s they play, without predefining an inventory of frames or semantic roles. • We present a novel, lightweight templatebased scheme (Section 3) that enables the high quality QA-SRL data annotation with very little training and no linguistic expertise. • We define two new QA-SRL sub-tasks, question generation and answer identification, and present baseline learning approaches for both (Sections 4 and 5). The results demonstrate that our data is high-quality and supports the study of better learning algorithms. 2 Related Work The success of syntactic annotation projects such as the Penn Treebank (Marcus et al., 1993) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We"
D15-1076,W04-2705,0,0.0131978,"eatures of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3 Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 2 Questions starting with a wh-word, such as who, what, when, how, etc. 644 Sentence (1) Stock-fund managers , meantime , went into October with less cash on hand than they held earlier this year . (2) Mr. Spielvogel added pointedly : “ The pressure on commissions did n’t begin with Al Achenbaum . ” (3) He"
D15-1076,J05-1004,0,0.0439619,"ned inventory of semantic roles or frames, or build on any existing gramIntroduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles 1 The PropBank annotation guide is 89 pages (Bonial et al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of cruc"
D15-1076,N06-2015,0,0.0262262,"is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3 Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 2 Questions starting with a wh-word, such as who, what, when, how, etc. 644 Sentence (1) Stock-fund managers , meantime , went into October with less cash on hand than they held earlier this year . (2) Mr. Spielvogel added pointedly : “ The pressure on commissions did n’t begin with Al Achenbaum . ” (3) He claimed losses totaling $ 42,455 –"
D15-1076,N06-1025,0,0.0226308,"uppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gildea, 2010) and coreference resolution (Ponzetto and Strube, 2006). The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches. For example, in “It was his mother’s birthday, so he was going to play her favorite tune”, annotators created the QA pair “When would someone play something? His mother’s birthday” which describes an implicit temporal relation. Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natural language to label the structure of natural language."
D15-1076,D08-1008,0,0.0210688,"Missing"
D15-1076,Q15-1034,0,0.0414828,"Missing"
D15-1076,P14-2120,0,0.0170093,"n annotation template. answer pairs. While existing work on natural logic has relied on small entailment datasets for training, our method allows practical large-scale annotation of training data. Parser evaluation using textual entailment (Yuret et al., 2010) is a method for evaluating syntactic parsers based on entailment examples. In a similar spirit to our work, they abstract away from linguistic formalisms by using natural language inference. We focus on semantic rather than syntactic annotation, and introduce a scalable method for gathering data that allows both training and evaluation. Stern and Dagan (2014) applied textual entailment to recognize implicit predicate-argument structure that are not explicitly expressed in syntactic structure. 3 To speed annotation and simplify downstream processing, we define a small grammar over possible questions. The questions are constrained with a template with seven fields, q ∈ WH × AUX × SBJ × TRG × OBJ1 × PP × OBJ2, each associated with a list of possible options. Descriptions for each field are shown in Table 2. The grammar is sufficiently general to capture a wide-range of questions about predicate-argument structure— some examples are given in Table 3."
D15-1076,E12-1003,0,0.0149395,"R2 OBJ2 = φ OBJ2 → R2[p] Table 7: Mapping question fields to roles in R. The mapping is based on whether certain question fields are empty and the voice of the verb in the question (active or passive). φ indicates that a field is either an empty string or equals “do/doing”. If a question is in passive voice and contains the preposition “by”, then OBJ2 is tagged with R0 instead, as in What is built by someone? this mapping heuristically with reasonable accuracy. In the future, we might try to induce the set of possible roles given each target verb, following the semantic role induction work of Titov and Klementiev (2012) and Lang and Lapata (2011), or use crowdsourcing to label proto-roles, following Reisinger et al. (2015). R ={R0, R1, R2, R2[p], w, w[p]} w ∈{Where, When, Why, How, HowMuch} p ∈Prepositions Predicting Question Roles Given this space of possible roles, our first step in generation is to determine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions. We formulate this task as a supervised multi-label learning problem. We define the set of possible labels L by combining the roles in R with different pronoun values: We then"
D15-1076,N03-1033,0,0.0450275,"Missing"
D15-1076,W04-3212,0,0.041608,"rd Predicate-Answer Question Generation Answer Identification Token, Predicted POS-tag, Lemma extracted from Wiktionary Dependency parent and edge label, dependency children and edge label Question role label, Wh-word, Preposition / Syntactic parent and edge label, Left/Right-most syntactic children, / Relative position (left or right), Syntactic relation, Syntactic path Table 10: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser (Klein and Manning, 2003). Classifier Random Newswire 78.7 26.3 Wikipedia 82.3 26.9 schemes that require new lexicons to be created for each language. The biggest challenge in annotating sentences with our scheme is choosing the questions. We introduced a method for generating candidate questions automatically, which has the potential to enable very large-scale annotation by only asking the annotators to provide answers. This will only be possible if performance can be improved to the point where we achi"
D15-1076,S10-1009,0,0.0271875,"Missing"
D15-1076,C98-1013,0,\N,Missing
D15-1076,D15-1169,1,\N,Missing
D15-1114,Q13-1005,1,0.727935,"ctions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without takin"
D15-1114,E14-1006,0,0.0128097,"ored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations"
D15-1114,D13-1178,0,0.0127042,"but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient compone"
D15-1114,E03-1061,0,0.0178609,"on models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning"
D15-1114,P09-1010,1,0.929137,"eneral-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. 1 The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural texts. 982 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 982–992, c"
D15-1114,J93-2003,0,0.0652798,"ous actions: hi = (e1 , . . . , ei−1 ). The probability of a recipe R given a set of connections C can be factored by the chain rule: P (R|C) = Y sem P (Sij |tsyn ij , tij , C, hi ) = Part-composite model When the encompassing argument is a food and the origin is a previous verb sem = f ood, origin(sk ) 6= (i.e., P (skij |tsyn ij ij , tij 0, C, hi )), then the probability of the span depends on the ingredients that the span represents given the connections in C. For example, “dressing” is more likely given ingredients “oil” and “vinegar” than given “chicken” and “noodles”. We use IBM Model 1 (Brown et al., 1993) to model the probability of a composite destination phrase given a set of origin food tokens. Let f ood(skij , C) be the set of spans in food arguments such that there is a directed path from those arguments to skij . IBM Model 1 defines the probability of a span given the propagated food spans, P (skij |f ood(skij , C)).3 Given C and a history hi , we assume the verb and arguments of an action are independent: P (aij |C, hi ). j Since the set of connections deterministically defines a verb signature gi for a verb vi , we can simplify P (vi |C, hi ) to the multinomial distribution P (vi |gi )"
D15-1114,W15-2206,0,0.205752,"high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. 1 The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural"
D15-1114,P09-1068,0,0.0086009,"(2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledg"
D15-1114,W14-2407,0,0.141997,"breast, vegetables, noodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al."
D15-1114,N15-1015,0,0.0445054,"oodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al."
D15-1114,P12-1057,0,0.0228171,"part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater semantic coverage (e.g., durations, tools, amounts), and training and evaluating o"
D15-1114,mori-etal-2014-flow,0,0.0887091,"tions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et"
D15-1114,P91-1003,0,0.48446,"(Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for i"
D15-1114,D14-1109,0,0.0607763,"Missing"
D15-1114,P86-1004,0,0.723823,"ecipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent"
D15-1114,E14-1024,0,0.0110329,"erpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures"
D15-1114,P10-1100,0,0.00888942,"xtraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of"
D15-1114,R11-1064,0,0.0182553,"nowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., iden"
D15-1114,Q13-1003,0,0.0563374,"re, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013,"
D15-1114,H86-1011,0,\N,Missing
D15-1114,S12-1001,0,\N,Missing
D15-1169,P11-1048,0,0.162692,"ntactic arguments with semantic roles, such as open : S NPARG1 and open : (S NPARG0 )/NPARG1 . Figure 3 shows a detailed trace of how the example from Figure 2 is parsed with our model. We also present a new A∗ algorithm for the joint model. Because CCG is strongly lexicalized, we are able to introduce a new type of extended lexical entries that allows us to factor the model over words and develop effective new upper bounds on the Viterbi outside parse score. A∗ parsing algorithms have previously been developed for models with tree-structured syntactic dependencies (Klein and Manning, 2003; Auli and Lopez, 2011b), and models with no bi-lexical dependencies, including supertag-factored CCGs (Lewis and Steedman, 2014a). We generalize these techniques to SRL-style graph-structured dependencies. Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser. PropBank SRL performance is 1.6 points higher than comparable existing work, and semantic features improve syntactic accuracy by 1.6 points. Our A∗ algorithm is 5 times faster than CKY parsing, with no loss in accuracy. The combination of CCG-based joint modelling"
D15-1169,P11-1158,0,0.363098,"ntactic arguments with semantic roles, such as open : S NPARG1 and open : (S NPARG0 )/NPARG1 . Figure 3 shows a detailed trace of how the example from Figure 2 is parsed with our model. We also present a new A∗ algorithm for the joint model. Because CCG is strongly lexicalized, we are able to introduce a new type of extended lexical entries that allows us to factor the model over words and develop effective new upper bounds on the Viterbi outside parse score. A∗ parsing algorithms have previously been developed for models with tree-structured syntactic dependencies (Klein and Manning, 2003; Auli and Lopez, 2011b), and models with no bi-lexical dependencies, including supertag-factored CCGs (Lewis and Steedman, 2014a). We generalize these techniques to SRL-style graph-structured dependencies. Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser. PropBank SRL performance is 1.6 points higher than comparable existing work, and semantic features improve syntactic accuracy by 1.6 points. Our A∗ algorithm is 5 times faster than CKY parsing, with no loss in accuracy. The combination of CCG-based joint modelling"
D15-1169,P98-1013,0,0.0464194,"tructured dependencies required for semantic role labelling. Lewis and Steedman (2014a) demonstrate an efficient A∗ algorithm for CCG, but cannot model dependencies. 9 Conclusions and Future Work We have shown that using CCG can allow joint models of syntax and semantics to outperform pipelines, and achieve state-of-the-art results on PropBank SRL. Our new A∗ parsing algorithm is 5 times faster than CKY parsing, without loss of accuracy. Using latent syntax allows us to train the model purely from semantic dependencies, enabling future work to train against other annotations such as FrameNet (Baker et al., 1998), Ontonotes (Hovy et al., 2006) or QASRL (He et al., 2015). The semantic labels provided by PropBank can also be integrated into wide-coverage CCG semantic parsers (Bos, 2008; Lewis and Steedman, 2013) to improve performance on downstream applications. Acknowledgements This research was supported in part by the NSF (IIS-1252835), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We thank Nicholas Fitzgerald, Dan Garrette, Kenton Lee, Swabha Swayamdipta, Mark Yatskar and the anonymous reviewers for their helpful"
D15-1169,W08-2222,0,0.0172135,"Missing"
D15-1169,P09-1005,0,0.0201963,"d Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is limited by relying on 1-best parses—a problem we resolved with a joint model. A∗ parsing A∗ parsing has previously been explored for less general models than ours. Klein and Manning (2003) and Auli and Lopez (2011b) use A∗ parsing for models with tree-structured dependencies. The best reported speed improvement is parsing 1.2 times faster, whereas we improve by a factor of 5. Our model also allows the more complex graph-structured dependencies required for semantic role labelling. Lewis and Steedman (2014a) demonstrate an efficient A∗ algorithm for CCG, but cannot model de"
D15-1169,W08-2134,0,0.3149,"entification (Hajiˇc et al., 2009; FitzGerald et al., 2015). This setting is particularly unrealistic for our joint model, where gold predicate identification would be a highly useful feature for the supertagger; we only compare with models that use automatic predicate identification. To the best of our knowledge, the best SRL results with automatic predicate identification were achieved at CoNLL-2008. A number of other models have been evaluated on CoNLL-2008 data. While we cannot compare directly on our metric, the best reported joint model (Johansson, 2009) scored 1.4 points lower than the Che et al. (2008) system we compare to on the CoNLL metric on PropBank. Other joint models, such as those of Titov et al. (2009) and Lluís et al. (2013), achieve similar performance. Evaluation Metric Comparing fairly with existing work is complicated by the mismatch between heads found by our model and those used in other evaluations. Headedness decisions are often arbitrary—for example, whether a prepositional phrase is headed by the noun or preposition—and different choices were made in the design of CCGbank and the CoNLL-2008 headedness rules. To solve this problem, we introduce a new within-constituent me"
D15-1169,J07-4004,0,0.176933,", ?i and hdeny, (S NP )/NP, 2, ∅, ?, ?i.2 At the end of the derivation, the dependencies are specified as: hdeny, (S NP)/NP , 1, ∅, he, ARG0i and hdeny, (S NP )/NP, 2, ∅, reports, ARG1i. The preposition p is lexically underspecified for P P arguments, but otherwise ∅. Prepositions are marked lexically on PP /∗ categories, and then propagated, for example: Model Our joint model of CCG and SRL parsing simply involves labelling CCG syntactic dependencies (which are implicit in CCG parses), with SRL roles (or null). This formulation allows us to easily adapt the log-linear CCG parsing model of Clark and Curran (2007) to the joint setting by working with extended dependencies that including syntactic and semantic information. More formally, we can define a notion of consistency to specify the labelling of syntactic dependencies. A set of semantic dependencies π is consistent with a CCG derivation d if each semantic dependency corresponds to a single syntactic dependency—for example, see the dependencies in Figure 3. The CCG derivation in Figure 3 would also be consistent with the SRL dependency ARG2 ARG1 ref use −−−−→he, but not ref use −−−−→reports (because the derivation does not produce the required syn"
D15-1169,D15-1112,0,0.053825,"ine first parses with a supertag-factored A∗ model, and chooses a semantic role for each CCG dependency with a log-linear classifier. The classifier uses the role and attachment features used by the parser. 7.2 Semantic Role Labelling We evaluate our parser as a dependency-based SRL model on PropBank, comparing with CoNLL-2008 systems (Surdeanu et al., 2008). Comparison systems Following Täckström et al. (2015), we compare with the best ‘single parser’ SRL models, which use only a single syntactic parse.3 Much recent work has evaluated using gold predicate identification (Hajiˇc et al., 2009; FitzGerald et al., 2015). This setting is particularly unrealistic for our joint model, where gold predicate identification would be a highly useful feature for the supertagger; we only compare with models that use automatic predicate identification. To the best of our knowledge, the best SRL results with automatic predicate identification were achieved at CoNLL-2008. A number of other models have been evaluated on CoNLL-2008 data. While we cannot compare directly on our metric, the best reported joint model (Johansson, 2009) scored 1.4 points lower than the Che et al. (2008) system we compare to on the CoNLL metric"
D15-1169,W03-1008,0,0.0449953,"th latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is limited by relying on 1-best parses—a problem we resolved with a joint model. A∗ parsing A∗ parsing has previously been explored for less general models than ours. Klein and Manning (2003) and Auli and Lopez (2011b) use A∗ parsing for models with tree-structured dependencies. The best reported speed improvement is parsing 1.2 times faster, whereas we improve by a factor of 5. Our model also allows the more complex graph-structured dependencies required for semantic role labelling. Lewis and Steedman (2014a) demonstrate an efficient A∗ algorithm for C"
D15-1169,J13-4006,0,0.158678,"be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies. Introduction Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Lluís et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Lluís et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A∗ parsing algorithm. Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint models. For example, in the Figure 1, the semantic dependency between He and deny spans three syntactic edges. This fact makes it difficult to jointly parse syntactic and semantic dependencies with dynamic pr"
D15-1169,P10-1022,0,0.266177,"over categories, dependencies, and local rule instantiations. 2 Bold-face is used to highlight the argument of a category corresponding to a dependency. For example (S NP)/NP denotes the first (subject) dependency of a transitive verb. 1446 3.2.1 Supertagging Features Supertagging features φCAT score categories for words. A single feature is used, which is the (unnormalized) score from Lewis and Steedman (2014b)’s supertagging model. The supertagger outputs a distribution over categories for each word independently. The model is trained on supertags extracted from an adaptation of CCGrebank (Honnibal et al., 2010). The adaptation makes minor changes to better match PropBank. 3.2.2 Preposition Features Preposition features φP P score whether the nth argument of the word at index f with category c should take a P P argument headed by preposition p. We use features xf +p, lf +c and lf +c+n+p, where xf is the f th word, and lf is its lemma. 3.2.3 Labelling Features Labelling features φROLE determine whether the nth argument of a word with lemma l, and category c should take a role r. We use n+r, c+n+r, c+n+p+r, l+c+n+p+r, n+r, r, l+p+r, c+n+r, l+r, h+r, where h is an indicator of whether l is hyphenated, a"
D15-1169,N06-2015,0,0.0642944,"for semantic role labelling. Lewis and Steedman (2014a) demonstrate an efficient A∗ algorithm for CCG, but cannot model dependencies. 9 Conclusions and Future Work We have shown that using CCG can allow joint models of syntax and semantics to outperform pipelines, and achieve state-of-the-art results on PropBank SRL. Our new A∗ parsing algorithm is 5 times faster than CKY parsing, without loss of accuracy. Using latent syntax allows us to train the model purely from semantic dependencies, enabling future work to train against other annotations such as FrameNet (Baker et al., 1998), Ontonotes (Hovy et al., 2006) or QASRL (He et al., 2015). The semantic labels provided by PropBank can also be integrated into wide-coverage CCG semantic parsers (Bos, 2008; Lewis and Steedman, 2013) to improve performance on downstream applications. Acknowledgements This research was supported in part by the NSF (IIS-1252835), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We thank Nicholas Fitzgerald, Dan Garrette, Kenton Lee, Swabha Swayamdipta, Mark Yatskar and the anonymous reviewers for their helpful comments on earlier versions of"
D15-1169,W08-2123,0,0.0126626,"formance. Evaluation Metric Comparing fairly with existing work is complicated by the mismatch between heads found by our model and those used in other evaluations. Headedness decisions are often arbitrary—for example, whether a prepositional phrase is headed by the noun or preposition—and different choices were made in the design of CCGbank and the CoNLL-2008 headedness rules. To solve this problem, we introduce a new within-constituent metric, which awards depen3 Model Vickrey Che Zhao Riedel Pipeline Joint The best models use reranking with powerful global features (Toutanova et al., 2008; Johansson and Nugues, 2008) or ensemble methods (Surdeanu et al., 2007; Punyakanok et al., 2008). These techniques have the potential to improve any SRL system, including ours, at some expense in speed. PropBank P R F1 87.3 77.3 82.0 85.3 78.6 81.8 82.4 79.8 81.1 83.6 74.7 78.9 79.2 73.9 76.4 84.8 82.2 83.5 P 74.0 71.1 66.6 69.3 69.3 71.2 Brown R F1 64.5 68.9 65.7 68.0 64.9 65.7 62.7 65.8 64.0 66.1 69.2 70.2 Table 1: Comparison with the best single-parser SRL models on PropBank from CoNLL-2008. The comparison models are Vickrey and Koller (2008), Che et al. (2008), Zhao and Kit (2008) and Riedel and Meza-Ruiz (2008). de"
D15-1169,D09-1059,0,0.77524,"G1 ARG0 ARG0 Figure 2: Dependencies produced by a CCG parse. SRL dependencies can be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies. Introduction Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Lluís et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Lluís et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A∗ parsing algorithm. Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint models. For example, in the Figure 1, the semantic dependency between He and deny spans three syntactic edges. This fa"
D15-1169,N03-1016,0,0.287927,"ical entries that pair syntactic arguments with semantic roles, such as open : S NPARG1 and open : (S NPARG0 )/NPARG1 . Figure 3 shows a detailed trace of how the example from Figure 2 is parsed with our model. We also present a new A∗ algorithm for the joint model. Because CCG is strongly lexicalized, we are able to introduce a new type of extended lexical entries that allows us to factor the model over words and develop effective new upper bounds on the Viterbi outside parse score. A∗ parsing algorithms have previously been developed for models with tree-structured syntactic dependencies (Klein and Manning, 2003; Auli and Lopez, 2011b), and models with no bi-lexical dependencies, including supertag-factored CCGs (Lewis and Steedman, 2014a). We generalize these techniques to SRL-style graph-structured dependencies. Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser. PropBank SRL performance is 1.6 points higher than comparable existing work, and semantic features improve syntactic accuracy by 1.6 points. Our A∗ algorithm is 5 times faster than CKY parsing, with no loss in accuracy. The combination of CCG"
D15-1169,P14-1112,0,0.0217236,"who, like us, score CCG parses based jointly on supertagging and dependency model scores. Decoding their model requires dual-decomposition, to maximize agreement between the separate models. We avoid the need for this technique by using a unigram supertagging model, rather than a sequence model. CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is limited by relying on 1-best parses—a problem we resolved with a joint model. A∗ parsing A∗ parsing has previously been explored for less general models than ours. Klein an"
D15-1169,D10-1119,1,0.806792,"o that of Clark and Curran (2007), but we model SRL dependencies instead of CCG dependencies. The best CCG parsing results were achieved by Auli and Lopez (2011a), who, like us, score CCG parses based jointly on supertagging and dependency model scores. Decoding their model requires dual-decomposition, to maximize agreement between the separate models. We avoid the need for this technique by using a unigram supertagging model, rather than a sequence model. CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is"
D15-1169,D13-1161,1,0.803165,"(2007), but we model SRL dependencies instead of CCG dependencies. The best CCG parsing results were achieved by Auli and Lopez (2011a), who, like us, score CCG parses based jointly on supertagging and dependency model scores. Decoding their model requires dual-decomposition, to maximize agreement between the separate models. We avoid the need for this technique by using a unigram supertagging model, rather than a sequence model. CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is limited by relying on 1-be"
D15-1169,P14-2050,0,0.00686022,"ith lemma l, and category c should take a role r. We use n+r, c+n+r, c+n+p+r, l+c+n+p+r, n+r, r, l+p+r, c+n+r, l+r, h+r, where h is an indicator of whether l is hyphenated, and p is the preposition of P P arguments. 3.2.4 Dependency Features Dependency features φDEP score dependencies, based on the functor index f , argument index a and role r. We use lf +r+la , lf +r+ca , d+r, cf +o +r, ca+o +r, where o is an offset from −3 . . . +3, d is the distance between f and a, ci is a cluster or POS tag for word i, and li is the lemma of word i. Clusters were created from pre-trained word embeddings (Levy and Goldberg (2014)) using k-means, with k = 20, 50, 200, 1000, 2500. These features only apply when the role r 6= ∅. 3.2.5 Derivation Features Derivation features φDERIV score properties of the syntactic derivation. To simplify computation of the upper bounds for the A∗ parsing algorithm given in Section 5, the weights of these features are constrained to be ≤ 0. For simplicity, we only use features for unary rules—for example, a feature records when the unary rule N → NP converts a determiner-less N to a NP . 4 Lexical Factorization In this section, we show how to factor the model into extended lexical entries"
D15-1169,Q13-1015,1,0.88873,"Missing"
D15-1169,D14-1107,1,0.769338,"e 3 shows a detailed trace of how the example from Figure 2 is parsed with our model. We also present a new A∗ algorithm for the joint model. Because CCG is strongly lexicalized, we are able to introduce a new type of extended lexical entries that allows us to factor the model over words and develop effective new upper bounds on the Viterbi outside parse score. A∗ parsing algorithms have previously been developed for models with tree-structured syntactic dependencies (Klein and Manning, 2003; Auli and Lopez, 2011b), and models with no bi-lexical dependencies, including supertag-factored CCGs (Lewis and Steedman, 2014a). We generalize these techniques to SRL-style graph-structured dependencies. Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser. PropBank SRL performance is 1.6 points higher than comparable existing work, and semantic features improve syntactic accuracy by 1.6 points. Our A∗ algorithm is 5 times faster than CKY parsing, with no loss in accuracy. The combination of CCG-based joint modelling and A∗ decoding gives an efficient, accurate, and linguistically principled parser.1 1 The parser is avai"
D15-1169,W09-1212,0,0.0220698,"∅ ARG1 ARG1 ARG0 ARG1 ARG0 ARG0 Figure 2: Dependencies produced by a CCG parse. SRL dependencies can be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies. Introduction Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Lluís et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Lluís et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A∗ parsing algorithm. Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint models. For example, in the Figure 1, the semantic dependency between He and deny spans three syntact"
D15-1169,Q13-1018,0,0.141008,"RL dependencies can be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies. Introduction Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Lluís et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Lluís et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A∗ parsing algorithm. Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint models. For example, in the Figure 1, the semantic dependency between He and deny spans three syntactic edges. This fact makes it difficult to jointly parse syntactic and semantic dep"
D15-1169,D12-1074,0,0.0324104,"Missing"
D15-1169,W08-2121,0,0.0412015,"Missing"
D15-1169,W05-0636,0,0.222725,"NP)/NP conj (S NP)/NP NP ∅ ARG1 ARG1 ARG0 ARG1 ARG0 ARG0 Figure 2: Dependencies produced by a CCG parse. SRL dependencies can be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies. Introduction Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Lluís et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Lluís et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A∗ parsing algorithm. Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint models. For example, in the Figure 1, the semantic dependency between He and deny"
D15-1169,Q15-1003,0,0.026044,"in the same way as Lewis and Steedman (2014a). 1449 7 7.1 Experiments Experimental Setup We used PropBank Section 00 for development, Sections 02-21 for training, and Section 23 for testing. The Pipeline baseline first parses with a supertag-factored A∗ model, and chooses a semantic role for each CCG dependency with a log-linear classifier. The classifier uses the role and attachment features used by the parser. 7.2 Semantic Role Labelling We evaluate our parser as a dependency-based SRL model on PropBank, comparing with CoNLL-2008 systems (Surdeanu et al., 2008). Comparison systems Following Täckström et al. (2015), we compare with the best ‘single parser’ SRL models, which use only a single syntactic parse.3 Much recent work has evaluated using gold predicate identification (Hajiˇc et al., 2009; FitzGerald et al., 2015). This setting is particularly unrealistic for our joint model, where gold predicate identification would be a highly useful feature for the supertagger; we only compare with models that use automatic predicate identification. To the best of our knowledge, the best SRL results with automatic predicate identification were achieved at CoNLL-2008. A number of other models have been evaluate"
D15-1169,J08-2002,0,0.155778,"Missing"
D15-1169,W08-2140,0,0.0133053,"dels use reranking with powerful global features (Toutanova et al., 2008; Johansson and Nugues, 2008) or ensemble methods (Surdeanu et al., 2007; Punyakanok et al., 2008). These techniques have the potential to improve any SRL system, including ours, at some expense in speed. PropBank P R F1 87.3 77.3 82.0 85.3 78.6 81.8 82.4 79.8 81.1 83.6 74.7 78.9 79.2 73.9 76.4 84.8 82.2 83.5 P 74.0 71.1 66.6 69.3 69.3 71.2 Brown R F1 64.5 68.9 65.7 68.0 64.9 65.7 62.7 65.8 64.0 66.1 69.2 70.2 Table 1: Comparison with the best single-parser SRL models on PropBank from CoNLL-2008. The comparison models are Vickrey and Koller (2008), Che et al. (2008), Zhao and Kit (2008) and Riedel and Meza-Ruiz (2008). dencies as correct if they attach anywhere within the original PropBank-annotated argument spans. For example, if the PropBank annotates that the ARG0 of owned is by Google, a dependency to either by or Google is judged correct. We compute new scores for the CoNLL-2008 submissions on our metric, filtering reference and continuation arguments (which are artifacts of the CoNLL conversion of PropBank, but not required by our metric), and nominal predicates based on POS tag. The ranking of the top 5 CoNLL-2008 open-track mod"
D15-1169,P09-1110,1,0.501006,"near model is closely related to that of Clark and Curran (2007), but we model SRL dependencies instead of CCG dependencies. The best CCG parsing results were achieved by Auli and Lopez (2011a), who, like us, score CCG parses based jointly on supertagging and dependency model scores. Decoding their model requires dual-decomposition, to maximize agreement between the separate models. We avoid the need for this technique by using a unigram supertagging model, rather than a sequence model. CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing. They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our results, jointly modelling the semantics lowers their model’s syntactic accuracy. Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al.,"
D15-1169,W08-2127,0,0.0600755,"es (Toutanova et al., 2008; Johansson and Nugues, 2008) or ensemble methods (Surdeanu et al., 2007; Punyakanok et al., 2008). These techniques have the potential to improve any SRL system, including ours, at some expense in speed. PropBank P R F1 87.3 77.3 82.0 85.3 78.6 81.8 82.4 79.8 81.1 83.6 74.7 78.9 79.2 73.9 76.4 84.8 82.2 83.5 P 74.0 71.1 66.6 69.3 69.3 71.2 Brown R F1 64.5 68.9 65.7 68.0 64.9 65.7 62.7 65.8 64.0 66.1 69.2 70.2 Table 1: Comparison with the best single-parser SRL models on PropBank from CoNLL-2008. The comparison models are Vickrey and Koller (2008), Che et al. (2008), Zhao and Kit (2008) and Riedel and Meza-Ruiz (2008). dencies as correct if they attach anywhere within the original PropBank-annotated argument spans. For example, if the PropBank annotates that the ARG0 of owned is by Google, a dependency to either by or Google is judged correct. We compute new scores for the CoNLL-2008 submissions on our metric, filtering reference and continuation arguments (which are artifacts of the CoNLL conversion of PropBank, but not required by our metric), and nominal predicates based on POS tag. The ranking of the top 5 CoNLL-2008 open-track models is identical under our metric and th"
D15-1169,W08-2125,0,0.0128053,"08; Johansson and Nugues, 2008) or ensemble methods (Surdeanu et al., 2007; Punyakanok et al., 2008). These techniques have the potential to improve any SRL system, including ours, at some expense in speed. PropBank P R F1 87.3 77.3 82.0 85.3 78.6 81.8 82.4 79.8 81.1 83.6 74.7 78.9 79.2 73.9 76.4 84.8 82.2 83.5 P 74.0 71.1 66.6 69.3 69.3 71.2 Brown R F1 64.5 68.9 65.7 68.0 64.9 65.7 62.7 65.8 64.0 66.1 69.2 70.2 Table 1: Comparison with the best single-parser SRL models on PropBank from CoNLL-2008. The comparison models are Vickrey and Koller (2008), Che et al. (2008), Zhao and Kit (2008) and Riedel and Meza-Ruiz (2008). dencies as correct if they attach anywhere within the original PropBank-annotated argument spans. For example, if the PropBank annotates that the ARG0 of owned is by Google, a dependency to either by or Google is judged correct. We compute new scores for the CoNLL-2008 submissions on our metric, filtering reference and continuation arguments (which are artifacts of the CoNLL conversion of PropBank, but not required by our metric), and nominal predicates based on POS tag. The ranking of the top 5 CoNLL-2008 open-track models is identical under our metric and the original one (up to statistica"
D15-1169,J08-2005,0,\N,Missing
D15-1169,C98-1013,0,\N,Missing
D15-1169,W09-1201,0,\N,Missing
D15-1169,Q14-1026,1,\N,Missing
D15-1169,D15-1198,1,\N,Missing
D15-1169,D15-1076,1,\N,Missing
D15-1189,S13-2012,0,0.0183845,"Missing"
D15-1189,de-marneffe-etal-2006-generating,0,0.0489017,"Missing"
D15-1189,J12-2003,0,0.186647,"Missing"
D15-1189,W09-3012,0,0.0812391,"d in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem"
D15-1189,W14-3333,0,0.0211697,"tive using our features, and (3) a regression model (P RABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Results Detection Results Figure 5 shows development and test results for detection event mentions.5 We see a small drop in precision and large gains in recall, but a significant increase in F1, primarily 4 Experimental Setup http://tiny.cc/cplex We performed two-sided bootstrap resampling statistical significance tests (Graham et al., 2014). In Figures 5 and 6, asterisks indicate that the difference from the best system is statistically significant (p &lt; 0.05). 5 Baselines For detection, we include a baseline reimplementation of the NAVY T IME (Chambers, 1646 Model Our system NAVY T IME P Dev. R F1 P Test R Error type Missed lexical cue (unseen in training) Missed lexical cue (seen in training) Long distance inference World knowledge & pragmatics Annotation error F1 90.1 90.9 90.5 85.5* 87.8 86.6 84.7* 79.6* 82.1* 87.7 78.3* 82.7* Figure 5: Results for the detection task. Model Dev. MAE r Test MAE r Our system SVR D ISCRETE P RAB"
D15-1189,W09-1401,0,0.0178809,"that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for re"
D15-1189,W04-2705,0,0.0141944,"a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), wh"
D15-1189,D08-1027,0,0.0555857,"Missing"
D15-1189,W15-0812,0,0.0187521,"advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factualit"
D15-1189,P14-2068,0,0.0832869,"ctuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Twitter, and they also crowdsourced factuality annotations to learn regression models. While their approach is similar, we focus on predicting factuality for events that occur in every sentence. Without the restrictions of their task, we must reason about a larger variety of contextual cues. Our method of evaluating annotator agreement (Section 3) is related to the crowdsourcing study by Snow et al. (2008), who showed that pooled non-experts can match or outperform single expert annotators. In contrast, we approximate expert judgments by"
D15-1189,P10-1040,0,0.0156029,"[neg]—h∗i—[xcomp]→h∗i h∗i←[neg]—h∗i—[xcomp]→h∗i Implementation Details The SVM models (NAVY T IME, D ISCRETE, SVR, P RABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features. Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean absolute error (MAE) relative to the gold standard labels and Pearson’s correlation coefficient. While MAE is an intuitive metric that evaluates the absolute fit of the model, Pearson’s r better captures how well a system is able to recover the variation of the annotations. Pearson’s r is also conveniently normalized such that r = 0 for a system that blindly chooses the best a priori output and r = 1 for a system that makes"
D15-1189,S13-2001,0,0.016869,"describing things that the author claims could have happened, and rate each possibility on a scale of -3 (certainly did not happen) to 3 (certainly did). Figure 1 shows that non-expert workers—when their judgments are aggregated—consistently find a wide range of events and recognize the subtle differences in implied factuality. For example, the event set gets a score of 2.6, indicating that it likely but not certainly occurred, since it was ordered, whereas the ordered event, gets a score of 3.0. We gather data for event detection and factuality, reusing sentences from the TempEval-3 corpus (Uzzaman et al., 2013). Our approach produces high-quality labels with modest costs. We also introduce simple but highly effective models for both tasks that outperform strong baselines. In 1643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–1648, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. particular, our factuality regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgm"
D15-1189,J05-1004,0,0.0227027,"regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marne"
D15-1189,C10-2117,0,0.0669716,"studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Tw"
D15-1198,D14-1134,1,0.843581,"esult in z. 6 Learning Learning the two-stage model requires inducing the entries of the CCG lexicon Λ and estimating the parameters θ, which score both stages of the derivation. We assume access to a training set of N examples D = {(xi , zi ) : i = 1 . . . N }, each containing a sentence xi and a logical form zi . This data does not include information about the lexical entries and CCG parsing operations required to construct the correct derivations. We consider all these decisions as latent. The main learning algorithm (Algorithm 1) starts by initializing the lexicon (line 1) and then 3 See Artzi et al. (2014) for a description of this process and how to approximate the partition function in Equation 1. 4 Experiments with loopy belief propagation showed it to be slower and less effective for our task. Algorithm 1 The main learning algorithm. Input: Training set D = {(xi , zi ) : i = 1 . . . N }, number of iterations T , mini-batch size M , seed lexicon Λ0 and learning rate µ. Definitions: S UB(D, i, j) is the set of the next j samples from D starting at i. G EN M AX(x, z, θ, Λ) is the set of viterbi derivations from x with the final result z given parameters θ and lexicon Λ. L EX(d) is the set of l"
D15-1198,Q13-1005,1,0.834772,"r graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperforming the previous state of the art. 1 Introduction Semantic parsers map sentences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington. AMR meaning bank provides a large new corpus that, for the first ti"
D15-1198,W13-2322,0,0.835574,"o formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington. AMR meaning bank provides a large new corpus that, for the first time, enables us to study the problem of grammar induction for broad-coverage semantic parsing. However, it also presents significant challenges for existing algorithms, including much longer sentences, more complex syntactic phenomena and in"
D15-1198,W08-2222,0,0.0494864,"Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG has also been used for broad-coverage recovery of firstorder logic representations (Bos, 2008; Lewis and Steedman, 2013). However, this work lacked corpora to evaluate the logical forms recovered. AMR (Banarescu et al., 2013) is a generalpurpose meaning representation and has been used in a number of applications (Pan et al., 2015; Liu et al., 2015). There is also work on recovering Happy people dance N[x] /N[x] λf.λx.f (x) ∧ ARG1-of(x, A(λc.content-01(c))) N[pl] λp.people(p) SN P[pl] λx.λd.dance-01(d) ∧ARG0(d, x) &gt; N[pl] λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))) N P[pl] A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c)))) S λd.dance-01(d) ∧ ARG0(d, A(λp.people(p) ∧ ARG1-of(x, A(λc"
D15-1198,S13-1045,0,0.0132916,"alternates between expanding the lexicon and updating the parameters. Learning new lexical entries relies on a two-pass process that combines learning the meaning of words and new syntactic structures, and supports learning with and without alignment heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates fr"
D15-1198,P13-2131,0,0.571757,"invertible conversion process between AMRs and lambdacalculus logical forms; roughly speaking, each AMR variable gets its own lambda term, which is scoped as low as possible, and each AMR role becomes a binary predicate applied to these variables. Figure 1 shows an example, and the full details are provided in the supplementary materials. Therefore, henceforth we discuss the task of mapping a sentence x ∈ X to a logical form z ∈ Z, where Z is the set of all logical forms. For example, in Figure 1, we would map the sentence x to the logical form z. We evaluate system performance using S MATCH (Cai and Knight, 2013). Model Given a sentence x and lexicon Λ, we generate the set of possible derivations G EN(x, Λ) using a two-stage process (Section 5). First, we use a weighted CCG to map x to an underspecified logical form u (Section 5.1), a logical form with placeholder constants for unresolved elements. For example, in the underspecified logical form u in Figure 1, the constants REL-of, REL and ID are placeholders. We then resolve 1700 these placeholders by defining a factor graph to find their optimal mapping and generate the final logical form z. In the figure, REL-of is mapped to ARG0-of, REL to ARG2 an"
D15-1198,P12-1045,0,0.0161926,"and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperforming the previous state of the art. 1 Introduction Semantic parsers map sentences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington. AMR meaning bank provides a large new"
D15-1198,J07-4004,0,0.00928219,"onstant, is a disambiguation of Su . Applying M to all constants in u results in the final logical form z. Decomposing the derivation provides two advantages. First, we are able to defer decisions from the CCG parse to the factor graph, thereby considering fewer hypotheses during parsing and simplifying the computation. Second, we can represent distant references while avoiding the complex parse trees that would have been required to represent these dependencies with scoped variables instead of Skolem IDs.2 5.3 Model Given a sentence x, we use a weighted log-linear CCG (Lafferty et al., 2001; Clark and Curran, 2007) to rank the space of possible parses under the grammar Λ. At the root of each CCG derivation is the underspecified logical form u. To represent a probability distribution over M, we build for each u a factor graph Gu = hV, F, Ei, 2 Similar to mention clustering methods for co-reference resolution (Ng, 2010), IDs can be viewed as creating clusters. 1702 (a) CCG parse y: Maps the sentence x to an underspecified logical form u (Section 5.1) with placeholders for unresolved decisions: ID for reference identifiers and the predicates REL and REL-of for unresolved relations. x: Pyongyang N P[sg] A1"
D15-1198,W10-2903,0,0.0433473,"ine a learning procedure (Section 6) that alternates between expanding the lexicon and updating the parameters. Learning new lexical entries relies on a two-pass process that combines learning the meaning of words and new syntactic structures, and supports learning with and without alignment heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer a"
D15-1198,P04-1015,0,0.114585,"logical form z, parameters θ and lexicon Λ, the procedure C OMPUTE G RAD(x, z, θ, Λ) computes the gradient for the sample (x, z). Let D∗ (z) = G EN M AX(x, z, θ, Λ), the set of max-scoring correct derivations. The hard gradient update is: 1 |D∗ (z)| X φ(xi , d) − Ep(d,|xi ;θ,Λ) [φ(xi , d)] , (2) d∈D ∗ (z) where φ(x, d) ∈ Rl is a l-dimensional feature vector (Section 5.3) and the positive portion of the gradient, rather than using expected features, averages over all max-scoring correct derivations. Early updates To generate an effective update when no correct derivation is observed, we follow Collins and Roark (2004) and do an early update if D∗ (z) is empty or if G EN(x, Λ), the set 6 Unlike Kwiatkowski et al. (2010), we also introduce syntactic attributes (e.g., pl, sg) when splitting. 1705 of derivations for x, does not contain a derivation with the correct final logical form z. Given the partial derivations, our gradient computation is identical to Equation 2. However, in contrast to Collins and Roark (2004) our data does not include gold derivations. Therefore, we attempt to identify max-scoring partial derivations that may lead to the correct derivation. We extract sub-expressions from u,7 the under"
D15-1198,P14-1134,0,0.718353,"ning We assume access to a training set of N examples {(xi , zi ) : i = 1 . . . N }, each containing a sentence xi and a logical form zi . Our goal is to learn a CCG, which constitutes learning the lexicon and estimating the parameters of both the grammar and the factor graph. We define a learning procedure (Section 6) that alternates between expanding the lexicon and updating the parameters. Learning new lexical entries relies on a two-pass process that combines learning the meaning of words and new syntactic structures, and supports learning with and without alignment heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semanti"
D15-1198,C10-1053,0,0.0329196,"ences, and leave them in the template. We use backward and forward binary combinators for application, composition and crossing composition. We allow non-crossing composition up to the third order. We also add rules to handle punctuation and unary rules for typeshifting non-adjectives in adjectival positions and verb phrases in adverbial positions. We allow 7 We extract all sub-expressions of type e, he, ti, he, ti, he, tii or he, he, tii from u. shifting of bare plurals, mass nouns and named entities to noun phrases. To avoid spurious ambiguity during parsing, we use normal-form constraints (Hockenmaier and Bisk, 2010). We use five basic lambda calculus types: entity e, truth value t, identifier id, quoted text txt and integer i. Features During CCG parsing, we use indicator features for unary type shifting, crossing composition, lexemes, templates and dynamically generated lexical entries. We also use indicators for co-occurrence of part-of-speech tags and syntactic attributes, repetitions in logical conjunctions and attachments in the logical form. In the factor graph, we use indicator features for control structures, parent-relation-child selectional preferences and for mapping a relation to its final fo"
D15-1198,J07-3004,0,0.0871086,"to do a second pass, where we try to generate new templates to parse the sentence. Second Pass: R EC S PLIT In this pass we try to generate max-scoring derivations in a top-down process. Starting from u, the underspecified form of z, we search for CCG parsing steps that will connect to existing partial derivations in the CKY chart to create a complete parse tree. Since the space of possible operations is extremely large, 5 Named-entity constants are created from name instances when converting from AMR to lambda calculus. See the supplementary material for the exact procedure. we use CCGBank (Hockenmaier and Steedman, 2007) categories to prune, as described below. The second pass is executed by calling R EC S PLIT(x, z, θ, Λ+ ), which returns a set of lexical entries to add to the model (line 10). We recursively apply the splitting operation introduced by Kwiatkowski et al. (2010). Given a CCG category, splitting outputs all possible category pairs that could have originally generated it. For example, given the category SN P ` λy.λd.deny-01(d) ∧ ARG0(d, y) ∧ ARG1(d, A1 (λi.involve-01(i) ∧ ARG1(i, R(ID)))), one of the possible splits will include the categories SN P/N P ` λx.λy.λd.deny-01(d) ∧ ARG0(d, y) ∧ ARG1"
D15-1198,P10-1022,0,0.211098,"Missing"
D15-1198,C12-1083,0,0.0127753,"ntations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington. AMR meaning bank provides a large new corpus that, for the first time, enables us to study the problem of grammar induction for broad-coverage semantic parsing. However, it also presents significant challenges for existing algorithms, including much longer sentences, more complex syntactic phenomena and increased use of noncompositional semantics, such as within-sentence coreference. In this paper, we introduce a new, scalable Combinatory Categorial Grammar (CCG; Steedman, 1996, 2000) induction approach tha"
D15-1198,D12-1069,0,0.0917698,"pports learning with and without alignment heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG"
D15-1198,Q15-1019,0,0.0245967,"Missing"
D15-1198,D13-1161,1,0.851167,"CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperforming the previous state of the art. 1 Introduction Semantic parsers map sentences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington"
D15-1198,E12-1024,1,0.546363,"Missing"
D15-1198,D10-1119,1,0.899397,"find the referent for a pronoun. Although primarily motivated by non-compositional reasoning, we also use this mechanism to underspecify certain relations during parsing, allowing for more effective search. Following most work in semantic parsing, we consider two learning challenges: grammar induction, which assigns meaning representations to words and phrases, and parameter estimation, where we learn a model for combining these pieces to analyze full sentences. We introduce a new CCG grammar induction algorithm which incorporates ideas from previous algorithms (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010) in a way that scales to the longer sentences and more varied syntactic constructions observed in newswire text. During lexical generation (Section 6.1), the algorithm first attempts to use a set of templates to hypothesize new lexical entries. It then attempts to combine bottom-up parsing with top-down recursive splitting to select the best entries and learn new templates for complex syntactic and semantic phenomena, which are re-used in later sentences to hypothesize new entries. Finally, while previous algorithms (e.g., Zettlemoyer and Collins, 2005) have assumed the existence of a grammar"
D15-1198,D11-1140,1,0.927764,"ic meaning. The typing system includes basic types (e.g., entity e, truth value t) and functional types (e.g., he, ti is the type of a function from e to t). In the example category, λx.λd.dance-01(d) ∧ ARG0(d, x) is a he, he, tii-typed function expecting an ARG0 argument, and the conjunction specifies the roles of the dance-01 frame. A CCG is defined by a lexicon and a set of combinators. The lexicon pairs words and phrases with their categorial meaning. For example, dance ` λx.λd.dance-01(d) ∧ ARG0(d, x) pairs dance with the category above. We adopt a factored representation of the lexicon (Kwiatkowski et al., 2011), where entries are dynamically generated by 1701 combining lexemes and templates. For example, the above lexical entry can be generated by pairing the lexeme hdance, {dance-01}i with the template λv1 .[SN P : λx.λa.v1 (a) ∧ ARG0(a, x)]. Skolem Terms and IDs Generalized Skolem terms (henceforth, Skolem terms) for CCG were introduced by Steedman (2011) to capture complex dependencies with relatively local quantification. We define here a simplified version of the theory to represent entities and allow distant references. Let A be a hhe, ti, ei-typed predicate. Given a he, ti-typed logical expr"
D15-1198,Q13-1015,0,0.0201503,"are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG has also been used for broad-coverage recovery of firstorder logic representations (Bos, 2008; Lewis and Steedman, 2013). However, this work lacked corpora to evaluate the logical forms recovered. AMR (Banarescu et al., 2013) is a generalpurpose meaning representation and has been used in a number of applications (Pan et al., 2015; Liu et al., 2015). There is also work on recovering Happy people dance N[x] /N[x] λf.λx.f (x) ∧ ARG1-of(x, A(λc.content-01(c))) N[pl] λp.people(p) SN P[pl] λx.λd.dance-01(d) ∧ARG0(d, x) &gt; N[pl] λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))) N P[pl] A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c)))) S λd.dance-01(d) ∧ ARG0(d, A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))))) &gt; Figure"
D15-1198,D14-1107,0,0.0651395,"Missing"
D15-1198,P11-1060,0,0.692804,"AMR Bank provides a unique opportunity to induce a single model for understanding broad-coverage newswire text and support a wide range of applications. We present a new model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperforming the previous state of the art. 1 Introduction Semantic parsers map sentences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, an"
D15-1198,N15-1114,0,0.218418,"ly query a database (Liang et al., 2011; Kwiatkowski et al., 2013) or instruct a robotic agent (Chen, 2012; Artzi and Zettlemoyer, 2013b). However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives. Recently, AMR (Banarescu et al., 2013) was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015). The ∗ Work done at the University of Washington. AMR meaning bank provides a large new corpus that, for the first time, enables us to study the problem of grammar induction for broad-coverage semantic parsing. However, it also presents significant challenges for existing algorithms, including much longer sentences, more complex syntactic phenomena and increased use of noncompositional semantics, such as within-sentence coreference. In this paper, we introduce a new, scalable Combinatory Categorial Grammar (CCG; Steedman, 1996, 2000) induction approach that solves these challenges with a lear"
D15-1198,P14-5010,0,0.00363596,"Missing"
D15-1198,P10-1142,0,0.00865501,"econd, we can represent distant references while avoiding the complex parse trees that would have been required to represent these dependencies with scoped variables instead of Skolem IDs.2 5.3 Model Given a sentence x, we use a weighted log-linear CCG (Lafferty et al., 2001; Clark and Curran, 2007) to rank the space of possible parses under the grammar Λ. At the root of each CCG derivation is the underspecified logical form u. To represent a probability distribution over M, we build for each u a factor graph Gu = hV, F, Ei, 2 Similar to mention clustering methods for co-reference resolution (Ng, 2010), IDs can be viewed as creating clusters. 1702 (a) CCG parse y: Maps the sentence x to an underspecified logical form u (Section 5.1) with placeholders for unresolved decisions: ID for reference identifiers and the predicates REL and REL-of for unresolved relations. x: Pyongyang N P[sg] A1 (λc.city(c)∧ name(c, A2 (λn.name(n)∧ op(n, PYONGYANG)))) officials denied their involvement N[pl] (N[pl] /N[pl] ) λf.λp.person(p)∧ REL-of(p, A3 (f (λh.have-org-role-91(h)∧ REL(h, A4 (λo.official(o)))))) SN P/N P λx.λy.λd.deny-01(d)∧ ARG0(d, y)∧ ARG1(d, x) N P[pl] R(ID) N[nb] λi.involve-01(i) &lt; u: &gt; &gt; &lt; A A"
D15-1198,N15-1119,0,0.0116251,"(e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG has also been used for broad-coverage recovery of firstorder logic representations (Bos, 2008; Lewis and Steedman, 2013). However, this work lacked corpora to evaluate the logical forms recovered. AMR (Banarescu et al., 2013) is a generalpurpose meaning representation and has been used in a number of applications (Pan et al., 2015; Liu et al., 2015). There is also work on recovering Happy people dance N[x] /N[x] λf.λx.f (x) ∧ ARG1-of(x, A(λc.content-01(c))) N[pl] λp.people(p) SN P[pl] λx.λd.dance-01(d) ∧ARG0(d, x) &gt; N[pl] λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))) N P[pl] A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c)))) S λd.dance-01(d) ∧ ARG0(d, A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))))) &gt; Figure 2: Example CCG tree with three lexical entries, two forward applications (&gt;) and type-shifting of a plural noun to a noun phrase. AMRs, including graph parsing (Flanigan et al., 2014), methods to build AMRs from"
D15-1198,P13-1092,0,0.132331,"k The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG has also been used for broad-coverage recovery of firstorder logic representations"
D15-1198,D14-1048,0,0.0255614,"x] λf.λx.f (x) ∧ ARG1-of(x, A(λc.content-01(c))) N[pl] λp.people(p) SN P[pl] λx.λd.dance-01(d) ∧ARG0(d, x) &gt; N[pl] λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))) N P[pl] A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c)))) S λd.dance-01(d) ∧ ARG0(d, A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))))) &gt; Figure 2: Example CCG tree with three lexical entries, two forward applications (&gt;) and type-shifting of a plural noun to a noun phrase. AMRs, including graph parsing (Flanigan et al., 2014), methods to build AMRs from dependency trees (Wang et al., 2015) and algorithms for aligning words to AMRs (Pourdamghani et al., 2014). 4 Background Combinatory Categorial Grammar CCG is a categorial formalism that provides a transparent interface between syntax and semantics (Steedman, 1996, 2000). Section 7 details our instantiation of CCG. In CCG trees, each node is a category. Figure 2 shows a simple CCG tree. For example, SN P[pl] : λx.λd.dance-01(d) ∧ ARG0(d, x) is a category for an intransitive verb phrase. The syntactic type SN P[pl] indicates that an argument of type N P[pl] is expected and the returned syntactic type will be S. The backward slash  indicates the argument is expected on the left, while a forward s"
D15-1198,W09-1119,0,0.0232004,"Missing"
D15-1198,Q14-1030,0,0.055492,"nt heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either used hand-engineered lexical templates (e.g., Zettlemoyer and Collins, 2005) or learned templates from the data directly (e.g., Kwiatkowski et al., 2010, 2012). Our two-pass reasoning for lexical generation combines ideas from both methods in a way that greatly improves scalability to long, newswire-style sentences. CCG has also been used for broa"
D15-1198,N15-1040,0,0.435019,". There is also work on recovering Happy people dance N[x] /N[x] λf.λx.f (x) ∧ ARG1-of(x, A(λc.content-01(c))) N[pl] λp.people(p) SN P[pl] λx.λd.dance-01(d) ∧ARG0(d, x) &gt; N[pl] λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))) N P[pl] A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c)))) S λd.dance-01(d) ∧ ARG0(d, A(λp.people(p) ∧ ARG1-of(x, A(λc.content-01(c))))) &gt; Figure 2: Example CCG tree with three lexical entries, two forward applications (&gt;) and type-shifting of a plural noun to a noun phrase. AMRs, including graph parsing (Flanigan et al., 2014), methods to build AMRs from dependency trees (Wang et al., 2015) and algorithms for aligning words to AMRs (Pourdamghani et al., 2014). 4 Background Combinatory Categorial Grammar CCG is a categorial formalism that provides a transparent interface between syntax and semantics (Steedman, 1996, 2000). Section 7 details our instantiation of CCG. In CCG trees, each node is a category. Figure 2 shows a simple CCG tree. For example, SN P[pl] : λx.λd.dance-01(d) ∧ ARG0(d, x) is a category for an intransitive verb phrase. The syntactic type SN P[pl] indicates that an argument of type N P[pl] is expected and the returned syntactic type will be S. The backward sla"
D15-1198,P07-1121,0,0.200596,"he parameters of both the grammar and the factor graph. We define a learning procedure (Section 6) that alternates between expanding the lexicon and updating the parameters. Learning new lexical entries relies on a two-pass process that combines learning the meaning of words and new syntactic structures, and supports learning with and without alignment heuristics (e.g., from Flanigan et al., 2014). 3 Related Work The problem of learning semantic parsers has received significant attention. Algorithms have been developed for learning from different forms of supervision, including logical forms (Wong and Mooney, 2007; Muresan, 2011), question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013), sentences paired with demonstrations (Goldwasser and Roth, 2011; Chen and Mooney, 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012, 2015; Reddy et al., 2014) and without explicit semantic supervision (Poon, 2013). Although we are first to consider using CCG to build AMR representations, our work is closely related to existing methods for CCG semantic parsing. Previous CCG induction techniques have either"
D15-1198,D11-1039,1,\N,Missing
D16-1032,D10-1049,0,0.00992082,"hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a soft checklist of what agenda items have been used so f"
D16-1032,D13-1178,0,0.03676,"use of items on the agenda. For example, in the cooking recipe domain, the goal is the recipe title (“pico de gallo” in Fig. 1), and the agenda is the ingredient list (e.g., “lime,” “salt”). For dialogue systems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g =“inform” and E = {name(Hotel Stratford), has internet(no)}, an output text might be x =“Hotel Stratford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1"
D16-1032,H05-1042,0,0.182645,"a new available Et x items new used used items ft 2 E agenda Generate output ft hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vect"
D16-1032,D16-1053,0,0.0101665,"Missing"
D16-1032,D14-1179,0,0.00991178,"Missing"
D16-1032,W14-3348,0,0.00509666,"oes not contain new items. We also report the performance of our checklist model without the additional weak supervision of heuristic ingredient references (- no supervision) (see Sec. 4.6).7 we also evaluate two ablations of our checklist model on the recipe task. First, we remove the linear interpolation and instead use ht as the output (see Sec. 4.2). Second, we remove the previously used item reference model by changing ref -type() to a 2-way classifier between new ingredient references and all other tokens (see Sec. 4.4). Metrics We include commonly used metrics like BLEU-4,8 and METEOR (Denkowski and Lavie, 2014). Because neither of these metrics can measure how well the generated recipe follows the input goal and the agenda, we also define two additional metrics. The first measures the percentage of the agenda items corrected used, while the second measures the number of extraneous items incorrectly introduced. Both these metrics are computed based on simple string match and can miss certain referring expressions (e.g., “meat” to refer to “pork”). Because of the approximate nature of these automated metrics, we also report a human evaluation. 6 Recipe generation results Fig. 1 results for recipe gene"
D16-1032,P16-1014,0,0.0626654,"Missing"
D16-1032,P16-1002,0,0.0169785,"possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention while decoding. Generating recipes was an early task in planning (Hammond, 1986) and generating referring expression research (Dale, 1988). These can be seen as key steps in c"
D16-1032,P09-1011,0,0.296248,"new used used items ft 2 E agenda Generate output ft hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a sof"
D16-1032,D15-1166,0,0.0201846,"Missing"
D16-1032,N16-1086,0,0.0942945,"t dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still left to be said in complete texts. For example, consider"
D16-1032,D15-1044,0,0.0520666,"ipe title (“pico de gallo” in Fig. 1), and the agenda is the ingredient list (e.g., “lime,” “salt”). For dialogue systems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g =“inform” and E = {name(Hotel Stratford), has internet(no)}, an output text might be x =“Hotel Stratford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of"
D16-1032,N15-1020,0,0.0136184,"1: Example checklist recipe generation. A checklist (right dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still lef"
D16-1032,P16-5005,0,0.0158182,"gue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention w"
D16-1032,P16-1008,0,0.0116556,"gue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention w"
D16-1032,D15-1199,0,0.638511,"A checklist (right dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still left to be said in complete texts. For"
D16-1032,N16-1015,0,0.013667,"ford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al"
D16-1032,Q17-1007,0,\N,Missing
D16-1168,P09-1068,0,0.0697666,"essing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs new texts by substituting thematically a"
D16-1168,P11-1020,0,0.0392888,"l requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automati"
D16-1168,D14-1082,0,0.0399284,"Missing"
D16-1168,W14-3348,0,0.0586977,"Missing"
D16-1168,W08-1105,0,0.0318061,"s method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external parap"
D16-1168,P05-1045,0,0.0718412,"Missing"
D16-1168,D11-1108,0,0.0684546,"Missing"
D16-1168,N13-1092,0,0.0381253,"Missing"
D16-1168,S13-1035,0,0.0126664,"structed from fan-authored scripts of the first 10 episodes of the show (Springfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our"
D16-1168,N15-1113,0,0.043938,"Missing"
D16-1168,W11-2123,0,0.00909718,"pringfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematici"
D16-1168,D14-1058,1,0.830597,"iven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad defi"
D16-1168,P16-1084,0,0.0294871,"), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the utility of our work, a"
D16-1168,N15-1022,1,0.807902,"7) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Si"
D16-1168,P13-1151,0,0.0128537,"o stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven"
D16-1168,Q15-1042,1,0.885046,"Missing"
D16-1168,N16-1136,1,0.804369,"2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the u"
D16-1168,P14-1026,1,0.822938,"ckrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as them"
D16-1168,A97-1039,0,0.0925939,"tyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm. Then plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hw"
D16-1168,P14-2050,0,0.0415559,"lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematicity components and does not incorporate the syntactic compatibility score, -S EM replaces the semantic coher5 We set α"
D16-1168,P14-5010,0,0.00577863,"Missing"
D16-1168,P09-1025,0,0.190599,"the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the re"
D16-1168,P10-1158,0,0.294347,"rical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs n"
D16-1168,D15-1118,0,0.0160036,"ch as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic"
D16-1168,N16-1098,0,0.0219292,", discourse relations, and solvability is essential. Previous work mainly focuses on rewriting single sentences. Second, we build a theme from a text corpus and show how the stories can be adapted to new themes. Third, our method leverages the human-authored story to capture the semantic skeleton and the plot of the current story, rather than synthesizing the story plot. To our knowledge, we are the first to introduce a text rewriting formulation for story generation. Story generation has been of long interest to AI researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016). Recent methods in story generation first synthesize candidate plots for a story and then compile those plots into text. Li et al. (2013) use crowdsourcing to build plot graphs. McIntyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events invol"
D16-1168,D15-1202,0,0.0216471,"roduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a"
D16-1168,D16-1117,0,0.0127235,"es take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the"
D16-1168,D15-1044,0,0.029105,"highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (X"
D16-1168,D15-1171,1,0.815083,"ques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the scri"
D16-1168,D15-1135,0,0.0661954,"under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is def"
D16-1168,P08-1040,0,0.0150168,"apata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Z"
D16-1168,D11-1038,0,0.0274259,"izer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and"
D16-1168,P12-1107,0,0.0327745,"Missing"
D16-1168,Q16-1029,0,0.0113683,"), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3"
D16-1168,N10-1056,0,0.0213029,"3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Konc"
D16-1168,D15-1096,0,0.013681,"), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, th"
D16-1168,C10-1152,0,0.0280014,"gh the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are int"
D16-1258,P15-1118,0,0.0186211,"onstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly. Instead of using paraphrases, we build on the approach of QA-SRL (He et al., 2015), which shows that untrained crowd workers can annotate predicate–argument structures by writing question–answer pairs. Our experiments for newswire and biomedical 2337 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2337–2342, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics text demonstrate improvements to parsing accur"
D16-1258,W16-1718,0,0.0295738,"sets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly. Instead of using paraphrases, we build on the approach of QA-SRL (He et al., 2015), which shows that untrained crowd workers can annotate predicate–argument structures by writing question–answer pairs. Our experiments for newswire and biomedical 2337 Proceedings of the 2016 Conference on E"
D16-1258,D15-1076,1,0.727645,"Missing"
D16-1258,J07-3004,0,0.0367443,"Missing"
D16-1258,W10-0702,0,0.0346675,"cus et al., 1993; Petrov and McDonald, 2012). Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly. Instead of using paraphrases, we build on the approach of QA-SRL (He et al., 2015), which shows that untrained crowd workers can annotate predicate–argument structures by writing question–answer pairs. Our experiments for newswire and"
D16-1258,N16-1026,1,0.458144,"Missing"
D16-1258,J93-2004,0,0.0820447,"5 annotators correctly answered Temple, providing a signal that can be used to improve parse predictions. non-experts improve parsing accuracy by answering questions automatically generated from the parser’s output. We develop the approach for CCG parsing, leveraging the link between CCG syntax and semantics to convert uncertain attachment decisions into natural language questions. The answers are used as soft constraints when re-parsing the sentence. Introduction The size of labelled datasets has long been recognized as a bottleneck in the performance of natural language processing systems (Marcus et al., 1993; Petrov and McDonald, 2012). Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010"
D16-1258,C10-1120,0,0.0173151,"ntics to convert uncertain attachment decisions into natural language questions. The answers are used as soft constraints when re-parsing the sentence. Introduction The size of labelled datasets has long been recognized as a bottleneck in the performance of natural language processing systems (Marcus et al., 1993; Petrov and McDonald, 2012). Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, a"
D16-1258,P15-1032,0,0.0800529,"attachment decisions into natural language questions. The answers are used as soft constraints when re-parsing the sentence. Introduction The size of labelled datasets has long been recognized as a bottleneck in the performance of natural language processing systems (Marcus et al., 1993; Petrov and McDonald, 2012). Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015). We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Previous work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the s"
D16-1262,P16-1231,0,0.00980243,"ation of learning and decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum´e III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear. 8 Conclusion We have shown for the first time that a parsing model with global features can be decoded with optimality guarantees. This enables the use of powerful recursive neural networks for parsing without resorting to approximate decoding methods. Experiments show that this approach is effective for CCG parsing, resulting in a new state-of-the-art parser. In future work, we will apply our approach to other structured prediction tasks, where neural networks—and greedy beam search—have become ubiquitous. Acknowledgements"
D16-1262,P11-1158,0,0.0173863,"., 2013)–at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A∗ decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features. By using neural representations and exact search, we improve over their results. A∗ parsing has been previously proposed for lo2374 cally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification exam"
D16-1262,J07-4004,0,0.562414,"Missing"
D16-1262,P04-1015,0,0.0444778,"infeasible in our setting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum´e III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear. 8 Conclusion We have shown for the first time that a parsing model with global features can be decoded with optimality guarantees. This enables the use of powerful recursive neural networks for parsing without resorting to approximate decoding methods. Experiments show that this approach is effective for CCG parsing, resulting in a new state-of-the-art parser. In future work, we will apply our approach to other structured prediction tasks, where neural netw"
D16-1262,W02-1001,0,0.175475,"ls of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce the first global recursive neural parsing approach Finding a model that achieves these A∗ guarantees in practice is a challenging learning problem. Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012). This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time. We instead introduce a new objective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A∗ search agenda, and it penalizes the model whenever the top agenda item is not a part of the gold parse. 2366 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2366–2376, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics explored Fruit fl"
D16-1262,P15-1030,0,0.0251522,", and the time spent decoding relative to A∗ . we obtain n-best lists from the backoff model and rerank each result with the full model. In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner. Results indicate that both approximate methods are less accurate and slower than A∗ . 7 Related Work Many structured prediction problems are based around dynamic programs, which are incompatible with recursive neural networks because of their realvalued latent variables. Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search. Previous work on structured prediction with recursive or recurrent neural models has used beam search–e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)–at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and"
D16-1262,P15-1033,0,0.0430187,"016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure. Introduction Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce the first global recursive neural parsing approach Finding a model that achieves these A∗ guarantees in practice is a challenging learning problem. Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012). This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time. We instead introduce a new objective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A"
D16-1262,J07-3004,0,0.150715,". Initialize forest F 4: A←∅ . Initialize agenda A 5: for e ∈ TAG(x) do 6: PUSH (A, e) ˆ > 0 and SIZE OK(F, A) do 7: while |A ∩ E| ˆ A) > 0 then 8: if v(E, ˆ A)) 9: APPEND (V, v(E, . Record violation 10: emax ← EXTRACT MAX(A) . Pop agenda 11: ADD (F, HEAD (emax )) . Explore hyperedge 12: for e ∈ RULES(F, HEAD(emax ), θ) do 13: PUSH (A, e) . Expand hyperedge 14: return V 15: 16: function LEARN(D) 17: for i = 1 to T do ˆ ∈ D do 18: for x, E ˆ x, θ) 19: V ← VIOLATIONS(E, 20: L ← LOSS(V) 21: θ ← OPTIMIZE(L, θ) 22: return θ 6 Experiments 6.1 Data We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for test. To recover a single gold derivation for each sentence to use during training, we find the right-most branching parse that satisfies the gold dependencies. 6.2 Experimental Setup For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal (e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after t"
D16-1262,N12-1015,0,0.0240033,"Missing"
D16-1262,N03-1016,0,0.253091,"stive exploration (e.g. CKY parsing). However, the model scores can only depend on local properties of a parse. We refer to these models as locally factored models. In contrast, nodes in the parse forest are labeled with entire subtrees, as shown in Figure 1b. For example, there are two nodes spanning the phrase Fruit flies with the same category NP but different internal substructures. While the parse forest requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees. A∗ parsing A∗ parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016). We present a special case of A∗ parsing that is conceptually simpler, since the parse forest constrains each node to be reachable via a unique path. During exploration, we maintain the unique (and therefore highest scoring) path to a hyperedge e, which we define as PATH(e). Similar to the standard A∗ search algorithm, we maintain an agenda A of hyperedges to explore and a forest F of explored nodes that initially contains only the start node ∅. Each hyperedge e in the agenda is sorted by the sum of its inside score g(PATH(e))"
D16-1262,D14-1107,1,0.940872,"KY parsing). However, the model scores can only depend on local properties of a parse. We refer to these models as locally factored models. In contrast, nodes in the parse forest are labeled with entire subtrees, as shown in Figure 1b. For example, there are two nodes spanning the phrase Fruit flies with the same category NP but different internal substructures. While the parse forest requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees. A∗ parsing A∗ parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016). We present a special case of A∗ parsing that is conceptually simpler, since the parse forest constrains each node to be reachable via a unique path. During exploration, we maintain the unique (and therefore highest scoring) path to a hyperedge e, which we define as PATH(e). Similar to the standard A∗ search algorithm, we maintain an agenda A of hyperedges to explore and a forest F of explored nodes that initially contains only the start node ∅. Each hyperedge e in the agenda is sorted by the sum of its inside score g(PATH(e)) and an admissible heurist"
D16-1262,N16-1026,1,0.916835,"th a heuristic that is a bound on the future cost. Generalizing A∗ to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A∗ heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014). Rather than directly replacing local models, we show that they can simply be augmented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero. The global model, in effect, only needs to model the remaining non-local phenomena. In our experiments, we use a recent factored A∗ CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure. Introduction Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer"
D16-1262,P09-1108,0,0.0634156,"reranking (Socher et al., 2013)–at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A∗ decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features. By using neural representations and exact search, we improve over their results. A∗ parsing has been previously proposed for lo2374 cally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampli"
D16-1262,P13-1045,0,0.265318,"train a Tree-LSTM (Tai et al., 2015) to model global structure. Introduction Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce the first global recursive neural parsing approach Finding a model that achieves these A∗ guarantees in practice is a challenging learning problem. Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012). This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time. We instead introduce a new objective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A∗ search agenda, and it penalizes t"
D16-1262,P15-1150,0,0.070971,"to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A∗ heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014). Rather than directly replacing local models, we show that they can simply be augmented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero. The global model, in effect, only needs to model the remaining non-local phenomena. In our experiments, we use a recent factored A∗ CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure. Introduction Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We introduce t"
D16-1262,P10-1040,0,0.0438287,"8.5 309.6 538.5 610.5 Table 3: Ablations of our full model (Global A∗ ) on the development set. Explored refers to the size of the parse forest. Results show the importance of global features and lexical inOur full system is trained with all-violations updates. During training, we lower the forest size limit to 2000 to reduce training times. The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from Turian et al. (2010). Embeddings for categories have 16 dimensions and are randomly initialized. We also apply dropout with a probability of 0.4 at the word embedding layer during training. Since the structure of the neural network is dynamically determined, we do not use mini-batches. The neural networks are implemented using the CNN library,1 and the CCG parser is implemented using the EasySRL library.2 The code is available online.3 6.3 Model Supertag-factored − dynamic program Span-factored − dynamic program Global A∗ − lexical inputs − lexical context Parsing Results Table 2 shows parsing results on the test"
D16-1262,Q16-1014,0,0.0287098,"less efficient and less accurate than optimal A∗ decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features. By using neural representations and exact search, we improve over their results. A∗ parsing has been previously proposed for lo2374 cally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the a"
D16-1262,N16-1027,0,0.0909576,"ng parse that satisfies the gold dependencies. 6.2 Experimental Setup For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal (e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural network. Model C&C C & C + RNN Xu (2016) Vaswani et al. (2016) Supertag-factored Global A∗ Dev F1 83.8 86.3 87.5 87.8 87.5 88.4 Test F1 85.2 87.0 87.8 88.3 88.1 88.7 Table 2: Labeled F1 for CCGbank dependencies on the CCGbank development and test set for our system Global A∗ and the baselines. Baselines We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al. (2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al., 2015); and sup"
D16-1262,D16-1137,0,0.0117218,"d decoding has been shown to be beneficial for structured prediction. S EARN (Daum´e III et al., 2009) and DAG GER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states. We similarly generate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum´e III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear. 8 Conclusion We have shown for the first time that a parsing model with global features can be decoded with optimality guarantees. This enables the use of powerful recursive neural networks for parsing without resorting to approximate decoding methods. Experiments show that this approach is effective for CCG parsing, resulting in a new state-of-the-art parser. In future work, we will apply our approach to other structured prediction tasks, where neural networks—and greedy beam search—have become ubiquitous. Acknowledgements We thank Luheng He, Juli"
D16-1262,D16-1181,0,0.0157672,"st branching parse that satisfies the gold dependencies. 6.2 Experimental Setup For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal (e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural network. Model C&C C & C + RNN Xu (2016) Vaswani et al. (2016) Supertag-factored Global A∗ Dev F1 83.8 86.3 87.5 87.8 87.5 88.4 Test F1 85.2 87.0 87.8 88.3 88.1 88.7 Table 2: Labeled F1 for CCGbank dependencies on the CCGbank development and test set for our system Global A∗ and the baselines. Baselines We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al. (2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark"
D16-1262,D14-1109,0,0.012105,"less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search. Previous work on structured prediction with recursive or recurrent neural models has used beam search–e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)–at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A∗ decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features. By using neural representations and exact search, we improve over their results. A∗ parsing has been previously proposed for lo2374 cally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser. Their use of error"
D17-1018,D08-1031,0,0.0358796,"ains. However, all of these models still use parsers for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most likely configuration produces"
D17-1018,P14-1005,0,0.0201331,"Missing"
D17-1018,P15-1136,0,0.0799141,"nclude highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most likely configuration produces the correct clustering. We use a product of multinomials for each span: P ("
D17-1018,D16-1245,0,0.504908,"ference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. 1 Introduction We present the first state-of-the-art coreference resolution model that is learned end-to-end given only gold mention clusters. All recent coreference models, including neural approaches that achieved impressive performance gains (Wiseman et al., 2016; Clark and Manning, 2016b,a), rely on syntactic parsers, both for head-word features and as the input to carefully hand-engineered mention proposal algorithms. We demonstrate for the first time that these resources are not required, and in fact performance can be improved significantly without them, by training an end-to-end neural model that jointly learns which spans are entity mentions and how to best cluster them. 188 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 Re"
D17-1018,P16-1061,0,0.702335,"ference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. 1 Introduction We present the first state-of-the-art coreference resolution model that is learned end-to-end given only gold mention clusters. All recent coreference models, including neural approaches that achieved impressive performance gains (Wiseman et al., 2016; Clark and Manning, 2016b,a), rely on syntactic parsers, both for head-word features and as the input to carefully hand-engineered mention proposal algorithms. We demonstrate for the first time that these resources are not required, and in fact performance can be improved significantly without them, by training an end-to-end neural model that jointly learns which spans are entity mentions and how to best cluster them. 188 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 Re"
D17-1018,D13-1203,0,0.614912,"ct Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters. It includes a span-ranking model that decides, for each span, which of the previous spans (if any) is a good antecedent. At the core of our model are vector embeddings representing spans of text in the document, which combine context-dependent boundary representations with a head-finding attention mechanism over the span. The attention component is inspired by parser-derived head-word matching features from previous systems (Durrett and Klein, 2013), but is less susceptible to cascading errors. In our analyses, we show empirically that these learned attention weights correlate strongly with traditional headedness definitions. Scoring all span pairs in our end-to-end model is impractical, since the complexity would be quartic in the document length. Therefore we factor the model over unary mention scores and pairwise antecedent scores, both of which are simple functions of the learned span embedding. The unary mention scores are used to prune the space of spans and antecedents, to aggressively reduce the number of pairwise computations. O"
D17-1018,N10-1061,0,0.0211805,"rs for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most likely configuration produces the correct clustering. We use a product of multin"
D17-1018,P10-1040,0,0.146483,"Missing"
D17-1018,N16-1114,0,0.722424,"cedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. 1 Introduction We present the first state-of-the-art coreference resolution model that is learned end-to-end given only gold mention clusters. All recent coreference models, including neural approaches that achieved impressive performance gains (Wiseman et al., 2016; Clark and Manning, 2016b,a), rely on syntactic parsers, both for head-word features and as the input to carefully hand-engineered mention proposal algorithms. We demonstrate for the first time that these resources are not required, and in fact performance can be improved significantly without them, by training an end-to-end neural model that jointly learns which spans are entity mentions and how to best cluster them. 188 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Compu"
D17-1018,P15-1137,0,0.399718,"ralize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most likely configuration produces the correct clustering. We use a product of multinomials for each span: P (y1 , . . . , yN |D) = = N Y i=1 N Y i=1 P (yi |D) P exp(s(i, yi )) 0 y 0 ∈Y(i) exp(s(i, y )) where s(i, j) is a pairwise score for a coreference link between span i and span j in document D. We omit the document D"
D17-1018,Q15-1029,0,0.0268094,"can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most likely configuration produces the correct clustering. We use a product of multinomials for each span: P (y1 , . . . , yN |D) = = N Y i=1 N Y i=1 P (yi |D) P exp(s(i, yi )) 0 y 0 ∈Y(i) exp(s(i, y )) where s(i, j) is a pairwise score for a co"
D17-1018,P10-1142,0,0.00963874,"s Y(i) = {, 1, . . . , i − 1}, a dummy antecedent  and all preceding spans. True antecedents of span i, i.e. span j such that 1 ≤ j ≤ i − 1, represent coreference links between i and j. The dummy antecedent  represents two possible scenarios: (1) the span is not an entity mention or (2) the span is an entity mention but it is not coreferent with any previous span. These decisions implicitly define a final clustering, which can be recovered by grouping all spans that are connected by a set of antecedent predictions. Machine learning methods have a long history in coreference resolution (see Ng (2010) for a detailed survey). However, the learning problem is challenging and, until very recently, handengineered systems built on top of automatically produced parse trees (Raghunathan et al., 2010) outperformed all learning approaches. Durrett and Klein (2013) showed that highly lexical learning approaches reverse this trend, and more recent neural models (Wiseman et al., 2016; Clark and Manning, 2016b,a) have achieved significant performance gains. However, all of these models still use parsers for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems"
D17-1018,C02-1139,0,0.0291838,"ificant performance gains. However, all of these models still use parsers for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first non-pipelined results, while providing further performance gains. More generally, a wide variety of approaches for learning coreference models have been proposed. They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a). Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference. 3 4 Model We aim to learn a conditional probability distribution P (y1 , . . . , yN |D) whose most like"
D17-1018,D14-1162,0,0.0863798,"Missing"
D17-1018,W12-4501,0,0.440709,"63.0 61.5 60.5 60.4 59.6 58.7 58.6 57.5 55.5 54.3 53.9 54.9 53.0 52.3 54.2 52.3 54.4 59.2 58.7 57.7 57.1 56.0 55.7 55.2 55.6 54.3 Table 1: Results on the test set on the English data from the CoNLL-2012 shared task. The final column (Avg. F1) is the main evaluation metric, computed by averaging the F1 of MUC, B3 , and CEAFφ4 . We improve state-of-the-art performance by 1.5 F1 for the single model and by 3.1 F1. 7 Experiments genre, span distance, mention width) are represented as learned 20-dimensional embeddings. We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. This dataset contains 2802 training documents, 343 development documents, and 348 test documents. The training documents contain on average 454 words and a maximum of 4009 words. 7.1 Pruning We prune the spans such that the maximum span width L = 10, the number of spans per word λ = 0.4, and the maximum number of antecedents K = 250. During training, documents are randomly truncated to up to 50 sentences. Learning We use ADAM (Kingma and Ba, 2014) for learning with a minibatch size of 1. The LSTM weights are initialized with random orthonormal matrices as described in Saxe"
D17-1018,D10-1048,0,0.748639,"and j. The dummy antecedent  represents two possible scenarios: (1) the span is not an entity mention or (2) the span is an entity mention but it is not coreferent with any previous span. These decisions implicitly define a final clustering, which can be recovered by grouping all spans that are connected by a set of antecedent predictions. Machine learning methods have a long history in coreference resolution (see Ng (2010) for a detailed survey). However, the learning problem is challenging and, until very recently, handengineered systems built on top of automatically produced parse trees (Raghunathan et al., 2010) outperformed all learning approaches. Durrett and Klein (2013) showed that highly lexical learning approaches reverse this trend, and more recent neural models (Wiseman et al., 2016; Clark and Manning, 2016b,a) have achieved significant performance gains. However, all of these models still use parsers for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains. We present the first"
D17-1018,Q14-1037,0,\N,Missing
D18-1051,P14-2105,0,0.0419482,"bound is loose. The simple questions task involves mapping an English question (e.g. “Who wrote Gulliver’s travels?”) to an analogous Freebase (Bollacker et al., 2008) query, used to answer the question. The query consists of a Freebase relation (e.g. /film/film/story by) and subject (e.g. 090s 0 [gulliver’s travels]). To understand how we might bound performance on the SimpleQuestions dataset, our first contribution in this paper, consider the following examples: 2 Background Single-relation factoid questions (simple questions) are common in many settings (e.g. Microsoft’s search query logs (Yih et al., 2014) and WikiAnswers web questions (Fader et al., 2013)). The SimpleQuestions dataset is one of the most a. who wrote gulliver’s travels? (film/film/story by, 090s 0 [gulliver’s travels, 554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 554–558 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Subject 0btc7 090s 0 06znpjr 02py9bj commonly used benchmarks for studying such questions. The Freebase knowledge graph (KG) provides the facts for answering the questions in the SimpleQuestions dataset. It inclu"
D18-1051,Q17-1010,0,0.0470032,"Missing"
D18-1051,C16-1164,0,0.511747,"Missing"
D18-1051,P16-1076,0,0.362855,"Missing"
D18-1051,P13-1158,1,0.811972,"s mapping an English question (e.g. “Who wrote Gulliver’s travels?”) to an analogous Freebase (Bollacker et al., 2008) query, used to answer the question. The query consists of a Freebase relation (e.g. /film/film/story by) and subject (e.g. 090s 0 [gulliver’s travels]). To understand how we might bound performance on the SimpleQuestions dataset, our first contribution in this paper, consider the following examples: 2 Background Single-relation factoid questions (simple questions) are common in many settings (e.g. Microsoft’s search query logs (Yih et al., 2014) and WikiAnswers web questions (Fader et al., 2013)). The SimpleQuestions dataset is one of the most a. who wrote gulliver’s travels? (film/film/story by, 090s 0 [gulliver’s travels, 554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 554–558 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Subject 0btc7 090s 0 06znpjr 02py9bj commonly used benchmarks for studying such questions. The Freebase knowledge graph (KG) provides the facts for answering the questions in the SimpleQuestions dataset. It includes 3 billion triples of the form (subject, relatio"
D18-1051,D16-1166,0,0.307285,"Missing"
D18-1051,N18-2047,0,0.526395,"Missing"
D18-1051,D14-1162,0,0.0866538,"nal log likelihood loss objective. k candidates are inferred with the top-k Viterbi algorithm. Our model is trained on a dataset of questions each with their corresponding subject alias span delimited with IO tagging. The gold standard subject alias spans are determined by heuristically matching a phrase in the question with a Freebase alias for the subject. All hyperparameters are hand tuned and then a limited set are further tuned with grid search to increase validation accuracy. In total we evaluated at most 100 hyperparameter configurations. The word embeddings are initialized with GloVe (Pennington et al., 2014) and frozen. Adam (Kingma and Ba, 2014), initialized with a learning rate of 0.001, is employed to optimize the model weights. Finally, we halve the learning rate if the validation accuracy has not improved in 3 epochs. Baseline Model Our second main contribution is a baseline that sets a new state-of-the-art performance level, despite using standard methods. Our approach includes (1) a CRF tagger to determine the subject alias, and (2) a BiLSTM to classify the relation. 4.1 Model Details Approach Given a question q (e.g. “who wrote gulliver’s travels?”) our model must predict the correspondin"
D18-1051,D17-1307,0,0.216638,"Missing"
D18-1060,E06-1042,0,0.404381,"e a bidirectional LSTM to encode a sentence, and a feedforward neural network for classification, optimized for the log-likelihood of gold labels. ai = SoftMaxi (Wa hi + ba ) n X c= ai hi Sentence encoding For both sequence labeling and classification, we represent each token xi in the input sentence with a pre-trained word embedding wi . To further encode contextual information, we also concatenate ELMo (Embeddings from Language Models) vectors ei from Peters et al. (2018). These vectors have been shown to be useful for word sense disambiguation, a task closely related to metaphor detection (Birke and Sarkar, 2006). 1 3.1 people’s Figure 2: A classification model for metaphor detection. Only a single word per sentence is labeled as metaphorical or literal. that the prediction for the target verb can be extracted from the full sentence predictions. In addition, as will be shown in Section 5, we find that given accurate annotations for all words in a sentence, the sequence labeling model outperforms the classification model even when the evaluation is set up as a classification task. 3 the i=1 Finally, we feed c to a feedforward network to compute the label scores for target verb. 4 Dataset We evaluate pe"
D18-1060,S16-2003,0,0.148265,"or detection. given a sentence, detecting all of the metaphorical words (independent of their POS tags). We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data. We improve the previous state-of-the-art by 7.5 F1 on the VU Amsterdam Metaphor Corpus (VUA) for the sequence labeling task (Steen et al., 2010), by 2.5 F1 on the VUA verb clasificiation dataset, and by 4.9 F1 on the MOH-X dataset (Mohammad et al., 2016). Our code is publicly available at https://github.com/gao-g/ metaphor-in-context. Introduction Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of"
D18-1060,W18-0911,0,0.172954,"Missing"
D18-1060,W18-0915,0,0.0459541,"Missing"
D18-1060,W18-0916,0,0.116839,"Missing"
D18-1060,E17-2084,0,0.561234,"Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a sequence of binary l"
D18-1060,D16-1220,0,0.110577,"Missing"
D18-1060,D14-1162,0,0.0805731,"Missing"
D18-1060,W13-0907,0,0.175116,"model outperforms the CLS model on detecting personifications, indirect metaphors, and direct metaphors involving uncommon verbs. 6 Related Work There has been significant work on studying different features for metaphor detection, including concretenesss and abstractness (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and im Walde, 2017), imaginability (Broadwell et al., 2013; Strzalkowski et al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), bag-of-words features (K¨oper and im Walde, 2016), and semantic class using WordNet (Hovy et al., 2013; Tsvetkov et al., 2014). More recently, embedding-based approaches (K¨oper and im Walde, 2017; Rei et al., 2017) showed gains on various benchmarks. Many neural models with various features and architectures were introduced in the 2018 VUA Metaphor Detection Shared Task. They include LSTM-based models and CRFs augmented by linguistic features, such as WordNet, POS tags, concreteness score, unigrams, lemmas, verb clusters, 7 Conclusion In this paper, we present simple biLSTM models augmented with contextualized word representation for metaphor detection. Our models establish new state-of-the-a"
D18-1060,N18-1202,1,0.824212,"ese models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text. 1 Table 1: Metaphorical usages of the target word are bold faced, and literal usages are italicized. Full sentence context is crucial for metaphor detection. given a sentence, detecting all of the metaphorical words (independent of their POS tags). We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data. We improve the previous state-of-the-art by 7.5 F1 on the VU Amsterdam Metaphor Corpus (VUA) for the sequence labeling task (Steen et al., 2010), by 2.5 F1 on the VUA verb clasificiation dataset, and by 4.9 F1 on the MOH-X dataset (Mohammad et al., 2016). Our code is publicly available at https://github.com/gao-g/ metaphor-in-context. Introduction Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can actually happen. (Lako"
D18-1060,W18-0908,0,0.101406,"Missing"
D18-1060,P16-1021,0,0.141362,"Missing"
D18-1060,D17-1162,0,0.758662,"actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a"
D18-1060,P16-2017,0,0.515134,"Missing"
D18-1060,N16-1020,0,0.465298,"soning about whether specific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: G"
D18-1060,W14-2302,0,0.421199,"Missing"
D18-1060,N16-1039,0,0.102738,"Missing"
D18-1060,W18-0918,0,0.131137,"Missing"
D18-1060,W17-1903,0,0.473949,"Missing"
D18-1060,W13-0909,0,0.043831,"Missing"
D18-1060,W18-0914,0,0.143054,"Missing"
D18-1060,W15-1404,0,0.497634,"nature of the task. We sampled 257 dev examples that the CLS model gets wrong but the SEQ model gets correct. We found that the SEQ model outperforms the CLS model on detecting personifications, indirect metaphors, and direct metaphors involving uncommon verbs. 6 Related Work There has been significant work on studying different features for metaphor detection, including concretenesss and abstractness (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and im Walde, 2017), imaginability (Broadwell et al., 2013; Strzalkowski et al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), bag-of-words features (K¨oper and im Walde, 2016), and semantic class using WordNet (Hovy et al., 2013; Tsvetkov et al., 2014). More recently, embedding-based approaches (K¨oper and im Walde, 2017; Rei et al., 2017) showed gains on various benchmarks. Many neural models with various features and architectures were introduced in the 2018 VUA Metaphor Detection Shared Task. They include LSTM-based models and CRFs augmented by linguistic features, such as WordNet, POS tags, concreteness score, unigrams, lemmas, verb clusters, 7 Conclusion In this paper, we present simple"
D18-1060,P14-1024,0,0.626133,"Missing"
D18-1060,W13-0906,0,0.151614,"pecific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. ."
D18-1060,D11-1063,0,0.496762,"hich can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a sequence of binary labels l1 , . . . , ln to indicate the metaphoricity of each word. Classification: Given a sentence x1 , . . . , xn and a target verb index i, predict a binary label l to indicate the metaphoricity of the target xi . While both formulations hav"
D18-1060,W18-0913,0,0.113824,"Missing"
D18-1179,P17-1080,0,0.0280108,"studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe s"
D18-1179,P17-1152,0,0.0610817,"Missing"
D18-1179,P18-1198,0,0.058197,"l. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and consider a wider variety of neural architectures in addition to RNNs. Other work has focused on attributing network predictions. Li et al. (2016) examined the impact of erasing portions of a network’s representations on the output, Sundararajan et al. (2017) used a gradi"
D18-1179,N18-1091,0,0.018106,"assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and consider a wider variety of neural architectures in addition to RNNs. Other work has focus"
D18-1179,W18-2501,1,0.827835,"Missing"
D18-1179,P17-1044,1,0.84723,"Missing"
D18-1179,P18-1031,0,0.0261358,"Missing"
D18-1179,P18-1027,0,0.0153787,"al neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and"
D18-1179,P18-1249,0,0.0163139,"yer biLM from that work,1 we also trained a deeper 4-layer model to examine the impact of depth using the publicly available training code.2 To reduce the training time for this large 4-layer model, we reduced the number of parameters in the character encoder by first projecting the character CNN filters down to the model dimension before the two highway layers. 3.2 Transformer The Transformer, introduced by Vaswani et al. (2017), is a feed forward self-attention based architecture. In addition to machine translation, it has also provided strong results for Penn Treebank constituency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). Each identical layer in the encoder first computes a multi-headed attention between a given token and all other tokens in the history, then runs a position wise feed forward network. To adapt the Transformer for bidirectional language modeling, we modified a PyTorch based 1 http://allennlp.org/elmo 2 https://github.com/allenai/bilm-tf re-implementation (Klein et al., 2017)3 to mask out future tokens for the forward language model and previous tokens for the backward language model, in a similar manner to the decoder masking in the original implem"
D18-1179,P17-4012,0,0.0309732,"(2017), is a feed forward self-attention based architecture. In addition to machine translation, it has also provided strong results for Penn Treebank constituency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). Each identical layer in the encoder first computes a multi-headed attention between a given token and all other tokens in the history, then runs a position wise feed forward network. To adapt the Transformer for bidirectional language modeling, we modified a PyTorch based 1 http://allennlp.org/elmo 2 https://github.com/allenai/bilm-tf re-implementation (Klein et al., 2017)3 to mask out future tokens for the forward language model and previous tokens for the backward language model, in a similar manner to the decoder masking in the original implementation. We adopted hyper-parameters from the “base” configuration in Vaswani et al. (2017), providing six layers of 512 dimensional representations for each direction. Concurrent with our work, Radford et al. (2018) trained a large forward Transformer LM and fine tuned it for a variety of NLP tasks. 3.3 Gated CNN Convolutional architectures have also been shown to provide competitive results for sequence modeling incl"
D18-1179,E17-1117,0,0.0457171,"Missing"
D18-1179,D17-1018,1,0.885266,"Missing"
D18-1179,Q16-1037,0,0.0706718,"l. (2018) showed the biLM based representations outperformed CoVe in all considered tasks, we focus exclusively on biLMs. Liu et al. (2018) proposed using densely connected RNNs and layer pruning to speed up the use of context vectors for prediction. As their method is applicable to other architectures, it could also be combined with our approach. Several prior studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions,"
D18-1179,D18-1153,0,0.0169762,"mer focuses heavily on the word embedding layer, xk , and the first contextual layer. In all cases, the maximum layer weights occur below the top layers as the most transferable contextual representations tend to occur in the middle layers, while the top layers specialize for language modeling. 6 Related work In addition to biLM-based representations, McCann et al. (2017) learned contextualized vectors with a neural machine translation system (CoVe). However, as Peters et al. (2018) showed the biLM based representations outperformed CoVe in all considered tasks, we focus exclusively on biLMs. Liu et al. (2018) proposed using densely connected RNNs and layer pruning to speed up the use of context vectors for prediction. As their method is applicable to other architectures, it could also be combined with our approach. Several prior studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that bi"
D18-1179,P18-1110,1,0.805008,"Missing"
D18-1179,J93-2004,0,0.0613187,"Missing"
D18-1179,N18-1101,0,0.0283744,"Missing"
D18-1179,D14-1162,0,0.0875153,"Missing"
D18-1179,P17-1161,1,0.856209,"Missing"
D18-1179,N18-1202,1,0.937893,"fowicz et al., 2016). Similar to Kim et al. (2015), our character-toword encoder is a five-layer sub-module that first embeds single characters with an embedding layer then passes them through 2048 character n-gram CNN filters with max pooling, two highway layers (Srivastava et al., 2015), and a linear projection down to the model dimension. 2.3 Deep contextual word representations After pre-training on a large data set, the internal representations from the biLM can be transferred to a downstream model of interest as contextual word representations. To effectively use all of the biLM layers, Peters et al. (2018) introduced ELMo word representations, whereby all of the layers are combined with a weighted average pooling operPL ation, ELMok = j=0 sj hk,j . The parameters s are optimized as part of the task model so that it may preferentially mix different types of contextual information represented in different layers of the biLM. In Sec. 4 we evaluate the relative effectiveness of ELMo representations from three different biLM architectures vs. pre-trained word vectors in four different state-of-the-art models. 3 Architectures for deep biLMs The primary design choice when training deep biLMs for learn"
D18-1179,W18-5448,0,0.0904006,"STM model.5 Speed ups are relatively faster in the single element batch scenario where the sequential LSTM is most disadvantaged, but are still 2.3-3X for a 64 sentence batch. As the inference speed for the character based word embeddings could be mostly eliminated in a production setting, the table lists timings for both the contextual layers and all layers of the biLM necessary to compute context vectors. We also note that the faster architectures will allow training to scale to large unlabeled corpora, which has been shown to improve the quality of biLM representations for syntactic tasks (Zhang and Bowman, 2018). 4 Evaluation as word representations In this section, we evaluate the quality of the pre-trained biLM representations as ELMo-like contextual word vectors in state-of-the-art mod5 While the CNN and Transformer implementations are reasonably well optimized, the LSTM biLM is not as it does not use an optimized CUDA kernel due to the use of the projection cell. els across a suite of four benchmark NLP tasks. To do so, we ran a series of controlled trials by swapping out pre-trained GloVe vectors (Pennington et al., 2014) for contextualized word vectors from each biLM computed by applying the le"
D18-1179,W12-4501,0,0.136745,"Missing"
D18-1179,W00-0726,0,0.339652,"Missing"
D18-1192,P16-1223,0,0.0224346,"urce code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with short textual descriptions possibly paired with categorical data, whose values need to be copied onto the resulting code from a single domain. In this work, we collect a new dataset of over 100,000 NL and code 1650 pairs, together with the"
D18-1192,P16-1004,0,0.44428,"enerator, we use a specialized neural encoder-decoder model that (a) encodes the NL together with representations based on subword units for environment identifiers (member variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose"
D18-1192,P16-1154,0,0.381918,"ion of a(i) given q (i) and the environment (see Figure 2). We evaluate a number of encoder-decoder models that generate source code derivations from NL and the class environment. Our best model encodes all environment components broken down into subword units (Sennrich et al., 2016) separately, using Bi-LSTMs and decodes these contextual representations to produce a sequence of valid production rules that derive syntactically valid source code. The decoder also uses a two-step attention mechanism to match words in the NL with environment components, and then uses a supervised copy mechanism (Gu et al., 2016a) to incorporate environment elements in the resulting code. We describe this architecture below. 3.1 Encoder The encoder computes contextual representations of the NL and each component in the environment. Each word of the NL, qi , is embedded into a high dimensional space using Identifier matrix I (denoted as qi ) followed by the application of a nlayer bidirectional LSTM (Hochreiter and Schmidhuber, 1997). The hidden states of the last layer (h1 , · · · , hz ) are passed on to the attention layer, while the hidden states at the last token are used to initialize the decoder. place to scalar"
D18-1192,P17-1089,1,0.84755,"ialized neural encoder-decoder model that (a) encodes the NL together with representations based on subword units for environment identifiers (member variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose language (MTG from L"
D18-1192,D17-1160,0,0.0484673,"odels have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with short textual descriptions po"
D18-1192,N13-1103,0,0.111423,"Missing"
D18-1192,J13-2005,0,0.0408214,"Missing"
D18-1192,P16-1057,0,0.238854,"Missing"
D18-1192,D16-1197,0,0.069616,"Missing"
D18-1192,D15-1166,0,0.0709255,"r(nt ), st−1 , snt ) (2) We use an embedding matrix N to embed nt and matrix A to embed at−1 and par(nt ). If at−1 is a rule that generates a terminal node that represents an identifier or literal, it is represented using a special rule IdentifierOrLiteral to collapse all these rules into a single previous rule. Two-step Attention At time step t, the decoder first attends to every token in the NL representation, hi , using the current decoder state, st , to compute a set of attention weights αt , which are used to combine hi into an NL context vector zt . We use a general attention mechanism (Luong et al., 2015), extended to perform multiple steps. exp(sTt Fhi ) αt,i = P exp(sTt Fhi ) Xi zt = αt,i hi i The context vector zt is used to attend over every type (return type) and variable (method) name in the environment, to produce attention weights βt that are used to combine the entire context x = [t : v : r : m] into an environment context vector et .2 exp(ztT Gxj ) βt,j = P T j exp(zt Gxj ) X et = βt,j xj j Finally, ct is computed using the decoder state and both context vectors zt and et : ˆ [st : zt : et ]) ct = tanh(W 2 “:” denotes concatenation. st−1 FormalParameters (nt ) IdentifierNt—>identifie"
D18-1192,P15-1085,0,0.0242859,"ements is the vector to be augmented, and that the method must take in a scalar parameter as the element to be added. The model also needs to disambiguate between the member variables vecElements and weights. Introduction Natural language can be used to define complex computations that reuse the functionality of rich, existing code bases. However, existing approaches for automatically mapping natural language (NL) to executable code have considered limited language or code environments. They either assume fixed code templates (i.e., generate only parts of a method with a predefined structure; Quirk et al., 2015), a fixed context (i.e., generate the body of the same method within a single fixed class; Ling et al., 2016), or no context at all (i.e., generate code tokens from the text alone; Oda et al., 2015). In this paper, we introduce new data and methods for learning to map language to source code within the context of a real-world programming environment, with application to generating member functions from documentation for automatically collected Java class environments. The presence of rich context provided by an existing code environment better approximates the way programmers capitalize on cod"
D18-1192,P17-1105,0,0.0698702,"Neural encoder-decoder models have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with s"
D18-1192,P16-1162,0,0.024094,"ng source code. The environment comprises a list of variables names v (i) 1..|v(i) | and their corresponding types t(i) 1..|t(i) |, as well as method names m(i) 1..|m(i) |and their return types 1 The method parameters and body can be used as well but we leave this to future work. 1644 r(i) 1..|r(i) |. Our goal is to generate the derivation of a(i) given q (i) and the environment (see Figure 2). We evaluate a number of encoder-decoder models that generate source code derivations from NL and the class environment. Our best model encodes all environment components broken down into subword units (Sennrich et al., 2016) separately, using Bi-LSTMs and decodes these contextual representations to produce a sequence of valid production rules that derive syntactically valid source code. The decoder also uses a two-step attention mechanism to match words in the NL with environment components, and then uses a supervised copy mechanism (Gu et al., 2016a) to incorporate environment elements in the resulting code. We describe this architecture below. 3.1 Encoder The encoder computes contextual representations of the NL and each component in the environment. Each word of the NL, qi , is embedded into a high dimensional"
D18-1192,P17-1041,0,0.40752,"r variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose language (MTG from Ling et al. (2016) has 13k examples), contains a larger variety of output code templates than existing datasets built for a specific domain, and is the"
D18-1192,P02-1040,0,0.103067,"Missing"
D18-1192,P11-1060,0,\N,Missing
D18-1241,P18-1078,0,0.0356121,"r all other dialog acts (neither for affirmation and don’t follow up for continuation). Transition matrix We divide the supporting text into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section tex"
D18-1241,D17-1070,0,0.0153231,"ng Naively prepending the previous k questions to the current question did not show gains in initial experiments. We opt instead to simply encode the dialog turn number within the question embedding. 5.4 Results Table 4 summarizes our results (each cell displays dev/test scores), where dialog acts are Yes/No (affirmation) and Follow up (continuation). For comparison to other datasets, we report F1 without filtering low-agreement QA pairs (F1’). Pretrained InferSent To test the importance of lexical matching in our dataset, we output the sentence in s whose pretrained InferSent representation (Conneau et al., 2017) has the highest cosine similarity to that of the question. Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Feature-rich logistic regression We train a logistic regression using Vowpal Wabbit (Langford et al., 2007) to select answer sentences. We use simple matching features (e.g., n-gram overlap between questions and candidate answers), bias features (position and length of a candidate), and con"
D18-1241,H94-1010,0,0.675779,"Missing"
D18-1241,K17-1034,1,0.798034,"ext into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section text by concatenating marker embeddings to the existing word embeddings. Gold sentence + NA To see if can be treated as an answer"
D18-1241,D17-1259,0,0.0131978,"of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine co"
D18-1241,D16-1127,0,0.0402491,"xt (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data c"
D18-1241,P18-2124,1,0.903645,"the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions, we do not follow previous dialogst"
D18-1241,P17-1162,1,0.821322,"short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine comprehension datasets, significantly"
D18-1241,D16-1264,1,0.889651,", who does not see the section text, asks questions. The teacher provides a response in the form of a text span (or No answer ), optionally yes or no ( Yes / No ), and encouragement about continuing a ¯ , or should line of questioning (should, ,→ , could ,→ not 6,→ ask a follow-up question). Wikipedia page), which only the teacher can access. Given just the section’s heading, “Origin & History”, the student aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016)"
D18-1241,P17-1167,1,0.880503,"ible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles o"
D18-1241,P17-1147,1,0.902703,"hese questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions,"
D18-1241,D11-1054,0,0.0542229,"ic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, wh"
D18-1241,D18-1233,0,0.203396,"are the first to incorporate these into information-seeking dialog. Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018), but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions. Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994) and more recently with modern approaches (Saha et al., 2018), but with questions paraphrased from templates. Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing in"
D18-1241,N18-1059,0,0.1584,"aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd"
D18-1412,S07-1018,0,0.0955337,"meNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference. For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material. Frame SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational"
D18-1412,P98-1013,0,0.599303,"ssification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null. In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER. 3 Semantic Role Labeling We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds. The performance of this baseline itself is competitive with state-of-the-art methods (§7). FrameNet. In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements. A frame can be evoked by a word or phrase in a sentence, called a target. Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized). Arguments for a given frame do not overlap. (2) where wc is a parameter vector associated with category c. We sum the log loss terms for all the spans in a sentence to give its loss: X L2 (x, z) = − log p(zi: j |xi: j ). (3) 4.2 5 In the OntoNote"
D18-1412,P17-1110,0,0.015502,"es stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task"
D18-1412,D16-1245,0,0.0211932,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P16-1061,0,0.0192322,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P17-1044,1,0.303465,"e on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models"
D18-1412,P18-1192,0,0.120004,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,copestake-flickinger-2000-open,0,0.0377689,"nformation into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better gen"
D18-1412,S12-1029,1,0.796445,"sible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,yi: j i j−i6D where Z = αn , under the base case α0 = 1. The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1: γ j = max γi−1 exp Ψ(s, x). s=hi, j,yi: j i j−i6D (8) Our model formulation enforces that arguments do not overlap. We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012). 5.3 Input Span Representation This section describes the neural architecture used to obtain the span embedding, vi: j , corresponding to a span xi: j and the target in consideration, t = htstart , tend i. For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used. If there are no verbs, we use the first token in the sentence as a placeholder target. The parameters used to learn v are shared between the tasks. We construct an embedding for the span using"
D18-1412,W06-1673,0,0.0377993,"sting alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyan"
D18-1412,J13-4006,0,0.0237581,"mation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic pars"
D18-1412,P18-1035,0,0.0286481,"ant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebank D2 with sentences x, each with a phrase-structure tree z. 4.1 Loss Each task has an associated loss, and we seek to minimize the combination of task losses, X X L1 (x, y)"
D18-1412,P15-2036,1,0.898483,"SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational model (Rel), they treat the same problem as a span classification problem. Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints. Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5 Notably"
D18-1412,D15-1112,0,0.0644176,"Missing"
D18-1412,D17-1018,1,0.481129,"sks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach"
D18-1412,J02-3001,0,0.475993,"nimizes an auxiliary supervised loss function, derived from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a val"
D18-1412,N18-2108,1,0.84095,"rate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1 . All the above are orthogonal to our approach, and could be incorporated to yield higher gains. 8 Discussion To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline. We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2. Not surpris7 We"
D18-1412,P02-1031,0,0.06036,"g, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost"
D18-1412,Q13-1018,0,0.0760573,"Missing"
D18-1412,N10-1112,1,0.71015,"arameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2). The learner evaluates and adjusts segment scores ψ(sk , x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3). 5.2 Softmax-Margin Objective Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective. In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant X L1 = − (x,s∗ )∈D1 Z(x, s∗ ) = X log exp Ψ(s∗ , x) , Z(x, s∗ ) exp {Ψ(s, x) + cost(s, s∗ )}. (4) (5) s We design the cost function so that it factors by predicted span, in the same way Ψ does: X X cost(s, s∗ ) = cost(s, s∗ ) = I(s &lt; s∗ ). (6) s∈s s∈s The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,y"
D18-1412,D17-1206,0,0.0301315,"ods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic ro"
D18-1412,D13-1152,0,0.0594194,"Missing"
D18-1412,P18-2058,1,0.905144,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,W08-2124,0,0.0676864,"Missing"
D18-1412,D12-1074,0,0.084146,"Missing"
D18-1412,P10-1142,0,0.0212275,"from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never nee"
D18-1412,J05-1004,0,0.492254,"r, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6). Our models use the strongest ava"
D18-1412,P17-1186,1,0.932782,"n As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach, the syntactic scaf"
D18-1412,Q15-1003,0,0.0552312,"Missing"
D18-1412,N18-1135,1,0.842326,"5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2 Clark and Manning (2016a) 79.2 70.4 74.6 66.8 57.0 61.5 71.0 56.5 63.0 69.9 58.0 63.4 62.1 53.9 57.7 63.8 54.3 58.7 63.5 55.5 59.2 64.2 65.3 65.7 Lee et al. (2017) 78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 67.2 + common nonterminals 78.4 74.3 76.3 68.7 62.9 65.7 62.9 60.2 61.5 67.8 Model MUC Prec. Rec. F1 Table 3: Coreference resolution r"
D18-1412,D14-1162,0,0.0809139,"still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture. To the best of our knowledge, such simplified syntactic scaffolds have not been tried before. Word embeddings. Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tas"
D18-1412,N18-1202,1,0.82975,"here is only one task about whose performance we are concerned, denoted T 1 (in this paper, T 1 is either SRL or coreference resolution). We use the term “scaffold” to refer to a second task, T 2 , that can be combined with T 1 during multitask learning. A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T 1 , and after learning is completed, the scaffold is discarded. A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic 1 This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours. structure. It could be defined through a syntactic parser that shares some parameters with T 1 ’s model. Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees. As with multitask learning in general, we do not assume that the same data are annotated with outputs for T 1 and T 2 . In this work, T 2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013). We experiment with three settings: one where the corpus for T 2 does"
D18-1412,W12-4501,0,0.144135,"Missing"
D18-1412,J08-2005,0,0.160547,"jective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We"
D18-1412,D16-1264,0,0.0416351,"d syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for"
D18-1412,J08-2002,0,0.054317,"Missing"
D18-1412,N16-1114,0,0.073824,"pared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation"
D18-1412,D17-1128,0,0.427202,"SRL and coreference). Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization"
D18-1412,P16-1147,0,0.0316632,"raining data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. A"
D18-1412,P15-1109,0,0.217767,"petitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural"
D18-1412,P16-2038,0,0.0317744,"mantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural v"
D18-1412,K16-1019,1,0.877792,"o reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech"
D19-1163,H91-1060,0,0.138762,"tmax this way since we want dummy nodes to contribute a score of 0 to the tree score. To train the model, we again directly minimize the negative log-likelihood of the gold tree T ∗ . The loss function factors into a cross-entropy loss term for each span (i, j) and for each edge in T ∗ . IN:UNSUPPORTED * intents excluded, which contains 28410 training, 4032 development, and 8241 test examples. The main evaluation metric is exact match accuracy: the fraction of predicted parses that exactly match the annotated parses. We also report the labeled bracket F1 score for the parse tree constituents (Black et al., 1991). Training details. We highlight crucial modeling decisions of our models and defer other details to Appendix B. • For a fair comparison, we use the same token and sequence embedders as Gupta et al. (2018): 200-dimensional GloVe embeddings and 2-layer 164-dimensional biLSTMs. We expect contextual embeddings (Peters et al., 2018; Devlin et al., 2018) and a transformerbased sequence embedder (Kitaev and Klein, 2018) to further improve the results, as observed in Einolghozati et al. (2018). Discussion. The joint modeling of T [i, j] and π[i, j] is closely related to the parent annotation techniqu"
D19-1163,D16-1257,0,0.0262264,"e. We evaluate our models on the TOP dataset (Gupta et al., 2018) of compositional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual wor"
D19-1163,D16-1001,0,0.0922186,"maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic unit. Indeed, span-based models have recently shown great promise by achieving state-ofthe-art results for segmentation and tagging (Kong et al., 2016), coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018). For syntactic parsing, span embeddings have been used to score actions in shift-reduce parsing (Cross and Huang, 2016), or to score the existence and labels of tree nodes in bottom-up and top-down parsing (Stern et al., 2017; Kitaev and Klein, 2018). The analysis by Gaddy et al. (2018) shows that the span embeddings can learn to capture various information, such as label correlation and structural agreement, which was traditionally modeled by grammars or lexical features. 3 Setup Given an utterance x = (x0 , . . . , xn−1 ) with n tokens, the task is to predict a TOP parse tree, as illustrated in Figure 1. Each leaf node corresponds to a token xi , while each non-terminal node covers some span (i, j) with toke"
D19-1163,P16-1004,0,0.0668594,"on the TOP dataset (Gupta et al., 2018) of compositional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can"
D19-1163,P15-1030,0,0.0204691,"lated work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic unit. Indeed, span-based models have recently shown great promise by achieving state-ofthe-art results for segmentation and tagging (Kong et al., 2016), coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018). For syntactic"
D19-1163,N16-1024,0,0.292184,"independence assumption and allows the decisions at child nodes to influence the parent node. We evaluate our models on the TOP dataset (Gupta et al., 2018) of compositional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token span"
D19-1163,N18-1091,0,0.181133,"se a span-based semantic parser for parsing utterances into the TOP representation. In its most basic form, the parser embeds each token span (e.g., x2:5 = “John ’s party” in Figure 1) as a vector, and then uses it to predict the labels of the tree nodes covering the span (e.g., SL:DESTINATION and IN:FIND EVENT). While the label prediction is done independently for each span, a CKY decoding algorithm is used to decode a valid tree with the maximum tree score. Our main contributions are twofold. First, we reinterpret the ad-hoc tree score in previous spanbased parsing work (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018) as a joint distribution over the labels. Under this new framework, the loss function factors nicely, which allows us to train the model in a highly parallelized fashion instead of having to run the computation1520 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1520–1526, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ally expensive decoder during training. Second, we introduce edge scores to model the relation"
D19-1163,D18-1300,1,0.911265,"(e.g., IN:GET DIRECTION) and the slots are the parameters needed to complete the task (e.g., SL:DESTINATION). This limited representation typically allows only a single intent per utterance and at most one slot label per token. Dialog systems using such a flat representation would struggle to handle compositional tasks that involve invoking multiple backend services (e.g., “direction to John’s party”: find John’s address, and then find the direction to that address). To support compositional utterances, the hierarchical Task Oriented Parsing (TOP) representation has recently been introduced (Gupta et al., 2018). As illustrated in Figure 1, the TOP representation work done while at Facebook Assistant. party4 T = {(0, 5) : (IN:GET DIRECTION), (2, 5) : (SL:DESTINATION, IN:FIND EVENT), (2, 3) : (SL:ORGANIZER), (4, 5) : (SL:CATEGORY), (0, 1) : ∅, (0, 2) : ∅, (0, 3) : ∅, . . . } Introduction ∗ SL:DESTINATION is a tree where intents and slots are nested alternatively to model composition. The values inside intent and slot subtrees can then be used by downstream dialog modules to invoke appropriate services in a hierarchical fashion. We propose a span-based semantic parser for parsing utterances into the TO"
D19-1163,P18-2058,1,0.853017,"methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic unit. Indeed, span-based models have recently shown great promise by achieving state-ofthe-art results for segmentation and tagging (Kong et al., 2016), coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018). For syntactic parsing, span embeddings have been used to score actions in shift-reduce parsing (Cross and Huang, 2016), or to score the existence and labels of tree nodes in bottom-up and top-down parsing (Stern et al., 2017; Kitaev and Klein, 2018). The analysis by Gaddy et al. (2018) shows that the span embeddings can learn to capture various information, such as label correlation and structural agreement, which was traditionally modeled by grammars or lexical features. 3 Setup Given an utterance x = (x0 , . . . , xn−1 ) with n tokens, the task is to predict a TOP parse tree, as illustrate"
D19-1163,J98-4004,0,0.488296,"ighlight crucial modeling decisions of our models and defer other details to Appendix B. • For a fair comparison, we use the same token and sequence embedders as Gupta et al. (2018): 200-dimensional GloVe embeddings and 2-layer 164-dimensional biLSTMs. We expect contextual embeddings (Peters et al., 2018; Devlin et al., 2018) and a transformerbased sequence embedder (Kitaev and Klein, 2018) to further improve the results, as observed in Einolghozati et al. (2018). Discussion. The joint modeling of T [i, j] and π[i, j] is closely related to the parent annotation technique in syntactic parsing (Johnson, 1998; Klein and Manning, 2003; Petrov et al., 2006), where certain non-terminal labels are split into multiple labels based on their parents in the grammar rule (e.g., VP with parent S becomes VP^S). In our framework, we can view the label and edge scores (Equation 7) as a score over parentannotated labels (T [i, j], π[i, j]): X s(T ) = fa (xi:j , T [i, j], π[i, j]) (8) • The class weight α for empty chains is tuned on the development data: α = 0.4 for the basic model and α = 0.2 for the model with edge scores. We will later demonstrate how different choices of α affect the results. i&lt;j where fa ("
D19-1163,P18-1249,0,0.185506,"ntic parser for parsing utterances into the TOP representation. In its most basic form, the parser embeds each token span (e.g., x2:5 = “John ’s party” in Figure 1) as a vector, and then uses it to predict the labels of the tree nodes covering the span (e.g., SL:DESTINATION and IN:FIND EVENT). While the label prediction is done independently for each span, a CKY decoding algorithm is used to decode a valid tree with the maximum tree score. Our main contributions are twofold. First, we reinterpret the ad-hoc tree score in previous spanbased parsing work (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018) as a joint distribution over the labels. Under this new framework, the loss function factors nicely, which allows us to train the model in a highly parallelized fashion instead of having to run the computation1520 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1520–1526, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ally expensive decoder during training. Second, we introduce edge scores to model the relationship between parent and c"
D19-1163,P03-1054,0,0.0786741,"l modeling decisions of our models and defer other details to Appendix B. • For a fair comparison, we use the same token and sequence embedders as Gupta et al. (2018): 200-dimensional GloVe embeddings and 2-layer 164-dimensional biLSTMs. We expect contextual embeddings (Peters et al., 2018; Devlin et al., 2018) and a transformerbased sequence embedder (Kitaev and Klein, 2018) to further improve the results, as observed in Einolghozati et al. (2018). Discussion. The joint modeling of T [i, j] and π[i, j] is closely related to the parent annotation technique in syntactic parsing (Johnson, 1998; Klein and Manning, 2003; Petrov et al., 2006), where certain non-terminal labels are split into multiple labels based on their parents in the grammar rule (e.g., VP with parent S becomes VP^S). In our framework, we can view the label and edge scores (Equation 7) as a score over parentannotated labels (T [i, j], π[i, j]): X s(T ) = fa (xi:j , T [i, j], π[i, j]) (8) • The class weight α for empty chains is tuned on the development data: α = 0.4 for the basic model and α = 0.2 for the model with edge scores. We will later demonstrate how different choices of α affect the results. i&lt;j where fa (xi:j , ∅, ∅) = 0. From ou"
D19-1163,D17-1160,0,0.0444235,"pta et al., 2018) of compositional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where"
D19-1163,D17-1018,1,0.925782,"ain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic unit. Indeed, span-based models have recently shown great promise by achieving state-ofthe-art results for segmentation and tagging (Kong et al., 2016), coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018). For syntactic parsing, span embeddings have been used to score actions in shift-reduce parsing (Cross and Huang, 2016), or to score the existence and labels of tree nodes in bottom-up and top-down parsing (Stern et al., 2017; Kitaev and Klein, 2018). The analysis by Gaddy et al. (2018) shows that the span embeddings can learn to capture various information, such as label correlation and structural agreement, which was traditionally modeled by grammars or lexical features. 3 Setup Given an utterance x = (x0 , . . . , xn−1 ) with n tokens, the task"
D19-1163,D16-1262,1,0.855161,"neration. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic unit. Indeed, span-based models have recently shown great promise by achieving state-ofthe-art results for segmentation and tagging (Kong et al., 2016), coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018). For syntactic parsing, span embed"
D19-1163,Q17-1004,0,0.0255234,"ption and allows the decisions at child nodes to influence the parent node. We evaluate our models on the TOP dataset (Gupta et al., 2018) of compositional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform predictio"
D19-1163,P06-1055,0,0.164866,"ur models and defer other details to Appendix B. • For a fair comparison, we use the same token and sequence embedders as Gupta et al. (2018): 200-dimensional GloVe embeddings and 2-layer 164-dimensional biLSTMs. We expect contextual embeddings (Peters et al., 2018; Devlin et al., 2018) and a transformerbased sequence embedder (Kitaev and Klein, 2018) to further improve the results, as observed in Einolghozati et al. (2018). Discussion. The joint modeling of T [i, j] and π[i, j] is closely related to the parent annotation technique in syntactic parsing (Johnson, 1998; Klein and Manning, 2003; Petrov et al., 2006), where certain non-terminal labels are split into multiple labels based on their parents in the grammar rule (e.g., VP with parent S becomes VP^S). In our framework, we can view the label and edge scores (Equation 7) as a score over parentannotated labels (T [i, j], π[i, j]): X s(T ) = fa (xi:j , T [i, j], π[i, j]) (8) • The class weight α for empty chains is tuned on the development data: α = 0.4 for the basic model and α = 0.2 for the model with edge scores. We will later demonstrate how different choices of α affect the results. i&lt;j where fa (xi:j , ∅, ∅) = 0. From our preliminary experime"
D19-1163,P17-1076,0,0.357035,"al fashion. We propose a span-based semantic parser for parsing utterances into the TOP representation. In its most basic form, the parser embeds each token span (e.g., x2:5 = “John ’s party” in Figure 1) as a vector, and then uses it to predict the labels of the tree nodes covering the span (e.g., SL:DESTINATION and IN:FIND EVENT). While the label prediction is done independently for each span, a CKY decoding algorithm is used to decode a valid tree with the maximum tree score. Our main contributions are twofold. First, we reinterpret the ad-hoc tree score in previous spanbased parsing work (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018) as a joint distribution over the labels. Under this new framework, the loss function factors nicely, which allows us to train the model in a highly parallelized fashion instead of having to run the computation1520 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1520–1526, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ally expensive decoder during training. Second, we introduce edge scores t"
D19-1163,D18-2002,0,0.025602,"tional utterances on events and navigation domains. Most utterances have nested intents, with some utterances containing intents from different domains. We demonstrate that our model is fast to train and outperforms previous models on the dataset.1 2 Related work Neural tree generation. Previous work on syntactic and semantic parsing employs different strategies for generating trees. Approaches such as transition-based parsers (Dyer et al., 2016; Liu and Zhang, 2017) and top-down tree generation (Vinyals et al., 2015; Choe and Charniak, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017; Yin and Neubig, 2018) frame tree generation as predicting a sequence of actions for generating the tree. The decoding processing generally consists of local decisions, and beam search is used to retain uncertainty. In contrast, global decoding methods (Durrett and Klein, 2015; Lee et al., 2016) can incorporate non-local features and decode a tree with the maximum global score. Span-based models. Span-based models use the embeddings of token spans to perform prediction. By capturing the properties of the whole phrase instead of individual words, span embeddings can be suitable for tasks where phrases are the basic"
D19-1284,P17-1147,1,0.890082,"bers in the reference text. Such weak supervision is attractive because it is relatively easy to gather, allowing for large datasets, but complicates learning because there are many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural La"
D19-1284,P16-1086,0,0.176959,"ets, but complicates learning because there are many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China,"
D19-1284,Q18-1023,0,0.115292,"Missing"
D19-1284,D17-1160,0,0.0174131,"Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al."
D19-1284,P17-1171,1,0.854607,"lin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017) and further run BM25 (Robertson et al., 2009) to retrieve 20 (for train) or 80 (for development and test) paragraphs. We try 10, 20, 40 and 80 paragraphs on the development set to choose the number of paragraphs to use on the test set. To avoid local optima, we perform annealing: at training step t, the model optimizes on MML objective with a probability of min(t/τ , 1) and otherwise use our objective, where τ is a hyperparameter. We observe that the performance is improved by annealing while not being overly sensitive to the hyperparameter τ . We include full hyperparameters and detailed abl"
D19-1284,Q19-1026,0,0.43565,"Missing"
D19-1284,P18-1078,0,0.532296,"many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computatio"
D19-1284,P19-1612,0,0.739327,"ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Tas"
D19-1284,W10-2903,0,0.0201778,"highlight the learning challenge and show that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (I"
D19-1284,P17-1003,0,0.0312862,"3; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al., 2017). 3 Method In this section, we first formally define our general setup, which we will instantiate for specific tasks in Section 4 and then we describe our l"
D19-1284,D13-1160,0,0.213043,"that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 20"
D19-1284,N19-1423,0,0.0764522,"hat answers are likely to appear earlier in the paragraph. Second, while MML achieves comparable result to the First-Only baseline, our learning method outperforms others by 2+ F1/ROUGE-L/EM consistently on all datasets. Lastly, our method achieves the new state-ofthe-art on NARRATIVE QA, T RIVIAQA- OPEN and NATURAL Q UESTIONS - OPEN, and is comparable to the state-of-the-art on T RIVIAQA, despite our aggressive truncation of documents. 5.2 P(zi |Q, D) can be obtained by any model which outputs the start and end positions of the input document. In this work, we use a modified version of BERT (Devlin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017"
D19-1284,P18-1068,0,0.021492,"Missing"
D19-1284,N19-1246,0,0.16032,"on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Task & Dataset Train # Examples Dev Test Avg |Z| Median 1. Multi-mention reading comprehension T RIVIAQA (Joshi et al., 2017) NARRATIVE QA (Koˇcisk`y et al., 2018) T RIVIAQA- OPEN (Joshi et al., 2017) NATURAL Q UESTIONS - OPEN (Kwiatkowski et al., 2019) 61,888 32,747 78,785 79,168 7,993 3,461 8,837 8,757 7,701 10,557 11,313 3,610 2.7 4.3 6.7 1.8 2 5 4 1 46,973 5,850 - 8.2 3 56,355 8,421 15,878 346.1 5 2. Reading comprehension with discrete reasoning DROPnum (Dua et al., 2019) 3. Semantic Parsing W IKI SQL (Zhong et al., 2017) Table 1: Six QA datasets in three different categories used in this paper (detailed in Section 5) along with the size of each dataset. An average and median of the size of precomputed solution sets (denoted by Z) are also reported. Details on how to obtain Z are given in Section 4. the document or an equation to compute the answer). We demonstrate that for many recently introduced tasks, which we group into three categories as given in Table 1, it is relatively easy to precompute a discrete, task-specific set of possible solutions that contai"
D19-1284,J13-2005,0,0.0226519,"g challenge and show that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; L"
D19-1284,P19-1416,1,0.817092,"nd, while MML achieves comparable result to the First-Only baseline, our learning method outperforms others by 2+ F1/ROUGE-L/EM consistently on all datasets. Lastly, our method achieves the new state-ofthe-art on NARRATIVE QA, T RIVIAQA- OPEN and NATURAL Q UESTIONS - OPEN, and is comparable to the state-of-the-art on T RIVIAQA, despite our aggressive truncation of documents. 5.2 P(zi |Q, D) can be obtained by any model which outputs the start and end positions of the input document. In this work, we use a modified version of BERT (Devlin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017) and further run BM25 (Robertson et al., 2009) to retrieve"
D19-1284,P19-1220,0,0.0556534,"Missing"
D19-1284,P17-1167,0,0.0191902,"0; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al., 2017). 3 Method In this section, we first formally define our general setup, which we will instantiate for specific tasks in Section 4 and th"
D19-1284,D16-1264,0,0.333329,"Missing"
D19-1418,P18-1078,1,0.895638,"Missing"
D19-1418,P17-1152,0,0.11431,"Missing"
D19-1418,D18-1241,1,0.846795,"et al., 2019). Some questions in recent multi-hop QA datasets (Yang et al., 2018; Welbl et al., 2018) can be solved by single-hop models (Chen and Durrett, 2019; Min et al., 2019). Additional examples include story completion (Schwartz et al., 2017) and multiple choice questions (Clark et al., 2016, 2018). Recognizing that bias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision. Recent dataset construction protocols have tried to avoid certain kinds of bias. For example, both CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) take steps to prevent annotators from using words that occur in the context passage, VQA 2.0 (Goyal et al., 2018) selects examples to limit the effectiveness of question-only models, and others have filtered examples solvable by simple baselines (Yang et al., 2018; Zhang et al., 2018b; Clark et al., 2018; Zellers et al., 2018). While reducing bias is important, developing ways to prevent models from using known biases will allow us to continue to leverage existing datasets, and update our methods as our understanding of what biases we want to avoid evolve. 4070 Recent work has focused on bias"
D19-1418,D17-1215,0,0.512277,"ural models have shown remarkable results, these achievements have been tempered by the observation that they are often exploiting dataset-specific patterns that do not generalize well to out-of-domain or adversarial settings. For example, entailment models trained on MNLI (Bowman et al., 2015) will guess an answer based solely on the presence of particular keywords (Gururangan et al., 2018) or whether sentences pairs contain the same words (McCoy et al., 2019), while QA models trained on SQuAD (Rajpurkar et al., 2016) tend to select text near question-words as answers, regardless of context (Jia and Liang, 2017). We refer to these kinds of superficial patterns as bias. Models that rely on bias can perform well on in-domain data, but are brittle and easy to fool (e.g., SQuAD models are easily distracted by irrelevant sentences that contain many question words). Recent concern about dataset bias has led researchers to re-examine many popular datasets, resulting in the discovery of a wide variety of biases (Agrawal et al., 2018; Anand et al., 2018; Min et al., 2019; Schwartz et al., 2017). In this paper, we build on these works by showing that, once a dataset bias has been identified, we can improve the"
D19-1418,P17-1147,1,0.91631,"y model. Since the bias-only model will have already captured the target pattern, the robust model has no incentive to learn it, and thus does better on test data where the pattern is not reliable. 2019; Jia and Liang, 2017), which were designed to break models that adopt superficial strategies on well known textual entailment (Bowman et al., 2015), reading comprehension (Rajpurkar et al., 2016), and VQA (Antol et al., 2015) datasets. We additionally construct a new QA challenge dataset, TriviaQA-CP (for TriviaQA changing priors). This dataset was built by holding out questions from TriviaQA (Joshi et al., 2017) that ask about particular kinds of entities from the train set, and evaluating on those questions in the dev set, in order to challenge models to generalize between different types of questions. We are able to improve out-of-domain performance in all settings, including a 6 and 9 point gain on the two QA datasets. On the VQA challenge set, we achieve a 12 point gain, compared to a 3 point gain from prior work. In general, we find using an ensembling method that can dynamically choose when to trust the bias-only model is the most effective, and we present synthetic experiments and qualitative"
D19-1418,P19-1416,1,0.922634,"9), while QA models trained on SQuAD (Rajpurkar et al., 2016) tend to select text near question-words as answers, regardless of context (Jia and Liang, 2017). We refer to these kinds of superficial patterns as bias. Models that rely on bias can perform well on in-domain data, but are brittle and easy to fool (e.g., SQuAD models are easily distracted by irrelevant sentences that contain many question words). Recent concern about dataset bias has led researchers to re-examine many popular datasets, resulting in the discovery of a wide variety of biases (Agrawal et al., 2018; Anand et al., 2018; Min et al., 2019; Schwartz et al., 2017). In this paper, we build on these works by showing that, once a dataset bias has been identified, we can improve the out-of-domain performance of models by preventing them from making use of that bias. To do this, we use the fact that these biases can often be explicitly modelled with simple, constrained baseline methods to factor them out of a final model through ensemble-based training. Our method has two stages. First, we build a bias-only model designed to capture a naive solution that performs well on the training data, but generalizes poorly to out-of-domain sett"
D19-1418,Q19-1016,0,0.0231273,"Anand et al., 2018; Caglayan et al., 2019). Some questions in recent multi-hop QA datasets (Yang et al., 2018; Welbl et al., 2018) can be solved by single-hop models (Chen and Durrett, 2019; Min et al., 2019). Additional examples include story completion (Schwartz et al., 2017) and multiple choice questions (Clark et al., 2016, 2018). Recognizing that bias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision. Recent dataset construction protocols have tried to avoid certain kinds of bias. For example, both CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) take steps to prevent annotators from using words that occur in the context passage, VQA 2.0 (Goyal et al., 2018) selects examples to limit the effectiveness of question-only models, and others have filtered examples solvable by simple baselines (Yang et al., 2018; Zhang et al., 2018b; Clark et al., 2018; Zellers et al., 2018). While reducing bias is important, developing ways to prevent models from using known biases will allow us to continue to leverage existing datasets, and update our methods as our understanding of what biases we want to avoid evolve. 4070 Re"
D19-1418,K17-1004,0,0.299589,"s trained on SQuAD (Rajpurkar et al., 2016) tend to select text near question-words as answers, regardless of context (Jia and Liang, 2017). We refer to these kinds of superficial patterns as bias. Models that rely on bias can perform well on in-domain data, but are brittle and easy to fool (e.g., SQuAD models are easily distracted by irrelevant sentences that contain many question words). Recent concern about dataset bias has led researchers to re-examine many popular datasets, resulting in the discovery of a wide variety of biases (Agrawal et al., 2018; Anand et al., 2018; Min et al., 2019; Schwartz et al., 2017). In this paper, we build on these works by showing that, once a dataset bias has been identified, we can improve the out-of-domain performance of models by preventing them from making use of that bias. To do this, we use the fact that these biases can often be explicitly modelled with simple, constrained baseline methods to factor them out of a final model through ensemble-based training. Our method has two stages. First, we build a bias-only model designed to capture a naive solution that performs well on the training data, but generalizes poorly to out-of-domain settings. Next, we train a s"
D19-1418,P05-1003,0,0.0574702,"constrained baseline methods to factor them out of a final model through ensemble-based training. Our method has two stages. First, we build a bias-only model designed to capture a naive solution that performs well on the training data, but generalizes poorly to out-of-domain settings. Next, we train a second model in an ensemble with the pre-trained bias-only model, which incentivizes the second model to learn an alternative strategy, and use the second model alone on the test set. We explore several different ensembling methods, building on product-of-expert style approaches (Hinton, 2002; Smith et al., 2005). Figure 1 shows an example of applying this procedure to prevent a visual question answering (VQA) model from guessing answers because they are typical for the question, a flaw observed in VQA models (Goyal et al., 2018; Agrawal et al., 2018). We evaluate our approach on a diverse set of tasks, all of which require models to overcome a challenging domain-shift between the train and test data. First, we build a set of synthetic datasets that contain manually constructed biases by adding artificial features to MNLI. We then consider three challenge datasets proposed by prior work (Agrawal et al"
D19-1418,N18-2091,0,0.0300267,"mble from ignoring bi , we add an entropy penalty to the loss: R = wH(softmax (g(xi ) log(bi ))) P Where H(z) = − j zj log(zj ) is the entropy and w is a hyperparameter. Penalizing the entropy encourages the bias component to be non-uniform, and thus have a greater impact on the ensemble. 4 Evaluation Methodology We evaluate our methods on several datasets that have out-of-domain test sets. Some of these tasks, such as HANS (McCoy et al., 2019) or Adversarial SQuAD (Jia and Liang, 2017), can be solved easily by generating additional training examples similar to the ones in the test set (e.g., Wang and Bansal (2018)). We, instead, demonstrate that it is possible to improve performance on these tasks by exploiting knowledge of general, biased strategies the model is likely to adopt. Our evaluation setup consists of a training set, an out-of-domain test set, a bias-only model, and a main model. To run an evaluation we train the bias-only model on the train set, train the main model on the train set while employing one of the methods in Section 3, and evaluate the main model on the out-of-domain test set. We also report performance on the in-domain test set, when available. We use models that are known to w"
D19-1418,Q18-1021,0,0.0265396,"is the most effective, and we present synthetic experiments and qualitative analysis to illustrate the advantages of that approach. We release our datasets and code to facilitate future work.1 2 Related Work Researchers have raised concerns about bias in many datasets. For example, many joint natu1 github.com/chrisc36/debias ral language processing and vision datasets can be partially solved by models that ignore the vision aspect of the task (Jabri et al., 2016; Zhang et al., 2016; Anand et al., 2018; Caglayan et al., 2019). Some questions in recent multi-hop QA datasets (Yang et al., 2018; Welbl et al., 2018) can be solved by single-hop models (Chen and Durrett, 2019; Min et al., 2019). Additional examples include story completion (Schwartz et al., 2017) and multiple choice questions (Clark et al., 2016, 2018). Recognizing that bias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision. Recent dataset construction protocols have tried to avoid certain kinds of bias. For example, both CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) take steps to prevent annotators from using words that occur in the context passag"
D19-1418,D18-1259,0,0.0251315,"the bias-only model is the most effective, and we present synthetic experiments and qualitative analysis to illustrate the advantages of that approach. We release our datasets and code to facilitate future work.1 2 Related Work Researchers have raised concerns about bias in many datasets. For example, many joint natu1 github.com/chrisc36/debias ral language processing and vision datasets can be partially solved by models that ignore the vision aspect of the task (Jabri et al., 2016; Zhang et al., 2016; Anand et al., 2018; Caglayan et al., 2019). Some questions in recent multi-hop QA datasets (Yang et al., 2018; Welbl et al., 2018) can be solved by single-hop models (Chen and Durrett, 2019; Min et al., 2019). Additional examples include story completion (Schwartz et al., 2017) and multiple choice questions (Clark et al., 2016, 2018). Recognizing that bias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision. Recent dataset construction protocols have tried to avoid certain kinds of bias. For example, both CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) take steps to prevent annotators from using words that occur"
D19-1418,D18-1009,0,0.0300625,"ias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision. Recent dataset construction protocols have tried to avoid certain kinds of bias. For example, both CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) take steps to prevent annotators from using words that occur in the context passage, VQA 2.0 (Goyal et al., 2018) selects examples to limit the effectiveness of question-only models, and others have filtered examples solvable by simple baselines (Yang et al., 2018; Zhang et al., 2018b; Clark et al., 2018; Zellers et al., 2018). While reducing bias is important, developing ways to prevent models from using known biases will allow us to continue to leverage existing datasets, and update our methods as our understanding of what biases we want to avoid evolve. 4070 Recent work has focused on biases that come from ignoring parts of the input (e.g., guessing the answer to a question before seeing the evidence). Solutions include generative objectives to force models to understand all the input (Lewis and Fan, 2019), carefully designed model architecture (Agrawal et al., 2018; Zhang et al., 2016), or adversarial removal o"
D19-1418,D17-1323,1,0.736435,"om model’s internal representations (Ramakrishnan et al., 2018; Zhang et al., 2018a; Belinkov et al., 2019; Grand and Belinkov, 2019). In contrast, we consider biases beyond partial-input cases (Feng et al., 2019), and show our method is superior on VQA-CP. Concurrently, He et al. (2019) also suggested using a product-of-experts ensemble to train unbiased models, but we consider a wider variety of ensembling approaches and test on additional domains. A related task is preventing models from using particular problematic dataset features, which is often studied from the perspective of fairness (Zhao et al., 2017; Burns et al., 2018). A popular approach is to use an adversary to remove information about a target feature, often gender or ethnicity, from a model’s internal representations (Edwards and Storkey, 2016; Wang et al., 2018; Kim et al., 2019). In contrast, the biases we consider are related to features that are essential to the overall task, so they cannot simply be ignored. Evaluating models on out-of-domain examples built by applying minor perturbations to existing examples has also been the subject of recent study (Szegedy et al., 2014; Belinkov and Bisk, 2018; Carlini and Wagner, 2018; Glo"
D19-1539,I05-5002,0,0.0272817,"Missing"
D19-1539,W07-1401,0,0.115964,"Missing"
D19-1539,L18-1550,0,0.0519103,"Missing"
D19-1539,P18-1249,0,0.0230217,"79.3 80.4 Table 5: Different loss functions on the development sets of GLUE (cf. Table 2). Results are based on the CNN base model (Table 1) CRF with 1E-03 and pretrained language model with 1E-05 gave us the best result. Table 3 shows the results, with comparison to previous published ELMoBASE results (Peters et al., 2018) and the BERT models. Both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain. Constituency Parsing We also report parseval F1 for Penn Treebank constituency parsing. We adopted the current state-ofthe-art architecture (Kitaev and Klein, 2018). We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder. Table 4 shows the results. Here, fine tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings. 6.3 Average GLUE score Avg. GLUE score 6.2.2 81.5 Objective functions for pretraining The two-tower model is trained to predict the current token given representations of the entire left and right context (cloze). Next we compare this choice to two alternative"
D19-1539,K16-1006,0,0.0373645,"combined to predict center words. BERT is also a transformer encoder that has access to the entire input but this choice requires a special training regime. In particular, they multi-task between predicting a subset of masked input tokens, similar to a denoising autoencoder, and a next sentence prediction task. In comparison, we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens. We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset. Melamud et al. (2016) follow a similar approach to ours by predicting the center word but their architecture is based on LSTMs and we include the center word when we actually fine-tune on downstream tasks. BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence-pairs with input markers that distinguish between tokens of the two sentences. Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task duri"
D19-1539,N19-4009,1,0.87296,"Missing"
D19-1539,W18-6301,1,0.836844,"erated gradient method (Sutskever et al., 2013) with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 (Pascanu et al., 2013). The learning rate is linearly warmed up from 10−7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 (Loshchilov and Hutter, 2016). We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband. We also use the NCCL2 library and the torch.distributed package for interGPU communication. We train models with 16bit floating point precision, following Ott et al. (2018). The BPE model trains much faster than the character CNN models (Table 1). 6 6.1 Results GLUE First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in Wang et al. (2018). There are two singlesentence classification tasks: First, the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is a binary task to judge sentence grammaticality; evaluation is in terms of the Matthews correlation coefficient (mcc). Second, the Stanford Sentiment Treebank (SST-2"
D19-1539,N18-1202,1,0.936093,"s ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective. 1 comb Introduction Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018). However, existing work has either used unidirectional (left-to-right) language models (LMs) (Radford et al., 2018) or bi-directional (both left-to-right and right-to-left) LMs (BiLMs) where each direction is trained with an independent loss function (Peters et al., 2018). In this paper, we show that even larger performance gains are possible by jointly pretraining both directions of a large language-model-inspired self-attention cloze model. Our bi-directional transformer architecture predicts every token in the training data (Figure 1). We achieve this by introducing"
D19-1539,E17-2025,0,0.022973,"96 4096 12 32 32 Query formation (final layer) Sum Concat Concat Train time (days) 6 10 4.5 Table 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as measured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model. 1024, followed by a 160K band with dimensionality 256. The remaining types have dimensionality 64; there are 480K types for the small model and 780K for the large model. The BPE model uses a vocabulary of 55K types and we share input and output embeddings in a flat softmax with dimension 1024 (Inan et al., 2016; Press and Wolf, 2017). The BPE vocabulary was constructed by applying 30K merge operations over the training data, then applying the BPE code to the training data and retaining all types occurring at least three times. Every setup uses model dimensionaltiy d = 1024 with H = 16 attention heads for all but the final attention layer. Model based on character inputs use character embedding size 128 and we apply six filters of size 1x128, 2x256, 3x384, 4x512, 5x512, 6x512 followed by a single highway layer. The models are trained with model and attention dropout rate of 0.1 and ReLU dropout rate of 0.05. Different to V"
D19-1539,P16-1162,0,0.0572388,"pus + Wikipedia. This is similar to the training data used by BERT which comprises the BooksCorpus (Zhu et al., 2015) of about 800M words plus English Wikipedia data of 2.5B words. 5.2 Pretraining hyper-parameters We adapt the transformer implementation available in the fairseq toolkit to our two tower architecture (Ott et al., 2019). For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018). Our experiments consider three model sizes shown in Table 1: There are two CNN input models in a base and large configuration as well as a Byte-Pair-Encoding based model (BPE; Sennrich et al., 2016). The CNN models have unconstrained input vocabulary, and an output vocabulary limited to 1M most common types for the large model, and 700K most common types for the base model. CNN models use an adaptive softmax in the output: the head band contains the 60K most frequent types with dimensionality 5363 Model CNN Base CNN Large BPE Large Parameters Updates Blocks FFN Dim Attn Heads (final layer) 177M 330M 370M 600K 1M 1M 6 12 12 4096 4096 4096 12 32 32 Query formation (final layer) Sum Concat Concat Train time (days) 6 10 4.5 Table 1: Hyper-parameters for our models. Parameter count excludes t"
D19-1539,D13-1170,0,0.00600016,"The BPE model trains much faster than the character CNN models (Table 1). 6 6.1 Results GLUE First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in Wang et al. (2018). There are two singlesentence classification tasks: First, the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is a binary task to judge sentence grammaticality; evaluation is in terms of the Matthews correlation coefficient (mcc). Second, the Stanford Sentiment Treebank (SST-2; Socher et al., 2013) requires to judge if movie reviews have positive or negative sentiment; evaluation is in terms of accuracy (acc). There are three tasks assessing sentence similarity: The Microsoft Research Paragraph Corpus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we evaluate in terms of F1. The Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefficient (scc). Finally, there are four natural language inference tasks: the Multi-Genre Natural Langu"
D19-1539,W18-5446,0,0.0427983,"Missing"
D19-1539,N18-1101,0,0.0438876,"movie reviews have positive or negative sentiment; evaluation is in terms of accuracy (acc). There are three tasks assessing sentence similarity: The Microsoft Research Paragraph Corpus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we evaluate in terms of F1. The Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefficient (scc). Finally, there are four natural language inference tasks: the Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018), the Stanford Question Answering Dataset (QNLI; Rajpurkar et al., 2016), the Recognizing Textual Entailment (RTE; Dagan et al., 2006, Bar Haim et al., 2006, Ciampiccolo et al., 2007 Bentivogli et al., 2009). We exclude the Winograd NLI task from our results similar to Radford et al. (2018); Devlin et al. (2018) and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test. We also report an average over the GLUE metrics. This figure is not comparable to the average on the official GLUE leaderboard since we exclude Winograd and do not report MRPC accuracy 5364 C"
D19-1539,W18-5448,0,0.0209368,"lly (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately. 2 Related work There has been much recent work on learning sentence-specific representations for language understanding tasks. McCann et al. (2017) learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks. Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data (Zhang and Bowman, 2018). Our work was inspired by ELMo (Peters et al., 2018) and the generative pretraining (GPT) approach of Radford et al. (2018). ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task. GPT relies on a left to right language model and an added projection layer for each downstream task without a task-specific model. Our approach mostly follows GPT, though we show that our model also works well with an ELMo module on NER and constitue"
D19-1539,D16-1264,0,\N,Missing
D19-1539,S17-2001,0,\N,Missing
D19-1539,W18-6401,0,\N,Missing
D19-1539,N19-1423,0,\N,Missing
D19-1545,P16-1004,0,0.0345552,"to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5× the number of training examples). On the second task, i.e., mapping NL utterances into SQL queries for a flight information database (ATIS-SQL; Iyer et al. (2017)), using idioms significantly improves denotational accuracy over SOTA models, when a limited amount of training data is used, and also marginally outperforms the SOTA when the full training set is used (more details in Section 7). 2 Related Work Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017, 2018). Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Iyer"
D19-1545,P18-1068,0,0.0294317,"2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Iyer et al. (2018) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation. We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time. Another different but related method to produce source code is using sketches, which are code snippets containing slots in the place of low-level information such as variable names, method arguments, and literals. Dong and Lapata (2018) generate such sketches using programming languagespecific sketch creation rules and use them as intermediate representations to train token-based seq2seq models that convert NL to logical forms. Hayati et al. (2018) retrieve sketches from a large training corpus and modify them for the current input; Murali et al. (2018) use a combination of neural learning and type-guided combinatorial search to convert existing sketches into executable programs, whereas Nye et al. (2019) additionally also generate the sketches before synthesising programs. Our idiom-based decoder learns to produce commonly"
D19-1545,P18-1033,0,0.145345,"Missing"
D19-1545,D18-1111,0,0.0112944,"al encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time. Another different but related method to produce source code is using sketches, which are code snippets containing slots in the place of low-level information such as variable names, method arguments, and literals. Dong and Lapata (2018) generate such sketches using programming languagespecific sketch creation rules and use them as intermediate representations to train token-based seq2seq models that convert NL to logical forms. Hayati et al. (2018) retrieve sketches from a large training corpus and modify them for the current input; Murali et al. (2018) use a combination of neural learning and type-guided combinatorial search to convert existing sketches into executable programs, whereas Nye et al. (2019) additionally also generate the sketches before synthesising programs. Our idiom-based decoder learns to produce commonly used subtrees of programming syntaxtrees in one decoding step, where the non-terminal leaves function as slots that can be subsequently expanded in a grammer-aware fashion. Code idioms can be roughly viewed as a tree"
D19-1545,P17-1089,1,0.942621,"n When programmers translate Natural Language (NL) specifications into executable source code, they typically start with a high-level plan of the major structures required, such as nested loops, conditionals, etc. and then proceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b)) as code idioms (Allamanis and Sutton, 2014). In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016). State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the target programming language grammar (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps 5426 Proceedings of the 201"
D19-1545,D18-1192,1,0.181241,"roceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b)) as code idioms (Allamanis and Sutton, 2014). In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016). State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the target programming language grammar (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps 5426 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5426–5435, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computa"
D19-1545,D17-1160,0,0.0303155,"in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017, 2018). Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Iyer et al. (2018) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation. We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time. Another different but related method to produce source code is using sketches, which are code snippets containing slots in the place of low-level information such as variable names, method arguments, and literals. Dong and Lapata (2018) generate"
D19-1545,D11-1140,1,0.808685,"Missing"
D19-1545,P16-1057,0,0.170899,"Missing"
D19-1545,J84-3009,0,0.43029,"Missing"
D19-1545,P02-1040,0,0.110092,"ng Java source files from github.com. It contains 100K tuples of (NL, code, context) for training, 2K tuples for development, and 2K tuples for testing. We use a BPE vocabulary of 10K tokens (for matrix B) and get the best validation set results using the original hyperparameters used by Iyer et al. (2018). Since idiom aware training is significantly faster than without idioms, it enables us to train on an additional 400K training examples that Iyer et al. (2018) released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002), and training time for all these configurations. Model Model Exact BLEU Training Time (h) Seq2Seq† Seq2Prod† Iyer et al. (2018)† 2.9 5.6 7.1 21.0 20.6 21.3 12 36 40 Iyer-Simp + 100 idioms + 200 idioms + 300 idioms + 400 idioms + 600 idioms 9.8 9.8 9.8 9.6 9.7 9.9 23.2 24.5 24.0 23.8 23.8 22.7 27 15 13 12 11 11 Table 2: Variation in Exact Match, BLEU score, and training time on the validation set of CONCODE with number of idioms used. After top-200 idioms are used, accuracies start to reduce, since using more specialized idioms can hurt model generalization. Training time plateaus after consid"
D19-1545,P17-1105,0,0.284395,"itionals, etc. and then proceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b)) as code idioms (Allamanis and Sutton, 2014). In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016). State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the target programming language grammar (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps 5426 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5426–5435, c Hong Kong, China, November 3–7, 2019. 2019 Ass"
D19-1545,P16-1162,0,0.645983,"can be seen as a higher level structure such as shown in Figure 1 (b) that can be applied in one decoding step and reused in many different programs. In this paper, we refer to frequently recurring subtrees of programmatic parse trees as code idioms, and we equip semantic parsers with the ability to learn and directly generate idiomatic structures as in Figure 1 (b). We introduce a simple iterative method to extract idioms from a dataset of programs by repeatedly collapsing the most frequent depth-2 subtrees of syntax parse trees. Analogous to the byte pair encoding (BPE) method (Gage, 1994; Sennrich et al., 2016) that creates new subtokens of words by repeatedly combining frequently occurring adjacent pairs of subtokens, our method takes a depth-2 syntax subtree and replaces it with a tree of depth-1 by removing all the internal nodes. This method is in contrast with the approach using probabilistic tree substitution grammars (pTSG) taken by Allamanis and Sutton (2014), who use the explanation quality of an idiom to prioritize idioms that are more interesting, with an end goal to suggest useful idioms to programmers using IDEs. Once idioms are extracted, we greedily apply them to semantic parsing trai"
D19-1545,P17-1041,0,0.308474,"as nested loops, conditionals, etc. and then proceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b)) as code idioms (Allamanis and Sutton, 2014). In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016). State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the target programming language grammar (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps 5426 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5426–5435, c Hong Kong, China, Nov"
D19-1545,D18-1425,0,0.053872,"o automatically mine idioms from large code bases. They focus on finding interesting and explainable idioms, e.g., those that can be included as preset code templates in programming IDEs. Instead, we learn frequently used idioms that can be easily associated with NL phrases in our dataset. The production of large subtrees in a single step directly translates to a large speedup in training and inference. Concurrent with our research, Shin et al. (2019) also develop a system (PATOIS) for idiom-based semantic parsing and demonstrate its benefits on the Hearthstone (Ling et al., 2016) and Spider (Yu et al., 2018) datasets. While we extract idioms by collapsing frequently occurring depth-2 AST subtrees and apply them greedily during training, they use non-parametric Bayesian inference for idiom extraction and train neural models to either apply entire idioms or generate its full body. 3 Idiom Aware Encoder-Decoder Models coder is trained to learn to apply idioms. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 We aim to train semantic parsers having the ability to use idioms during code generation. To do this, we first extract frequently used idioms from the training set, and then provide them as supervision"
D19-1545,D15-1166,0,\N,Missing
D19-1546,J84-2007,0,0.640143,"Missing"
D19-1546,P16-1004,0,0.0282952,"n-curated test set of 3,725 NL statements with interactive history. Neural encoder-decoder models have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism (Gu et al., 2016a) to generate source code. Recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically-determined modular structure paralleling the structure of the abstract syntax tree of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017; Iyer et al., 2018). Iyer et al. (2018) use a specialized context encoder that uses subword units (Sennrich et al., 2016) for representing code tokens and a grammar-aware decoder that attends to both NL and context to produce source code parses. Although syntax aware models are more accurate, they are significantly slower than Seq2Seq to train on large datasets. We present strong Seq2seq baselines with sub-word units for our task, that take interactive context into account and scale to large datasets. Train Dev/Test #"
D19-1546,P16-1154,0,0.543804,"est NL utterance alone does not specify that a Decision Tree model must be trained, or which variables should be used as train and test data. Automatically making these decisions conditioned on prior history is one of the main challenges of this task. Existing tasks for mapping NL to source code primarily use a single NL utterance (Zettlemoyer and Collins, 2005; Iyer et al., 2017) to generate database queries (semantic parsing), single line python code (Yin et al., 2018; Oda et al., 2015), multi-line domain-specific code (Ling et al., 2016; Rabinovich et al., 2017), or sequences of API calls (Gu et al., 2016b). A recent task by Iyer et al. (2018) on the CONCODE dataset maps a single utterance to an entire method, conditioned on environment variables and methods. In contrast, we tackle the task of general purpose code generation in an interactive setting, using an entire sequence of prior NL and code blocks as context. More closely related to our task is the context dependent semantic parsing task on the ATIS dataset (Zettlemoyer and Collins, 2009; Suhr et al., 2018) for mapping NL to database queries based on a prior history of NL and query pairs. One main difference is that while future queries"
D19-1546,P17-1089,1,0.93655,"ational Joint Conference on Natural Language Processing, pages 5436–5446, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifier), based on the previous markdown, and NL and code context. The closest NL utterance alone does not specify that a Decision Tree model must be trained, or which variables should be used as train and test data. Automatically making these decisions conditioned on prior history is one of the main challenges of this task. Existing tasks for mapping NL to source code primarily use a single NL utterance (Zettlemoyer and Collins, 2005; Iyer et al., 2017) to generate database queries (semantic parsing), single line python code (Yin et al., 2018; Oda et al., 2015), multi-line domain-specific code (Ling et al., 2016; Rabinovich et al., 2017), or sequences of API calls (Gu et al., 2016b). A recent task by Iyer et al. (2018) on the CONCODE dataset maps a single utterance to an entire method, conditioned on environment variables and methods. In contrast, we tackle the task of general purpose code generation in an interactive setting, using an entire sequence of prior NL and code blocks as context. More closely related to our task is the context dep"
D19-1546,D18-1192,1,0.448708,"ify that a Decision Tree model must be trained, or which variables should be used as train and test data. Automatically making these decisions conditioned on prior history is one of the main challenges of this task. Existing tasks for mapping NL to source code primarily use a single NL utterance (Zettlemoyer and Collins, 2005; Iyer et al., 2017) to generate database queries (semantic parsing), single line python code (Yin et al., 2018; Oda et al., 2015), multi-line domain-specific code (Ling et al., 2016; Rabinovich et al., 2017), or sequences of API calls (Gu et al., 2016b). A recent task by Iyer et al. (2018) on the CONCODE dataset maps a single utterance to an entire method, conditioned on environment variables and methods. In contrast, we tackle the task of general purpose code generation in an interactive setting, using an entire sequence of prior NL and code blocks as context. More closely related to our task is the context dependent semantic parsing task on the ATIS dataset (Zettlemoyer and Collins, 2009; Suhr et al., 2018) for mapping NL to database queries based on a prior history of NL and query pairs. One main difference is that while future queries can be built by modifying queries in pr"
D19-1546,P17-1167,0,0.0501127,"Missing"
D19-1546,D17-1160,0,0.0286203,"nteractive history. Neural encoder-decoder models have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism (Gu et al., 2016a) to generate source code. Recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically-determined modular structure paralleling the structure of the abstract syntax tree of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017; Iyer et al., 2018). Iyer et al. (2018) use a specialized context encoder that uses subword units (Sennrich et al., 2016) for representing code tokens and a grammar-aware decoder that attends to both NL and context to produce source code parses. Although syntax aware models are more accurate, they are significantly slower than Seq2Seq to train on large datasets. We present strong Seq2seq baselines with sub-word units for our task, that take interactive context into account and scale to large datasets. Train Dev/Test # Examples Avg Context Cells Avg NL Tokens Avg Code Tok"
D19-1546,N13-1103,0,0.0175806,"s varying training data size. In summary, we introduce the task of code generation under the paradigm of IC. We collect and release a new large-scale dataset together with a manually curated test set based on nbgrader. Finally, we evaluate strong neural baselines on two code generation tasks, which achieve reasonable performance, with significant room for improvement. 2 Related Work There is significant existing research on mapping single NL utterances directly to executable programs in the form of logical forms (Zettlemoyer and Collins, 2005), λ-DCS (Liang et al., 2013), regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), database queries (Iyer et al., 2017; Zhong et al., 2017) and general purpose programs (Balog et al., 2016; Allamanis et al., 2015). Ling et al. (2016) generate Java and Python source code from NL for card games, conditioned on categorical card attributes. Iyer et al. (2018) generate Java class methods using a single NL instruction and conditioned on a list of class environment variables and methods. Yin et al. (2018) mine a large NL-code dataset from Stackoverflow to train models to map a NL programming question into a short example solution snippet. Gu et al. (2016c)"
D19-1546,J13-2005,0,0.0159423,"NL and code context considered, as well as varying training data size. In summary, we introduce the task of code generation under the paradigm of IC. We collect and release a new large-scale dataset together with a manually curated test set based on nbgrader. Finally, we evaluate strong neural baselines on two code generation tasks, which achieve reasonable performance, with significant room for improvement. 2 Related Work There is significant existing research on mapping single NL utterances directly to executable programs in the form of logical forms (Zettlemoyer and Collins, 2005), λ-DCS (Liang et al., 2013), regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), database queries (Iyer et al., 2017; Zhong et al., 2017) and general purpose programs (Balog et al., 2016; Allamanis et al., 2015). Ling et al. (2016) generate Java and Python source code from NL for card games, conditioned on categorical card attributes. Iyer et al. (2018) generate Java class methods using a single NL instruction and conditioned on a list of class environment variables and methods. Yin et al. (2018) mine a large NL-code dataset from Stackoverflow to train models to map a NL programming question into a"
D19-1546,P16-1057,0,0.128932,"Missing"
D19-1546,D16-1197,0,0.0342853,"Missing"
D19-1546,P16-1138,0,0.0549898,"Missing"
D19-1546,D15-1166,0,0.0241498,"ting both NL sequences as vectors using tf-idf representations for their dimensions and measuring the cosine distance between these vectors. We convert the retrieved code cell to an API sequence for the API sequence task. 6.2 LSTM with Attention This baseline is a neural encoder-decoder model where the encoder computes contextualized representations of input sequence embeddings using an n-layer BiLSTM, and an LSTM-based decoder produces a sequence of code (API) tokens, while attending to the encoder representations at every time step t. We make use of the general global attention mechanism by Luong et al. (2015). To use this model for our task, we concatenate all K context cells, c1...K , using the type of each cell as separator symbols i.e. CODE or MARKDOWN. The output sequence is supervised to be either an API sequence or a sequence of code tokens. 6.3 Transformer This baseline utilizes the Transformer (Vaswani et al., 2017) architecture where the encoder and decoder consist of multiple layers of multi-headed self-attention and position-wise feed forward layers. The inputs and outputs are the same as the LSTM model described above. 7 Experiments We run code generation experiments to evaluate our ba"
D19-1546,P17-1041,0,0.0478035,"ncoder-decoder models have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism (Gu et al., 2016a) to generate source code. Recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically-determined modular structure paralleling the structure of the abstract syntax tree of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017; Iyer et al., 2018). Iyer et al. (2018) use a specialized context encoder that uses subword units (Sennrich et al., 2016) for representing code tokens and a grammar-aware decoder that attends to both NL and context to produce source code parses. Although syntax aware models are more accurate, they are significantly slower than Seq2Seq to train on large datasets. We present strong Seq2seq baselines with sub-word units for our task, that take interactive context into account and scale to large datasets. Train Dev/Test # Examples Avg Context Cells Avg NL Tokens Avg Code Tokens # Unique NL Token"
D19-1546,N19-4009,0,0.0244829,"ariables X train and y train, or the API sequence, DecisionTreeClassifier fit. Formally, let a(i) denote the API sequence, and s(i) denote the code token sequence of the target (i) cell in the ith training example. Further, if cd represents the context cell at a distance d, d &gt; 0, above the target cell of example i, our tasks are to generate a(i) and s(i) , conditioned on the context (i) cells. Note that cd can be a sequence of NL words or code tokens. In models that we present in later sections, we limit d to a maximum of K cells. 6 We train and evaluate a selection of neural (using fairseq: Ott et al. (2019)) and non-neural baselines that are known to perform well for code generation tasks, particularly on large scale datasets. 6.1 We investigate two tasks for target cell generation in notebooks, conditioned on the context cells up to the target cell: (1) The Full code generation task, which involves generating the entire code snippet in the target cell. (2) The API sequence task, which involves generating only the sequence of constructor and method calls within the target Baselines Retrieval This baseline presents the programmer with an existing code cell from elsewhere in the training corpus as"
D19-1546,P09-1110,1,0.847019,"single line python code (Yin et al., 2018; Oda et al., 2015), multi-line domain-specific code (Ling et al., 2016; Rabinovich et al., 2017), or sequences of API calls (Gu et al., 2016b). A recent task by Iyer et al. (2018) on the CONCODE dataset maps a single utterance to an entire method, conditioned on environment variables and methods. In contrast, we tackle the task of general purpose code generation in an interactive setting, using an entire sequence of prior NL and code blocks as context. More closely related to our task is the context dependent semantic parsing task on the ATIS dataset (Zettlemoyer and Collins, 2009; Suhr et al., 2018) for mapping NL to database queries based on a prior history of NL and query pairs. One main difference is that while future queries can be built by modifying queries in previous utterances, code snippets in our task are almost always disjoint from previously generated code. We collect an open-domain large-scale dataset (JuICe) of over 659K publicly available Jupyter notebooks from github.com to train models for our task. Each notebook contains an average of 39 NL/code cells and uses python packages from thousands of domains. Since obtaining source-code with NL annotations"
D19-1546,P02-1040,0,0.10478,"and (d) show the effects of context cells on performance of LSTM trained with 200K examples. Performance improves for up to 3 cells, after which reasoning over long contexts becomes challenging for current methods. 6 layers and 4 attention heads. The dimension of the model is 512 and the dimension of the feed forward network is 1024. We use a learning rate of 0.0005 with 2,000 warmup steps, train for 200 epochs, and pick the model at the epoch with highest validation performance. 7.2 Metrics Similar to prior code generation tasks, we use Exact Match (EM) accuracy and corpus-level BLEU score (Papineni et al., 2002) as performance metrics for full code generation. While EM is a strict metric measuring the ability of models to generate fully functioning code, BLEU serves as a measure of partial credit for models that can provide users with a partially correct snippet. For the API sequence task, we find BLEU and EM inadequately measure performance since the average API sequence length for our target code is 3.8. We therefore treat the predicted API calls as a set of values and compute precision/recall metrics against the gold set of API calls. 8 Results and Discussion We present EM and BLEU scores for the"
D19-1546,P17-1105,0,0.264417,"previous markdown, and NL and code context. The closest NL utterance alone does not specify that a Decision Tree model must be trained, or which variables should be used as train and test data. Automatically making these decisions conditioned on prior history is one of the main challenges of this task. Existing tasks for mapping NL to source code primarily use a single NL utterance (Zettlemoyer and Collins, 2005; Iyer et al., 2017) to generate database queries (semantic parsing), single line python code (Yin et al., 2018; Oda et al., 2015), multi-line domain-specific code (Ling et al., 2016; Rabinovich et al., 2017), or sequences of API calls (Gu et al., 2016b). A recent task by Iyer et al. (2018) on the CONCODE dataset maps a single utterance to an entire method, conditioned on environment variables and methods. In contrast, we tackle the task of general purpose code generation in an interactive setting, using an entire sequence of prior NL and code blocks as context. More closely related to our task is the context dependent semantic parsing task on the ATIS dataset (Zettlemoyer and Collins, 2009; Suhr et al., 2018) for mapping NL to database queries based on a prior history of NL and query pairs. One m"
D19-1546,P16-1162,0,0.0666253,"Missing"
D19-1546,N18-1203,1,0.905412,"Missing"
D19-1588,P15-1136,0,0.547985,"Missing"
D19-1588,D16-1245,0,0.283549,"xample, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretraining span representations achieving current state of the art results on OntoNotes with the independ"
D19-1588,D08-1069,0,0.0429317,"errors suggest that models are still unable to resolve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretrainin"
D19-1588,N19-1423,0,0.641498,"1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available1 . 1 Introduction Recent BERT-based models have reported dramatic gains on multiple semantic benchmarks including question-answering, natural language inference, and named entity recognition (Devlin et al., 2019). Apart from better bidirectional reasoning, one of BERT’s major improvements over previous methods (Peters et al., 2018; McCann et al., 2017) is passage-level training,2 which allows it to better model longer sequences. We fine-tune BERT to coreference resolution, achieving strong improvements on the GAP (Webster et al., 2018) and OntoNotes (Pradhan et al., 2012) benchmarks. We present two ways of extending the c2f-coref model in Lee et al. (2018). The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT. The overlap variant splits the docum"
D19-1588,D13-1203,0,0.0958185,"lve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretraining span representations achieving current state of"
D19-1588,P19-1064,0,0.467621,"Missing"
D19-1588,W12-4502,0,0.0428419,"Missing"
D19-1588,P19-1066,0,0.511259,"Missing"
D19-1588,D17-1018,1,0.948252,"s from using longer context windows (384 word pieces) while BERT-base performs better with shorter contexts (128 word pieces). Yet, both variants perform much worse with longer context windows (512 tokens) in spite of being trained on 512-size contexts. Moreover, the overlap variant, which artificially extends the context window beyond 512 tokens provides no improvement. This indicates that using larger context windows for pretraining might not translate into effective long-range features for a downstream task. Larger models also exacerbate the memory-intensive nature of span representations (Lee et al., 2017), which have driven recent improvements in coreference resolution. Together, these observations suggest that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. 2 Method For our experiments, we use the higher-order coreference model in Lee et al. (2018) which is the current state of the art for the English OntoNotes dataset (Pradhan et al., 2012). We refer to this as c2f-coref in the paper. 2.1 Overview of c2f-coref For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : 5803 Proceedings of t"
D19-1588,N18-2108,1,0.806336,"Missing"
D19-1588,P15-1137,0,0.743197,"ning. Likewise, we use a batch size of 1 document following (Lee et al., 2018). While training the large model requires 32GB GPUs, all models can be tested on 16GB GPUs. We use the cased English variants in all our experiments. Baselines We compare the c2f-coref + BERT system with two main baselines: (1) the original ELMo-based c2f-coref system (Lee et al., 2018), and (2) its predecessor, e2e-coref (Lee et al., 3 http://github.com/kentonl/e2e-coref/ https://github.com/google-research/ bert 5804 4 P MUC R F1 B3 R P F1 P CEAFφ4 R F1 Avg. F1 Martschat and Strube (2015) (Clark and Manning, 2015) (Wiseman et al., 2015) Wiseman et al. (2016) Clark and Manning (2016) e2e-coref (Lee et al., 2017) c2f-coref (Lee et al., 2018) Fei et al. (2019) EE (Kantor and Globerson, 2019) 76.7 76.1 76.2 77.5 79.2 78.4 81.4 85.4 82.6 68.1 69.4 69.3 69.8 70.4 73.4 79.5 77.9 84.1 72.2 72.6 72.6 73.4 74.6 75.8 80.4 81.4 83.4 66.1 65.6 66.2 66.8 69.9 68.6 72.2 77.9 73.3 54.2 56.0 55.8 57.0 58.0 61.8 69.5 66.4 76.2 59.6 60.4 60.5 61.5 63.4 65.0 70.8 71.7 74.7 59.5 59.4 59.4 62.1 63.5 62.7 68.2 70.6 72.4 52.3 53.0 54.9 53.9 55.5 59.0 67.1 66.3 71.1 55.7 56.0 57.1 57.7 59.2 60.8 67.6 68.4 71.8 62.5 63.0 63.4 64.2 65.7 67.2 73.0 73.8"
D19-1588,N16-1114,0,0.609599,"Missing"
D19-1588,P19-1593,1,0.896497,"Missing"
D19-1588,Q15-1029,0,0.11685,"Missing"
D19-1588,C02-1139,0,0.193019,"he document. Lastly, a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies."
D19-1588,N18-1202,1,0.900076,"ase, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available1 . 1 Introduction Recent BERT-based models have reported dramatic gains on multiple semantic benchmarks including question-answering, natural language inference, and named entity recognition (Devlin et al., 2019). Apart from better bidirectional reasoning, one of BERT’s major improvements over previous methods (Peters et al., 2018; McCann et al., 2017) is passage-level training,2 which allows it to better model longer sequences. We fine-tune BERT to coreference resolution, achieving strong improvements on the GAP (Webster et al., 2018) and OntoNotes (Pradhan et al., 2012) benchmarks. We present two ways of extending the c2f-coref model in Lee et al. (2018). The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT. The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens. BERT-large improves over ELMo-b"
D19-1588,W12-4501,0,0.470215,"r context windows for pretraining might not translate into effective long-range features for a downstream task. Larger models also exacerbate the memory-intensive nature of span representations (Lee et al., 2017), which have driven recent improvements in coreference resolution. Together, these observations suggest that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. 2 Method For our experiments, we use the higher-order coreference model in Lee et al. (2018) which is the current state of the art for the English OntoNotes dataset (Pradhan et al., 2012). We refer to this as c2f-coref in the paper. 2.1 Overview of c2f-coref For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : 5803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5803–5808, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics P (y) = P es(x,y) y 0 ∈Y es(x,y0 ) (1) The scoring function s(x, y) between spans x and y uses fixed-length span representations, gx and gy to represent its in"
D19-1588,D08-1031,0,\N,Missing
D19-1633,Q19-1042,0,0.330019,"sked words in the target translation. We use transformer CMLMs, where the decoder’s self attention (Vaswani et al., 2017) can attend to the ∗ Equal contribution, sorted alphabetically. Our code is publicly available at: https://github.com/facebookresearch/Mask-Predict 1 Luke Zettlemoyer entire sequence (left and right context) to predict each masked word. We train with a simple masking scheme where the number of masked target tokens is distributed uniformly, presenting the model with both easy (single mask) and difficult (completely masked) examples. Unlike recently proposed insertion models (Gu et al., 2019; Stern et al., 2019), which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training. We also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of CMLMs to support highly parallel decoding. Mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire sequence (Lee et al., 2018). Decoding starts with a comp"
D19-1633,D18-1149,0,0.161842,"tly proposed insertion models (Gu et al., 2019; Stern et al., 2019), which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training. We also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of CMLMs to support highly parallel decoding. Mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire sequence (Lee et al., 2018). Decoding starts with a completely masked target text, to predict all of the words in parallel, and ends after a constant number of mask-predict cycles. This overall strategy allows the model to repeatedly reconsider word choices within a rich bi-directional context and, as we will show, produce high-quality translations in just a few cycles. Experiments on benchmark machine translation datasets show the strengths of mask-predict decoding for transformer CMLMs. With just 4 iterations, BLEU scores already surpass the performance of the best non-autoregressive and parallel decoding models.2 Wit"
D19-1633,D18-1336,0,0.199333,"Missing"
D19-1633,N19-4007,0,0.0181455,"into a uni-modal distribution. The decrease in repetitions also correlates with the steep rise in translation quality (BLEU), supporting the conjecture of Gu et al. (2018) that multi-modality is a major roadblock for purely non-autoregressive machine translation. 5.2 Do Longer Sequences Need More Iterations? A potential concern with using a constant amount of decoding iterations is that it may be effective for short sequences (where the number of iterations T is closer to the output’s length N ), but insufficient for longer sequences. To determine whether this is the case, we use compare-mt (Neubig et al., 2019) to bucket the evaluation data by target sentence length and compute the performance with different values of T . Table 4 shows that increasing the number of decoding iterations (T ) appears to mainly improve the performance on longer sequences. Having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ≤ N ). 5.3 Do More Length Candidates Help? Traditional autoregressive models can dynamically decide the length of the target sequence by generating a sp"
D19-1633,P02-1040,0,0.104444,"of standard autoregressive models (Section 4.2), while decoding significantly faster (Section 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions, for comparison with previous parallel decoding models (Gu et al., 2018; Lee et al., 2018). We follow the weight initialization scheme from BERT (Devlin et al., 2018), which samples weights from N (0, 0.02), initializes biases to zero, and sets"
D19-1633,W18-6319,0,0.0196285,"n 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions, for comparison with previous parallel decoding models (Gu et al., 2018; Lee et al., 2018). We follow the weight initialization scheme from BERT (Devlin et al., 2018), which samples weights from N (0, 0.02), initializes biases to zero, and sets layer normalization parameters to β = 0, γ = 1. For regularization, we use 0.3 dr"
D19-1633,P16-1162,0,0.267068,"). 4 Experiments We evaluate CMLMs with mask-predict decoding on standard machine translation benchmarks. We find that our approach significantly outperforms prior parallel decoding machine translation methods and even approaches the performance of standard autoregressive models (Section 4.2), while decoding significantly faster (Section 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions"
D19-1633,W19-2304,0,0.0577819,"encoder and decoder parameters (as in our model) and pre-training them jointly in an autoregressive version of masked language modeling, although with monolingual data. While this work demonstrates that pretraining CMLMs can improve autoregressive machine translation, it does not try to leverage the parallel and bi-directional nature of CMLMs to generate text in a non-left-to-right manner. Generating from Masked Language Models One such approach for generating text from a masked language model casts BERT (Devlin et al., 2018), a non-conditional masked language model, as a Markov random field (Wang and Cho, 2019). By masking a sequence of length N and then iteratively sampling a single token at each time from the model (either sequentially or in arbitrary order), one can produce grammatical examples. While this sampling process has a theoretical justification, it also requires N forward passes of the model; mask-predict decoding, on the other hand, can produce text in a constant number of iterations. Parallel Decoding for Machine Translation There have been several advances in parallel decoding machine translation by training nonautoregressive models. Gu et al. (2018) introduce a transformer-based app"
E12-1024,D11-1131,0,0.0825834,"Missing"
E12-1024,D10-1119,1,0.517337,"learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once. We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus (MacWhinney, 2000) using online variational Bayesian EM. We evaluate the learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1 Similar to referential uncertainty but relating to propositions rather than referents. 234 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguis"
E12-1024,D11-1140,1,0.744611,"Missing"
E12-1024,D08-1082,1,0.28321,"learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approac"
E12-1024,sagae-etal-2004-adding,0,0.0577893,"ex function to read all of the lexical items off from the derivations in each {t}0 . In the parameter update step, the training algorithm updates the pseudocounts associated with each of the productions a → b that have ever been seen during training according to Equation (17). Only non-zero pseudocounts are stored in our model. The count vector is expanded with a new entry every time a new production is used. While Data The Eve corpus, collected by Brown (1973), contains 14, 124 English utterances spoken to a single child between the ages of 18 and 27 months. These have been hand annotated by Sagae et al. (2004) with labelled syntactic dependency graphs. An example annotation is shown in Figure 3. While these annotations are designed to represent syntactic information, the parent-child relationships in the parse can also be viewed as a proxy for the predicate-argument structure of the semantics. We developed a template based deterministic procedure for mapping this predicateargument structure onto logical expressions of the type discussed in Section 2.1. For example, the dependency graph in Figure 3 is automatically transformed into the logical expression λe.have(you,another(y, cookie(y)), e) (18) ∧"
E12-1024,N06-1056,0,0.0359022,"een data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. I"
E12-1024,P07-1121,0,0.25359,"Missing"
E12-1024,D07-1071,1,0.775604,"Missing"
E12-1024,P09-1110,1,0.394634,"Missing"
K17-1034,P15-2061,0,0.037345,"015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do. Related Work We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type RN +1 at test time. The only information we have about RN +1 are parametrized questions. This setting differs from prior art in relation extraction. Bronstein et al. (2015) explore a similar zero-shot setting for event-trigger identification, in 334 3 that might participate in this relation.1 Each time the model returns a non-null answer a for a given question qe , it extracts the relation RN +1 (e, a). Ultimately, all we need to do for a new relation is define our information need in the form of a question.2 Our approach provides a naturallanguage API for application developers who are interested in incorporating a relation-extraction component in their programs; no linguistic knowledge or pre-defined schema is needed. To implement our approach, we require two"
K17-1034,D16-1146,0,0.0118323,"Missing"
K17-1034,D15-1076,1,0.160901,"ontext can therefore be defined by: Discussion Some recent QA datasets were collected by expressing knowledge-base assertions in natural language. The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(T uring, P rinceton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset. For the task of identifying predicate-argument structures, QASRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences. In these efforts, the costs scale linearly in the number of instances, requiring significant investments for large datasets. In contrast, schema querification can generate an enormous amount of data for a fraction of the cost by labeling at the relation level; as ev"
K17-1034,P16-1145,0,0.276022,"7), pages 333–342, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics which RN +1 is specified by a set of trigger words at test time. They generalize by measuring the similarity between potential triggers and the given seed set using unsupervised methods. We focus instead on slot filling, where questions are more suitable descriptions than trigger words. in data and models. We use distant supervision for a relatively large number of relations (120) from Wikidata (Vrandeˇci´c, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al., 2016). We introduce a crowdsourcing approach for gathering and verifying the questions for each relation. This process produced about 10 questions per relation on average, yielding a dataset of over 30,000,000 questionsentence-answer examples in total. Because questions are paired with relation types, not instances, this overall procedure has very modest costs. The key modeling challenge is that most existing reading-comprehension problem formulations assume the answer to the question is always present in the given text. However, for relation extraction, this premise does not hold, and the model ne"
K17-1034,P11-1055,1,0.180397,"Missing"
K17-1034,N16-1104,0,0.00903376,"to train a reading comprehension model through our reduction. However, at test time, we are asked about a previously unseen relation type RN +1 . Rather than providing labeled data for the new relation, we simply list questions that define the relation’s slot values. Assuming we learned a good reading comprehension model, the correct values should be extracted. Our zero-shot setup includes innovations both Introduction Relation extraction systems populate knowledge bases with facts from an unstructured text corpus. When the type of facts (relations) are predefined, one can use crowdsourcing (Liu et al., 2016) or distant supervision (Hoffmann et al., 2011) to collect examples and train an extraction model for each relation type. However, these approaches are incapable of extracting relations that were not specified in advance and observed during training. In this paper, we propose an alternative approach for relation extraction, which can potentially extract facts of new types that were neither specified nor observed a priori. 333 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Associatio"
K17-1034,P16-1105,0,0.0341351,"ways improves performance (Question Ensemble). We create an ensemble by sampling 3 questions per test instance and predicting the answer for each. We then choose the answer with the highest sum of confidence scores. In addition to our model, we compare three other systems. The first is a random baseline that chooses a named entity in the sentence that does not appear in the question (Random NE). We also reimplement the RNN Labeler that was shown to have good results on the extractive portion of WikiReading (Hewlett et al., 2016). Lastly, we retrain an off-the-shelf relation extraction system (Miwa and Bansal, 2016), which has shown promising results on a number of benchmarks. This system (and many like it) represents relations as indicators, and cannot extract unseen relations. Experiments To understand how well our method can generalize to unseen data, we design experiments for unseen entities (Section 6.1), unseen question templates (Section 6.2), and unseen relations (Section 6.3). Evaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.6 Precision is the true positive count divided by the number of times the system returned a n"
K17-1034,E17-1058,0,0.0181729,"extracting facts from text. While open IE systems need no relation-specific training data, they often treat different phrasings as different relations. In this work, we hope to extract a canonical slot value independent of how the original text is phrased. Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations. The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques. Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1). Rockt¨aschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations. This setting is akin to generalizing across different manifestations of the same relation (see Section 6.2) since a natural-language description of each target relation appears in the training data. Moreover, the information about the unseen relations is a set of expli"
K17-1034,D16-1199,0,0.0336193,"i is to end at that index. Assuming the answer exists, we can transform these confidence scores into pseudo-probability distributions pstart , pend via softmax. The probability of each i-to-j-span of the context can therefore be defined by: Discussion Some recent QA datasets were collected by expressing knowledge-base assertions in natural language. The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(T uring, P rinceton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset. For the task of identifying predicate-argument structures, QASRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences. In these efforts, the costs scale linearly in the number of instan"
K17-1034,D14-1162,0,0.111821,"een relations (Section 6.3). Evaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.6 Precision is the true positive count divided by the number of times the system returned a non-null answer. Recall is the true positive count divided by the number of instances that have an answer. 6.1 Unseen Entities We show that our reading-comprehension approach works well in a typical relation-extraction setting by testing it on unseen entities and texts. Hyperparameters In our experiments, we initialized word embeddings with GloVe (Pennington et al., 2014), and did not fine-tune them. The typical training set was an order of 1 million examples, for which 3 epochs were enough for convergence. All training sets had a ratio of 1:1 positive and negative examples, which was chosen to match the test sets’ ratio. Setup We partitioned our dataset along entities in the question, and randomly clustered each entity into one of three groups: train, dev, or test. For instance, Alan Turing examples appear only in training, while Steve Jobs examples are exclusive to test. We then sampled 1,000,000 examples for train, 1,000 for dev, and 10,000 for test. This p"
K17-1034,D16-1264,0,0.0721379,"the example set, the name of the relation (e.g. country), and another instance where it was hidden. Out of a potential 54 question templates, 40 were unique on average. In the verification phase, we measure the question templates’ quality by sampling additional sentences and instantiating each question template with the example entity e. Annotators are then asked to answer the question from the sentence s, or mark it as unanswerable; if the annotators’ anNegative Examples To support relation extraction, our dataset deviates from recent reading comprehension formulations (Hermann et al., 2015; Rajpurkar et al., 2016), and introduces negative examples – question-sentence pairs that have no answers (A = ∅). Following the methodology of InfoboxQA (Morales et al., 2016), we generate negative examples by matching (for the same entity e) a question q that pertains to one relation with a sentence s that expresses another relation. We also assert that the sentence does not contain the answer to q. For instance, we match “Who 3 We used this relatively lenient measure because many annotators selected the correct answer, but with a slightly incorrect span; e.g. “American businessman” instead of “businessman”. We the"
K17-1034,N13-1008,0,0.0781624,"answer to “Where” is a location), as well as detecting relation paraphrases to a certain extent. We also find that there are many feasible cases that our model does not quite master, providing an interesting challenge for future work. 2 Open information extraction (open IE) (Banko et al., 2007) is a schemaless approach for extracting facts from text. While open IE systems need no relation-specific training data, they often treat different phrasings as different relations. In this work, we hope to extract a canonical slot value independent of how the original text is phrased. Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations. The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques. Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1). Rockt¨aschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-b"
K17-1034,N15-1118,0,0.0672247,"Missing"
K17-1034,D15-1174,0,0.0166238,"et of explicit inference rules, as opposed to implicit natural-language questions. Our zero-shot scenario, in which no manifestation of the test relation is observed during training, is substantially more challenging (see Section 6.3). In universal-schema terminology, we add a new empty column (the target knowledgebase relation), plus a few new columns with a single entry each (reflecting the textual relations in the sentence). These columns share no entities with existing columns, making the rest of the matrix irrelevant. To fill the empty column from the others, we match their descriptions. Toutanova et al. (2015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do. Related Work We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type RN +1 at test time. The only information we have about RN +1 are parametrized questions. This setting differs from prior art in relation extraction. Bron"
L18-1491,P16-1004,0,0.024817,"Some other works (Oda et al., 2015) have adopted fuzzy evaluation metrics, such as BLEU, which is widely used to measure the translation quality between natural languages (Doddington, 2002). Appendix C shows that the n-gram overlap captured by BLEU is not effective in measuring the semantic similarity for formal languages. Baseline System Performance To establish performance levels for future work, we evaluated two neural machine translation models that have demonstrated strong performance in both NL-to-NL translation and NL-to-code translation tasks, namely, Seq2Seq (Sutskever et al., 2014; Dong and Lapata, 2016) and CopyNet (Gu et al., 2016). We also evaluated a stage-wise natural language programing model, Tellina (Lin et al., 2017), which includes manually-designed heuristics for argument translation. Seq2Seq The Seq2Seq (sequence-to-sequence) model defines the conditional probability of an output sequence given the input sequence using an RNN (recurrent neural network) encoder-decoder (Jain and Medsker, 1999; Sutskever et al., 2014). When applied to the NL-to-code translation problem, the input natural language and output commands are treated as sequences of tokens. At test time, the command seque"
L18-1491,P16-1154,0,0.042212,"have adopted fuzzy evaluation metrics, such as BLEU, which is widely used to measure the translation quality between natural languages (Doddington, 2002). Appendix C shows that the n-gram overlap captured by BLEU is not effective in measuring the semantic similarity for formal languages. Baseline System Performance To establish performance levels for future work, we evaluated two neural machine translation models that have demonstrated strong performance in both NL-to-NL translation and NL-to-code translation tasks, namely, Seq2Seq (Sutskever et al., 2014; Dong and Lapata, 2016) and CopyNet (Gu et al., 2016). We also evaluated a stage-wise natural language programing model, Tellina (Lin et al., 2017), which includes manually-designed heuristics for argument translation. Seq2Seq The Seq2Seq (sequence-to-sequence) model defines the conditional probability of an output sequence given the input sequence using an RNN (recurrent neural network) encoder-decoder (Jain and Medsker, 1999; Sutskever et al., 2014). When applied to the NL-to-code translation problem, the input natural language and output commands are treated as sequences of tokens. At test time, the command sequences with the highest conditio"
L18-1491,P17-1097,0,0.0192196,"ve a large set of option flags, and multiple commands can be combined to solve more complex tasks. This often results in multiple correct solutions for one task (§3.3.), and poses challenges for both training and evaluation. Idiomatic Syntax The Bash interpreter uses a shallow syntactic grammar to parse pipelines, code blocks, and other high-level syntax structures. It parses command options using pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difficult to apply. 6. Table 7: Inter-annotator agreement. Previous approaches Previous NL-to-code translation work also noticed similar problems. (Kushman and Barzilay, 2013; Locascio et al., 2016) formally verify the equivalence of different regular expressions by converting them to minimal deterministic finite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash commands is undecidable"
L18-1491,N16-1088,0,0.0399241,"Missing"
L18-1491,P16-1195,1,0.0487575,"ash 640 391 880 284 917 – 5,410 936 4,737 – 824 715 18,805 – 9,305 7,790 # tokens – 91,156 1,287 179Y – – 137,123 21,413 45Y= – Avg. # Avg. # NL w. in nl t. in code collection 7.0 21.8 12 38 scraped 9 46 36.4 58.8Y 7 352Y game card description 21 1,080Y 9 86 extracted 9 60 using ML 10.6 26Y synthesized & – – paraphrased synthesized – 10.9 16.0 given code 58= 9.8 22.9 60= 7.6 19.1 user written – – – = 176 11.1 28.1 – – – search log 85Y= 7.1 19.0Y turker written – 14.3 – expert written 6,234 11.7 7.7 given code Code Semantic collection alignment scraped Noisy Introduced by (Quirk et al., 2015) (Iyer et al., 2016) (Zhong et al., 2018) game card source code extracted using ML synthesized Good@ Noisy (Yao et al., 2018) Very Good (Locascio et al., 2016) (Zhong et al., 2017) expert written expert written given NL scraped (Ling et al., 2016) (Haas and Riezler, 2016) Very Good (Tang and Mooney, 2001) (Zelle and Mooney, 1996) (Cai and Yates, 2013) (Dahl et al., 1994) (Yih et al., 2016) (Kushman and Barzilay, 2013) (Oda et al., 2015) Ours Table 12: Comparison of datasets for translation of natural language to (short) code snippets. *: Both C#2NL and SQL2NL were originally collected to train systems that explai"
L18-1491,P17-1089,1,0.908983,"n be seen as a type of semantic parsing, where the goal is to map sentences to formal representations of their underlying meaning (Mooney, 2014). The dataset we introduce provides a new type of target meaning representations (Bash1 commands), and is significantly larger (from two to ten times) than most existing semantic parsing benchmarks (Dahl et al., 1994; Popescu et al., 2003). Other recent work in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkowski et al., 2013; Iyer et al., 2017; Zhong et al., 2017). However, the shell command data we consider raises unique challenges, due to its irregular syntax (no syntax tree representation for the command options), wide domain coverage (&gt; 100 Bash utilities), and a large percentage of unseen words (e.g. commands can manipulate arbitrary files). * Work done at the University of Washington. The Bourne-again shell (Bash) is the most popular Unix shell and command language: https://www.gnu.org/software/bash/. Our data collection approach and baseline models can be trivially generalized to other command languages. We constructed the N"
L18-1491,N13-1103,0,0.0190726,"ses challenges for both training and evaluation. Idiomatic Syntax The Bash interpreter uses a shallow syntactic grammar to parse pipelines, code blocks, and other high-level syntax structures. It parses command options using pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difficult to apply. 6. Table 7: Inter-annotator agreement. Previous approaches Previous NL-to-code translation work also noticed similar problems. (Kushman and Barzilay, 2013; Locascio et al., 2016) formally verify the equivalence of different regular expressions by converting them to minimal deterministic finite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash commands is undecidable. Alternatively, one can check command equivalence using test examples: two commands can be executed in a virtual environment and their execution outcome can be compared. We leave this"
L18-1491,D13-1161,1,0.125285,"sk. The NL2Bash problem can be seen as a type of semantic parsing, where the goal is to map sentences to formal representations of their underlying meaning (Mooney, 2014). The dataset we introduce provides a new type of target meaning representations (Bash1 commands), and is significantly larger (from two to ten times) than most existing semantic parsing benchmarks (Dahl et al., 1994; Popescu et al., 2003). Other recent work in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkowski et al., 2013; Iyer et al., 2017; Zhong et al., 2017). However, the shell command data we consider raises unique challenges, due to its irregular syntax (no syntax tree representation for the command options), wide domain coverage (&gt; 100 Bash utilities), and a large percentage of unseen words (e.g. commands can manipulate arbitrary files). * Work done at the University of Washington. The Bourne-again shell (Bash) is the most popular Unix shell and command language: https://www.gnu.org/software/bash/. Our data collection approach and baseline models can be trivially generalized to other command languages. W"
L18-1491,P16-1057,0,0.0276497,"Missing"
L18-1491,D16-1197,0,0.168573,"tions, along with baseline methods to establish performance levels on this task. The NL2Bash problem can be seen as a type of semantic parsing, where the goal is to map sentences to formal representations of their underlying meaning (Mooney, 2014). The dataset we introduce provides a new type of target meaning representations (Bash1 commands), and is significantly larger (from two to ten times) than most existing semantic parsing benchmarks (Dahl et al., 1994; Popescu et al., 2003). Other recent work in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkowski et al., 2013; Iyer et al., 2017; Zhong et al., 2017). However, the shell command data we consider raises unique challenges, due to its irregular syntax (no syntax tree representation for the command options), wide domain coverage (&gt; 100 Bash utilities), and a large percentage of unseen words (e.g. commands can manipulate arbitrary files). * Work done at the University of Washington. The Bourne-again shell (Bash) is the most popular Unix shell and command language: https://www.gnu.org/software/bash/. Our data collection approach"
L18-1491,P16-1138,0,0.0724763,"using pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difficult to apply. 6. Table 7: Inter-annotator agreement. Previous approaches Previous NL-to-code translation work also noticed similar problems. (Kushman and Barzilay, 2013; Locascio et al., 2016) formally verify the equivalence of different regular expressions by converting them to minimal deterministic finite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash commands is undecidable. Alternatively, one can check command equivalence using test examples: two commands can be executed in a virtual environment and their execution outcome can be compared. We leave this evaluation approach to future work. Some other works (Oda et al., 2015) have adopted fuzzy evaluation metrics, such as BLEU, which is widely used to measure the translation quality between natural languages (Do"
L18-1491,P02-1040,0,0.124741,"Missing"
L18-1491,P15-1085,0,0.45559,"ds: Natural Language Programming, Natural Language Interface, Semantic Parsing 1. Introduction The dream of using English or any other natural language to program computers has existed for almost as long as the task of programming itself (Sammet, 1966). Although significantly less precise than a formal language (Dijkstra, 1978), natural language as a programming medium would be universally accessible and would support the automation of highly repetitive tasks such as file manipulation, search, and application-specific scripting (Wilensky et al., 1984; Wilensky et al., 1988; Dahl et al., 1994; Quirk et al., 2015; Desai et al., 2016). This work presents new data and semantic parsing methods on a novel and ambitious domain — natural language control of the operating system. Our long-term goal is to enable any user to perform tasks on their computers by simply stating their goals in natural language (NL). We take a first step in this direction, by providing a large new dataset (NL2Bash) of challenging but commonly used commands and expert-written descriptions, along with baseline methods to establish performance levels on this task. The NL2Bash problem can be seen as a type of semantic parsing, where th"
L18-1491,J88-4003,0,0.684663,"Missing"
L18-1491,P16-2033,0,0.00639071,"Missing"
L18-1491,P17-1041,0,0.209412,"Many bash commands have a large set of option flags, and multiple commands can be combined to solve more complex tasks. This often results in multiple correct solutions for one task (§3.3.), and poses challenges for both training and evaluation. Idiomatic Syntax The Bash interpreter uses a shallow syntactic grammar to parse pipelines, code blocks, and other high-level syntax structures. It parses command options using pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difficult to apply. 6. Table 7: Inter-annotator agreement. Previous approaches Previous NL-to-code translation work also noticed similar problems. (Kushman and Barzilay, 2013; Locascio et al., 2016) formally verify the equivalence of different regular expressions by converting them to minimal deterministic finite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash comm"
L18-1491,H94-1010,0,\N,Missing
L18-1491,P13-1042,0,\N,Missing
N07-2053,W06-3123,0,0.0276401,"Missing"
N07-2053,W06-3105,0,0.0673368,"Missing"
N07-2053,W06-1607,0,0.0279155,"e 1. A full list using phrases of up to three words would include 28 pairs. For each extracted phrase pair (s, t), feature values φ(s, t) = hlog p(s|t), log p(t|s), log l(s, t)i are computed. The first two features, the log translation and inverse translation probabilities, are estimated by counting phrase cooccurrences, following Koehn et al. (2003). The third feature is the logarithm of a lexical score l(s, t) that provides a simple form of smoothing by weighting a phrase pair based on how likely individual words within the phrases are to be translations of each other. We use a version from Foster et al. (2006), modified from (Koehn et al., 2003), which is an average of pairwise word translation probabilities. In phrase-based SMT, the decoder produces translations by dividing the source sentence into a sequence of phrases, choosing a target language phrase 210 Source Lang. Phrase Monsieur Monsieur le Monsieur le Orateur le Orateur Orateur ... le R`eglement le R`eglement le R`eglement R`eglement R`eglement R`eglement Target Lang. Phrase Mr. Mr. Mr. Speaker Speaker Speaker ... point of order of order order point of order of order order Figure 2: Phrase pairs consistent with the word alignment in Figur"
N07-2053,N03-1017,0,0.0254231,"according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric. 1 Introduction Phrase translation tables are the heart of phrasebased statistical machine translation (SMT) systems. They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set.1 The most widely used method for building phrase translation tables (Koehn et al., 2003) selects, from a word alignment of a parallel bilingual training corpus, all pairs of phrases (up to a given length) that are consistent with the alignment. This procedure 1 A “phrase” in this sense can be any contiguous sequence of words, and need not be a complete linguistic constituent. typically generates many phrase pairs that are not remotely reasonable translation candidates.2 To avoid creating translations that use these pairs, a set of features is computed for each pair. These features are used to train a translation model, and phrase pairs that produce low scoring translations are av"
N07-2053,koen-2004-pharaoh,0,0.040776,"order of order order Figure 2: Phrase pairs consistent with the word alignment in Figure 1. as a translation for each source language phrase, and ordering the target language phrases to build the final translated sentence. Each potential translation is scored according to a weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters λi of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using"
N07-2053,W02-1018,0,0.138599,"Missing"
N07-2053,J00-2004,0,0.0564255,"rase pair (s, t) in the phrase translation table, and λ is a vector of the three parameter values that were learned for these features by the full translation model. The rest of the features are ignored because they are either constant or depend on the target language sentence which is fixed during phrase extraction. In essence, we are using the subpart of a full translation model that looks at phrase pair identity and scoring the pair based on how the full model would like it. This scoring metric is used in a phrase pair selection algorithm inspired by competitive linking for word alignment (Melamed, 2000). Local competitive linking extracts high scoring phrase pairs while enforcing a redundancy constraint that minimizes the number of phrase pairs that share a common phrase. For each sentence pair in the training set, this algorithm marks the highest scoring phrase pair, according to q(s, t), containing each source language phrase and the highest scoring phrase pair containing each target language phrase. Each of these marked phrase pairs is selected and the phrase translation table is rebuilt. This is a soft redundancy constraint because a phrase pair will only be excluded if there is a higher"
N07-2053,W03-0301,0,0.0267616,"ter estimate is computed. Then, based on that estimate, a subset of the phrases is selected which, in turn, supplies a new estimate for the parameters. One question is how many times to run this reestimation procedure. We found, on the development set, that it never helped to run more than one iteration. Perhaps because of the hard nature of the algorithm, repeated iterations caused slight decreases in phrase translation table size and overall performance. 4 Experiments In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLTNAACL word-alignment workshop (Mihalcea and Pedersen, 2003). Phrase pairs are extracted from 500,000 word-aligned French-English sentence pairs. Translation quality is evaluated according to the BLEU metric (with one reference translation). Three additional disjoint data sets (from the same source) were used, one with 500 sentence pairs for minimum error rate training, another with 1000 pairs for development testing, and a final set of 2000 sentence pairs for the final test. For each experiment, we trained the full translation model as described in Section 2. Each trial varied only in the phrase translation table that was used.3 One important question"
N07-2053,P06-1065,1,0.828852,"teur , je invoque le R`egement , &quot;&quot; , Mr. Speaker , # 1 2 3 4 5 ... 23 24 25 26 27 28 &quot;&quot; I rise on a point of order Figure 1: A word aligned sentence pair. t is constructed from s. The weights λi associated with each feature fi are tuned to maximize the quality of the translations. The training procedure starts by computing a word alignment for each sentence pair in the training corpus. A word alignment is a relation between the words in two sentences where, intuitively, words are aligned to their translation in the other language. In this work, we use a discriminatively trained word aligner (Moore et al., 2006) that has state of the art performance. Figure 1 presents a high quality alignment produced by this aligner. Given a word aligned corpus, the second step is to extract a phrase translation table. Each entry in this table contains a source language phrase s, a target language phrase t, and a list of feature values φ(s, t). It is usual to extract every phrase pair, up to a certain phrase length, that is consistent with the word alignment that is annotated in the corpus. Each consistent pair must have at least one word alignment between words within the phrases and no words in either phrase can b"
N07-2053,P03-1021,0,0.0165118,"weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters λi of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using a function q(s, t) that returns a high score for source, target phrase pairs (s, t) that lead to high quality translations. • Redundancy Constraints: Our intuition is that each occurrence of a source or target language phrase really has at most one translation for that s"
N07-2053,2006.amta-papers.2,0,\N,Missing
N07-2053,D08-1076,0,\N,Missing
N13-1043,Q13-1005,1,0.800807,"clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012)"
N13-1043,P09-1010,1,0.794158,"(2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and un"
N13-1043,P11-1028,0,0.0172845,"learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-gene"
N13-1043,P11-1020,0,0.0176367,"ptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand"
N13-1043,W02-1001,0,0.012853,") These features will allow the model to capture correlations between our feature norms which provide descriptions of visual attributes, like black, and sentimental words, like gothic. Word happi student friend music confid sport casual youth waitress smart fashion monei cool relax game musician parti content friendli smooth S-Independent is used for both word prediction and ranking. For prediction, we train a linear model using averaged binary perceptron. For ranking, we try to rank all positive instances above negative instances. We use an averaged structured perceptron to train the ranker (Collins, 2002). To rank with respect to an entire query q~i , we sum the scores of each word q ∈ q~i . 6.2 Joint Sentimental Model The second approach (S-Joint) jointly models the query words to learn the relationships between literal and sentimental words with score s: s(~a|~q, D) = |~a ||~ q| X X θT f (~ ai , q~j , d~ai ) i=1 j=1 Where every word in the query has a separate factor and every position is treated independently subject to the constraint that ~a is valid. The feature function f uses the same features as the word independent model above. This model is used for ranking and generation. For rankin"
N13-1043,D09-1100,0,0.0174581,"tructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal De"
N13-1043,N10-1125,0,0.0254996,"queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has al"
N13-1043,W10-4210,0,0.0712232,"Missing"
N13-1043,W02-1011,0,0.0137282,"level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figu"
N13-1043,P10-1071,0,0.0175316,"eeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figure 2 shows the distribution over object types. We restricted descriptions to be between 3 and 15 words. I"
N13-1043,N10-1147,0,\N,Missing
N13-1043,D11-1041,0,\N,Missing
N16-1026,N15-1006,0,0.16316,"these report both state-ofthe-art speed and accuracy. Vinyals et al. (2015) instead propose embedding entire sentences in a vector space, and then generating parse trees as strings. Our model achieves state-of-the-art accuracy with a non-ensemble model trained on the standard training data, whereas their model requires ensembles or extra supervision to match the state of the art. Most work on CCG parsing has either used CKY chart parsing (Hockenmaier, 2003; Clark and Curran, 2007; Fowler and Penn, 2010; Auli and Lopez, 2011a) or shift-reduce algorithms (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). These methods rely on beam-search to cope with the huge space of possible CCG parses. Instead, we use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to reduce sparsity. Xu et al. (2015) showed"
N16-1026,N16-1052,0,0.062837,"Missing"
N16-1026,P11-1048,0,0.278995,"xical tags (we use 425). The binary rules are conjectured to be language-universal, and most language-specific 222 information is lexicalized (Steedman, 2000). The large tag set means that most (but not all) attachment decisions are determined by tagging decisions. Figure 1 shows how a prepositional phrase attachment decision can be encoded in the choice of tags. The process of assigning CCG categories to words is called supertagging. All supertaggers used in practice are probabilistic, providing a distribution over possible tags for each word. Parsing models either use these scores directly (Auli and Lopez, 2011b), or as a form of beam search (Clark and Curran, 2007), typically in conjunction with models of the dependencies or derivation. Supertag-Factored A∗ CCG Parsing Lewis and Steedman (2014a) introduced supertag-factored CCG parsers, in which the score for a parse is simply the sum of the scores of its supertags. The parser takes in a distribution over supertags for each word, and outputs the highest scoring parse—subject to the hard constraint that the parse only uses standard CCG combinators (resolving any remaining ambiguity by attaching low). One advantage of the supertag-factored model is t"
N16-1026,D11-1031,0,0.215006,"xical tags (we use 425). The binary rules are conjectured to be language-universal, and most language-specific 222 information is lexicalized (Steedman, 2000). The large tag set means that most (but not all) attachment decisions are determined by tagging decisions. Figure 1 shows how a prepositional phrase attachment decision can be encoded in the choice of tags. The process of assigning CCG categories to words is called supertagging. All supertaggers used in practice are probabilistic, providing a distribution over possible tags for each word. Parsing models either use these scores directly (Auli and Lopez, 2011b), or as a form of beam search (Clark and Curran, 2007), typically in conjunction with models of the dependencies or derivation. Supertag-Factored A∗ CCG Parsing Lewis and Steedman (2014a) introduced supertag-factored CCG parsers, in which the score for a parse is simply the sum of the scores of its supertags. The parser takes in a distribution over supertags for each word, and outputs the highest scoring parse—subject to the hard constraint that the parse only uses standard CCG combinators (resolving any remaining ambiguity by attaching low). One advantage of the supertag-factored model is t"
N16-1026,E03-1036,0,0.0742603,"ndencies Model 85.44 93.67 95.86 98.09 Table 8: Per-relation accuracy for several dependencies whose attachments are often ambiguous given the supertags. Results are only on sentences where the parsers assign the correct supertags. 100 LSTM LSTM+Dependencies 80 F1 CCG parser. With a very relaxed grammar in which all atoms can unify, dependencies features help compensate for the weakened grammar. Future work should explore further strengthening the grammar—-e.g. marking plurality on NP s to enforce plural agreement, or using slash-modalities to prevent over-generation arising from composition (Baldridge and Kruijff, 2003). 60 40 0 10 20 30 Dependency Length 40 Figure 3: F1 on dependencies of various lengths. on the ‘attach low’ heuristic with current models. Supertag-factored model is accurate on longrange dependencies One motivation for CCG parsing is to recover long-range dependencies. While we do not explicitly model these dependencies, they can still be extracted from the parse. Instead, we rely on the LSTM supertagger to implicitly model the dependencies—a task that becomes more challenging with longer dependencies. We investigate the accuracy of our parser for dependencies of different lengths. Figure 3"
N16-1026,D13-1195,0,0.0205631,"improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combination of deep learning, linguistics and classic AI search can be used to build a parser with both state-of-the-art speed and accuracy. Future work will explore using our parser to recover other representations from CCG, such as Universal Dependencies (McDonald et al., 2013) or 229 semantic roles. The major obstacle is the mismatch between these representations and CCGbank—we will therefore investigate new techniques for obtaining other representati"
N16-1026,D14-1082,0,0.335385,"ptually simple, allows for the reuse of existing optimal and efficient parsing algorithms, benefits significantly from semi-supervised learning, and is highly accurate both in and out of domain. The parser is publicly released.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a so"
N16-1026,P15-1038,0,0.0052737,"hms, the efficiency of our model depends directly on the accuracy of the supertagger in guiding the search. We therefore measure the efficiency empirically. Results are shown in Table 4.5 Our parser runs more slowly than E ASY CCG on CPU, due to the more complex tagging model (but is 4.8 F1 more accurate). Adding dependencies substantially reduces efficiency, due to calculating sparse features. Without dependencies, the run time is dominated by the LSTM supertagger. Running the supertagger on a GPU reduces parsing times dramatically— outperforming SpaCy, the fastest publicly available parser (Choi et al., 2015). Roughly half the parsing time is spent on GPU supertagging, and half on CPU parsing. To better exploit batching in the GPU, our implementation dynamically buckets sentences by length (bins of width 10), and tags batches when the bucket size reaches 3072 (the number of threads on our GPU). We are not aware of any GPU implementations of shift-reduce parsers or lexicalized chart parsers, so it is unclear if most other state-ofthe-art parsers can be adapted to exploit GPUs. 4 honnibal.github.io/spaCy All timing experiments use a single 3.5GHz core and (where applicable) a single NVIDIA TITAN X G"
N16-1026,J07-4004,0,0.870638,"ured to be language-universal, and most language-specific 222 information is lexicalized (Steedman, 2000). The large tag set means that most (but not all) attachment decisions are determined by tagging decisions. Figure 1 shows how a prepositional phrase attachment decision can be encoded in the choice of tags. The process of assigning CCG categories to words is called supertagging. All supertaggers used in practice are probabilistic, providing a distribution over possible tags for each word. Parsing models either use these scores directly (Auli and Lopez, 2011b), or as a form of beam search (Clark and Curran, 2007), typically in conjunction with models of the dependencies or derivation. Supertag-Factored A∗ CCG Parsing Lewis and Steedman (2014a) introduced supertag-factored CCG parsers, in which the score for a parse is simply the sum of the scores of its supertags. The parser takes in a distribution over supertags for each word, and outputs the highest scoring parse—subject to the hard constraint that the parse only uses standard CCG combinators (resolving any remaining ambiguity by attaching low). One advantage of the supertag-factored model is that it allows a simple A∗ parsing algorithm, which prova"
N16-1026,W02-2203,0,0.159233,"upervision to match the state of the art. Most work on CCG parsing has either used CKY chart parsing (Hockenmaier, 2003; Clark and Curran, 2007; Fowler and Penn, 2010; Auli and Lopez, 2011a) or shift-reduce algorithms (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). These methods rely on beam-search to cope with the huge space of possible CCG parses. Instead, we use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to reduce sparsity. Xu et al. (2015) showed further improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al"
N16-1026,P15-1030,0,0.0321292,"leased.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a softmax potential over tags for each word in the sentence. During training, we jointly optimize all LSTM parameters, including the word embeddings, to maximize the conditional likelihood of supertag sequences. For inference,"
N16-1026,P15-1033,0,0.0396832,"ting optimal and efficient parsing algorithms, benefits significantly from semi-supervised learning, and is highly accurate both in and out of domain. The parser is publicly released.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a softmax potential over tags for each word"
N16-1026,P10-1035,0,0.0379059,"derson et al., 2013; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). Unlike our approach, none of these report both state-ofthe-art speed and accuracy. Vinyals et al. (2015) instead propose embedding entire sentences in a vector space, and then generating parse trees as strings. Our model achieves state-of-the-art accuracy with a non-ensemble model trained on the standard training data, whereas their model requires ensembles or extra supervision to match the state of the art. Most work on CCG parsing has either used CKY chart parsing (Hockenmaier, 2003; Clark and Curran, 2007; Fowler and Penn, 2010; Auli and Lopez, 2011a) or shift-reduce algorithms (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). These methods rely on beam-search to cope with the huge space of possible CCG parses. Instead, we use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural ne"
N16-1026,P14-1020,0,0.216452,"ng the lower-right parse). highest probability sequence of tags that combine to produce a complete parse tree. Whenever there is parsing ambiguity not specified by the supertags, the model attaches low (see Figure 1). This approach is not only conceptually simple but also highly effective, as we demonstrate with extensive experiments. Because the A* algorithm is extremely efficient and the LSTMs can be run in parallel on GPUs, the end-to-end parser can process over 2600 sentences per second. This is more than three times the speed of any publicly available parser for any formalism. Apart from Hall et al. (2014), we are not aware of efficient algorithms for running other state-of-art-parsers on GPUs. The LSTM parameters also benefit from semi-supervised training, which we demonstrate by employing a recently introduced tri-training scheme (Weiss et al., 2015). Finally, the recurrent nature of the LSTM allows for effective modelling of long distance dependencies, as we show empirically. Our approach significantly advances the state-of-the-art on benchmark datasets—improving accuracy by 1.1 F1 in domain and up to 4.5 F1 out of domain. 2 Background Combinatory Categorial Grammar (CCG) Compared to a phras"
N16-1026,Q15-1007,0,0.0111969,", Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combination of deep learning, linguistics and classic AI search can be used to build a parser with both state-of-the-art speed and accuracy. Future work will explore using our parser to recover other representations from CCG, such as Universal Dependencies (McDonald et al., 2013) or 229 semantic roles. The major obstacle is the mismatch between these representations and CCGbank—we will therefore investigate new techniques for obtaining other representations from CCG parses. We will also explore new A∗ parsing algorithms that explicitly mode"
N16-1026,J13-4006,0,0.0203766,"advantages: it is conceptually simple, allows for the reuse of existing optimal and efficient parsing algorithms, benefits significantly from semi-supervised learning, and is highly accurate both in and out of domain. The parser is publicly released.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-direc"
N16-1026,J07-3004,0,0.295093,"such implementations. In contrast, our supertagger only uses matrix operations, and does not take any parse state as input— meaning it is straightforward to run on a GPU. To exploit the parallelism of GPUs, we process thousands of sentences simultaneously—improving parsing efficiency by an order-of-magnitude over CPU. A major advantage of our model is that it allows all of the computationally intensive decisions to occur on GPUs. Unlike existing GPU parsers, the LSTM can be run with generic library code.2 7 7.1 Experiments Experimental setup We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development, and Section 23 for test. Our experiments use a supertagger beam of 10−4 — which does not affect the final scores, but reduces overheads such as building the initial agenda. 2 We use TensorFlow (Abadi et al., 2015). 225 Dev 91.5 91.3 93.1 94.1 94.9 Test 92.0 91.6 93.0 94.3 94.7 Table 1: Supertagging accuracy on CCGbank. Model C&C C&C + RNN E ASY CCG Dependencies LSTM LSTM + Dependencies LSTM + Tri-training LSTM + Tri-training + Dependencies P 86.2 87.7 83.7 86.5 87.7 88.2 88.6 88.2 R 84.2 86.4 83.0 85.8 86.7 87.3 87.5 87.3 F1 85.2 87.0 83.3 86.1 87.2 87.8 88."
N16-1026,U11-1006,0,0.0283559,"showed further improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combination of deep learning, linguistics and classic AI search can be used to build a parser with both state-of-the-art speed and accuracy. Future work will explore using our parser to recover other representations from CCG, such as Universal Dependencies (McDonald et al., 2013) or 229 semantic roles. The major obstacle is the mismatch between these representations and CCGbank—we will therefore investigate new techniques for obtainin"
N16-1026,D14-1107,1,0.92549,"a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a softmax potential over tags for each word in the sentence. During training, we jointly optimize all LSTM parameters, including the word embeddings, to maximize the conditional likelihood of supertag sequences. For inference, we use a recently introduced A* CCG parsing algorithm (Lewis and Steedman, 2014a), which efficiently searches for the 1 http://github.com/mikelewis0/EasySRL 221 Proceedings of NAACL-HLT 2016, pages 221–231, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics I saw squirrels NP (S NP )/NP NP with I nuts NP suburbs NP (NP NP )/NP NP NP NP &gt; &lt; NP NP binoculars &gt; NP ((S NP )(S NP )/NP NP S NP &lt; &gt; of &lt; &lt; S Paris (NP NP )/NP NP NP NP &gt; (S NP )(S NP ) S NP &lt; S in with &gt; S NP houses squirrels NP (S NP )/NP (NP NP )/NP NP NP NP saw houses in suburbs NP (NP NP )/NP NP of (NP NP )/NP NP NP NP &gt; NP &lt; NP NP NP Paris &gt; &lt; &gt; &lt; F"
N16-1026,P13-2017,0,0.0196274,"ore long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combination of deep learning, linguistics and classic AI search can be used to build a parser with both state-of-the-art speed and accuracy. Future work will explore using our parser to recover other representations from CCG, such as Universal Dependencies (McDonald et al., 2013) or 229 semantic roles. The major obstacle is the mismatch between these representations and CCGbank—we will therefore investigate new techniques for obtaining other representations from CCG parses. We will also explore new A∗ parsing algorithms that explicitly model the global parse structure using neural networks, while maintaining optimality guarantees. Acknowledgements We thank Bharat Ram Ambati, Greg Coppola, Chloé Kiddon, Luheng He, Yannis Konstas and the anonymous reviewers for comments on an earlier version, and Mark Yatskar for helpful discussions. This research was supported in part"
N16-1026,D08-1050,0,0.0497354,"ser SpaCy*4 Berkeley GPU* (Hall et al., 2014) Chen and Manning (2014)* C&C E ASY CCG LSTM LSTM + Dependencies LSTM GPU 74.5 77.2 77.7 80.5 79.4 82.2 Table 3: Out-of-domain experiments. and the Dependency model outperforms the LSTM alone, showing that dependency features are capturing some generalizations that the LSTM does not. However, semi-supervised learning substantially improves the LSTM, matching the accuracy of the ensemble—showing that the LSTM is expressive enough to compensate given sufficient data. 7.4 Out-of-domain Experiments We also evaluate on two out-of-domain datasets used by Rimell and Clark (2008), but did no development on this data. In both cases, we use Rimell and Clark’s scripts for converting CCG parses to the target dependency representations. The datasets are: QUESTIONS 500 questions from TREC (Rimell and Clark, 2008). Questions frequently contain very long range dependencies, providing an interesting test of the LSTM supertagger’s ability to capture unbounded dependencies. We follow Rimell and Clark by re-training the supertagger on the concatenation of the CCGbank training data and 10 copies of the QUESTIONS training data. BIOINFER 500 sentences from biomedical abstracts. This"
N16-1026,P13-1045,0,0.0672444,"fits significantly from semi-supervised learning, and is highly accurate both in and out of domain. The parser is publicly released.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a softmax potential over tags for each word in the sentence. During training, we jointly optimize"
N16-1026,P10-1040,0,0.0120661,"rain the model using stochastic gradient descent, with a minibatch size of 1, a learning rate of 0.01, and using momentum with µ = 0.7. We then fine-tune models using a larger minibatch size of 32. Gradients whose L2 norm exceeds 5 are clipped. Training was run for 30 epochs, shuffling the order of sentences after each epoch, and we used the model parameters with the highest development supertagging accuracy. The input layer uses dropout with a rate of 0.5. All trainable parameters have L2 regularization of Λ = 10−6 . Word embedding are initialized using 50-dimensional pre-trained values from Turian et al. (2010). For prefix and suffix embeddings, we use randomly initialized 32dimensional vectors—features occurring less than 3 times are replaced with an ‘unknown’ embedding. We add special start and end tokens to each sentence, with trainable parameters. The LSTM state size is 128 and the R E LU layer has a size of 64. 4 Parsing Models Our experiments focus on two parsing models: Supertag-Factored We use the supertagging model described in Section 3 to build a supertagfactored parser, closely following the approach described in Section 2. We also add a penalty of 0.1 224 (tuned on development data) for"
N16-1026,N16-1027,0,0.293257,"a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to reduce sparsity. Xu et al. (2015) showed further improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combinati"
N16-1026,P15-1032,0,0.307485,"or the reuse of existing optimal and efficient parsing algorithms, benefits significantly from semi-supervised learning, and is highly accurate both in and out of domain. The parser is publicly released.1 Neural networks have shown strong performance in a range of NLP tasks; however they can break the dynamic programs for structured prediction problems, such as parsing, when vector embeddings are recursively computed for subparts of the output. Existing neural net parsers either (1) use greedy inference techniques including shift-reduce parsing (Henderson et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015), constituency parse re-ranking (Socher et al., 2013), and stringto-string transduction (Vinyals et al., 2015), or (2) avoid recursive computations entirely (Durrett and Klein, 2015). Our approach gives a simple alternative: we only train a model for tagging decisions, where we can easily use recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997), and rely on the highly lexicalized nature of the CCG grammar to allow this tagger to specify nearly every aspect of the complete parse. Our LSTM supertagger is bi-directional and includes a softmax potential over"
N16-1026,P14-1021,0,0.250488,"pproach, none of these report both state-ofthe-art speed and accuracy. Vinyals et al. (2015) instead propose embedding entire sentences in a vector space, and then generating parse trees as strings. Our model achieves state-of-the-art accuracy with a non-ensemble model trained on the standard training data, whereas their model requires ensembles or extra supervision to match the state of the art. Most work on CCG parsing has either used CKY chart parsing (Hockenmaier, 2003; Clark and Curran, 2007; Fowler and Penn, 2010; Auli and Lopez, 2011a) or shift-reduce algorithms (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). These methods rely on beam-search to cope with the huge space of possible CCG parses. Instead, we use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to reduce sparsity. X"
N16-1026,N16-1025,0,0.0774333,"use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to reduce sparsity. Xu et al. (2015) showed further improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015)."
N16-1026,W11-2921,0,0.0283919,"Xu et al. (2015) showed further improvements by using RNNs to condition on non-local context. Concurrently with this work, Xu et al. (2016) explored bidirectional RNN models, and Vaswani et al. (2016) use bidirectional LSTMs with a different training procedure. Our tagging model is closely related to the bidirectional LSTM POS tagging model of Ling et al. (2015). We see larger gains over the state-ofthe-art—likely because supertagging involves more long-range dependencies than POS tagging. Other work has successfully applied GPUs to parsing, but has required GPU-specific code and algorithms (Yi et al., 2011; Johnson, 2011; Canny et al., 2013; Hall et al., 2014). GPUs have also been used for machine translation (He et al., 2015). 10 Conclusions and Future Work We have shown that a combination of deep learning, linguistics and classic AI search can be used to build a parser with both state-of-the-art speed and accuracy. Future work will explore using our parser to recover other representations from CCG, such as Universal Dependencies (McDonald et al., 2013) or 229 semantic roles. The major obstacle is the mismatch between these representations and CCGbank—we will therefore investigate new techniqu"
N16-1026,P11-1069,0,0.0551518,"l., 2015). Unlike our approach, none of these report both state-ofthe-art speed and accuracy. Vinyals et al. (2015) instead propose embedding entire sentences in a vector space, and then generating parse trees as strings. Our model achieves state-of-the-art accuracy with a non-ensemble model trained on the standard training data, whereas their model requires ensembles or extra supervision to match the state of the art. Most work on CCG parsing has either used CKY chart parsing (Hockenmaier, 2003; Clark and Curran, 2007; Fowler and Penn, 2010; Auli and Lopez, 2011a) or shift-reduce algorithms (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). These methods rely on beam-search to cope with the huge space of possible CCG parses. Instead, we use Lewis and Steedman (2014a)’s A∗ algorithm. By using a semi-supervised LSTM supertagger, we improved over Lewis and Steedman’s parser by 4.8 F1. CCG supertagging was first attempted with maximum-entropy Markov models (Clark, 2002)— in practice, the combination of sparse features and a large tag set makes such models brittle. Lewis and Steedman (2014b) applied feed-forward neural networks to supertagging, motivated by using pretrained work embeddings to r"
N16-1026,J99-2004,0,\N,Missing
N16-1026,D15-1176,0,\N,Missing
N16-1026,Q14-1026,1,\N,Missing
N16-1026,P15-2041,0,\N,Missing
N16-1026,D15-1169,1,\N,Missing
N18-1081,D11-1142,0,0.960397,"corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4)."
N18-1081,N03-1013,0,0.11271,"Missing"
N18-1081,W12-3010,0,0.210272,"Missing"
N18-1081,P17-1044,1,0.941233,"ue to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independen"
N18-1081,P15-1034,0,0.374367,"o the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling"
N18-1081,D15-1076,1,0.951313,"partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader e"
N18-1081,J93-2004,0,0.0607468,"imilar to other cases in NLP, we would like to allow some variability in the predicted tuples. For example, for the sentence The sheriff standing against the wall spoke in a very soft voice we would want to treat both (The Sheriff; spoke; in a soft voice) and (The sheriff standing against the wall; spoke; in a very soft voice) as acceptable extractions. To that end, we follow He et al. (2015) which judge an argument as correct if and only if it includes the syntactic head of the gold argument (and similarly for predicates). For OIE2016, we use the available Penn Treebank gold syntactic trees (Marcus et al., 1993), while for the other test sets, we use predicted trees instead. While this metric may sometimes be too lenient, it does allow a more balanced and fair comparison between systems which can make different, but equally valid, span boundary decisions. 6.3 Performance Analysis In our analysis, we find that RnnOIE generalizes to unseen predicates, produces more and shorter arguments on average than are in the gold extractions, and, like all of the systems we tested, struggles with nominal predicates. Unseen predicates We split the propositions in the gold and predicted OIE2016 test set into two par"
N18-1081,P11-1062,1,0.79897,"e labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for"
N18-1081,D12-1048,0,0.899671,"and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed whil"
N18-1081,D16-1006,0,0.270204,"tions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et al., 2016) postprocess dependency trees, and Open IE42 extracts tuples from Semantic Role"
N18-1081,N18-2089,1,0.915945,"rom an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps d"
N18-1081,D13-1043,0,0.0752202,"extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in prev"
N18-1081,W16-1307,0,0.223307,") was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et a"
N18-1081,D14-1162,0,0.0820803,"Missing"
N18-1081,W95-0107,0,0.240007,"ed below the dashed lines, where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL gener"
N18-1081,E99-1023,0,0.376532,"where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL generally deals with single-wor"
N18-1081,W17-5402,0,0.151396,"allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in previous Open IE efforts (WEB"
N18-1081,D16-1252,1,0.932243,"the-art in Open IE on benchmark datasets. 1 (mercury filling; particularly prevalent; in the USA) (mercury filling; causes; antibiotic resistance) (mercury filling; was banned; in the EU; partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic p"
N18-1081,P10-1013,0,0.735933,"Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectivel"
N18-1081,N13-1107,0,0.0293875,"ce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets a"
N18-1081,D14-1038,0,0.0579622,"(Banko et al., 2007) was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) an"
N18-1081,P15-1109,0,0.0829896,"However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were co"
N18-1170,P05-1074,0,0.748798,"inal test sets (§5). Together these results not only establish the first general purpose syntactically controlled paraphrase approach, but also suggest that this general paradigm could be used for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nemat"
N18-1170,N03-1003,0,0.138616,"Missing"
N18-1170,D08-1021,0,0.0149106,"Missing"
N18-1170,2005.mtsummit-ebmt.3,0,0.272082,"Missing"
N18-1170,D17-1091,0,0.091119,"Missing"
N18-1170,W17-5401,0,0.101857,"Missing"
N18-1170,D12-1139,0,0.0195225,"odel is trained to produce s2 . To overcome learned biases of the NMT system, we also include reversed pairs hs2 , s1 i during training. 2.2.1 Syntactic templates To provide syntactic control, we linearize the bracketed parse structure without leaf nodes (i.e., tokens). For example, the corresponding linearized parse 2 Syntactic diversity was measured by the entropy of the top two levels of parse trees in the corpora. 3 Similar automated filtering could be used to produce data for many other transformations, such as tense changes, pointof-view shifts, and even stylometric pattern differences (Feng et al., 2012). This is an interesting area for future work. 4 Because of the large dataset size, we use the faster but less accurate shift-reduce parser written by John Bauer. 1876 tree for the sentence “She drove home.” is (S(NP(PRP))(VP(VBD)(NP(NN)))(.)). A system that requires a complete linearized target parse at test-time is unwieldy; how do we go about choosing the target parse? To simplify test-time usage, we relax the target syntactic form to a parse template, which we define as the top two levels of the linearized parse tree (the level immediately below the root along with the root); the prior exa"
N18-1170,W17-4912,0,0.0524541,"Missing"
N18-1170,D17-1215,0,0.169957,"nguage processing datasets often suffer from a dearth of linguistic variation, which can hurt the generalization of models trained on them. Recent work has shown it is possible to easily “break” many learned models by evaluating them on adversarial examples (Goodfellow et al., 2015), which are generated by manually introducing lexical, pragmatic, and syntactic variation not seen in the training set (Ettinger et al., 2017). Robustness to such adversarial examples can potentially be improved by augmenting the training data, as shown by prior work that introduces rulebased lexical substitutions (Jia and Liang, 2017; FAuthors contributed equally. The man is standing in the water at the base of a waterfall Liang et al., 2017). However, more complex transformations, such as generating syntactically adversarial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the"
N18-1170,N10-1017,0,0.045833,"22.3 18.3 17.7 Table 1: A crowdsourced paraphrase evaluation on a three-point scale (0 = no paraphrase, 1 = ungrammatical paraphrase, 2 = grammatical paraphrase) shows both that NMT- BT and SCPN produce mostly grammatical paraphrases. Feeding parse templates to SCPN instead of full parses does not impact its quality. 4.1 Intrinsic Experiments w/ full parses w/ templates NMT- BT SCPN 2 Paraphrase quality & grammaticality To measure paraphrase quality and grammaticality, we perform a crowdsourced experiment in which workers are asked to rate a paraphrase pair hs, gi on the three-point scale of Kok and Brockett (2010), where s is the source sentence and g is the generated sentence. A 0 on this scale indicates no paraphrase relationship, while 1 means that g is an ungrammatical paraphrase of s and 2 means that g is a grammatical paraphrase of s. We select 100 paraphrase pairs from the development set of our PARA NMT-50M split (after the postprocessing steps detailed in Section 3.3) and have three workers rate each pair.7 To focus the evaluation on the effect of syntactic manipulation on quality, we minimum paraphrastic similarity to 0.7. 7 We use the Crowdflower platform for our experiments. 1878 only selec"
N18-1170,P16-1094,0,0.0138315,"is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effective way of generating adversarial examples. Using a parser, we"
N18-1170,J10-3003,0,0.0423198,"ency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this dataset. Shen et al. (2006) extend this to deeper syntactic analysis. All of these approaches use syntax to 7 Related Work Paraphrase generation (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) has been tackled using many different methods, including those based on hand-crafted rules (McKeown, 1983), synonym substitution (Bolshakov and Gelbukh, 2004), machine translation (Quirk et al., 2004), and, most recently, deep learning (Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017). Our syntactically controlled setting also relates to controlled language generation tasks in which one desires to generate or rewrite a sentence with particular characteristics. We review related work in both 13 A configuration without the copy mechanism copies input syntax even more, with a 47."
N18-1170,E17-1083,0,0.0546964,"d for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nematus NMT system (Sennrich et al., 2017). The training data of this system includes four sources: Common Crawl, CzEng 1.6, Europarl, and News Commentary. The CzEng corpus is the larges"
N18-1170,S14-2001,0,0.00800909,"he problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification proposed by Li et al. (2016b) for NMT- BT but found that it dr"
N18-1170,P06-2096,0,0.106453,"Missing"
N18-1170,J83-1001,0,0.445484,"rial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the target. General purpose syntactically controlled paraphrase generation is a challenging task. Approaches that rely on handcrafted rules and grammars, such as the question generation system of McKeown (1983), support only a limited number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exist"
N18-1170,N16-3013,0,0.122415,"Missing"
N18-1170,D17-1299,0,0.0213328,"Missing"
N18-1170,D13-1170,0,0.0143659,"hrases for adversarial example generation. To formalize the problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification pro"
N18-1170,N03-1024,0,0.454247,"Missing"
N18-1170,P17-3007,0,0.0644294,"Missing"
N18-1170,D14-1162,0,0.12426,"k and ignore all phrase-level labels (because our paraphrase models are trained on only sentences). Table 4 shows that for both datasets, SCPN breaks many more examples than NMT- BT. Moreover, as shown in Table 5, NMT- BT’s paraphrases differ from the original example mainly by lexical substitutions, while SCPN often produces dramatically different syntactic structures. 5.3 Are the adversarial examples valid? We have shown that we can break pretrained models with controlled paraphrases, but are these paraon the three-point scale. 10 We initialize both models using pretrained GloVe embeddings (Pennington et al., 2014) and set the LSTM hidden dimensionality to 300. 11 Since the SICK development dataset is tiny, we additionally generate adversarial examples on its test set. phrases actually valid adversarial examples? After all, it is possible that the syntactic modifications cause informative clauses or words (e.g., negations) to go missing. To measure the validity of our adversarial examples, we turn again to crowdsourced experiments. We ask workers to choose the appropriate label for a given sentence or sentence pair (e.g., positive or negative for SST), and then we compare the worker’s judgment to the or"
N18-1170,P15-1150,0,0.0186117,"ime comes . ” you said . can i get a good burglar when the time comes ? look at the time the thief comes . Table 3: Syntactically controlled paraphrases generated by SCPN for two examples from the PARA NMT-50M development set. For each input sentence, we show the outputs of four different templates; the fourth template is a failure case (highlighted in green) exhibiting semantic divergence and/or ungrammaticality, which occurs when the target template is unsuited for the input. for which to generate adversarial examples; we use the bidirectional LSTM baseline for both SST and SICK outlined in Tai et al. (2015) since it is a relatively simple architecture that has proven to work well for a variety of problems.10 Since the SICK task involves characterizing the relationship between two sentences, for simplicity we only generate adversarial examples for the first sentence and keep the second sentence fixed to the ground truth. 5.2 Breaking pretrained models For each dataset, we generate paraphrases for held-out examples and then run a pretrained model over them.11 We consider a development example x broken if the original prediction yx is correct, but the prediction yx0 for at least one paraphrase x0 i"
N18-1170,C16-1275,0,0.301174,"Missing"
N18-1170,D17-1228,0,0.0560075,"Missing"
N18-1170,W04-3219,0,0.398798,"paraphrase generation, noting two primary families: template-based and translationbased. The first family includes approaches that use hand-crafted rules (McKeown, 1983), thesaurus-based substitution (Bolshakov and Gelbukh, 2004; Zhang and LeCun, 2015), lattice matching (Barzilay and Lee, 2003), and templatebased “shake & bake” paraphrasing (Carl et al., 2005). These methods often yield grammatical outputs but they can be limited in diversity. The second family includes methods that rewrite the input using methods based on parallel text (Bannard and Callison-Burch, 2005), machine translation (Quirk et al., 2004; Napoles et al., 2016; Suzuki et al., 2017), or related statistical techniques (Zhao et al., 2009). Of particular relevance to our work are methods that incorporate syntax to improve fluency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this"
N18-1170,P17-1099,0,0.0157426,"either parse templates or full parses depending on their desired level of control. 3 Syntactically Controlled Paraphrase Networks The SCPN encoder-decoder architecture is built from standard neural modules, as we describe in this section. 3.1 Neural controlled paraphrase generation Given a sentential paraphrase pair hs1 , s2 i and a corresponding target syntax tree p2 for s2 , we encode s1 using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and our decoder is a two-layer LSTM augmented with soft attention over the encoded states (Bahdanau et al., 2014) as well as a copy mechanism (See et al., 2017). Following existing work in NMT (Sennrich et al., 2015), we preprocess s1 and s2 into subword units using byte pair encoding, and we perform decoding using beam search. For all attention computations, we use a bilinear product with a learned parameter matrix W: given vectors u and v, we score them by uT Wv. We incorporate the target syntax p2 into the generation process by modifying the inputs to the decoder. In particular, a standard decoder LSTM receives two inputs at every time step: (1) the embedding wt−1 of the ground-truth previous word in s2 , and (2) an attention-weighted average at o"
N18-1170,D17-1026,1,0.789198,"ed number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exists publicly, we follow Wieting et al. (2017) and automatically generate millions of paraphrase pairs using neural backtranslation. Backtranslation naturally injects linguistic variation between the original sentence and its backtranslated counterpart. By running the process at a very large scale and testing for the specific variations we want to produce, we can gather ample input-output pairs for a wide range of phenomena. Our focus is on syntactic transformations, which we define using templates derived from linearized constituency parses (§2). Given such parallel data, we can easily train an encoder-decoder model that takes a sentence"
N18-1170,E17-3017,0,0.0397283,"Missing"
N18-1170,D11-1038,0,0.00804775,"ation than existing uncontrolled paraphrase generation systems, instead preferring purely syntactic modifications. It is capable of generating adversarial examples that fool pretrained NLP models. Furthermore, by training on such examples, we increase the robustness of these models to syntactic variation. Controlled language generation There is growing interest in generating language with the ability to influence the topic, style, or other properties of the output. Most related to our methods are those based on syntactic transformations, like the tree-to-tree sentence simplification method of Woodsend and Lapata (2011) based on quasi-synchronous grammar (Smith and Eisner, 2006). Our method is more general since we do not require a grammar and there are only soft constraints. Perhaps the closest to the proposed method is the conditioned recurrent language model of Ficler and Goldberg (2017), which produces language with user-selected properties such as sentence length and formality but is incapable of generating paraphrases. For machine translation output, Niu et al. (2017) 8 Conclusion Acknowledgments We thank the reviewers for their insightful comments. We would also like to thank Mark Yatskar for many use"
N18-1170,N16-1005,0,0.00722262,"generator to produce viable full parses. improve grammaticality, which is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effect"
N18-1170,P09-1094,0,0.169719,"Missing"
N18-1170,W06-3104,0,\N,Missing
N18-1170,P14-5010,0,\N,Missing
N18-1202,P17-1080,0,0.874363,"eneralize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using lan"
N18-1202,Q17-1010,0,0.531036,"eled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual"
N18-1202,D15-1075,0,0.770513,"Missing"
N18-1202,P17-1152,0,0.665298,"- 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstr"
N18-1202,Q16-1026,0,0.779805,"Missing"
N18-1202,W14-4012,0,0.0858828,"Missing"
N18-1202,D16-1245,0,0.0232835,"Missing"
N18-1202,D13-1203,0,0.0178787,"Missing"
N18-1202,D17-1206,0,0.793935,"to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mi"
N18-1202,P17-1044,1,0.395137,"Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to pred"
N18-1202,P16-1085,0,0.422547,"nd “player”, “game” as nouns) but concentrated in the sportsrelated senses of “play”. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM’s context representation of “play” in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence. These observations can be quantified using an 2233 resentations have F1 of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et"
N18-1202,P16-1101,0,0.700562,"compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder. Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important"
N18-1202,J93-2004,0,0.116591,"al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD"
N18-1202,K16-1006,0,0.86068,"eviously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful mo"
N18-1202,N16-1030,0,0.746938,"Missing"
N18-1202,D17-1018,1,0.121179,"Missing"
N18-1202,H94-1046,0,0.123123,"2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers. Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training. Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a). Overall, the biLM top layer rep5.3 What information is captured by the biLM’s representations? Since adding ELMo improves task performance over word vec"
N18-1202,E17-1002,0,0.0183651,"Missing"
N18-1202,D14-1113,0,0.0176931,"nnington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed wi"
N18-1202,J05-1004,0,0.0609658,"Missing"
N18-1202,D14-1162,0,0.137952,"the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language u"
N18-1202,P17-1161,1,0.697802,"ting models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems. Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on superv"
N18-1202,W12-4501,0,0.343829,"Missing"
N18-1202,D17-1120,0,0.865474,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,E17-1010,0,0.821604,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,N16-1114,0,0.0336672,"Missing"
N18-1202,P15-1109,0,0.0952398,"Missing"
N18-1202,C16-1329,0,0.694006,"Missing"
N18-1202,D13-1170,0,0.0814613,"Missing"
N18-1202,P16-2038,0,0.361634,"biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai a"
N18-1202,P10-1040,0,0.0362285,"neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform 2227 Proceedings of NAACL-HLT 2018, pages 2227–2237 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors f"
N18-1202,P17-1018,0,0.676855,"Missing"
N18-1202,D16-1157,0,0.311052,"from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approache"
N18-1202,D15-1176,0,\N,Missing
N18-1202,W13-3516,0,\N,Missing
N18-1202,D16-1264,0,\N,Missing
N18-2089,W04-2705,0,0.491097,"here is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwn"
N18-2089,P13-1023,0,0.0755469,"Missing"
N18-2089,P98-1013,0,0.843785,"ated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-H"
N18-2089,J05-1004,0,0.608029,"(He et al., 2015), each question-answer pair corresponds to a predicateargument relationship. There is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations"
N18-2089,W13-2322,0,0.308993,"r from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-HLT 2018, pages 560–568 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Asso"
N18-2089,D16-1252,1,0.904315,"Missing"
N18-2089,J93-2004,0,0.0727984,"Missing"
N18-2108,P15-1136,0,0.303812,"Missing"
N18-2108,D16-1245,0,0.4354,"ction. Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark. We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model. Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work. Introduction Recent coreference resolution systems have heavily relied on first order models (Clark and Manning, 2016a; Lee et al., 2017), where only pairs of entity mentions are scored by the model. These models are computationally efficient and scalable to long documents. However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent. Figure 1 shows an example from Wiseman et al. (2016) that illustrates this failure case. The plurality of [you] is underspecified, making it locally compatible with both [I] and [all of you], while the full cluster would have mixed plurality, resulting in global incons"
N18-2108,P16-1061,0,0.379479,"ction. Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark. We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model. Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work. Introduction Recent coreference resolution systems have heavily relied on first order models (Clark and Manning, 2016a; Lee et al., 2017), where only pairs of entity mentions are scored by the model. These models are computationally efficient and scalable to long documents. However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent. Figure 1 shows an example from Wiseman et al. (2016) that illustrates this failure case. The plurality of [you] is underspecified, making it locally compatible with both [I] and [all of you], while the full cluster would have mixed plurality, resulting in global incons"
N18-2108,D08-1069,0,0.100833,"Missing"
N18-2108,D13-1203,0,0.243127,"Missing"
N18-2108,D17-1018,1,0.901053,"w that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark. We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model. Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work. Introduction Recent coreference resolution systems have heavily relied on first order models (Clark and Manning, 2016a; Lee et al., 2017), where only pairs of entity mentions are scored by the model. These models are computationally efficient and scalable to long documents. However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent. Figure 1 shows an example from Wiseman et al. (2016) that illustrates this failure case. The plurality of [you] is underspecified, making it locally compatible with both [I] and [all of you], while the full cluster would have mixed plurality, resulting in global inconsistency. We introduc"
N18-2108,C02-1139,0,0.495708,"Missing"
N18-2108,D14-1162,0,0.0812569,"1, setting a new state of the art for coreference resolution. Compared to the heuristic pruning with up to 250 antecedents, our coarse-to-fine model only computes the expensive scores sa (i, j) for 50 antecedents. Despite using far less computation, it outperforms the baseline because the coarse scores • We used embedding representations from a language model (Peters et al., 2018) at the input to the LSTMs (ELMo in the results). • We changed several hyperparameters: 1. increasing the maximum span width from 10 to 30 words. 2. using 3 highway LSTMs instead of 1. 3. using GloVe word embeddings (Pennington et al., 2014) with a window size 1 Results https://github.com/kentonl/e2e-coref 690 Acknowledgements sc (i, j) can be computed for all antecedents, enabling the model to potentially predict a coreference link between any two spans in the document. As a result, we observe a much higher recall when adopting the coarse-to-fine approach. The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We also thank the UW NLP group for helpful conver"
N18-2108,N18-1202,1,0.891659,"rence links can reach much further in natural language discourse. First stage Keep the top M spans based on the mention score sm (i) of each span. Second stage Keep the top K antecedents of each remaining span i based on the first three factors, sm (i) + sm (j) + sc (i, j). 689 MUC Prec. Rec. F1 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Martschat and Strube (2015) Clark and Manning (2015) Wiseman et al. (2015) Wiseman et al. (2016) Clark and Manning (2016b) Clark and Manning (2016a) 76.7 76.1 76.2 77.5 79.9 79.2 66.1 65.6 66.2 66.8 71.0 69.9 59.5 59.4 59.4 62.1 63.8 63.5 Lee et al. (2017) + ELMo (Peters et al., 2018) + hyperparameter tuning 78.4 73.4 75.8 80.1 77.2 78.6 80.7 78.8 79.8 + coarse-to-fine inference + second-order inference 80.4 79.9 80.1 81.4 79.5 80.4 68.1 69.4 69.3 69.8 69.3 70.4 72.2 72.6 72.6 73.4 74.2 74.6 54.2 56.0 55.8 57.0 56.5 58.0 59.6 60.4 60.5 61.5 63.0 63.4 52.3 53.0 54.9 53.9 54.3 55.5 Avg. F1 55.7 56.0 57.1 57.7 58.7 59.2 62.5 63.0 63.4 64.2 65.3 65.7 68.6 61.8 65.0 69.8 66.5 68.1 71.7 68.7 70.2 62.7 59.0 60.8 66.4 62.9 64.6 67.2 66.8 67.0 67.2 70.4 72.3 71.0 70.0 70.5 72.2 69.5 70.8 67.5 67.2 67.3 68.2 67.1 67.6 72.6 73.0 Table 1: Results on the test set on the English CoNLL-2"
N18-2108,W12-4501,0,0.555437,"dow size of 10 for the LSTM inputs. The baseline model considers up to 250 antecedents per span. As shown in Figure 2, the coarse-to-fine model is quite insensitive to more aggressive pruning. Therefore, our final model considers only 50 antecedents per span. On the development set, the second-order model (N = 2) outperforms the first-order model by 0.8 F1, but the third order model only provides an additional 0.1 F1 improvement. Therefore, we only compute test results for the secondorder model. Experimental Setup We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available.1 Our models reuse the hyperparameters from Lee et al. (2017), with a few exceptions mentioned below. In our results, we report two improvements that are orthogonal to our contributions. 6 We report the precision, recall, and F1 of the the MUC, B3 , and CEAFφ4 metrics using the official CoNLL-2012 evaluation scripts. The main evaluation is the average F1 of the three metrics. Results on the test set are shown in Table 1. We include performance of systems proposed in the past 3 years for reference. The baseline re"
N18-2108,N16-1114,0,0.54331,"insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work. Introduction Recent coreference resolution systems have heavily relied on first order models (Clark and Manning, 2016a; Lee et al., 2017), where only pairs of entity mentions are scored by the model. These models are computationally efficient and scalable to long documents. However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent. Figure 1 shows an example from Wiseman et al. (2016) that illustrates this failure case. The plurality of [you] is underspecified, making it locally compatible with both [I] and [all of you], while the full cluster would have mixed plurality, resulting in global inconsistency. We introduce an approximation of higher-order inference that uses the span-ranking architecture from Lee et al. (2017) in an iterative manner. At each iteration, the antecedent distribution is used as an attention mechanism to optionally update existing span representations, enabling later corefer687 Proceedings of NAACL-HLT 2018, pages 687–692 c New Orleans, Louisiana, J"
N18-2108,P15-1137,0,0.242639,"Missing"
N19-1273,Q13-1005,1,0.929968,"Missing"
N19-1273,D13-1160,0,0.754253,"the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive"
N19-1273,W18-2501,1,0.861033,"Missing"
N19-1273,P96-1024,0,0.380485,"logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic MML to bootstrap its way to finding good logical forms. To solve this problem, we interleave static MML, which has a consistent super"
N19-1273,P17-1097,0,0.348125,"tself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic M"
N19-1273,P17-1167,0,0.0510016,"ntic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very"
N19-1273,P16-1002,0,0.0484511,"our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2"
N19-1273,D16-1032,1,0.829456,"ore details. In addition, we slightly modify the constrained decoding architecture from (Krishnamurthy et al., 2017) to bias the predicted actions towards those that would decrease the value of S(yi , xi ). This is done using a coverage vector, viS for each training instance that keeps track of the production rules triggered by xi , and gets updated whenever one of those desired productions is produced by the decoder. That is, viS is a vector of 1s and 0s, with 1s indicating the triggered productions that are yet to be produced by the decoder. This is similar to the idea of checklists used by Kiddon et al. (2016). The decoder in the original architecture scores output actions at each time step by computing a dot product of the predicted action representation with the embeddings of each of the actions. We add a weighted sum of all the actions that are yet to produced: sai = ea .(pi + γ ∗ viS .E) (4) where sai is the score of action a at time step i, ea is the embedding of that action, pi is the predicted action representation, E is the set of embeddings of all the actions, and γ is a learned parameter for regularizing the bias towards yet-to-be produced triggered actions. 6.2 Experimental setup NLVR We"
N19-1273,D17-1160,1,0.873567,"in frequent search failures early during training when model parameters are close to random, and in general may only yield spurious logical forms in the absence of any guidance. Since modern semantic parsers typically operate without a lexicon, new techniques are essential to provide guidance to the search procedure (Goldman et al., 2018). One way of providing this guidance during search is to perform some kind of heuristic search up front to find a set of logical forms that evaluate to the correct denotation, and use those logical forms to approximate the inner summation (Liang et al., 2011; Krishnamurthy et al., 2017). The particulars of the heuristic search can have a large impact on performance; a smaller candidate set has lower noise, while a larger set makes it more likely that the correct logical form is in it, and one needs to strike the right balance. In this paper, we refer to the MML that does search during training as dynamic MML, and the one that does an offline search as static MML. The main benefit of dynamic MML is that it adapts its training signal over time. As the model learns, it can increasingly focus its probability mass on a small set of very likely logical forms. The main benefit of s"
N19-1273,D11-1140,1,0.781683,"and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists (yellow (top (object in box (member count equals all boxes 3))))) The tower with three blocks has a yellow block over a black block (object count greater equals (yellow (above (black (object in box (member count equals all boxes 3))))) 1) Table 7: Complexity of logical forms produced at"
N19-1273,P17-1003,0,0.0331307,"e denotation given the utterance. The semantic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where t"
N19-1273,P11-1060,0,0.732007,"aning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi an"
N19-1273,D18-1266,0,0.0200734,"), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangl"
N19-1273,P15-1142,0,0.14764,"Missing"
N19-1273,P16-1003,0,0.420486,"Missing"
N19-1273,P17-1105,0,0.0325794,"r work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all box"
N19-1273,P17-1099,0,0.0230164,"esent modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists"
N19-1273,P06-2101,0,0.125269,"Missing"
N19-1273,N18-1203,0,0.023553,"Missing"
N19-1273,P17-2034,0,0.113175,"Missing"
N19-1273,N18-2071,0,0.019839,"t al., 2011; Berant et al., 2013), or trying to automatically infer logical forms from denotations (Pasupat and Liang, 2016). However, matching the performance of a fully supervised semantic parser with only weak supervision remains a significant challenge (Yih et al., 2016). The main contributions of this work deal with training semantic parsers with weak supervision, and we gave a detailed discussion of related training methods in §2.2. We evaluate our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include M"
N19-1273,P16-1008,0,0.0132738,"018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with"
N19-1273,P16-2033,0,0.0236319,"duce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015; Guu et al., 2017, inter alia) but also dealing with spurious logical forms that evaluate to the correct denotation while not being s"
N19-1273,P17-1041,0,0.134843,"omparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required b"
N19-1273,D17-1125,0,0.503711,"Missing"
N19-1362,W11-2501,0,0.0688554,"ocus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inf"
N19-1362,D18-1454,0,0.034005,"Missing"
N19-1362,Q17-1010,0,0.0645686,"about a quarter of the overall gain. This suggests that while the bulk of the signal is mined from the pair-context interactions, there is also valuable information in other interactions as well. We also test whether specific pre-training of word pair representations is useful by replacing pair2vec embeddings with the vector offsets of pre-trained word embeddings (Unsupervised: Pair Dist). We follow the PairDistance method for word analogies (Mikolov et al., 2013b), and represent the pair (x, y) as the L2 normalized difference of single-word vectors: (x − y)/kx − yk. We use the same fastText (Bojanowski et al., 2017) word vectors with which we initialized pair2vec before training. We observe a gain of only 0.34 F1 over the baseline. 5 Ablating parts of pair2vec shows that all components of the model (Section 2) are useful. We ablate each component and report the EM and F1 on the development set of SQuAD 2.0 (Table 6). The full model, which uses a 4-layer MLP for R(x, y) and trains with multivariate negative sampling, achieves the highest F1 of 72.68. We experiment with two alternative composition functions, a 2-layer MLP (Composition: 2 Layers) and element-wise multiplication (CompoAnalysis In Section 4,"
N19-1362,D15-1075,0,0.0291221,"◦ bi ; ˆ a (12) In the later layers, ainf is recontextualized using a BiGRU and self attention. Finally a prediction layer predicts the start and end tokens. BiDAF++ with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (12): i   ainf = ai ; bi ; ai ◦ bi ; ˆ a; ri (13) i 3.3 Benchmark ESIM + pair2vec Matched 79.68 Mismatched 78.80 81.03 80.12 ∆ +1.35 +1.32 Table 4: Performance on MultiNLI, with and without pair2vec. All models have ELMo. Natural Language Inference For NLI, we augment the ESIM model (Chen et al., 2017), which was previously state-of-theart on both SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) benchmarks. ESIM Let a and b be the outputs of the premise and hypothesis encoders respectively (in place of the standard p and h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair"
N19-1362,P18-1224,0,0.124062,"h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (14): i   ainf = ai ; bi ; ai ◦ bi ; ai − bi ; ri (15) i A similar augmentation of ESIM was recently proposed in KIM (Chen et al., 2018). However, their pair vectors are composed of WordNet features, while our pair embeddings are learned directly from text (see further discussion in Section 6). 4 Table 3: Performance on SQuAD 2.0 and adversarial SQuAD (AddSent and AddOneSent) benchmarks, with and without pair2vec. All models have ELMo. Experiments For experiments on QA (Section 4.1) and NLI (Section 4.2), we use our full model which includes multivariate and typed negative sampling. We discuss ablations in Section 4.3 Data We use the January 2018 dump of English Wikipedia, containing 96M sentences to train pair2vec. We restric"
N19-1362,P17-1152,0,0.57696,"ise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages 3597–3608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compositional function on individual words alleviates the sparsity that necessarily comes with embedding pairs of words, even at a very large scale. We show that our embeddings can be added to existing cross-sentence inference models, such as BiDAF++ (Seo et al., 2017; Clark and Gardner, 2018) for QA and ESIM (Chen et al., 2017) for NLI. Instead of changing the word embeddings that are fed into the encoder, we add the pretrained pair representations to higher layers in the network where cross sentence attention mechanisms are used. This allows the model to use the background knowledge that the pair embeddings implicitly encode to reason about the likely relationships between the pairs of words it aligns. Experiments show that simply adding our wordpair embeddings to existing high-performing models, which already use ELMo (Peters et al., 2018), results in sizable gains. We show 2.72 F1 points over the BiDAF++ model (C"
N19-1362,W04-3205,0,0.0608506,"ts. First, our goal is to represent word pairs, not individual words. Second, our new PMI formulation models the trivariate word-word-context distribution. Experiments show that our pair embeddings can complement single-word embeddings. Mining Textual Patterns There is extensive literature on mining textual patterns to predict relations between words (Hearst, 1992; Snow et al., 2005; Turney, 2005; Riedel et al., 2013; Van de Cruys, 2014; Toutanova et al., 2015; Shwartz and Dagan, 2016). These approaches focus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes"
N19-1362,P18-1078,0,0.0386834,"Missing"
N19-1362,W11-1303,0,0.0775844,"Missing"
N19-1362,D14-1004,0,0.0554251,"Missing"
N19-1362,C18-1225,0,0.129932,"Missing"
N19-1362,W18-2501,1,0.868432,"Missing"
N19-1362,N16-2002,0,0.0411555,"erb 3pSg:Ved Verb Ving:Ved Verb Inf:Ved Noun+less Substance Meronym 3CosAdd +pair2vec α∗ 1.2 1.8 0.1 0.7 4.0 49.1 61.1 58.5 4.8 3.8 86.1 44.6 42.0 31.7 28.4 61.7 73.3 70.1 16.0 14.5 0.9 0.8 0.9 1.0 0.8 0.6 0.5 0.5 0.2 0.6 Table 7: The top 10 analogy relations for which interpolating with pair2vec improves performance. α∗ is the optimal interpolation parameter for each relation. 5.1 Quantitative Analysis: Word Analogies Word Analogy Dataset Given a word pair (a, b) and word x, the word analogy task involves predicting a word y such that a : b :: x : y. We use the Bigger Analogy Test Set (BATS, Gladkova et al., 2016) which contains four groups of relations: encyclopedic semantics (e.g., personprofession as in Einstein-physicist), lexicographic semantics (e.g., antonymy as in cheap-expensive), derivational morphology (e.g., noun forms as in oblige-obligation), and inflectional morphology (e.g., noun-plural as in bird-birds). Each group contains 10 sub-relations. Method We interpolate pair2vec and 3CosAdd (Mikolov et al., 2013b; Levy et al., 2014) scores on fastText embeddings, as follows: score(y) = α · cos(ra,b , rx,y ) + (1 − α) · cos(b − a + x, y) (16) where a, b, x, and y represent fastText embeddings6"
N19-1362,P18-2103,0,0.0246428,"swering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this, we define the word pair probability as the product of unigram probabilities. 3601 100 Rule-based Models WordNet Baseline 85.8 80 Models with GloVe ESIM (Chen et al., 2017) KIM (Chen et al., 2018) ESIM + pair2vec 77.0 87.7 92.9 Models with ELMo ESIM (Peters et al., 2018) ESIM + pair2vec 84.6 93.4 Accuracy Accuracy Model EM (∆) 40 0 0.0 69.20 72.68 Composition: 2 Layers Composition: Multiply Objective: Bivariate NS Unsupervised: Pair Dist 68.35 (-0.85) 67.10 (-2.20) 68"
N19-1362,C92-2082,0,0.431945,"be learned with word pair vectors (pair2vec1 ), 1 https://github.com/mandarjoshi90/ pair2vec Y which are trained unsupervised, and which significantly improve performance when added to existing cross-sentence attention mechanisms. Unlike single-word representations, which typically model the co-occurrence of a target word x with its context c, our word-pair representations are learned by modeling the three-way cooccurrence between words (x, y) and the context c that ties them together, as seen in Table 1. While similar training signals have been used to learn models for ontology construction (Hearst, 1992; Snow et al., 2005; Turney, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a varia"
N19-1362,P18-1003,0,0.241061,"tains pairs occurring within a sentence. For better generalization to cross-sentence tasks, where the pair distribution differs from that of the training data, we need a multivariate objective that captures the full three-way (x, y, c) interaction. Multivariate Negative Sampling We introduce negative sampling of target words, x and y, in addition to negative sampling of contexts c (Table 2, J3N S ). Our new objective also converges to a novel multivariate generalization of PMI, different from previous PMI extensions that were inspired by information theory (Van de Cruys, 2011) and heuristics (Jameel et al., 2018).2 Following Levy and Goldberg (2014), we can show that when replacing target words in addition to contexts, our objective will converge3 to the optimal value in Equation 1: R(x, y) · C(c) = log 2 3 P (x, y, c) Zx,y,c (1) See supplementary material for their exact formulations. A full proof is provided in the supplementary material. where Zx,y,c , the denominator, is: Zx,y,c = kc P (·, ·, c)P (x, y, ·) + kx P (x, ·, ·)P (·, y, c) + ky P (·, y, ·)P (x, ·, c) (2) This optimal value deviates from previous generalizations of PMI by having a linear mixture of marginal probability products in its de"
N19-1362,D17-1215,0,0.0279102,"moves all out-of-vocabulary words in the corpus. We consider each word pair within a window of 5 in the preprocessed corpus, and subsample5 instances based on pair probability with a threshold of 5·10−7 . We define the context as one word each to the left and right, and all the words in between each pair, replacing both target words with placeholders X and Y (see Table 1). More details can be found in the supplementary material. 4.1 Question Answering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this, we define the word pa"
N19-1362,W14-1610,1,0.897363,"Missing"
N19-1362,K17-1034,1,0.85942,"nce layer effectively learns word-pair relationships from training data, and it should, therefore, help to augment its input with pair2vec. We augment ainf (7) with the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation o"
N19-1362,D17-1257,0,0.0581715,"Missing"
N19-1362,P18-1076,0,0.0232903,"enchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. (2017), Bauer et al. (2018), and Mihaylov and Frank (2018) dynamically refine word representations by reading assertions from ConceptNet and Wikipedia abstracts. Our approach, on the other hand, relies on a relatively simple extension of existing cross-sentence inference models. Furthermore, we do not need to dynamically retrieve and process knowledge base facts or Wikipedia texts, and just pretrain our pair vectors in advance. KIM (Chen et al., 2017) integrates word-pair vectors into the ESIM model for NLI in a very similar way to ours. However, KIM’s wordpair vectors contain only hand-engineered wordrelation indicators from WordNet, whereas our wor"
N19-1362,N13-1090,0,0.737968,"ey, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages 3597–3608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compositional function on individual words alleviates the sparsity that necessarily comes with embedding pairs of words, even at a very large scale. We show that our embeddings can be added to existing cross-sentence inference models, such as BiDAF++ (Seo et al., 2017; Clark and Gardner, 2018) for QA and ESIM (Chen et al., 2017) for NLI. Instead of changing the word embeddings that are fed into the encoder, we add the pretraine"
N19-1362,N18-1202,1,0.901698,"and it should, therefore, help to augment its input with pair2vec. We augment ainf (7) with the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation of the elements in a, with better-aligned elements receiving larger weights:"
N19-1362,P18-2124,0,0.141089,"the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation of the elements in a, with better-aligned elements receiving larger weights: µ = softmaxi (max si,j ) j X ˆ ai = µi a i Benchmark ∆ BiDAF + pair2vec SQuAD 2.0 EM F1 65.66"
N19-1362,D16-1264,0,0.0570696,"words. Preprocessing removes all out-of-vocabulary words in the corpus. We consider each word pair within a window of 5 in the preprocessed corpus, and subsample5 instances based on pair probability with a threshold of 5·10−7 . We define the context as one word each to the left and right, and all the words in between each pair, replacing both target words with placeholders X and Y (see Table 1). More details can be found in the supplementary material. 4.1 Question Answering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this,"
N19-1362,N13-1008,0,0.0738311,"are trained unsupervised, and which significantly improve performance when added to existing cross-sentence attention mechanisms. Unlike single-word representations, which typically model the co-occurrence of a target word x with its context c, our word-pair representations are learned by modeling the three-way cooccurrence between words (x, y) and the context c that ties them together, as seen in Table 1. While similar training signals have been used to learn models for ontology construction (Hearst, 1992; Snow et al., 2005; Turney, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages"
N19-1362,W16-5309,0,0.0214379,"s of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. ("
N19-1362,N18-1101,0,0.0469063,", ainf is recontextualized using a BiGRU and self attention. Finally a prediction layer predicts the start and end tokens. BiDAF++ with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (12): i   ainf = ai ; bi ; ai ◦ bi ; ˆ a; ri (13) i 3.3 Benchmark ESIM + pair2vec Matched 79.68 Mismatched 78.80 81.03 80.12 ∆ +1.35 +1.32 Table 4: Performance on MultiNLI, with and without pair2vec. All models have ELMo. Natural Language Inference For NLI, we augment the ESIM model (Chen et al., 2017), which was previously state-of-theart on both SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) benchmarks. ESIM Let a and b be the outputs of the premise and hypothesis encoders respectively (in place of the standard p and h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair vectors, we simply concatenate ri (3)"
N19-1362,W16-5304,0,0.012518,"ivariate distribution of target words and contexts is modeled. Our work deviates from the word embedding literature in two major aspects. First, our goal is to represent word pairs, not individual words. Second, our new PMI formulation models the trivariate word-word-context distribution. Experiments show that our pair embeddings can complement single-word embeddings. Mining Textual Patterns There is extensive literature on mining textual patterns to predict relations between words (Hearst, 1992; Snow et al., 2005; Turney, 2005; Riedel et al., 2013; Van de Cruys, 2014; Toutanova et al., 2015; Shwartz and Dagan, 2016). These approaches focus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction ta"
N19-1362,P17-1132,0,0.0242905,"(individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. (2017), Bauer et al. (2018), and Mihaylov and Frank (2018) dynamically refine word representations by reading assertions from ConceptNet and Wikipedia abstracts. Our approach, on the other hand, relies on a relatively simple extension of existing cross-sentence inference models. Furthermore, we do not need to dynamically retrieve and process knowledge base facts or Wikipedia texts, and just pretrain our pair vectors in advance. KIM (Chen et al."
N19-1362,P16-1226,0,0.0463128,"Missing"
N19-1362,D15-1174,0,0.205549,"g, we typically replace x and y by sampling from their unigram distributions. In addition to this, we also sample uniformly from the top 100 words according to cosine similarity using distributional word vectors. This is done to encourage the model to learn relations between specific instances as opposed to more general types. For example, using California as a negative sample for Oregon helps the model to learn that the pattern “X is located in Y” fits the pair (Portland, Oregon), but not the pair (Portland, California). Similar adversarial constraints were used in knowledge base completion (Toutanova et al., 2015) and word embeddings (Li et al., 2017).4 3 Integrating pair2vec into Models We first present a general outline for incorporating pair2vec into attention-based architectures, and then discuss changes made to BiDAF++ and ESIM. The key idea is to inject our pairwise representations into the attention layer by reusing the cross-sentence attention weights. In addition to attentive pooling over single word representations, we also pool over cross-sentence word pair embeddings (Figure 1). 4 Applying typed sampling also changes the value to which our objective will converge, and will replace the unigr"
N19-1362,N18-1102,0,0.3372,"ong with their element-wise product, are fed into a four-layer perceptron: R(x, y) = M LP 4 (x, y, x ◦ y) The context c = c1 ...cn is encoded as a ddimensional vector using the function C(c). C(c) embeds each token ci with a lookup matrix Ec , contextualizes it with a single-layer Bi-LSTM, and then aggregates the entire context with attentive pooling: ci = Ec (ci ) h1 ...hn = BiLSTM(c1 ...cn ) w = softmaxi (khi ) X C(c) = wi Whi i where W ∈ Rd×d and k ∈ Rd . All parameters, including the lookup tables Ea and Ec , are trained. Our representation is similar to two recentlyproposed frameworks by Washio and Kato (2018a,b), but differs in that: (1) they use dependency paths as context, while we use surface form; (2) they encode the context as either a lookup table or the last state of a unidirectional LSTM. We also use a different objective, which we discuss next. 2.2 Objective To optimize our representation functions, we consider two variants of negative sampling (Mikolov et al., 2013a): bivariate and multivariate. The original bivariate objective models the two-way distribution of context and (monolithic) word pair co-occurrences, while our multivariate extension models the three-way distribution of word-"
N19-1362,D18-1058,0,0.396801,"ong with their element-wise product, are fed into a four-layer perceptron: R(x, y) = M LP 4 (x, y, x ◦ y) The context c = c1 ...cn is encoded as a ddimensional vector using the function C(c). C(c) embeds each token ci with a lookup matrix Ec , contextualizes it with a single-layer Bi-LSTM, and then aggregates the entire context with attentive pooling: ci = Ec (ci ) h1 ...hn = BiLSTM(c1 ...cn ) w = softmaxi (khi ) X C(c) = wi Whi i where W ∈ Rd×d and k ∈ Rd . All parameters, including the lookup tables Ea and Ec , are trained. Our representation is similar to two recentlyproposed frameworks by Washio and Kato (2018a,b), but differs in that: (1) they use dependency paths as context, while we use surface form; (2) they encode the context as either a lookup table or the last state of a unidirectional LSTM. We also use a different objective, which we discuss next. 2.2 Objective To optimize our representation functions, we consider two variants of negative sampling (Mikolov et al., 2013a): bivariate and multivariate. The original bivariate objective models the two-way distribution of context and (monolithic) word pair co-occurrences, while our multivariate extension models the three-way distribution of word-"
P09-1010,H89-1033,0,0.71984,"ons. Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998). We assume that supervision is provided in the form of a reward function that defines the quality of executed actions. During training, the learner repeatedly constructs action sequences for a set of given documents, executes those actions, and observes the resulting reward. The learner’s goal is to estimate a Introduction The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992). Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation. Examples include configuring software based on how-to guides and operating simulators using instruction manuals. In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples. For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in Figure 1. We aim to map 1 Code, data, and annotations use"
P09-1010,P92-1016,0,0.509104,"Missing"
P09-1010,W05-0614,0,0.147141,"(a0 , . . . , an−1 ). Actions are predicted and executed sequentially.2 An action a = (c, R, W 0 ) encompasses a command c, the command’s parameters R, and the words W 0 specifying c and R. Elements of R refer to objects available in the environment state, as described below. Some parameters can also refer to words in document d. Additionally, to account for words that do not describe any actions, c can be a null command. Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. The Environment The environment state E specifies the set of objects available for interaction, and their p"
P09-1010,C00-1073,0,0.0282889,"e E specifies the set of objects available for interaction, and their properties. In Figure 2, E is shown on the right. The environment state E changes in response to the execution of command c with parameters R according to a transition distribution p(E 0 |E, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. State To predict actions sequentially, we need to track the state of the document-to-actions mapping over tim"
P09-1010,P00-1013,0,0.0170029,"e environment state E specifies the set of objects available for interaction, and their properties. In Figure 2, E is shown on the right. The environment state E changes in response to the execution of command c with parameters R according to a transition distribution p(E 0 |E, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. State To predict actions sequentially, we need to track the state of the document-to-act"
P09-1010,E12-1061,0,\N,Missing
P09-1110,C04-1021,0,0.0129951,". Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, Figure 2: An online learning algorithm. 8 Featu"
P09-1110,C04-1180,0,0.0365058,"Missing"
P09-1110,J83-3001,0,0.444479,"of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the"
P09-1110,W03-1013,0,0.0196546,"Missing"
P09-1110,W02-1001,1,0.066962,"binator rules. For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. A/B : f B : g B : g AB : f ⇒ ⇒ A :"
P09-1110,W00-1317,0,0.020042,"i,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse"
P09-1110,H94-1010,0,0.2237,"Missing"
P09-1110,W04-3201,0,0.0261329,"For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. A/B : f B : g B : g AB : f ⇒ ⇒ A : f (g) A : f (g) (>) ("
P09-1110,W05-0602,0,0.0419396,"zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It r"
P09-1110,H94-1039,0,0.0190751,"from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, Figu"
P09-1110,P99-1069,0,0.0094594,"nstruct parse trees according to a set of combinator rules. For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test example"
P09-1110,W99-0909,0,0.183667,"Missing"
P09-1110,P07-1121,0,0.592714,"ectness) • Let d∗ = arg maxd∈GEN(wi,j ;C) θ · φ(d) . • If L(d∗ ) = zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully sup"
P09-1110,P96-1008,0,0.863406,"φ(d) . • If L(d∗ ) = zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning repre"
P09-1110,D07-1071,1,0.89181,"light(x) ∧ f rom(x, bos) ∧ to(x, phi) ∧ during(x, morning) constructed from the variable x, where the subexpression aircraf t(x) = y has been removed because it contains the free variable y. 7 Learning Figure 2 details the complete learning algorithm. Training is online and error-driven. Step 1 parses the current sentence in context. If the optimal logical form is not correct, Step 2 finds the best derivation that produces the labeled logical form7 and does an additive, perceptron-style parameter update. Step 3 updates the context. This algorithm is a direct extension of the one introduced by Zettlemoyer and Collins (2007). It maintains the context but does not have the lexical induction step that was previously used. Elaboration Expressions Finally, E(z) is a set of elaboration expressions constructed Sj−1 from a logical form z. We define E(C) = i=1 E(zi ). E(z) is defined by enumerating the places where embedded variables are found in z. For each logical variable x and each coordination (conjunction or disjunction) in the scope of x, a new expression is created by defining a function λf.z 0 where z 0 has the function f (x) added to the appropriate coordination. This procedure would 6 7 A lambda-calculus expre"
P10-1129,P09-1010,1,0.88521,"n word spans Wa , and translating each instruction into the sequence ~c of one or more commands it describes. During learning, the correct output command sequence is not provided to the algorithm. instructions enables us to bias exploration toward transitions relevant for language learning. This approach yields superior performance compared to a policy that relies on an environment model constructed via random exploration. 2 Related Work Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than l"
P10-1129,C92-4181,0,0.700947,"Missing"
P10-1129,P92-1016,0,0.295405,"Missing"
P10-1129,D09-1100,0,0.0843068,"learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, automatic approaches for processing such instructions have relied on hand-engineered world kno"
P10-1129,W05-0614,0,0.0542742,"via random exploration. 2 Related Work Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et a"
P10-1129,E09-1058,0,0.0108413,"n of relevant environment knowledge. Reinforcement Learning Our work combines ideas of two traditionally disparate approaches to reinforcement learning (Sutton and Barto, 1998). The first approach, model-based learning, constructs a model of the environment in which the learner operates (e.g., modeling location, velocity, and acceleration in robot navigation). It then computes a policy directly from the rich information represented in the induced environment model. In the NLP literature, model-based reinforcement learning techniques are commonly used for dialog management (Singh et al., 2002; Lemon and Konstas, 2009; Schatzmann and Young, 2009). However, if the environment cannot be accurately approximated by a compact representation, these methods perform poorly (Boyan and Moore, 1995; Jong and Stone, 2007). Our instruction interpretation task falls into this latter category,2 rendering standard model-based learning ineffective. The second approach – model-free methods such as policy learning – aims to select the opti2 For example, in the Windows GUI domain, clicking on the File menu will result in a different submenu depending on the application. Thus it is impossible to predict the effects of a previo"
P10-1129,P09-1011,0,0.123719,"Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, autom"
P10-1129,H89-1033,0,0.197931,"d, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, automatic approaches for processing such instructions have relied on hand-engineered world knowledge to reason about the preconditions and effects of environment commands. The assumption of a fully specified environment model is also common in work on semantics in the linguistics literature (Lascarides and Asher, 2004). While our approach learns to analyze instructions in a goaldirected manner, it does not require manual specification of relevant environment knowledge. Reinforcement Learning Our work combines id"
P11-1055,P08-1004,0,0.051435,"el of extractions at the individual sentence level. Perhaps surprisingly, our 548 model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), T EXT RUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and W OE (Wu and Weld, 2010)) can scale to millions of documents, but don’t output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extract"
P11-1055,P07-1073,0,0.109605,"N ELL system (Carlson et al., 2010) can also be viewed as performing weak supervision. Its initial knowledge consists of a selectional preference constraint and 20 ground fact seeds. N ELL then matches entity pairs from the seeds to a Web corpus, but instead of learning a probabilistic model, it bootstraps a set of extraction patterns using semisupervised methods for multitask learning. 8.2 Multi-Instance Learning Multi-instance learning was introduced in order to combat the problem of ambiguously-labeled training data when predicting the activity of different drugs (Dietterich et al., 1997). Bunescu and Mooney (2007) connect weak supervision with multi-instance learning and extend their relational extraction kernel to this context. Riedel et al. (2010), combine weak supervision and multi-instance learning in a more sophisticated manner, training a graphical model, which assumes only that at least one of the matches between the arguments of a Freebase fact and sentences in the corpus is a true relational mention. Our model may be seen as an extension of theirs, since both models include sentence-level and aggregate random variables. However, Riedel et al. have only a single aggregate variable that takes va"
P11-1055,W02-1001,0,0.0298078,"irs {(xi , yi )|i = 1 . . . n}, where i is an index corresponding to a particular entity pair (ej , ek ), xi contains all of the sentences with mentions of this pair, and yi = relVector(ej , ek ). Given this form of supervision, we would like to find the setting for θ with the highest likelihood: Y YX O(θ) = p(yi |xi ; θ) = p(yi , z|xi ; θ) i i z However, this objective would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to data sets of the size we consider. Instead, we make two approximations, described below, leading to a Perceptron-style additive (Collins, 2002) parameter update scheme which has been modified to reason about hidden variables, similar in style to the approaches of (Liang et al., 2006; Zettlemoyer and Collins, 2007), but adapted for our specific model. This approximate algorithm is computationally efficient and, as we will see, works well in practice. Our first modification is to do online learning instead of optimizing theP full objective. Define the feature sums φ(x, z) = j φ(xj , zj ) which range over the sentences, as indexed by j. Now, we can define an update based on the gradient of the local log likelihood for example i: ∂ log O"
P11-1055,P05-1045,0,0.113938,"ional extractions that increase the overall probability of the assignment. Given the computational advantage, we use it in all of the experimental evaluations. 6 Experimental Setup We follow the approach of Riedel et al. (2010) for generating weak supervision data, computing features, and evaluating aggregate extraction. We also introduce new metrics for measuring sentential extraction performance, both relation-independent and relation-specific. 6.1 Data Generation We used the same data sets as Riedel et al. (2010) for weak supervision. The data was first tagged with the Stanford NER system (Finkel et al., 2005) and then entity mentions were found by collecting each continuous phrase where words were tagged identically (i.e., as a person, location, or organization). Finally, these phrases were matched to the names of Freebase entities. Given the set of matches, define Σ to be set of NY Times sentences with two matched phrases, E to be the set of Freebase entities which were mentioned in one or more sentences, ∆ to be the set of Freebase facts whose arguments, e1 and e2 were mentioned in a sentence in Σ, and R to be set of relations names used in the facts of ∆. These sets define the weak supervision"
P11-1055,P10-1030,1,0.209983,"aining data by heuristically matching the contents of a database to corresponding text (Craven and Kumlien, 1999). For example, suppose that r(e1 , e2 ) = Founded(Jobs, Apple) is a ground tuple in the database and s =“Steve Jobs founded Apple, Inc.” is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1 , e2 ) holds and could be a useful training example. While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles). To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e1 and e2 are expressing r(e1 , e2 ), and their method yields a substantial improvement in extraction performance. However, Riedel et al.’s model (like that of previous systems (Mintz et al., 2009)) assumes that relations do not overlap — there cannot exis"
P11-1055,P06-1096,0,0.0376891,"Missing"
P11-1055,P09-1113,0,0.782804,"ined as follows: def p(Y = y, Z = z|x; θ) = Y 1 Y join r Φ (y , z) Φextract (zi , xi ) Zx r i where the parameter vector θ is used, below, to define the factor Φextract . The factors Φjoin are deterministic OR operators ( 1 if y r = true ∧ ∃i : zi = r def join r Φ (y , z) = 0 otherwise which are included to ensure that the ground fact r(e) is predicted at the aggregate level for the assignment Y r = y r only if at least one of the sen543 where the features φj are sensitive to the relation name assigned to extraction variable zi , if any, and cues from the sentence xi . We will make use of the Mintz et al. (2009) sentence-level features in the expeiments, as described in Section 7. 3.3 Discussion This model was designed to provide a joint approach where extraction decisions are almost entirely driven by sentence-level reasoning. However, defining the Y r random variables and tying them to the sentencelevel variables, Zi , provides a direct method for modeling weak supervision. We can simply train the model so that the Y variables match the facts in the database, treating the Zi as hidden variables that can take any value, as long as they produce the correct aggregate predictions. This approach is rela"
P11-1055,W04-2407,0,0.0181202,"t manually check the much larger set of sentences where no approach predicted extractions. 6.4 Precision / Recall Curves To compute precision / recall curves for the tasks, we ranked the M ULTI R extractions as follows. For sentence-level evaluations, we ordered according to 1 http://opennlp.sourceforge.net/ 546 1.0 M ULT IR S OLO R Riedel et al., 2010 0.8 Precision veloped by Mintz et al. (2009). These include indicators for various lexical, part of speech, named entity, and dependency tree path properties of entity mentions in specific sentences, as computed with the Malt dependency parser (Nivre and Nilsson, 2004) and OpenNLP POS tagger1 . However, unlike the previous work, we did not make use of any features that explicitly aggregate these properties across multiple mention instances. The M ULTI R algorithm has a single parameter T , the number of training iterations, that must be specified manually. We used T = 50 iterations, which performed best in development experiments. 0.6 0.4 0.2 0.0 0.00 0.05 0.10 0.15 Recall 0.20 0.25 0.30 Figure 4: Aggregate extraction precision / recall curves for Riedel et al. (2010), a reimplementation of that approach (S OLO R), and our algorithm (M ULTI R). the extracti"
P11-1055,N06-1039,0,0.0299435,"entity-level relation predictions and has a less detailed model of extractions at the individual sentence level. Perhaps surprisingly, our 548 model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), T EXT RUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and W OE (Wu and Weld, 2010)) can scale to millions of documents, but don’t output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (20"
P11-1055,P10-1013,1,0.374883,"sentence level. Perhaps surprisingly, our 548 model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), T EXT RUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and W OE (Wu and Weld, 2010)) can scale to millions of documents, but don’t output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relati"
P11-1055,D10-1099,0,0.115897,"Missing"
P11-1055,D07-1071,1,0.256581,"this pair, and yi = relVector(ej , ek ). Given this form of supervision, we would like to find the setting for θ with the highest likelihood: Y YX O(θ) = p(yi |xi ; θ) = p(yi , z|xi ; θ) i i z However, this objective would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to data sets of the size we consider. Instead, we make two approximations, described below, leading to a Perceptron-style additive (Collins, 2002) parameter update scheme which has been modified to reason about hidden variables, similar in style to the approaches of (Liang et al., 2006; Zettlemoyer and Collins, 2007), but adapted for our specific model. This approximate algorithm is computationally efficient and, as we will see, works well in practice. Our first modification is to do online learning instead of optimizing theP full objective. Define the feature sums φ(x, z) = j φ(xj , zj ) which range over the sentences, as indexed by j. Now, we can define an update based on the gradient of the local log likelihood for example i: ∂ log Oi (θ) ∂θj = Ep(z|xi ,yi ;θ) [φj (xi , z)] −Ep(y,z|xi ;θ) [φj (xi , z)] where the deterministic OR Φjoin factors ensure that the first expectation assigns positive probabili"
P12-1089,W02-1001,0,0.578954,"assignments. Such an approach allows us to learn, for each target relation, an integrated model to weight the different extraction options, including for example the likely lengths for events, or the fact that start times should come before end times. However, there are significant computation challenges that come with this style of joint learning. We demonstrate empirically that these challenges can be solved with a combination of greedy beam decoding, performed directly in the joint space of possible mention clusters and field assignments, and structured Perceptronstyle learning algorithm (Collins, 2002). We report experimental evaluations on two benchmark datasets in different genres, the CMU seminar announcements and corporate acquisitions (Freitag and McCallum, 2000). In each case, we evaluated both template extraction and mention detection performance. Our joint learning approach provides consistently strong results across every setting, including new state-of-the-art results. We also demonstrate, through ablation studies on the feature set, the need for joint modeling and the relative importance of the different types of joint constraints. 2 Related Work Research on the task of template"
P12-1089,P05-1045,0,0.0261404,"Missing"
P12-1089,P10-2054,0,0.384089,"need for joint modeling and the relative importance of the different types of joint constraints. 2 Related Work Research on the task of template filling has focused on the extraction of field value mentions from the underlying text. Typically, these values are extracted based on local evidence, where the most likely entity is assigned to each slot (Roth and Yih, 2001; Siefkes, 2008). There has been little effort towards a comprehensive approach that includes mention unification, as well as considers the structure of the target relational schema to create semantically valid outputs. Recently, Haghighi and Klein (2010) presented a generative semi-supervised approach for template filling. In their model, slot-filling entities are first generated, and entity mentions are then realized in text. Thus, their approach performs coreference at 846 slot level. In addition to proper nouns (named entity mentions) that are considered in this work, they also account for nominal and pronominal noun mentions. This work presents a discriminative approach to this problem. An advantage of a discriminative framework is that it allows the incorporation of rich and possibly overlapping features. In addition, we enforce label co"
P12-1089,H05-1056,1,0.777015,"be generated, ranked according to Equation 1. 5 Seminar Extraction Task Dataset The CMU seminar announcement dataset (Freitag and McCallum, 2000) includes 485 emails containing seminar announcements. The dataset has been originally annotated with text spans referring to four slots: speaker, location, stime, and etime. We have annotated this dataset with two additional attributes: date and title.2 We consider this corpus as an example of semi-structured text, where some of the field values appear in the email header, in a tabular structure, or using special formatting (Califf and Mooney, 1999; Minkov et al., 2005).3 We used a set of rules to extract candidate named entities per the types specified in Figure 2.4 The rules encode information typically used in NER, including content and contextual patterns, as well as lookups in available dictionaries (Finkel et al., 2005; Minkov et al., 2005). The extracted candidates are high-recall and overlapping. In order to increase recall further, additional candidates were extracted based on document structure (Siefkes, 2008). The 2 A modified dataset is available on the author’s homepage. Such structure varies across messages. Otherwise, the problem would reduce"
P12-1089,C02-1151,0,0.0341661,"at 846 slot level. In addition to proper nouns (named entity mentions) that are considered in this work, they also account for nominal and pronominal noun mentions. This work presents a discriminative approach to this problem. An advantage of a discriminative framework is that it allows the incorporation of rich and possibly overlapping features. In addition, we enforce label consistency and semantic coherence at record level. Other related works perform structured relation discovery for different settings of information extraction. In open IE, entities and relations may be inferred jointly (Roth and Yih, 2002; Yao et al., 2011). In this IE task, the target relation must agree with the entity types assigned to it; e.g., born-in relation requires a place as its argument. In addition, extracted relations may be required to be consistent with an existing ontology (Carlson et al., 2010). Compared with the extraction of tuples of entity mention pairs, template filling is associated with a more complex target relational schema. Interestingly, several researchers have attempted to model label consistency and high-level relational constraints using state-of-the-art sequential models of named entity recogni"
P12-1089,D11-1135,0,0.015728,"In addition to proper nouns (named entity mentions) that are considered in this work, they also account for nominal and pronominal noun mentions. This work presents a discriminative approach to this problem. An advantage of a discriminative framework is that it allows the incorporation of rich and possibly overlapping features. In addition, we enforce label consistency and semantic coherence at record level. Other related works perform structured relation discovery for different settings of information extraction. In open IE, entities and relations may be inferred jointly (Roth and Yih, 2002; Yao et al., 2011). In this IE task, the target relation must agree with the entity types assigned to it; e.g., born-in relation requires a place as its argument. In addition, extracted relations may be required to be consistent with an existing ontology (Carlson et al., 2010). Compared with the extraction of tuples of entity mention pairs, template filling is associated with a more complex target relational schema. Interestingly, several researchers have attempted to model label consistency and high-level relational constraints using state-of-the-art sequential models of named entity recognition (NER). Mainly,"
P13-1158,P05-1074,0,0.236624,"ave yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are en"
P13-1158,N03-1003,0,0.0100333,"ets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 n"
P13-1158,P01-1008,0,0.0129458,"ng and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2"
P13-1158,W02-1033,0,0.233788,"n extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yate"
P13-1158,P13-1042,0,0.425016,"t al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al.,"
P13-1158,D08-1021,0,0.0105295,"neral, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabula"
P13-1158,W10-2903,0,0.0253509,"made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question int"
P13-1158,W02-1001,0,0.0430583,"perceptron approach. We first randomly partition the training data T into K equally-sized subsets T1 , . . . , TK . We then perform perceptron learning on each partition in parallel. Finally, the learned weights from each parallel run are aggregated by taking a uniformly weighted average of each partition’s parameter vector. This procedure is repeated for T iterations. The training data consists of (question x, query z) pairs, but our scoring model is over (question x, derivation y) pairs, which are unobserved in the training data. We use a hidden variable version of the perceptron algorithm (Collins, 2002), where the model parameters are updated using the highest scoring derivation y ∗ that will generate the correct query z using the learned lexicon L. 6 Data For our database D, we use the publicly available set of 15 million R E V ERB extractions (Fader et al., 2011).3 The database consists of a set of triples r(e1 , e2 ) over a vocabulary of approximately 600K relations and 2M entities, extracted from the ClueWeb09 corpus.4 The R E V ERB database contains a large cross-section of general world-knowledge, and thus is a good testbed for developing an open-domain QA system. However, the extracti"
P13-1158,D11-1142,1,0.437775,"problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for learning about lexical variation, but there are a number Table 1: Examples of paraphrase clusters from the WikiAnswers corpus. Within each cluster, there is a wide range of syntactic and lexical variations. of challenges. Given that the data is communityauthored, it will inevitably be incomplete, contain incorrectly tagged p"
P13-1158,P10-1030,0,0.0256217,"Missing"
P13-1158,P11-1055,1,0.487038,"and show that it outperforms baseline systems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle an"
P13-1158,N03-1017,0,0.0397793,"Missing"
P13-1158,P11-1060,0,0.0666912,"ext into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The lear"
P13-1158,D09-1040,0,0.0132155,"stion answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabulary E. We assume that t"
P13-1158,P09-1113,0,0.0109104,"questions from WikiAnswers using a database of web extractions, and show that it outperforms baseline systems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to aut"
P13-1158,P00-1056,0,0.0222414,"Missing"
P13-1158,W08-0509,0,0.0122163,"Missing"
P13-1158,C04-1021,1,0.607014,"2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these sys"
P13-1158,W04-3219,0,0.218387,"Missing"
P13-1158,D12-1035,0,0.167604,"rom data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the A"
P13-1158,P12-1075,0,0.00682637,"ems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Z"
P13-1158,N10-1069,0,\N,Missing
P13-1164,aggarwal-etal-2012-twins,0,0.0170181,"s. 1 U: “I want to add page numbers and a title” S: “Top or Bottom of the page?” U: “Top” S: “Please select page design from the templates” (*System shows drop down menu*) U: *User selects from menu* S: “Enter header or footer content” U: “C.V.” S: “Task completed.” Figure 1: An example dialog interaction between a system (S) and user (U) that can be automatically achieved by learning from instructional web page and query click logs. Introduction Procedural dialog systems aim to assist users with a wide range of goals. For example, they can guide visitors through a museum (Traum et al., 2012; Aggarwal et al., 2012), teach students physics (Steinhauser et al., 2011; Dzikovska et al., 2011), or enable interaction with a health care system (Morbini et al., 2012; Rizzo et al., 2011). However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and dialog management strategies. In this paper, we present a new approach for learning procedural dialog systems from taskoriented textual resources in combination with light, non-expert supervision. Specifically, we assume access to task knowledge in textual form (e.g., instructional"
P13-1164,Q13-1005,1,0.816066,"s that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 201"
P13-1164,P09-1010,1,0.828959,"t” S: *System Restarts* (D) Figure 6: Four example dialogs from the user study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of vi"
P13-1164,P10-1129,1,0.884129,"r study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Simila"
P13-1164,P11-1028,0,0.0163665,"procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters havi"
P13-1164,P12-1035,0,0.0266953,"dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct di"
P13-1164,P09-1011,0,0.022129,"and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn compl"
P13-1164,W12-1620,0,0.0313178,"shows drop down menu*) U: *User selects from menu* S: “Enter header or footer content” U: “C.V.” S: “Task completed.” Figure 1: An example dialog interaction between a system (S) and user (U) that can be automatically achieved by learning from instructional web page and query click logs. Introduction Procedural dialog systems aim to assist users with a wide range of goals. For example, they can guide visitors through a museum (Traum et al., 2012; Aggarwal et al., 2012), teach students physics (Steinhauser et al., 2011; Dzikovska et al., 2011), or enable interaction with a health care system (Morbini et al., 2012; Rizzo et al., 2011). However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and dialog management strategies. In this paper, we present a new approach for learning procedural dialog systems from taskoriented textual resources in combination with light, non-expert supervision. Specifically, we assume access to task knowledge in textual form (e.g., instructional web pages) and examples of user intent statements (e.g., search query logs or dialog interactions). Such instructional resources are available in"
P13-1164,P11-1020,0,0.0164534,"rafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning"
P13-1164,P12-1059,0,0.0173017,"g management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using"
P13-1164,N10-1048,0,0.0703818,"Missing"
P13-1164,P11-2042,0,0.0420886,"Missing"
P13-1164,P08-1073,0,0.0271142,"systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL wit"
P13-1164,N07-1034,0,0.0204458,"imited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state update rules (Heeman, 2007). However, our approach is unique in the use of inducing task and domain knowledge with light supervision to assist the user with many goals. 8 Conclusions and Future Work This paper presented a novel approach for automatically constructing procedural dialog systems with light supervision, given only textual resources such as instructional text and search query click logs. Evaluations demonstrated highly accurate performance, on automatic benchmarks and through a user study. Although we showed it is possible to build complete systems, more work will be required to scale the approach to new dom"
P13-1164,N09-2023,0,0.0152868,"NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state update rules (Heeman, 2007). However, our approach is unique in the use of inducing task and domain knowledge with light supervision to assist the user with many goals. 8 Conclusions and Future Work This paper presented a novel approach for automatically constructing procedural dialog systems with light supervision, given"
P13-1164,P10-1083,0,0.0224588,"(D) Figure 6: Four example dialogs from the user study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dol"
P13-1164,W12-1816,0,\N,Missing
P13-5002,W10-2903,0,\N,Missing
P13-5002,J07-4004,0,\N,Missing
P13-5002,W02-1001,0,\N,Missing
P13-5002,H94-1010,0,\N,Missing
P13-5002,W07-1202,0,\N,Missing
P13-5002,P94-1004,0,\N,Missing
P13-5002,D11-1039,1,\N,Missing
P13-5002,D07-1071,1,\N,Missing
P13-5002,D12-1040,0,\N,Missing
P13-5002,P09-1110,1,\N,Missing
P13-5002,P06-1096,0,\N,Missing
P13-5002,D13-1197,1,\N,Missing
P13-5002,P12-1045,0,\N,Missing
P13-5002,P11-1060,0,\N,Missing
P13-5002,P96-1008,0,\N,Missing
P13-5002,D10-1119,1,\N,Missing
P13-5002,P13-1042,0,\N,Missing
P13-5002,P13-1127,0,\N,Missing
P13-5002,N13-1103,0,\N,Missing
P13-5002,Q13-1016,0,\N,Missing
P13-5002,Q13-1005,1,\N,Missing
P13-5002,D11-1140,1,\N,Missing
P13-5002,S13-1045,0,\N,Missing
P13-5002,D13-1161,1,\N,Missing
P13-5002,P09-1010,1,\N,Missing
P13-5002,P10-1129,1,\N,Missing
P14-1026,de-marneffe-etal-2006-generating,0,0.0419568,"Missing"
P14-1026,D11-1039,1,0.809714,"Missing"
P14-1026,P08-1030,0,0.012397,"Missing"
P14-1026,D13-1160,0,0.0581256,"sed to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement park sells 2 kinds of tickets. Tickets for"
P14-1026,D12-1040,0,0.0271902,"Missing"
P14-1026,P09-1010,1,0.896819,"ion (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. pairs (“$1.50”, “children”) and (“$4”,“adults”) both surround the word “cost,” suggesting an output equation with a sum of two constant-variable products. We consider learning with two different levels of supervision. In the first scenario, we assume access to each problem’s numeric solution (see Figure 1) for most of the data, along with a s"
P14-1026,N13-1103,1,0.56024,"Missing"
P14-1026,P10-1129,1,0.377659,"rning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automaticall"
P14-1026,D10-1119,1,0.608985,"n, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learnin"
P14-1026,P13-1042,0,0.0113296,"supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement"
P14-1026,D13-1161,1,0.712521,"Missing"
P14-1026,S13-1045,0,0.00788108,"supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement"
P14-1026,P13-1127,1,0.848031,"Missing"
P14-1026,P11-1098,0,0.00930203,"e performance when trained exclusively on answers. 2 Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body"
P14-1026,W04-0902,0,0.4035,"d Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the admission fees collected totaled $ 792 . How many children were admitted on that day? How many adults were admitted? u11 + u12 − n1 = 0 n2 × u21 + n3 × u22 − n4 = 0 x + y − 278 = 0 1.5x + 4y − 792 ="
P14-1026,P07-1075,0,0.0442321,"Missing"
P14-1026,P12-1045,0,0.0140584,"Missing"
P14-1026,W10-2903,0,0.045354,"iven varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated e"
P14-1026,N12-1008,1,0.824517,"Missing"
P14-1026,P10-1083,0,0.00836505,"pping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of"
P14-1026,N06-1056,0,0.0645808,"ain areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee an"
P14-1026,P09-1110,1,0.72552,"Missing"
P14-1026,Q13-1005,1,\N,Missing
P14-1135,P13-1009,0,0.265163,"neered rules for reasoning about time expressions (Str¨otgen and Gertz, 2013). This includes both detection, identifying a phrase as a time expression, and resolution, mapping such a phrase into a standardized time value. While rule-based approaches provide a natural way to express expert knowledge, it is relatively difficult to en∗ Work conducted at the University of Washington. code preferences between similar competing hypotheses and provide prediction confidence. Recently, methods for learning probabilistic semantic parsers have been shown to address such limitations (Angeli et al., 2012; Angeli and Uszkoreit, 2013). However, these approaches do not account for any surrounding linguistic context and were mainly evaluated with gold standard mentions. We propose to use a context-dependent semantic parser for both detection and resolution of time expressions. For both tasks, we make use of a hand-engineered Combinatory Categorial Grammar (CCG) to construct a set of meaning representations that identify the time being described. For example, this grammar maps the phrase “2nd Friday of July” to the meaning representation intersect(nth(2 , friday), july), which encodes the set of all such days. Detection is th"
P14-1135,N12-1049,0,0.501877,"methods use handengineered rules for reasoning about time expressions (Str¨otgen and Gertz, 2013). This includes both detection, identifying a phrase as a time expression, and resolution, mapping such a phrase into a standardized time value. While rule-based approaches provide a natural way to express expert knowledge, it is relatively difficult to en∗ Work conducted at the University of Washington. code preferences between similar competing hypotheses and provide prediction confidence. Recently, methods for learning probabilistic semantic parsers have been shown to address such limitations (Angeli et al., 2012; Angeli and Uszkoreit, 2013). However, these approaches do not account for any surrounding linguistic context and were mainly evaluated with gold standard mentions. We propose to use a context-dependent semantic parser for both detection and resolution of time expressions. For both tasks, we make use of a hand-engineered Combinatory Categorial Grammar (CCG) to construct a set of meaning representations that identify the time being described. For example, this grammar maps the phrase “2nd Friday of July” to the meaning representation intersect(nth(2 , friday), july), which encodes the set of a"
P14-1135,D11-1039,1,0.840556,"et a probability threshold for retaining a resolved time expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for"
P14-1135,J07-4004,0,0.0175618,"e optional modifier values for non-TIMEX properties (e.g., the modifier would contain EARLY for the phrase “early march”). We do recover these modifiers but omit them from the discussion since they are not part of the official evaluation metrics. map each m ∈ M to the referred time expression e. This paper addresses both of these tasks. Approach We learn separate, but related, models for detection and resolution. For both tasks, we define the space of possible compositional meaning representations Z, where each z ∈ Z defines a unique time expression e. We use a log-linear CCG (Steedman, 1996; Clark and Curran, 2007) to rank possible meanings z ∈ Z for each mention m in a document D, as described in Section 4. Both detection (Section 5) and resolution (Section 6) rely on the semantic parser to identify likely mentions and resolve them within context. For learning we assume access to TimeML data containing documents labeled with time expressions. Each document D has a set {(mi , ei )|i = 1 . . . n}, where each mention mi marks a phrase that resolves to the time expression ei . Evaluation We evaluate performance (Section 8) for both newswire text and Wikipedia articles. We compare to the state-of-the-art sy"
P14-1135,W10-2903,0,0.0416775,"us gold mentions to fill in missing context, where necessary. After parameter estimation, we set a probability threshold for retaining a resolved time expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic p"
P14-1135,de-marneffe-etal-2006-generating,0,0.0394593,"Missing"
P14-1135,S13-2009,0,0.165658,"Missing"
P14-1135,Q13-1005,1,0.360482,"ir underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005; Zett"
P14-1135,S13-2002,0,0.586194,"Both detection (Section 5) and resolution (Section 6) rely on the semantic parser to identify likely mentions and resolve them within context. For learning we assume access to TimeML data containing documents labeled with time expressions. Each document D has a set {(mi , ei )|i = 1 . . . n}, where each mention mi marks a phrase that resolves to the time expression ei . Evaluation We evaluate performance (Section 8) for both newswire text and Wikipedia articles. We compare to the state-of-the-art systems for end-to-end resolution (Str¨otgen and Gertz, 2013) and resolution given gold mentions (Bethard, 2013b), both of which do not use any machine learning techniques. 3 Representing Time We use simply typed lambda calculus to represent time expressions. Our representation draws heavily from the representation proposed by Angeli et al. (2012), who introduced semantic parsing for this task. There are five primitive types: duration d, sequence s, range r, approximate reference a, and numeral n, as described below. Table 1 lists the available constants for each type. Duration A period of time. Each duration is a multiple of one of a closed set of possible base durations (e.g., hour, day, and quarter)"
P14-1135,D13-1078,0,0.360323,"Both detection (Section 5) and resolution (Section 6) rely on the semantic parser to identify likely mentions and resolve them within context. For learning we assume access to TimeML data containing documents labeled with time expressions. Each document D has a set {(mi , ei )|i = 1 . . . n}, where each mention mi marks a phrase that resolves to the time expression ei . Evaluation We evaluate performance (Section 8) for both newswire text and Wikipedia articles. We compare to the state-of-the-art systems for end-to-end resolution (Str¨otgen and Gertz, 2013) and resolution given gold mentions (Bethard, 2013b), both of which do not use any machine learning techniques. 3 Representing Time We use simply typed lambda calculus to represent time expressions. Our representation draws heavily from the representation proposed by Angeli et al. (2012), who introduced semantic parsing for this task. There are five primitive types: duration d, sequence s, range r, approximate reference a, and numeral n, as described below. Table 1 lists the available constants for each type. Duration A period of time. Each duration is a multiple of one of a closed set of possible base durations (e.g., hour, day, and quarter)"
P14-1135,S13-1045,0,0.0200164,"(see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a n"
P14-1135,S13-2012,0,0.216262,"a number of different formal models for temporal reasoning, e.g. Allen (1983), Moens and Steedman (1988). In 2013, HeidelTime (Str¨otgen and Gertz, 2013) was the top performing system. It used deterministic rules defined over regular expressions to perform both detection and resolution, and will provide a comparison system for our evaluation in Section 9. In Corpus TempEval-3 (Dev) TempEval-3 (Test) WikiWars (Dev) WikiWars (Test) Doc. 256 20 17 5 Token 95,391 6,375 98,746 19,052 TimeEx 1,822 138 2,228 363 Figure 3: Corpus statistics. general, many different rule-based systems, e.g. NavyTime (Chambers, 2013) and SUTime (Chang and Manning, 2012), and learning systems, e.g. ClearTK (Bethard, 2013a) and MANTime (Filannino et al., 2013), did well for detection. However, rule-based approaches dominated in resolution; none of the top performers attempted to learn to do resolution. Our approach is a hybrid of rule based and learning, by using latent-variable learning techniques to estimate CCG parsing and context resolution models from the provided data. 8 Experimental Setup Data We evaluate performance on the TempEval-3 (Uzzaman et al., 2013) and WikiWars (Mazur and Dale, 2010) datasets. Figure 3 shows"
P14-1135,D12-1069,0,0.0292142,"expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resol"
P14-1135,D10-1119,1,0.432561,"introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised parameter updates (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of the TimeEx task, including 9"
P14-1135,D11-1140,1,0.852552,"arning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised parameter updates (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of the TimeEx task, including 9 teams who competed in the"
P14-1135,chang-manning-2012-sutime,0,0.403292,"Missing"
P14-1135,D13-1161,1,0.702846,"xt, where necessary. After parameter estimation, we set a probability threshold for retaining a resolved time expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). Howev"
P14-1135,P11-1060,0,0.0209998,"ill in missing context, where necessary. After parameter estimation, we set a probability threshold for retaining a resolved time expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effect"
P14-1135,D10-1089,0,0.424729,"Missing"
P14-1135,J88-2003,0,0.122204,"er, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of the TimeEx task, including 9 teams who competed in the 2013 TempEval-3 competition (Uzzaman et al., 2013). This line of work builds on ideas from TimeBank (Pustejovsky et al., 2003) and a number of different formal models for temporal reasoning, e.g. Allen (1983), Moens and Steedman (1988). In 2013, HeidelTime (Str¨otgen and Gertz, 2013) was the top performing system. It used deterministic rules defined over regular expressions to perform both detection and resolution, and will provide a comparison system for our evaluation in Section 9. In Corpus TempEval-3 (Dev) TempEval-3 (Test) WikiWars (Dev) WikiWars (Test) Doc. 256 20 17 5 Token 95,391 6,375 98,746 19,052 TimeEx 1,822 138 2,228 363 Figure 3: Corpus statistics. general, many different rule-based systems, e.g. NavyTime (Chambers, 2013) and SUTime (Chang and Manning, 2012), and learning systems, e.g. ClearTK (Bethard, 2013a)"
P14-1135,D07-1071,1,0.826521,"2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve time expressions (Angeli et al., 2012) and showed that the approach can generalize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised parameter updates (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of th"
P14-1135,P09-1110,1,0.7667,"eneralize to multiple languages (Angeli and Uszkoreit, 2013). Similarly, Bethard demonstrated that a hand-engineered semantic parser is also effective (Bethard, 2013b). However, these approaches did not use the semantic parser for detection and did not model linguistic context during resolution. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised parameter updates (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of the TimeEx task, including 9 teams who competed in the 2013 TempEval-3 competition (Uzzaman et al., 2013). This line of work builds on ideas from TimeBank (Pustejovsky"
P14-1135,strotgen-gertz-2012-temporal,0,0.0214624,"Missing"
P14-1135,N03-1033,0,0.0223773,"the probability distribution: P (t|m, D; Λ, θ) = eθ·φ(m,D,Λ) 1 + eθ·φ(m,D,Λ) where t indicates whether m is a time expression. Features We use three types of indicator features that test properties of the words in and around the potential mention m. Context tokens Indicate the presence of a set of manually specified tokens near the mention. These include quotations around the mention, the word “old” after the mention, and prepositions of time (such as “in”, “until”, and “during”) before. Part of speech Indicators that pair each word with its part of speech, as assigned by the Stanford tagger (Toutanova et al., 2003). Lexical group Each lexical entry belongs to one of thirteen manually defined lexical groups which cluster entries that contribute to the final time expression similarly. These groups include numbers, days of the week, months, seasons, etc. For each group, we include a feature indicating whether the parse includes a lexical entry from that group. Determiner dependency Indicates the presence of a determiner in the mention and whether its parent in the dependency tree (generated by the Stanford parser (de Marneffe et al., 2006)) also resides within the mention. Learning Finally, we construct th"
P14-1135,S13-2001,0,0.286535,"nsform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised parameter updates (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013b). However, we are the first to use a semantic parsing grammar within a mention detection algorithm, thereby avoiding the need to represent the meaning of complete sentences, and the first to develop a context-dependent model for semantic parsing of time expressions. Time expressions have been extensively studied as part of the TimeEx task, including 9 teams who competed in the 2013 TempEval-3 competition (Uzzaman et al., 2013). This line of work builds on ideas from TimeBank (Pustejovsky et al., 2003) and a number of different formal models for temporal reasoning, e.g. Allen (1983), Moens and Steedman (1988). In 2013, HeidelTime (Str¨otgen and Gertz, 2013) was the top performing system. It used deterministic rules defined over regular expressions to perform both detection and resolution, and will provide a comparison system for our evaluation in Section 9. In Corpus TempEval-3 (Dev) TempEval-3 (Test) WikiWars (Dev) WikiWars (Test) Doc. 256 20 17 5 Token 95,391 6,375 98,746 19,052 TimeEx 1,822 138 2,228 363 Figure 3"
P14-1135,P07-1121,0,0.0257662,", 2011) to optimize the conditional, marginal log-likelihood of the data. For each mention, we marginalize over all possible context-dependent parses, using the predictions from the model on the previous gold mentions to fill in missing context, where necessary. After parameter estimation, we set a probability threshold for retaining a resolved time expression by optimizing value F1 (see Section 8) over the training data. 7 Related Work Semantic parsers map sentences to logical representations of their underlying meaning, e.g., Zelle 1442 and Mooney (1996), Zettlemoyer and Collins (2005), and Wong and Mooney (2007). Recently, research in this area has focused on learning for various forms of relatively weak but easily gathered supervision. This includes learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b). Recently, Angeli et al. introduced the idea of learning semantic parsers to resolve ti"
P14-1135,L14-1000,0,\N,Missing
P15-1127,P14-1133,0,0.0344416,"Missing"
P15-1127,D13-1160,0,0.369869,"Missing"
P15-1127,D14-1067,0,0.101232,"Missing"
P15-1127,P13-1042,0,0.0681122,"Missing"
P15-1127,D13-1197,1,0.862442,"Missing"
P15-1127,P98-1067,0,0.0327646,"Missing"
P15-1127,P10-1160,0,0.0134093,"Missing"
P15-1127,P11-1055,1,0.874806,"Missing"
P15-1127,D14-1117,0,0.0740773,"Missing"
P15-1127,D12-1069,0,0.452556,"Missing"
P15-1127,P14-1112,0,0.0681801,"Missing"
P15-1127,D13-1161,1,0.639666,"e on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 1 Figure 1: Example noun phrases from Wikipedia category labels and appositives in newswire text. concepts that may or may not be representable using the Freebase ontology. Introduction Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB) (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Reddy et al., 2014). Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.1 In this paper, we instead consider the problem of learning semantic parsers for open domain text containing † Now at Google, NY. To ensure all questions are answerable, the data is manually filtered. For example, the WebQuestions dataset introduced by"
P15-1127,meyers-etal-2004-annotating,0,0.0780855,"Missing"
P15-1127,P09-1113,0,0.0247445,": Entity attribute extraction performance on the Wikipedia category development set. Data Wikipedia Appos System IE Baseline Our Approach IE Baseline Our Approach P 56.7 61.2 4.9 33.2 P* 58.7 72.6 13.9 61.4 fact # 1.6 2.0 1.3 0.9 Table 6: Manual evaluation for entity attribute extraction on the test sets. is trained using the annotated Wikipedia categories. This dataset contains hundreds of unary and binary relations, which the IE baseline can predict. Each classifier is further anchored on a specific word, and includes n-gram and POS context features around that word, following features from Mintz et al (2009). To predict binary relations, we used named entities as anchors. For unary attributes we anchored on all possible nouns and adjectives. The final logical form includes the best relation predicted by each classifier. We use the Stanford CoreNLP5 toolkit for tokenization, named entity recognition, and part-of-speech tagging. 8 Results Tables 3 and 4 show performance on the referring expression resolution task. Tables 5 and 6 show performance on the extraction task. Reported precision is lower on the labeled development set than on the test set, where predicted logical forms are manually evaluat"
P15-1127,Q14-1030,0,0.0682404,"Missing"
P15-1127,N13-1008,0,0.0316864,"Missing"
P15-1127,P08-1039,0,0.0731563,"Missing"
P15-1127,D14-1038,0,0.0390253,"Missing"
P15-1127,P14-1090,0,0.225541,"Missing"
P15-1127,C98-1064,0,\N,Missing
P16-1032,P14-2015,0,0.0301803,"ψr to the ψsocial . 2.3 Discussion While many studies exist on homophily, social balance, and reciprocity, no prior work has reported quantitative analysis on the sentiment dynamics among the real world entities that appear in unstructured text. Thus we report the data statistics based on the development set in Table 1. We find that the global constraints hold commonly but are not universal, motivating the use of soft constraints (see Sec. 6). 3 Document Features Previous work has shown that notions related to salience (e.g., proximity to sentiment words) can help to detect sentiment targets (Ben-Ami et al., 2014). In our data, we found that an entity’s occurrence pattern is highly indicative of being involved in sentiment, for example the most frequently mentioned entity is 3.4 times more likely to be polarized and an entity in the headline is two times more likely to be polarized. Pairwise features include the NER type of ei and ej and the percentage of sentences they cooccur in. We also use features indicating whether ei and ej (1) are mentioned in the headline and (2) appear only once in the document. When they are the two most frequent entities, we add the document sentiment label as a feature. Fo"
P16-1032,N16-1180,0,0.0367009,"Missing"
P16-1032,E14-1040,0,0.0311492,"y sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, includi"
P16-1032,N15-1185,0,0.0171205,"effectiveness of jointly considering multiple sentences. Yang and Mitchell (2016) proposed joint extraction of entities and events with the document context, improving on the event extraction. Most work focuses on events, while we primarily study sentiment relations. Social Network Analysis While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explicitly available social network structure or considered dialogues and speech acts for which opinion holders are given (Tan et al., 2011; Hu et al., 2013; Li et al., 2014; West et al., 2014; Krishnan and Eisenstein, 2015). Compared to the recent work that focused on relationships among fictional characters in movie summaries and stories (Chaturvedi et al., 2016; Srivastava et al., 2016; Iyyer et al., 2016), we consider a broader types of named entities on news domains. Related Work Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task,14 in that we aim to find opinion target and holder. While we study the complete document-level analysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially releva"
P16-1032,D15-1018,0,0.798225,"es among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing cases where the global con"
P16-1032,N15-1146,0,0.473481,"es among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing cases where the global con"
P16-1032,P14-1016,0,0.0290864,"omain, previous research showed the effectiveness of jointly considering multiple sentences. Yang and Mitchell (2016) proposed joint extraction of entities and events with the document context, improving on the event extraction. Most work focuses on events, while we primarily study sentiment relations. Social Network Analysis While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explicitly available social network structure or considered dialogues and speech acts for which opinion holders are given (Tan et al., 2011; Hu et al., 2013; Li et al., 2014; West et al., 2014; Krishnan and Eisenstein, 2015). Compared to the recent work that focused on relationships among fictional characters in movie summaries and stories (Chaturvedi et al., 2016; Srivastava et al., 2016; Iyyer et al., 2016), we consider a broader types of named entities on news domains. Related Work Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task,14 in that we aim to find opinion target and holder. While we study the complete document-level analysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entit"
P16-1032,C14-1009,0,0.0643484,"ge pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity dete"
P16-1032,N10-3009,0,0.0248868,"Thus we include document-level 336 features encoding this intuition. For example, the sentence “We’re pleased to put this behind us,” said Michael DuVally implies positive sentiment from DuVally. We extract direct quotations using regular expressions. We include the sentiment label of the direct quotation from the speaker to the entities in it, excluding entities that appear less than three times in the document. We add the sentiment label of the quotation as a feature to (speaker, the most frequent entity) pair as well. To extract indirect quotations, we follow studies (Bethard et al., 2004; Lu, 2010) and use a list of 20 verbs indicating speech events (e.g., say, speak, and announce) to detect direct quotations and their opinion holders. We then add the sentiment label of words connected to ej via a dependency path of length up to two that also includes the subject of quotation verb to ej (e.g. Hassanal said that cooperation between Brunei and China were fruitful). We also include an indicator feature for whether ei is the subject of the quotation verb. 3.2 Document count Avg. sentence count Avg. entity count Avg. mentions / entity MPQA 54 12.7 10.6 2.7 Crowdsourced 914 14.8 8.8 3.5 Table"
P16-1032,P14-5010,0,0.00671592,"stics annotations at the corpus-level (Ellis et al., 2014), by providing document-level annotations for all entity pairs (see Sec. 7 for discussion). 4.1 Document Preprocessing All-pair annotation can be expensive, as there are N 2 pairs to annotate for each document with N entities. We determined that it would be more cost efficient to cover a large number of short documents than a small number of very long documents. We therefore selected articles with less than eleven entities from KBP and less than fifteen from MPQA and took the first 15 sentences for annotation. We used Stanford CoreNLP (Manning et al., 2014) for sentence splitting, part-of-speech tagging, named entity recognition, co-reference resolution and dependency parsing. We discarded entities of type date, duration, money, time and number and merged named entities using several heuristics, such as merging acronyms, merging named entity of person type with the same last name (e.g., Tiger Woods to Woods). We merged names listed as alias in when there is an exact match from Freebase. We included all mentions in a co-reference chain with the named entity, discarding chains with more than one entity. The corpus statistics are shown in Table 2."
P16-1032,W10-0204,0,0.0154027,"irs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment"
P16-1032,P13-1174,1,0.841626,"retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions fo"
P16-1032,W04-2401,0,0.0700491,"elationship. 2 A Document-level Sentiment Model Given a news document d, and named entities e1 ,...,en in d, where each entity ei has mentions mi1 · · · mik , the task is to decide directed sentiment between all pairs of entities. We predict the directed sentiment from ei to ej at the document level, i.e., sent(ei →ej ) ∈ {positive, unbiased, negative}, for all ei , ej ∈ d where i 6= j, assuming that sentiment is consistent within the document. We introduce a document-level ILP that includes base models and soft social constraints. ILP has been used successfully for a wide range of NLP tasks (Roth and Yih, 2004), perhaps because they easily support incorporating different types of global constraints. We use two base models: (1) a learned pairwise sentiment classifier (Sec 3.1) that combines sentence- and discourse-level features to make predictions for each entity pair and (2) a pattern-based faction extractor (Sec 3.2) that detects alliances among a subset of the entities. The ILP is solved by maximizing: plicit sentiment, while preserving consistency. The sentiment dynamics in social groups, motivated by social science theories, are encoded as soft ILP constraints. They include a notion of homophil"
P16-1032,N09-1057,0,0.0250358,"alysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach c"
P16-1032,D13-1170,0,0.00828576,"ver care to work with a future perspective in mind,” Alexei Skripkov of the Federal Medical and Biological Agency said. “It’s a big systemic mistake.” Skripkov never appears together with Russia in any sentence, but he manifests negative sentiment towards it. When a document revolves around a theme (in this example Russia), sentiment is often directed to it without being explicitly mentioned. (Pair) and randomly assigning labels according to their empirical distribution (Random). The first existing method adaptation (Sentence) uses the publicly released sentence-level RNN sentiment model from Socher et al (2013). For each entity pair, we collect sentiment labels from sentences they co-occur in and assign a positive label if a positive-labeled sentence exists, negative if there exists more than one sentence with a negative label and no positives.9 We also report a proxy for doing similar aggregation over a state-of-the-art entity-entity sentiment classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annotations, we can simply predict the union of the original gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper boun"
P16-1032,R11-1028,0,0.0727201,"Missing"
P16-1032,Q14-1024,0,0.150553,"li (in Figure 2a), we can infer that Russia is negative towards Saakhashvilli (in Figure 2c). When considered in aggregate, these constraints can greatly improve the consistency over the overall document-level predictions. Our work stands in contrast to previous approaches in three aspects. First, we apply social dynamics motivated by social science theories to entity-entity sentiment analysis in unstructured text. In contrast, most previous studies focused on social media or dialogue data with overt social network structure when integrating social dynamics (Tan et al., 2011; Hu et al., 2013; West et al., 2014). Second, we aim to recover sentiment F =ψsocial + ψf act + n X n X ψij i=1 j=1 where F combines soft constraints (ψsocial , ψf act defined in detail in this section) with pairwise potentials ψij defined as: 1 All data will be made publicly available. You can browse it at http://homes.cs.washington. edu/˜eunsol/project_page/acl16, and download it from the author’s webpage. 334 Sentence Canadian Prime Minister Harper. . . . . . Reid, the Democratic leader. . . Goldman spokesman DuVally . . . Djibouti, a key U.S. ally. i Canada Reid Goldman Djibouti j Harper Democratic DuVally U.S. (b) Visual re"
P16-1032,H05-1044,0,0.103269,"Missing"
P16-1032,P13-1161,0,0.0276777,"based on the factual ties among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing case"
P16-1032,P11-2101,0,0.209066,"ers must be inferred based on the factual ties among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error"
P16-1032,N16-1033,0,\N,Missing
P16-1195,P15-2017,0,0.017322,"using rules to create dependency trees for each section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets g"
P16-1195,C12-2040,0,0.0433296,"Missing"
P16-1195,W11-2123,0,0.00870051,"nerate text from source code, and hence we adapt them slightly for this task, as explained below. IR is an information retrieval baseline that outputs the title associated with the code cj in the training set that is closest to the input code c in terms of token Levenshtein distance. In this case s from Eq.1 becomes, s(c, nj ) = −1 × lev(cj , c), 1 ≤ j ≤ J MOSES (Koehn et al., 2007) is a popular phrase-based machine translation system. We perform generation by treating the tokenized code snippet as the source language, and the title as the target. We train a 3-gram language model using KenLM (Heafield, 2011) to use with MOSES, and perform MIRA-based tuning (Cherry and Foster, 2012) of hyper-parameters using DEV. SUM-NN is the neural attention-based abstractive summarization model of Rush et al. (2015). 7 http://torch.ch It uses an encoder-decoder architecture with an attention mechanism based on a fixed context window of previously generated words. The decoder is a feed-forward neural language model that generates the next word based on previous words in a context window of size k. In contrast, we decode using an LSTM network that can model long range dependencies and our attention weights are ti"
P16-1195,P07-2045,0,0.0437632,"beam size to 10, and the maximum summary length to 20 words. 6 Experimental Setup 6.1 GEN Task 6.1.1 Baselines For the GEN task, we compare CODE-NN with a number of competitive systems, none of which had been previously applied to generate text from source code, and hence we adapt them slightly for this task, as explained below. IR is an information retrieval baseline that outputs the title associated with the code cj in the training set that is closest to the input code c in terms of token Levenshtein distance. In this case s from Eq.1 becomes, s(c, nj ) = −1 × lev(cj , c), 1 ≤ j ≤ J MOSES (Koehn et al., 2007) is a popular phrase-based machine translation system. We perform generation by treating the tokenized code snippet as the source language, and the title as the target. We train a 3-gram language model using KenLM (Heafield, 2011) to use with MOSES, and perform MIRA-based tuning (Cherry and Foster, 2012) of hyper-parameters using DEV. SUM-NN is the neural attention-based abstractive summarization model of Rush et al. (2015). 7 http://torch.ch It uses an encoder-decoder architecture with an attention mechanism based on a fixed context window of previously generated words. The decoder is a feed-"
P16-1195,N13-1103,0,0.0216271,"k well for retrieval but cannot be used for generation. We learn a neural generation model without using search logs and show that it can also be used to score code for retrieval, with much higher accuracy. Synthesizing code from language is an alternative to code retrieval and has been studied in both the Systems and NLP research communities. Giordani and Moschitti (2012), Li and Jagadish (2014), and Gulwani and Marron (2014) synthesize source code from NL queries for database and spreadsheet applications. Similarly, Lei et al. (2013) interpret NL instructions to machine-executable code, and Kushman and Barzilay (2013) convert language to regular expressions. Unlike most synthesis methods, CODE-NN is domain agnostic, as we demonstrate its applications on both C# and SQL. 4 Dataset We collected data from StackOverflow (SO), a popular website for posting programming-related questions. Anonymized versions of all the posts can be freely downloaded.3 Each post can have multiple tags. Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively.4 Each post comprises a short title, a detailed question, and one or more responses, of which one"
P16-1195,P13-1127,0,0.00791456,"plicative models to score (code, language) pairs, an approach that could work well for retrieval but cannot be used for generation. We learn a neural generation model without using search logs and show that it can also be used to score code for retrieval, with much higher accuracy. Synthesizing code from language is an alternative to code retrieval and has been studied in both the Systems and NLP research communities. Giordani and Moschitti (2012), Li and Jagadish (2014), and Gulwani and Marron (2014) synthesize source code from NL queries for database and spreadsheet applications. Similarly, Lei et al. (2013) interpret NL instructions to machine-executable code, and Kushman and Barzilay (2013) convert language to regular expressions. Unlike most synthesis methods, CODE-NN is domain agnostic, as we demonstrate its applications on both C# and SQL. 4 Dataset We collected data from StackOverflow (SO), a popular website for posting programming-related questions. Anonymized versions of all the posts can be freely downloaded.3 Each post can have multiple tags. Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively.4 Each post"
P16-1195,C04-1072,0,0.0258451,"g automatic metrics, and also perform a human study. Automatic Evaluation We report METEOR (Banerjee and Lavie, 2005) and sentence level BLEU-4 (Papineni et al., 2002) scores. METEOR is recall-oriented and measures how well our model captures content from the references in our output. BLEU-4 measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. Since the generated summaries are short and there are multiple alternate summaries for a given code snippet, higher order n-grams may not overlap. We remedy this problem by using +1 smoothing (Lin and Och, 2004). We compute these metrics on the tuning set DEV and the held-out evaluation set EVAL. Human Evaluation Since automatic metrics do not always agree with the actual quality of the results (Stent et al., 2005), we perform human evaluation studies to measure the output of our system and baselines across two modalities, namely naturalness and informativeness. For the former, we asked 5 native English speakers to rate each title against grammaticality and fluency, on a scale between 1 and 5. For informativeness (i.e., the amount of content carried over from the input code to the NL summary, ignorin"
P16-1195,D11-1149,0,0.00588652,"he code (e.g., consider the phrase ‘second largest’ in Example 3 in Figure 1). In addition to being directly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor techniques to a large corpus of automatically summarized code. Natural language generation has traditionally been addressed as a pipeline of modules that decide ‘what to say’ (content selection) and ‘how to say it’ (realization) separately (Reiter and Dale, 2000; Wong and Mooney, 2007; Chen et al., 2010; Lu and Ng, 2011). Such approaches require supervision at each stage and do not scale well to large domains. We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks. The system generates a summary one word at a time, guided by an attention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network (Hochreiter and Schmidhuber, 1997). The simplicity of the model allows it to be learned from the training data"
P16-1195,D15-1166,0,0.0285122,"q. 1) as a product of the conditional next-word probabilities l Y k X αi,j · cj F j=1 Figure 2: Generation of a title n = n1 , . . . , END given code snippet c1 , ..., ck . The attention cell computes a distributional representation ti of the code snippet based on the current LSTM hidden state hi . A combination of ti and hi is used to generate the next word, ni , which feeds back into the next LSTM cell. This is repeated until a fixed number of words or END is generated. ∝ blocks denote softmax operations. s(c, n) = Attention The generation of each word is guided by a global attention model (Luong et al., 2015), which computes a weighted sum of the embeddings of the code snippet tokens based on the current LSTM state (see right part in Figure 2). Formally, we represent c as a set of 1-hot vectors c1 , . . . , ck ∈ {0, 1}|C |for each source code token; C is the vocabulary of all tokens in our code snippets. Our attention model computes, p(ni |n1 , . . . , ni−1 ) i=1 with, p(ni |n1 , . . . , ni−1 ) ∝ W tanh(W1 hi + W2 ti ) where, W ∈ R|N |×H and W1 , W2 ∈ RH×H , H being the embedding dimensionality of the summaries. ti is the contribution from the attention model on the source code (see below). hi rep"
P16-1195,N16-1086,0,0.00817074,"cribe them. There is also work on improving program comprehension (Haiduc et al., 2074 2010), identifying cross-cutting source code concerns (Rastkar et al., 2011), and summarizing software bug reports (Rastkar et al., 2010). To the best of our knowledge, we are the first to use learning techniques to construct completely new sentences from arbitrary code snippets. Source code summarization is also related to generation from formal meaning representations. Wong and Mooney (2007) present a system that learns to generate sentences from lambda calculus expressions by inverting a semantic parser. Mei et al. (2016), Konstas and Lapata (2013), and Angeli et al. (2010) create learning algorithms for text generation from database records, again assuming data that pairs sentences with formal meaning representations. In contrast, we present algorithms for learning from easily gathered web data. In the database community, Simitsis and Ioannidis (2009) recognize the need for SQL database systems to talk back to users. Koutrika et al. (2010) built an interactive system (LOGOS) that translates SQL queries to text using NL templates and database schemas. Similarly there has been work on translating SPARQL queries"
P16-1195,P13-2007,0,0.152214,"snippet c ∈ UC , the goal is to produce a NL sentence n∗ ∈ UN that maximizes some scoring function s ∈ (UC × UN → R): n∗ = argmax s(c, n) (1) n RET We also use the scoring function s to retrieve the highest scoring code snippet c∗j from our training corpus, given a NL question n ∈ UN : c∗j = argmax s(cj , n), 1 ≤ j ≤ J cj (2) In this work, s is computed using an LSTM neural attention model, to be described in Section 5. 3 Related Work Although we focus on generating high-level summaries of source code snippets, there has been work on producing code descriptions at other levels of abstraction. Movshovitz-Attias and Cohen (2013) study the task of predicting class-level comments by learning n-gram and topic models from open source Java projects and testing it using a character-saving metric on existing comments. Allamanis et al. (2015a) create models for suggesting method and class names by embedding them in a high dimensional continuous space. Sridhara et al. (2010) present a pipeline that generates summaries of Java methods by selecting relevant content and generating phrases using templates to describe them. There is also work on improving program comprehension (Haiduc et al., 2074 2010), identifying cross-cutting"
P16-1195,P02-1040,0,0.118383,"attention weights are tied to the LSTM hidden states. We set the embedding and hidden state dimensions and context window size by tuning on our validation set. We found this model to generate overly short titles like ‘sql server 2008’ when a length restriction was not imposed on the output text. Therefore, we fix the output length to be the average title length in the training set while decoding. 6.1.2 Evaluation Metrics We evaluate the GEN task using automatic metrics, and also perform a human study. Automatic Evaluation We report METEOR (Banerjee and Lavie, 2005) and sentence level BLEU-4 (Papineni et al., 2002) scores. METEOR is recall-oriented and measures how well our model captures content from the references in our output. BLEU-4 measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. Since the generated summaries are short and there are multiple alternate summaries for a given code snippet, higher order n-grams may not overlap. We remedy this problem by using +1 smoothing (Lin and Och, 2004). We compute these metrics on the tuning set DEV and the held-out evaluation set EVAL. Human Evaluation Since automatic metrics do not always agree w"
P16-1195,D15-1044,0,0.201488,"ch section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets given NL questions and vice versa. They const"
P16-1195,P15-1150,0,0.0230533,"Missing"
P16-1195,N15-1173,0,0.0185772,"ueries to natural language using rules to create dependency trees for each section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retriev"
P16-1195,D15-1199,0,0.012087,"Missing"
P16-1195,N07-1022,0,0.023521,"an include complex, non-local aspects of the code (e.g., consider the phrase ‘second largest’ in Example 3 in Figure 1). In addition to being directly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor techniques to a large corpus of automatically summarized code. Natural language generation has traditionally been addressed as a pipeline of modules that decide ‘what to say’ (content selection) and ‘how to say it’ (realization) separately (Reiter and Dale, 2000; Wong and Mooney, 2007; Chen et al., 2010; Lu and Ng, 2011). Such approaches require supervision at each stage and do not scale well to large domains. We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks. The system generates a summary one word at a time, guided by an attention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network (Hochreiter and Schmidhuber, 1997). The simplicity of the model allows it"
P16-1195,D14-1074,0,0.0139263,"on step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets given NL questions and vice versa. They construct distributional representations of code structure an"
P16-1195,W05-0909,0,\N,Missing
P16-1195,N12-1047,0,\N,Missing
P17-1014,D15-1198,1,0.896171,"Missing"
P17-1014,P05-1045,0,0.0446321,"c) named entity clustering, and (d) insertion of scope markers. group, :ARG2-of, :ARG1-of, :ARG0.4 The order traverses children in the sequence they are presented in the AMR. We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above. ation, we render the corresponding format when predicted. Figure 2(b) contains an example of all preprocessing up to this stage. Named Entity Clusters When performing AMR generation, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc. This reduces the sparsity associated with many rarely occurring entity types. Figure 2 (c) contains an example with named entity clusters. Rendering Function Our rendering function marks scope, and generates tokens following the pre-order traversal of the graph: (1) if the element is a node, it emits the type of the node. (2) if the element is an edge, it emits the type of the edge and then recursively emits a bracketed string for the (concept) node immediately after it. In case the node has only one child we omit the scope markers (denoted with left"
P17-1014,N16-1087,0,0.360548,"ifying the contributions that come from preprocessing and the paired training procedure. 2 Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearizat"
P17-1014,S16-1176,0,0.461842,"mance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2s"
P17-1014,P14-1134,0,0.507344,"Missing"
P17-1014,bender-2014-language,0,0.0121142,"ucially, we avoid relying on resources such as knowledge bases and externally trained parsers. We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8). For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)). This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014). Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000). state :arg0 report :arg1 ( obligate :arg1 ( government-organization :arg0-of ( govern :arg1 loc_0 ) ) :arg2 ( help :arg1 ( and :op1 ( stabilize :arg1 ( state :mod weak ) ) :op2 ( push :arg1 ( regulate :mod international :arg0-of ( stop :arg1 terror"
P17-1014,S16-1182,0,0.0387204,"et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn hi"
P17-1014,S16-1180,0,0.0589977,"ications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences ("
P17-1014,S16-1179,0,0.037075,"ermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR pars"
P17-1014,P16-1025,0,0.0126843,"t enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al."
P17-1014,P13-2131,0,0.267935,"M (Barzdins and Gosko, 2016) Prec 72.3 67.2 62.2 61.9 59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Al"
P17-1014,C12-1083,0,0.153954,"Missing"
P17-1014,N15-1114,0,0.0887244,". Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categ"
P17-1014,W16-6603,0,0.309947,"(Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model t"
P17-1014,D15-1136,0,0.106718,"MR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn high quality represent"
P17-1014,D15-1166,0,0.261978,"igure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity. Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely,"
P17-1014,D16-1183,0,0.0137661,"establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can b"
P17-1014,S16-1178,0,0.0388043,"esentation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unl"
P17-1014,W12-3018,0,0.0220269,"Missing"
P17-1014,P16-1009,0,0.0228418,"external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output, Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017), 147 et al., 2016).1 Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015). The model uses a stacked bidirectional-LSTM encoder to encode an input sequ"
P17-1014,J05-1004,0,0.015856,"e use (section 3.2), graph-to-sequence conversion (section 3.3), and our paired training procedure (section 3.4). 3.1 Tasks We assume access to a training dataset D where each example pairs a natural language sentence s with an AMR a. The AMR is a rooted directed acylical graph. It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect.01, Obama, and person in Figure 1. One of these nodes is a distinguished root, for example, the node and in Figure 1. Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a set of tokens C, and every word in a sentence is drawn from a vocabulary W . We study the task of training an AMR parser, i.e., finding a set of parameters θP for model f , that predicts an AMR graph a ˆ, given a sentence s: a ˆ = argmax f a|s; θP a  3.3 Our seq2seq models require that both the input and target be presented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nod"
P17-1014,D16-1224,0,0.150248,"d reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) ("
P17-1014,P02-1040,0,0.101398,"59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Algorithm 1 – line 1), we use a single stack version of o"
P17-1014,D16-1112,0,0.02967,"roduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretra"
P17-1014,E17-1035,0,0.732145,"e-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome,"
P17-1014,D16-1065,0,0.0687239,"into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target lang"
P17-1014,P07-2045,0,\N,Missing
P17-1014,D14-1048,0,\N,Missing
P17-1014,S16-1181,0,\N,Missing
P17-1014,E17-1051,0,\N,Missing
P17-1044,D16-1262,1,0.303684,"arn a locally decomposed Pn scoring function conditioned on the input: t=1 log p(yt |w). To incorporate additional information (e.g., structural consistency, syntactic input), we augment the scoring function with penalization terms: f (w, y) = P (BV ) Transform Gates y∈Y n X P (IARG0 ) Softmax Two major factors contribute to the success of our deep SRL model: (1) applying recent advances in training deep recurrent neural networks such as highway connections (Srivastava et al., 2015) and RNN-dropouts (Gal and Ghahramani, 2016),2 and (2) using an A∗ decoding algorithm (Lewis and Steedman, 2014; Lee et al., 2016) to enforce structural consistency at prediction time without adding more complexity to the training process. Formally, our task is to predict a sequence y given a sentence-predicate pair (w, v) as input. Each yi ∈ y belongs to a discrete set of BIO tags T . Words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags Br and Ir respectively. Let n = |w |= |y |be the length of the sequence. Predicting an SRL structure under our model involves finding the highest-scoring tag sequence over the space of all possibilities Y: yˆ = arg"
P17-1044,D14-1107,1,0.666758,"tional LSTM (BiLSTM) to learn a locally decomposed Pn scoring function conditioned on the input: t=1 log p(yt |w). To incorporate additional information (e.g., structural consistency, syntactic input), we augment the scoring function with penalization terms: f (w, y) = P (BV ) Transform Gates y∈Y n X P (IARG0 ) Softmax Two major factors contribute to the success of our deep SRL model: (1) applying recent advances in training deep recurrent neural networks such as highway connections (Srivastava et al., 2015) and RNN-dropouts (Gal and Ghahramani, 2016),2 and (2) using an A∗ decoding algorithm (Lewis and Steedman, 2014; Lee et al., 2016) to enforce structural consistency at prediction time without adding more complexity to the training process. Formally, our task is to predict a sequence y given a sentence-predicate pair (w, v) as input. Each yi ∈ y belongs to a discrete set of BIO tags T . Words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags Br and Ir respectively. Let n = |w |= |y |be the length of the sequence. Predicting an SRL structure under our model involves finding the highest-scoring tag sequence over the space of all possib"
P17-1044,W05-0620,0,0.54535,"Missing"
P17-1044,K17-1041,0,0.508378,"cting long-distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct-argument distinctions; (3) the role of syntax, showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL. Introduction Semantic role labeling (SRL) systems aim to recover the predicate-argument structure of a sentence, to determine essentially “who did what to whom”, “when”, and “where.” Recently breakthroughs involving end-to-end deep models for SRL without syntactic input (Zhou and Xu, 2015; Marcheggiani et al., 2017) seem to overturn the long-held belief that syntactic parsing is a prerequisite for this task (Punyakanok et al., 2008). In this paper, we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding, again significantly moving the state of the art (another 2 points on CoNLL 2005). We also present a careful empirical analysis to determine what works well and what might be done to progress even further. Our model combines a number of best practices in the recent deep learning literature. FolIn summary, our main contributions incluede: • A new stat"
P17-1044,A00-2018,0,0.116617,"Missing"
P17-1044,D14-1162,0,0.120331,"81.5 67.5 64.6 83.5 81.7 83.3 81.6 83.4 81.7 68.5 66.0 Zhou FitzGerald (Struct.,PoE) 81.0 T¨ackstr¨om (Struct.) 80.5 Pradhan (revised) - 78.5 77.8 - 81.1 79.7 79.1 - 60.9 60.1 - 81.2 80.6 78.5 79.0 78.2 76.6 81.3 80.1 79.4 77.5 62.6 61.8 55.8 Ours (PoE) Ours Table 2: Experimental results on CoNLL 2012 in the same metrics as above. We compare our best single and ensemble (PoE) models against Zhou and Xu (2015), FitzGerald et al. (2015), T¨ackstr¨om et al. (2015) and Pradhan et al. (2013). All tokens are lower-cased and initialized with 100-dimensional GloVe embeddings pre-trained on 6B tokens (Pennington et al., 2014) and updated during training. Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding. and CoNLL 2012 development sets. Only the BIO hard constraints significantly improve over the ensemble model. Therefore, in our final results, we only use BIO hard constraints during decoding. 5 3.3 Training We use Adadelta (Zeiler, 2012) with  = 1e−6 and ρ = 0.95 and mini-batches of size 80. We set RNN-dropout probability to 0.1 and clip gradients with norm larger than 1. All the models are trained for 500 epochs with early stopping based on development results. 4 Result"
P17-1044,D16-1257,0,0.0300695,"Missing"
P17-1044,W05-0634,0,0.0151511,"o previous work, we address the following questions with a suite of empirical analyses: • What is the model good at and what kinds of mistakes does it make? • How well do LSTMs model global structural consistency, despite conditionally independent tagging decisions? • Is our model implicitly learning syntax, and could explicitly modeling syntax still help? All the analysis in this section is done on the CoNLL 2005 development set with gold predicates, unless otherwise stated. We are also able to compare to previous systems whose model predictions are available online (Punyakanok et al., 2005; Pradhan et al., 2005).7 Ablations Figure 2 shows learning curves of our model ablations on the CoNLL 2005 development set. We ablate our full model by removing highway connections, RNN-dropout, and orthonormal initialization independently. Without dropout, the model overfits at around 300 epochs at 78 F1. Orthonormal parameter initialization is surprisingly important—without this, the model achieves only 65 F1 within the first 50 epochs. All 8 layer ablations suffer a loss of more than 1.7 in absolute F1 compared to the full model. 3.5 Analysis 4.1 End-to-end SRL Error Types Breakdown Inspired by Kummerfeld et al."
P17-1044,D15-1112,0,0.583084,"Missing"
P17-1044,W05-0625,0,0.04735,"model and its relation to previous work, we address the following questions with a suite of empirical analyses: • What is the model good at and what kinds of mistakes does it make? • How well do LSTMs model global structural consistency, despite conditionally independent tagging decisions? • Is our model implicitly learning syntax, and could explicitly modeling syntax still help? All the analysis in this section is done on the CoNLL 2005 development set with gold predicates, unless otherwise stated. We are also able to compare to previous systems whose model predictions are available online (Punyakanok et al., 2005; Pradhan et al., 2005).7 Ablations Figure 2 shows learning curves of our model ablations on the CoNLL 2005 development set. We ablate our full model by removing highway connections, RNN-dropout, and orthonormal initialization independently. Without dropout, the model overfits at around 300 epochs at 78 F1. Orthonormal parameter initialization is surprisingly important—without this, the model achieves only 65 F1 within the first 50 epochs. All 8 layer ablations suffer a loss of more than 1.7 in absolute F1 compared to the full model. 3.5 Analysis 4.1 End-to-end SRL Error Types Breakdown Inspir"
P17-1044,J08-2005,0,0.946969,"t distinctions; (3) the role of syntax, showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL. Introduction Semantic role labeling (SRL) systems aim to recover the predicate-argument structure of a sentence, to determine essentially “who did what to whom”, “when”, and “where.” Recently breakthroughs involving end-to-end deep models for SRL without syntactic input (Zhou and Xu, 2015; Marcheggiani et al., 2017) seem to overturn the long-held belief that syntactic parsing is a prerequisite for this task (Punyakanok et al., 2008). In this paper, we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding, again significantly moving the state of the art (another 2 points on CoNLL 2005). We also present a careful empirical analysis to determine what works well and what might be done to progress even further. Our model combines a number of best practices in the recent deep learning literature. FolIn summary, our main contributions incluede: • A new state-of-the-art deep network for endto-end SRL, supported by publicly available code and models.1 • An in-depth error anal"
P17-1044,J13-4006,0,0.00617703,"to learn long-range dependencies, syntactic constituency structure, and global constraints without coding task-specific mechanisms for doing so. An alternative line of work has attempted to reduce the dependency on syntactic input for semantic role labeling models. Collobert et al. (2011) first introduced an end-to-end neural-based approach with sequence-level training and uses a convolutional neural network to model the context window. However, their best system fell short of traditional feature-based systems. Neural methods have also been used as classifiers in transition-based SRL systems (Henderson et al., 2013; Swayamdipta et al., 2016). Most recently, several successful LSTM-based architecAcknowledgments The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We are grateful to Mingxuan Wang for sharing his highway LSTM implementation and Sameer Pradhan for help with the CoNLL 2012 dataset. We thank Nicholas FitzGerald, Dan Garrette, Julian Michael, Hao Peng, and Swabha Swayamdipta for helpful comments, and the anonymous reviewe"
P17-1044,P16-1113,0,0.382962,"eep SRL models. CoNLL-2012 Dev. Dev. Test BC BN NW MZ PT TC WB L8+PoE 82.7 84.6 81.4 82.8 82.8 80.4 93.6 84.8 81.0 +AutoSyn 83.2 84.8 81.5 82.8 83.2 80.6 93.7 84.9 81.1 Table 8: F1 on CoNLL 2005, and the development set of CoNLL 2012, broken down by genres. Syntax-constrained decoding (+AutoSyn) shows bigger improvement on in-domain data (CoNLL 05 and CoNLL 2012 NW). global consistency using integer linear programming (Punyakanok et al., 2008) or dynamic programs (T¨ackstr¨om et al., 2015). More recently, neural methods have been employed on top of syntactic features (FitzGerald et al., 2015; Roth and Lapata, 2016). Our experiments show that offthe-shelf neural methods have a remarkable ability to learn long-range dependencies, syntactic constituency structure, and global constraints without coding task-specific mechanisms for doing so. An alternative line of work has attempted to reduce the dependency on syntactic input for semantic role labeling models. Collobert et al. (2011) first introduced an end-to-end neural-based approach with sequence-level training and uses a convolutional neural network to model the context window. However, their best system fell short of traditional feature-based systems. N"
P17-1044,D12-1096,0,0.00758856,"han et al., 2005).7 Ablations Figure 2 shows learning curves of our model ablations on the CoNLL 2005 development set. We ablate our full model by removing highway connections, RNN-dropout, and orthonormal initialization independently. Without dropout, the model overfits at around 300 epochs at 78 F1. Orthonormal parameter initialization is surprisingly important—without this, the model achieves only 65 F1 within the first 50 epochs. All 8 layer ablations suffer a loss of more than 1.7 in absolute F1 compared to the full model. 3.5 Analysis 4.1 End-to-end SRL Error Types Breakdown Inspired by Kummerfeld et al. (2012), we define a set of oracle transformations that fix various prediction errors sequentially and observe the relative improvement after each operation (see Table 4). Figure 3 shows how our work compares to the preThe network for predicate detection (Section 2.3) contains 2 BiLSTM layers with 100-dimensional hidden units, and is trained for 30 epochs. For end-to-end evaluation, all arguments predicted for the false positive predicates are counted as precision loss, and all arguments for the false negative predicates are considered as recall loss. Table 3 shows the predicate detection F1 as well"
P17-1044,K16-1019,0,0.033471,"ndencies, syntactic constituency structure, and global constraints without coding task-specific mechanisms for doing so. An alternative line of work has attempted to reduce the dependency on syntactic input for semantic role labeling models. Collobert et al. (2011) first introduced an end-to-end neural-based approach with sequence-level training and uses a convolutional neural network to model the context window. However, their best system fell short of traditional feature-based systems. Neural methods have also been used as classifiers in transition-based SRL systems (Henderson et al., 2013; Swayamdipta et al., 2016). Most recently, several successful LSTM-based architecAcknowledgments The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We are grateful to Mingxuan Wang for sharing his highway LSTM implementation and Sameer Pradhan for help with the CoNLL 2012 dataset. We thank Nicholas FitzGerald, Dan Garrette, Julian Michael, Hao Peng, and Swabha Swayamdipta for helpful comments, and the anonymous reviewers for valuable feedback. 4"
P17-1044,Q15-1003,0,0.0618488,"Missing"
P17-1044,J08-2002,0,0.0249944,"yakanok (Ensemble) 79.7 81.2 81.2 80.1 79.6 78.9 78.6 78.6 77.4 55.1 54.4 58.7 50.7 82.9 82.5 82.3 81.9 82.3 82.8 78.2 77.6 78.8 76.8 82.8 80.3 79.9 80.3 79.4 57.3 56.0 60.1 53.8 70.7 74.5 74.3 73.4 68.2 70.0 68.6 62.9 69.4 72.2 71.3 68.8 67.8 41.3 39.8 40.8 32.3 81.1 77.9 79.4 76.7 76.2 74.8 Table 1: Experimental results on CoNLL 2005, in terms of precision (P), recall (R), F1 and percentage of completely correct predicates (Comp.). We report results of our best single and ensemble (PoE) model. The comparison models are Zhou and Xu (2015), FitzGerald et al. (2015), T¨ackstr¨om et al. (2015), Toutanova et al. (2008) and Punyakanok et al. (2008). Development Method Test P R F1 Comp. P R F1 Comp. 83.5 81.8 83.2 81.4 83.4 81.5 67.5 64.6 83.5 81.7 83.3 81.6 83.4 81.7 68.5 66.0 Zhou FitzGerald (Struct.,PoE) 81.0 T¨ackstr¨om (Struct.) 80.5 Pradhan (revised) - 78.5 77.8 - 81.1 79.7 79.1 - 60.9 60.1 - 81.2 80.6 78.5 79.0 78.2 76.6 81.3 80.1 79.4 77.5 62.6 61.8 55.8 Ours (PoE) Ours Table 2: Experimental results on CoNLL 2012 in the same metrics as above. We compare our best single and ensemble (PoE) models against Zhou and Xu (2015), FitzGerald et al. (2015), T¨ackstr¨om et al. (2015) and Pradhan et al. (2013). A"
P17-1044,D15-1186,0,0.221776,"Missing"
P17-1044,P15-1109,0,0.898847,"dels excel at predicting long-distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct-argument distinctions; (3) the role of syntax, showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL. Introduction Semantic role labeling (SRL) systems aim to recover the predicate-argument structure of a sentence, to determine essentially “who did what to whom”, “when”, and “where.” Recently breakthroughs involving end-to-end deep models for SRL without syntactic input (Zhou and Xu, 2015; Marcheggiani et al., 2017) seem to overturn the long-held belief that syntactic parsing is a prerequisite for this task (Punyakanok et al., 2008). In this paper, we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding, again significantly moving the state of the art (another 2 points on CoNLL 2005). We also present a careful empirical analysis to determine what works well and what might be done to progress even further. Our model combines a number of best practices in the recent deep learning literature. FolIn summary, our main contribu"
P17-1089,Q13-1005,1,0.947688,"Missing"
P17-1089,D13-1160,0,0.510211,"Missing"
P17-1089,P15-1127,1,0.43684,"Missing"
P17-1089,N13-1103,0,0.0606504,"Missing"
P17-1089,W10-2903,0,0.0282341,"Missing"
P17-1089,D13-1161,1,0.8402,"Missing"
P17-1089,P16-1004,0,0.623508,"Missing"
P17-1089,D11-1140,1,0.458728,"Missing"
P17-1089,J13-2005,0,0.537802,"short regular expressions (Locascio et al., 2016). Our work extends these results to the task of SQL generation. Finally, Ling et al. (2016) generate Java/Python code for trading cards given a natural language description; however, this system suffers from low overall accuracy. A final direction of related work studies methods for reducing the annotation effort required to train a semantic parser. Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al., 2013; Kwiatkowski et al., 2013; Berant et al., cluding batch learning of models that directly produce programs (e.g., regular expressions (Locascio et al., 2016)), learning from paraphrases (often gathered through crowdsourcing (Wang et al., 2015)), data augmentation (e.g. based on manually engineered semantic grammars (Jia and Liang, 2016)) and learning through direct interaction with users (e.g., where a single user teaches the model new concepts (Wang et al., 2016)). However, there are unique advantages to our approach, including showing (1) that non-linguists can write SQL to encode complex, c"
P17-1089,C12-2040,0,0.0994407,"Missing"
P17-1089,P16-1057,0,0.0840081,"Missing"
P17-1089,P16-1002,0,0.201283,"Missing"
P17-1089,D16-1197,0,0.0742209,"Missing"
P17-1089,D16-1032,1,0.290482,"Missing"
P17-1089,D15-1166,0,0.0825084,"(R) if f = correct then T ← T ∪ (n, q) else if f = wrong then qˆ ← annotate(n) T ← T ∪ (n, qˆ) end end end end Algorithm 1: Feedback-based learning. 4 Semantic Parsing to SQL We use a neural sequence-to-sequence model for mapping natural language questions directly to SQL queries and this allows us to scale our feedback-based learning approach, by easily crowdsourcing labels when necessary. We further present two data augmentation techniques which use content from the database schema and external paraphrase resources. 4.1 Model We use an encoder-decoder model with global attention, similar to Luong et al. (2015), where the anonymized utterance (see Section 4.2) is encoded using a bidirectional LSTM network, then decoded to directly predict SQL query tokens. Fixed pre-trained word embeddings from word2vec (Mikolov et al., 2013) are concatenated to the embeddings that are learned for source tokens from the training data. The decoder predicts a conditional probability distribution over possible values for the next SQL token given the previous tokens using a combination of the previous SQL token embedding, attention over the hidden states of the encoder network, and an attention signal from the previous"
P17-1089,D12-1069,1,0.486156,"Missing"
P17-1089,P14-1026,1,0.73715,"Missing"
P17-1089,D16-1183,0,0.0453999,"Missing"
P17-1089,P16-2033,0,0.0171502,"Missing"
P17-1089,P13-1092,0,0.0289636,"Missing"
P17-1089,D07-1071,1,0.537961,"Missing"
P17-1089,D16-1117,0,0.0112453,"ly, we do a small scale online experiment for a new domain, academic paper metadata search, demonstrating that actual users can provide useful feedback and our full approach is an effective method for learning a high quality parser that continues to improve over time as it is used. 2 Related Work Although diverse meaning representation languages have been used with semantic parsers – such as regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), Abstract Meaning Representations (AMR) (Artzi et al., 2015; Misra and Artzi, 2016), and systems of equations (Kushman et al., 2014; Roy et al., 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins, 2005), or λDCS (Liang et al., 2013) as the meaning represen1 964 http://www.upwork.com 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al., 2015), and binary correct/incorrect feedback signals (Clarke et al., 2010; Artzi and Zettlemoyer, 2013). Each of these schemes presents a particular trade-off between annotation effort and parser accuracy; however, recent work has suggested that labeled queries are the most effective (Yih et a"
P17-1089,D14-1135,1,0.697797,"Missing"
P17-1089,P16-1224,0,0.03279,"Missing"
P17-1089,P15-1129,0,0.488314,"Missing"
P17-1089,N07-1022,0,0.0186852,"Missing"
P17-1089,P11-1060,0,\N,Missing
P17-1089,D15-1198,1,\N,Missing
P17-1089,P16-1073,0,\N,Missing
P17-1147,D13-1160,0,0.281544,"tly released datasets include (Lai et al., 2017). 7.2 Open domain question answering The recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusiasts provided both questions and answers for our dataset. Knowledge base question answering involves converting natural language questions to logical forms that can be executed over a KB. Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB. A standard task for open domain IR-style QA is the annual TREC competitions (Voorhees and Tice, 2000), which contains questions from various domains but is limited in size. Many advances from the TREC competitions were used in the IBM Watson system for Jeopardy! (Ferrucci et al., 2010). Other datasets includes SearchQA 1608 (Dunn et al., 2017) where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset (Yang et al., 2015) for answer sente"
P17-1147,D12-1118,0,0.076931,"Missing"
P17-1147,P13-1042,0,0.0215412,"ectively. Other recently released datasets include (Lai et al., 2017). 7.2 Open domain question answering The recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusiasts provided both questions and answers for our dataset. Knowledge base question answering involves converting natural language questions to logical forms that can be executed over a KB. Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB. A standard task for open domain IR-style QA is the annual TREC competitions (Voorhees and Tice, 2000), which contains questions from various domains but is limited in size. Many advances from the TREC competitions were used in the IBM Watson system for Jeopardy! (Ferrucci et al., 2010). Other datasets includes SearchQA 1608 (Dunn et al., 2017) where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset (Yang et al., 2"
P17-1147,P16-1223,0,0.305805,"Missing"
P17-1147,D14-1070,0,0.0553695,"Missing"
P17-1147,D14-1117,1,0.746136,"Missing"
P17-1147,D16-1241,0,0.341143,"//nlp.cs. washington.edu/triviaqa/ recover from text (e.g. due to lexical and syntactic variation). Figure 1 shows examples of all these phenomena. This paper presents TriviaQA, a new reading comprehension dataset designed to simultaneously test all of these challenges. Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs (Rajpurkar et al., 2016) or using cloze-style sentences instead of questions (Hermann et al., 2015; Onishi et al., 2016) (see Table 1 for more examples). In general, system performance has improved rapidly as each resource is released. The best models of1601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1601–1611 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1147 Dataset Large scale Freeform Answer Well formed Independent of Evidence Varied Evidence TriviaQA 3 3 3 3 3 SQuAD (Rajpurkar et al., 2016) MS Marco (Nguyen et al., 2016) NewsQA(Trischler et al., 2016) WikiQA (Yang et al., 2016"
P17-1147,P16-1144,0,0.0333561,"te. 7 Related work Recent interest in question answering has resulted in the creation of several datasets. However, they are either limited in scale or suffer from biases stemming from their construction process. We group existing datasets according to their associated tasks, and compare them against TriviaQA. The analysis is summarized in Table 1. 7.1 Reading comprehension Reading comprehension tasks aims to test the ability of a system to understand a document using questions based upon its contents. Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document. Cloze-style datasets, while easier to construct large-scale automatically, do not contain natural language questions. Datasets with natural language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2016). MCTest is limited in scale with only 2640 multiple choice questions. SQuAD contains 100K crowdsourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to create questions solely from news articl"
P17-1147,D16-1264,0,0.80811,"ckground knowledge) and (3) individual facts can be difficult to 1 Data and code available at http://nlp.cs. washington.edu/triviaqa/ recover from text (e.g. due to lexical and syntactic variation). Figure 1 shows examples of all these phenomena. This paper presents TriviaQA, a new reading comprehension dataset designed to simultaneously test all of these challenges. Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs (Rajpurkar et al., 2016) or using cloze-style sentences instead of questions (Hermann et al., 2015; Onishi et al., 2016) (see Table 1 for more examples). In general, system performance has improved rapidly as each resource is released. The best models of1601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1601–1611 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1147 Dataset Large scale Freeform Answer Well formed Independent of Evidence Varied Evidence TriviaQA 3 3 3 3 3 SQuAD (Rajpurkar et al"
P17-1147,D13-1020,0,0.556168,"against TriviaQA. The analysis is summarized in Table 1. 7.1 Reading comprehension Reading comprehension tasks aims to test the ability of a system to understand a document using questions based upon its contents. Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document. Cloze-style datasets, while easier to construct large-scale automatically, do not contain natural language questions. Datasets with natural language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2016). MCTest is limited in scale with only 2640 multiple choice questions. SQuAD contains 100K crowdsourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to create questions solely from news article summaries in order to control potential bias. The crucial difference between SQuAD/NewsQA and TriviaQA is that TriviaQA questions have not been crowdsourced from preselected passages. Additionally, our evidence set consists of web documents, while SQuAD and NewsQA are limited to Wikipedia and ne"
P17-1147,D15-1237,0,0.252325,"Missing"
P17-1147,N16-1174,0,0.0195646,"Missing"
P17-1147,P15-2115,0,0.0227849,"Missing"
P17-1147,P11-1055,1,\N,Missing
P18-1009,D17-1284,0,0.106987,"Missing"
P18-1009,N06-2015,0,0.0905723,"Missing"
P18-1009,D17-1018,1,0.935109,"to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask five different annotators to validate the generate"
P18-1009,P14-5010,0,0.0118187,"resolution and question answering (e.g. “Which philanthropist is trying to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most com"
P18-1009,E17-1075,0,0.237414,"Missing"
P18-1009,N07-1071,0,0.0168201,"defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996; Pantel et al., 2007; Zapirain et al., 2013; de Cruys, 2014), where the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researcher"
P18-1009,D15-1103,0,0.108925,"growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996;"
P18-1009,D14-1004,0,0.0608809,"Missing"
P18-1009,P17-2052,0,0.107655,"76.8 66.1 71.8 7 Fine-grained NER has received growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related"
P18-1009,Q14-1037,0,0.0693507,"e sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” while “John” is more likely a “victim” because he was “robbed.” Such fine-grained types (victim, c"
P18-1009,D16-1144,0,0.719416,"We predict every type t for which yt &gt; 0.5, or arg max yt if there is no such type. Multitask Objective The distant supervision sources provide partial supervision for ultra-fine types; KBs often provide more general types, while head words usually provide only ultra-fine types, without their generalizations. In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”. Prior work used a customized hinge loss (Abhishek et al., 2017) or max margin loss (Ren et al., 2016a) to improve robustness to noisy or incomplete supervision. We propose a multitask objective that reflects the characteristic of our training dataset. Instead of updating all labels for each example, we divide labels into three bins (general, fine, and ultra-fine), and update labels only in bin containing at least one positive label. Specifically, the training objective is to minimize J where t is the target vector at each granularity: Model We design a model for predicting sets of types given a mention in context. The architecture resembles the recent neural AttentiveNER model (Shimaoka et a"
P18-1009,E17-1111,0,0.0716907,"Missing"
P18-1009,D11-1141,0,0.271575,"Missing"
P18-1009,D16-1015,0,0.109002,"form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” w"
P18-1009,E17-2119,0,0.117556,"Missing"
P18-1009,P15-2048,0,0.237857,"Missing"
P18-1009,C12-2133,0,0.234817,"f distant supervision: automatically extracted nominal head words from raw text (Section 3.2). Using head words as a form of distant supervision provides fine-grained information about named entities and nominal mentions. While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context. We bypass the challenge of automatically linking entities to Wikipedia by exploiting existing hyperlinks in web pages (Singh et al., 2012), following prior work (Ling and Weld, 2012; Yosef et al., 2012). Since our heuristic extraction of types from the definition sentence is somewhat noisy, we use a more conservative entity linking policy4 that yields a signal with similar overall accuracy to KB-linked data. 2 Data from: https://github.com/ shimaokasonse/NFGEC 3 We extract types by applying a dependency parser (Manning et al., 2014) to the definition sentence, and taking nouns that are dependents of a copular edge or connected to nouns linked to copulars via appositive or conjunctive edges. 4 Only link if the mention contains the Wikipedia entity’s name and the entity’s name contains the men"
P18-1009,N04-1002,0,0.150987,"Missing"
P18-1009,D12-1042,0,0.0575254,"ere the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning (Surdeanu et al., 2012; Yaghoobzadeh et al., 2017b) or custom losses (Abhishek et al., 2017; Ren et al., 2016a). Our multitask objective handles noisy supervision by pooling different distant supervision sources across different levels of granularity. Table 6: Results on the OntoNotes fine-grained entity typing test set. The first two models (AttentiveNER++ and AFET) use only KB-based supervision. LNR uses a filtered version of the KBbased training set. Our model uses all our distant supervision sources. Model Training Data Performance Acc. MaF1 MiF1 ONTO WIKI HEAD Attn. NER 3 3 3 3 46.5 53.7 63.3 72.8 58.3 68.0 Ou"
P18-1191,P13-1023,0,0.198784,"as something being used? tested for intelligence Figure 1: An annotated sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) Introduction Learning semantic parsers to predict the predicateargument structures of a sentence is a long standing, open challenge (Palmer et al., 2005; Baker et al., 1998). Such systems are typically trained from datasets that are difficult to gather,1 but recent research has explored training nonexperts to provide this style of semantic supervision (Abend and Rappoport, 2013; Basile et al., 2012; Reisinger et al., 2015; He et al., 2015). In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Question Who published something? 1 Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenhofer et al., 2016) annotation guides are 89 and 119 pages, respectively. scale dataset that can be used to train high quality parsers at modest cost. We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) (He et al.,"
P18-1191,P98-1013,0,0.568354,"In 1950 7 What can be tested? machines 8 What can something be tested for? intelligence 9 How can something be tested? using questions and answers 10 What was being used? questions and answers 11 Why was something being used? tested for intelligence Figure 1: An annotated sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) Introduction Learning semantic parsers to predict the predicateargument structures of a sentence is a long standing, open challenge (Palmer et al., 2005; Baker et al., 1998). Such systems are typically trained from datasets that are difficult to gather,1 but recent research has explored training nonexperts to provide this style of semantic supervision (Abend and Rappoport, 2013; Basile et al., 2012; Reisinger et al., 2015; He et al., 2015). In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Question Who published something? 1 Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenhofer et al., 2016) ann"
P18-1191,W13-2322,0,0.027243,"accuracy for questions and spans, as each model’s span detection threshold is varied. Questions are considered correct if 5 out of 6 annotators consider it valid. Spans are considered correct if their question was valid, and the span was among those labeled by human annotators for that question. The vertical line indicates a threshold value where the number of questions per sentence matches that of the original labeled data (2 questions / verb). 7 Related Work Resources and formalisms for semantics often require expert annotation and underlying syntax (Palmer et al., 2005; Baker et al., 1998; Banarescu et al., 2013). Some more recent semantic resources require less annotator training, or can be crowdsourced (Abend and Rappoport, 2013; Reisinger et al., 2015; Basile et al., 2012; Michael et al., 2018). In particular, the original QA-SRL (He et al., 2015) dataset is annotated by freelancers, while we developed streamlined crowdsourcing approaches for more scalable annotation. Crowdsourcing has also been used for indirectly annotating syntax (He et al., 2016; Duan et al., 2016), and to complement expert annotation of SRL (Wang et al., 2018). Our crowdsourcing approach draws heavily on that of Michael et al."
P18-1191,basile-etal-2012-developing,0,0.140262,"sted for intelligence Figure 1: An annotated sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) Introduction Learning semantic parsers to predict the predicateargument structures of a sentence is a long standing, open challenge (Palmer et al., 2005; Baker et al., 1998). Such systems are typically trained from datasets that are difficult to gather,1 but recent research has explored training nonexperts to provide this style of semantic supervision (Abend and Rappoport, 2013; Basile et al., 2012; Reisinger et al., 2015; He et al., 2015). In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Question Who published something? 1 Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenhofer et al., 2016) annotation guides are 89 and 119 pages, respectively. scale dataset that can be used to train high quality parsers at modest cost. We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) (He et al., 2015) annotation sch"
P18-1191,P07-1071,0,0.0194966,"plement expert annotation of SRL (Wang et al., 2018). Our crowdsourcing approach draws heavily on that of Michael et al. (2018), with automatic two-stage validation for the collected question-answer pairs. More recently, models have been developed for these newer semantic resources, such as UCCA (Teichert et al., 2017) and Semantic Proto-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explored neural models for SRL tasks (Collobert and Weston, 2007; FitzGerald et al., 2015; Swayamdipta et al., 2017; Yang and Mitchell, 2017), many of which employ a BIO encoding (Zhou and Xu, 2015; He et al., 2017). Recently, span-based models have proven to be useful for question answering (Lee et al., 2016) and coreference resolution (Lee et al., 2017), and PropBank SRL (He et al., 2018). 2058 A much larger super eruption in Colorado produced over 5,000 cubic kilometers of material. What produced something? Produced appeared In the video, the perpetrators never appeared to look at the camera. A much larger super eruption Where did something produce some"
P18-1191,W16-1718,0,0.0252059,"s and formalisms for semantics often require expert annotation and underlying syntax (Palmer et al., 2005; Baker et al., 1998; Banarescu et al., 2013). Some more recent semantic resources require less annotator training, or can be crowdsourced (Abend and Rappoport, 2013; Reisinger et al., 2015; Basile et al., 2012; Michael et al., 2018). In particular, the original QA-SRL (He et al., 2015) dataset is annotated by freelancers, while we developed streamlined crowdsourcing approaches for more scalable annotation. Crowdsourcing has also been used for indirectly annotating syntax (He et al., 2016; Duan et al., 2016), and to complement expert annotation of SRL (Wang et al., 2018). Our crowdsourcing approach draws heavily on that of Michael et al. (2018), with automatic two-stage validation for the collected question-answer pairs. More recently, models have been developed for these newer semantic resources, such as UCCA (Teichert et al., 2017) and Semantic Proto-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explored neural models for"
P18-1191,D15-1112,1,0.913513,"Missing"
P18-1191,P18-2058,1,0.824588,"-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explored neural models for SRL tasks (Collobert and Weston, 2007; FitzGerald et al., 2015; Swayamdipta et al., 2017; Yang and Mitchell, 2017), many of which employ a BIO encoding (Zhou and Xu, 2015; He et al., 2017). Recently, span-based models have proven to be useful for question answering (Lee et al., 2016) and coreference resolution (Lee et al., 2017), and PropBank SRL (He et al., 2018). 2058 A much larger super eruption in Colorado produced over 5,000 cubic kilometers of material. What produced something? Produced appeared In the video, the perpetrators never appeared to look at the camera. A much larger super eruption Where did something produce something? in Colorado What did something produce? over 5,000 cubic kilometers of material Where didn’t someone appear to do something? In the video Who didn’t appear to do something? the perpetrators When did someone appear? never look at the camera What didn’t someone appear to do? look to look at the camera Where didn't someone"
P18-1191,P17-1044,1,0.938265,"answers. Our proposed parsers construct these tuples in a three-step pipeline: 1. Verbal predicates are identified using the same POS-tags and heuristics as in data collection (see Section 2). 2. Unlabeled span detection selects a set Sv of spans as arguments for a given verb v. 3. Question generation predicts a question for each span in Sv . Spans are then grouped by question, giving each question a set of answers. We describe two models for unlabeled span detection in section 3.1, followed by question generation in section 3.2. All models are built on an LSTM encoding of the sentence. Like He et al. (2017), we start with an input X v = {x0 . . . xn }, where the representation xi at each time step is a concatenation of the token wi ’s embedding and an embedded binary feature (i = v) which indicates whether wi is the predicate under consideration. We then compute the output representation H v = B I LSTM(X v ) using a stacked alternating LSTM (Zhou and Xu, 2015) with highway connections (Srivastava et al., 2015) and recurrent dropout (Gal and Ghahramani, 2016). Since the span detection and question generation models both use an LSTM encoding, this component could in principle be shared between the"
P18-1191,D15-1076,1,0.93398,"sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) Introduction Learning semantic parsers to predict the predicateargument structures of a sentence is a long standing, open challenge (Palmer et al., 2005; Baker et al., 1998). Such systems are typically trained from datasets that are difficult to gather,1 but recent research has explored training nonexperts to provide this style of semantic supervision (Abend and Rappoport, 2013; Basile et al., 2012; Reisinger et al., 2015; He et al., 2015). In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Question Who published something? 1 Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenhofer et al., 2016) annotation guides are 89 and 119 pages, respectively. scale dataset that can be used to train high quality parsers at modest cost. We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) (He et al., 2015) annotation scheme. QA-SRL is appealing because it is int"
P18-1191,D16-1258,1,0.816085,"ted Work Resources and formalisms for semantics often require expert annotation and underlying syntax (Palmer et al., 2005; Baker et al., 1998; Banarescu et al., 2013). Some more recent semantic resources require less annotator training, or can be crowdsourced (Abend and Rappoport, 2013; Reisinger et al., 2015; Basile et al., 2012; Michael et al., 2018). In particular, the original QA-SRL (He et al., 2015) dataset is annotated by freelancers, while we developed streamlined crowdsourcing approaches for more scalable annotation. Crowdsourcing has also been used for indirectly annotating syntax (He et al., 2016; Duan et al., 2016), and to complement expert annotation of SRL (Wang et al., 2018). Our crowdsourcing approach draws heavily on that of Michael et al. (2018), with automatic two-stage validation for the collected question-answer pairs. More recently, models have been developed for these newer semantic resources, such as UCCA (Teichert et al., 2017) and Semantic Proto-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explor"
P18-1191,D16-1252,0,0.0671226,"the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenhofer et al., 2016) annotation guides are 89 and 119 pages, respectively. scale dataset that can be used to train high quality parsers at modest cost. We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) (He et al., 2015) annotation scheme. QA-SRL is appealing because it is intuitive to non-experts, has been shown to closely match the structure of traditional predicate-argument structure annotation schemes (He et al., 2015), and has been used for end tasks such as Open IE (Stanovsky and Dagan, 2016). In QA-SRL, each predicate-argument relationship is labeled with a question-answer pair (see Figure 1). He et al. (2015) showed that high precision QA-SRL annotations can be gathered with limited training but that high recall is challenging to achieve; it is relatively easy to gather answerable questions, but difficult to ensure that every possible question is labeled for every verb. For this reason, they hired and trained hourly annotators and only labeled a relatively small dataset (3000 sentences). Our first contribution is a new, scalable approach for crowdsourcing QA-SRL. We introduce a"
P18-1191,D17-3004,0,0.0492571,"Missing"
P18-1191,E17-2015,0,0.0420683,"Missing"
P18-1191,D17-1128,0,0.0499858,"h draws heavily on that of Michael et al. (2018), with automatic two-stage validation for the collected question-answer pairs. More recently, models have been developed for these newer semantic resources, such as UCCA (Teichert et al., 2017) and Semantic Proto-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explored neural models for SRL tasks (Collobert and Weston, 2007; FitzGerald et al., 2015; Swayamdipta et al., 2017; Yang and Mitchell, 2017), many of which employ a BIO encoding (Zhou and Xu, 2015; He et al., 2017). Recently, span-based models have proven to be useful for question answering (Lee et al., 2016) and coreference resolution (Lee et al., 2017), and PropBank SRL (He et al., 2018). 2058 A much larger super eruption in Colorado produced over 5,000 cubic kilometers of material. What produced something? Produced appeared In the video, the perpetrators never appeared to look at the camera. A much larger super eruption Where did something produce something? in Colorado What did something produce? over 5,000 cubic kilometers of"
P18-1191,P15-1109,0,0.249745,"en grouped by question, giving each question a set of answers. We describe two models for unlabeled span detection in section 3.1, followed by question generation in section 3.2. All models are built on an LSTM encoding of the sentence. Like He et al. (2017), we start with an input X v = {x0 . . . xn }, where the representation xi at each time step is a concatenation of the token wi ’s embedding and an embedded binary feature (i = v) which indicates whether wi is the predicate under consideration. We then compute the output representation H v = B I LSTM(X v ) using a stacked alternating LSTM (Zhou and Xu, 2015) with highway connections (Srivastava et al., 2015) and recurrent dropout (Gal and Ghahramani, 2016). Since the span detection and question generation models both use an LSTM encoding, this component could in principle be shared between them. However, in preliminary experiments we found that sharing hurt performance, so for the remainder of this work each model is trained independently. 3.1 Span Detection Given an encoded sentence H v , the goal of span detection is to select the spans Sv that correspond to arguments of the given predicate. We explore two models: a sequence-tagging model with"
P18-1191,D17-1018,1,0.834053,"hert et al., 2017) and Semantic Proto-Roles (White et al., 2017). Our work is the first highquality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing. Several recent works have explored neural models for SRL tasks (Collobert and Weston, 2007; FitzGerald et al., 2015; Swayamdipta et al., 2017; Yang and Mitchell, 2017), many of which employ a BIO encoding (Zhou and Xu, 2015; He et al., 2017). Recently, span-based models have proven to be useful for question answering (Lee et al., 2016) and coreference resolution (Lee et al., 2017), and PropBank SRL (He et al., 2018). 2058 A much larger super eruption in Colorado produced over 5,000 cubic kilometers of material. What produced something? Produced appeared In the video, the perpetrators never appeared to look at the camera. A much larger super eruption Where did something produce something? in Colorado What did something produce? over 5,000 cubic kilometers of material Where didn’t someone appear to do something? In the video Who didn’t appear to do something? the perpetrators When did someone appear? never look at the camera What didn’t someone appear to do? look to look"
P18-1191,P14-5010,0,0.00608742,"Missing"
P18-1191,N18-2089,1,0.821574,"t if their question was valid, and the span was among those labeled by human annotators for that question. The vertical line indicates a threshold value where the number of questions per sentence matches that of the original labeled data (2 questions / verb). 7 Related Work Resources and formalisms for semantics often require expert annotation and underlying syntax (Palmer et al., 2005; Baker et al., 1998; Banarescu et al., 2013). Some more recent semantic resources require less annotator training, or can be crowdsourced (Abend and Rappoport, 2013; Reisinger et al., 2015; Basile et al., 2012; Michael et al., 2018). In particular, the original QA-SRL (He et al., 2015) dataset is annotated by freelancers, while we developed streamlined crowdsourcing approaches for more scalable annotation. Crowdsourcing has also been used for indirectly annotating syntax (He et al., 2016; Duan et al., 2016), and to complement expert annotation of SRL (Wang et al., 2018). Our crowdsourcing approach draws heavily on that of Michael et al. (2018), with automatic two-stage validation for the collected question-answer pairs. More recently, models have been developed for these newer semantic resources, such as UCCA (Teichert e"
P18-1191,J05-1004,0,0.941252,"e propose something? In 1950 7 What can be tested? machines 8 What can something be tested for? intelligence 9 How can something be tested? using questions and answers 10 What was being used? questions and answers 11 Why was something being used? tested for intelligence Figure 1: An annotated sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) Introduction Learning semantic parsers to predict the predicateargument structures of a sentence is a long standing, open challenge (Palmer et al., 2005; Baker et al., 1998). Such systems are typically trained from datasets that are difficult to gather,1 but recent research has explored training nonexperts to provide this style of semantic supervision (Abend and Rappoport, 2013; Basile et al., 2012; Reisinger et al., 2015; He et al., 2015). In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Question Who published something? 1 Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. 1 The PropBank (Bonial et al., 2010) and FrameNet (Ruppenho"
P18-1191,Q15-1034,0,0.122212,"Missing"
P18-1191,C98-1013,0,\N,Missing
P18-2003,P17-1080,0,0.308427,"erties from the vector alone. Specifically, we predict the word’s part of speech (POS), as well as the first (parent), second (grand-parent), and third level (great-grandparent) constituent labels of the given word. Figure 1 shows how these labels correspond to an example constituency tree. 2.2 Analyzed Models We consider four different forms of supervision. Table 1 summarizes the differences in data, architecture, and hyperparameters.1 Our methodology follows Shi et al. (2016), who run syntactic feature prediction experiments over a number of different shallow machine translation models, and Belinkov et al. 2017a; 2017b, who use a similar process to study the morphological, part-of-speech, and semantic features learned by deeper machine translation encoders. We extend upon prior work by considering training signals for models other than machine translation, and by applying more stratified word-level syntactic tasks. 2.1 (1) Dependency Parsing We train a four-layer version of the Stanford dependency parser (Dozat and Manning, 2017) on the Universal Dependencies English Web Treebank (Silveira et al., 2014). We ran the parser with 4 bidirectional LSTM layers (the default is 3), yielding a UAS of 91.5 an"
P18-2003,I17-1001,0,0.16282,"Missing"
P18-2003,D14-1162,0,0.0801549,"Missing"
P18-2003,P17-1171,0,0.0180853,"els encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 14–19 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associa"
P18-2003,N18-1202,1,0.721736,"for example, in SRL we see that the parent constituent task peaks one layer after POS, and the grand-parent and great-grandparent tasks peak on the layer after that. One possible explanation is that each layer leverages the shallower syntactic information learned in the previous layer in order to construct a more abstract syntactic representation. In SRL and language modeling, it seems as though the syntactic information is then replaced by taskspecific information (semantic roles, word probabilities), perhaps making it redundant. This observation may also explain a modeling decision in ELMo (Peters et al., 2018), where injecting the contextualized word representations from a pre-trained language model was shown to boost performance on a wide variety of NLP tasks. ELMo represents each word using a task-specific weighted sum of the language model’s hidden layers, i.e. rather than use only the top layer, it selects which of the language model’s internal layers contain the most relevant information for the task at hand. Our results confirm that, in general, different types of information manifest at different layers, suggesting that post-hoc layer selection can be beneficial. Figure 3: Comparison between"
P18-2003,W13-3516,0,0.0426484,"Missing"
P18-2003,N09-1025,0,0.0490487,"emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short P"
P18-2003,W05-0639,0,0.044504,"soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computationa"
P18-2003,P81-1022,0,0.69212,"Missing"
P18-2003,D16-1159,0,0.408155,"nday” for the POS (green), parent constituent (blue), grandparent constituent (orange), and great-grandparent constituent (red) tasks. erties from the vector alone. Specifically, we predict the word’s part of speech (POS), as well as the first (parent), second (grand-parent), and third level (great-grandparent) constituent labels of the given word. Figure 1 shows how these labels correspond to an example constituency tree. 2.2 Analyzed Models We consider four different forms of supervision. Table 1 summarizes the differences in data, architecture, and hyperparameters.1 Our methodology follows Shi et al. (2016), who run syntactic feature prediction experiments over a number of different shallow machine translation models, and Belinkov et al. 2017a; 2017b, who use a similar process to study the morphological, part-of-speech, and semantic features learned by deeper machine translation encoders. We extend upon prior work by considering training signals for models other than machine translation, and by applying more stratified word-level syntactic tasks. 2.1 (1) Dependency Parsing We train a four-layer version of the Stanford dependency parser (Dozat and Manning, 2017) on the Universal Dependencies Engl"
P18-2003,N18-1108,0,0.0383606,"(red) constituent prediction tasks. itly. Specifically, we observe in Figure 3 that the language model and dependency parser perform nearly identically on the three constituent prediction tasks in the second layer of their respective networks. In deeper layers the parser continues to improve, while the language model peaks at layer 2 and drops off afterwards. These results may be surprising given the findings of Linzen et al. (2016), which found that RNNs trained on language modeling perform below baseline levels on the task of subject-verb agreement. However, the more recent investigation by Gulordava et al. (2018) are in line with our results. They find that language models trained on a number of different languages assign higher probabilities to valid long-distance dependencies than to incorrect ones. Therefore, LMs seem able to induce syntactic information despite being provided with no linguistic annotation. 4 Dependency Arc Prediction We run an additional experiment that seeks to clarify if the representations learned by deep NLP models capture information about syntactic structure. Using the internal representations from a deep RNN, we train a classifier to predict whether two words share an depen"
P18-2003,silveira-etal-2014-gold,0,0.193271,"Missing"
P18-2003,P17-1044,1,0.851187,"h one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at e"
P18-2003,P17-4012,0,0.199338,"y parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what s"
P18-2003,D17-1018,1,0.842022,"ectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l"
P18-2058,W05-0620,0,0.678935,"Missing"
P18-2058,P18-1191,1,0.824012,"akanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass. Our model builds on a recent coref"
P18-2058,D15-1112,0,0.148413,"Missing"
P18-2058,D17-1128,0,0.0386925,"P PRP PRP I-AM-PRP I-AM-PRP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, o"
P18-2058,P17-1044,1,0.911591,"RP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves"
P18-2058,D17-1018,1,0.89591,"presentations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipel"
P18-2058,P15-1109,0,0.155871,"B-AM- I-AM- I-AMPRP PRP PRP I-AM-PRP I-AM-PRP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predi"
P18-2058,N18-2108,1,0.90354,"Missing"
P18-2058,K17-1041,0,0.0504248,"ting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass. Our model builds on a recent coreference resolution model (Lee et al., 2017), by making central use of learned, contextualized span representations. We use these representations to predict SRL graphs directly over text spans."
P18-2058,P16-1105,0,0.0253707,"ly standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pa"
P18-2058,N18-1202,1,0.771516,"esentations as inputs, and computes a softmax over the label space L. 4 Results with gold predicates To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only. As shown in Table 2, our model significantly out-performs He et al. (2017), but falls short of Tan et al. (2018), a very recent attention-based (Vaswani et al., 2017) BIO-tagging model that was developed concurrently with our work. By adding the contextualized ELMo representations, we are able to out-perform all previous systems, including Peters et al. (2018), which applies ELMo to the SRL model introduced in He et al. (2017). Experiments We experiment on the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2012 (OntoNotes 5.0, (Pradhan et al., 2013)) benchmarks, using two SRL setups: end-to-end and gold predicates. In the end-to-end setup, a system takes a tokenized sentence as input, and predicts all the predicates and their arguments. Systems are evaluated on the micro-averaged F1 for correctly predicting (predicate, argument span, label) tuples. For comparison with previous systems, we also report results with gold predicates, in which the c"
P18-2058,W04-2421,0,0.123961,"Missing"
P18-2058,J08-2005,0,0.358856,"imply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al.,"
P18-2058,P17-1076,0,0.0230476,"d roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicat"
P18-2058,D18-1548,0,0.273641,"Missing"
P18-2058,P06-4018,0,\N,Missing
P18-2058,D14-1162,0,\N,Missing
P18-2116,P17-1044,1,0.742747,"ile theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model. By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing. We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al., 2017). We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation. Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance. We also show that, in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum. Together, these results suggest that the gates’"
P18-2116,P17-1171,0,0.0190863,"pact form: ht = OUTPUT t X wjt ◦ CONTENT(xj )  Language Modeling We evaluate the models on the Penn Treebank (PTB) (Marcus et al., 1993) language modeling benchmark. We use the implementation of Zaremba et al. (2014) from TensorFlow’s tutorial while replacing any invocation of LSTMs with simpler models. We test two of their configurations: medium and large (Table 1). Question Answering For question answering, we use two different QA systems on the Stanford question answering dataset (SQuAD) (Rajpurkar et al., 2016): the Bidirectional Attention Flow model (BiDAF) (Seo et al., 2016) and DrQA (Chen et al., 2017). BiDAF contains 3 LSTMs, which are referred to as the phrase layer, the modeling layer, and the span end encoder. Our experiments replace each of these LSTMs with their simplified counterparts. We directly use the implementation of BiDAF from AllenNLP (Gardner et al., 2017), and all experiments reuse the existing hyperparameters that were tuned for LSTMs. Likewise, we use an open-source implementation of DrQA1 and replace only the LSTMs, while leaving everything else intact. Table 2 shows the results. (9) j=0 where the content layer CONTENT(·) and the output layer OUTPUT(·) are both context-i"
P18-2116,J93-2004,0,\N,Missing
P18-2116,silveira-etal-2014-gold,0,\N,Missing
P18-2116,D16-1053,0,\N,Missing
P18-2116,D16-1264,0,\N,Missing
P18-2116,P17-4012,0,\N,Missing
P19-1156,P17-1080,0,\N,Missing
P19-1156,P17-1137,0,\N,Missing
P19-1156,P17-1184,0,\N,Missing
P19-1156,I17-1015,0,\N,Missing
P19-1156,P18-1036,0,\N,Missing
P19-1156,Q18-1032,0,\N,Missing
P19-1156,D18-1278,0,\N,Missing
P19-1156,D18-1029,0,\N,Missing
P19-1156,D18-1313,0,\N,Missing
P19-1164,P06-1084,0,0.0264317,"erms of alphabet, word order, or grammar), while still allowing for highly accurate automatic morphological analysis. These languages belong to four different families: (1) Romance languages: Spanish, French, and Italian, all of which have gendered noun-determiner agreement and spaCy morphological analysis support (Honnibal and Montani, 2017). (2) Slavic languages (Cyrillic alphabet): Russian and Ukrainian, for which we use the morphological analyzer developed by Korobov (2015). (3) Semitic languages: Hebrew and Arabic, each with a unique alphabet. For Hebrew, we use the analyzer developed by Adler and Elhadad (2006), while gender inflection in Arabic can be easily identified via the ta marbuta character, which uniquely indicates feminine inflection. (4) Germanic languages: German, for which we Results Our main findings are presented in Tables 2 and 3. For each tested MT system and target language we compute three metrics with respect to their ability to convey the correct gender in the target language. Ultimately, our analyses indicate that all tested MT systems are indeed gender biased. First, the overall system Accuracy is calculated by the percentage of instances in which the translation preserved the"
P19-1164,D17-1042,0,0.0262651,"Missing"
P19-1164,N18-1118,0,0.0150505,"h or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and"
P19-1164,D17-1263,0,0.0335542,"ders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might intro"
P19-1164,W18-6307,0,0.0594926,"Missing"
P19-1164,N13-1073,1,0.665237,"3,888 instances, and is equally balanced between male and female genders, as well as between stereotypical and nonstereotypical gender-role assignments (e.g., a female doctor versus a female nurse). Additional dataset statistics are presented in Table 1. We use WinoMT to estimate the gender-bias of an MT model, M , in target-language L by performing following steps (exemplified in Figure 1): (1) Translate all of the sentences in WinoMT into L using M , thus forming a bilingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, whi"
P19-1164,D18-1045,0,0.016978,"ingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pretrained models provided by the Fairseq toolkit.5 1680 1 https://translate.google.com https://www.bing.com/translator 3 https://aws.amazon.com/translate 4 http://www.systransoft.com 5 https://github.com/pytorch/fairseq 2 Google Translate Amazon Translate∗ Microsoft Translator SYSTRAN Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S ES FR IT 53.1 63.6 39.6 23.4 6.4 32.9 21.3 26.7 21.5 47.3 44.7 39.8 36.8 36.4 39.8 23.2 29.7 17.0 59.4"
P19-1164,D18-1002,0,0.0218372,"e languages, all annotated with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like t"
P19-1164,W19-3821,0,0.233151,"Missing"
P19-1164,W19-3621,0,0.0489826,"with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like to thank Mark Yatskar, Iz Be"
P19-1164,W18-6301,0,0.0342982,"Missing"
P19-1164,P02-1040,0,0.113146,"emale inflections for professions which were stereotypically associated with one of the genders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this"
P19-1164,W17-1609,0,0.15079,"Missing"
P19-1164,N18-2002,0,0.232322,"Missing"
P19-1164,Q18-1042,0,0.0532662,"ctions. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and evaluation. Ideally, WinoMT could be augmented with natural “in t"
P19-1164,D17-1323,0,0.163501,"Missing"
P19-1164,N18-2003,0,0.250997,"Missing"
P19-1223,P09-1113,0,0.0203357,"). The previous best system for ShARC (Saeidi et al., 2018) similarly breaks the CMR task into subtasks and combines handdesigned sub-models for decision classification, entailment, and follow-up generation. In contrast, the core reasoning (e.g. non-editor) components of E3 are jointly trained, and does not require complex hand-designed features. Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language document"
P19-1223,P17-1163,0,0.0292851,"Missing"
P19-1223,P15-1034,0,0.0185249,"for decision classification, entailment, and follow-up generation. In contrast, the core reasoning (e.g. non-editor) components of E3 are jointly trained, and does not require complex hand-designed features. Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language documents. Finally, these rules provide rationalization for the model’s decision making, in the sense that the user can visualize what rules the model"
P19-1223,D16-1244,0,0.0454526,"Missing"
P19-1223,D14-1059,0,0.0227124,"es handdesigned sub-models for decision classification, entailment, and follow-up generation. In contrast, the core reasoning (e.g. non-editor) components of E3 are jointly trained, and does not require complex hand-designed features. Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language documents. Finally, these rules provide rationalization for the model’s decision making, in the sense that the user can visualiz"
P19-1223,D14-1162,0,0.082452,"w line. 2 we discard the covered shorter rule. Section A.2 details how clause matching is used to obtain noisy supervision for rule extraction. We train the editor separately, as jointly training with a shared encoder worsens performance. The editor is trained by optimizing Ledit while the rest of the model is trained by optimizing Ldec + λLre . We use a rule extraction threshold of τ = 0.5 and a rule extraction loss weight of λ = 400. We perform early stopping using the product of the macro-averaged accuracy and the BLEU4 score. For the editor, we use fixed, pretrained embeddings from GloVe (Pennington et al., 2014), and use dropout after input attention with a rate of 0.4. Before editing retrieved rules, we remove prefix and suffix adpositions, auxiliary verbs, conjunctions, determiners, or punctuation. We find that doing so allows the editor to convert some extracted rules (e.g. or sustain damage) into sensible questions (e.g. did you sustain damage?). 4.2 Results Our performance on the development and the blind, held-out test set of ShARC is shown in Table 1. Compared to previous results, E3 achieves a new state-of-the-art, obtaining best performance on micro and macro-averaged decision classification"
P19-1223,E17-2025,0,0.0127907,"tmE3Y0QQv0rXjwo4tUf4s1/46bNQVsHFoZ5b3izEyScKe0431ZlbX1jc6u6XdvZ3ds/sA+PeipOJaFdEvNY9gOsKGeCdjXTnPYTSXEUcPoQTK+L+cMjlYrF4l5nCfUiPBYsZARrI/l2/c7P2TkaRlhPZJQb72zm2w2n6cyBVolbkgaU6Pj213AUkzSiQhOOlRq4TqK9HEvNCKez2jBVNMFkisd0YKjAEVVePg8/Q6dGGaEwluYJjebqb0eOI6WyKDCbRUi1PCvE/2aDVIdXXs5EkmoqyOJQmHKkY1Q0gUZMUqJ5ZggmkpmsiEywxESbvmqmBHf5y6ukd9F0naZ722q0W2UdVTiGEzgDFy6hDTfQgS4QyOAZXuHNerJerHfrY7FasUpPHf7A+vwB3eeU3Q==</latexit&gt; Post-span attentive decoder Post-span edit Ri,post Figure 3: The editor of E3 . To generate the tth token wt , we use weight tying between the output layer and the embedding matrix (Press and Wolf, 2017). (17) During inference, the model first determines the decision d = argmaxk zk . If the decision d is inquire, the model asks a follow-up question about the ith rule such that i = argmaxj rj . Otherwise, the model concludes the dialogue with d. ζk = softmax(ζ)k ∈ R X at = ζk Uedit,k ∈ RdU xD attentive decoder <latexit sha1_base64=&quot;0SaTPdbqamEmTDTmYKx4bmgeaa0=&quot;&gt;AAAB/XicbVDLSgMxFM3UV62v8bFzEyyCCykzUtBlwY3LKvYB7TBk0rQNzWNIMkIdBn/FjQtF3Pof7vwbM+0stPVA4HDOvdyTE8WMauN5305pZXVtfaO8Wdna3tndc/cP2lomCpMWlkyqboQ0YVSQlqGGkW6sCOIRI51ocp37nQeiNJXi3kxjEnA0EnRIMTJWCt2juzCl57DPkRkrnsZSmywL3apX82aAy8QvSBUUaIbu"
P19-1223,D18-1241,1,0.898565,"Missing"
P19-1223,N19-1423,0,0.495629,"ion 31 Extraction module The extraction module extracts spans from the document that correspond to latent rules Let xD xQ xS xH denote words in the rule text question scenario and the inquiry and user response during the ith previous turn of the dialogue after N turns have passed We concatenate these inputs into a single sequence x = [xQ ; xD ; xS ; xH 1 ; · · · xH N ] joined by sentinel tokens that mark the boundaries of each input To encode the input for the extraction module we use BERT a transformer-based model (Vaswani et al 2017) that achieves consistent gains on a variety of NLP tasks (Devlin et al 2019) We encode x using the BERT encoder which first converts words into word piece tokens (Wu et al 2016) then embeds these tokens along with their positional embeddings and segmentation embeddings These embeddings are subsequently encoded via a transformer network which allows for inter-token attention at each layer Let nx be the number of tokens in the concatenated input x and dU be the output dimension of the BERT encoder For brevity we denote the output of the BERT encoder as U = BERT(x) ∈ Rnx ×dU and refer readers to Devlin et al (2019) for detailed architecture In order to extract the implic"
P19-1223,W14-4337,0,0.0171743,"s Wikipedia). A consequence of this is that CMR requires the interpretation of complex decision rules in order to answer high-level questions, whereas dialogue QA typically contains questions whose answers are directly extractable from the text. In addition, CMR requires the formulation of free-form follow-up questions in order to identify whether the user satisfies decision rules, whereas dialogue QA does not. There has also been significant work on task-oriented dialogue, where the system must inquire about missing information in order to help the user achieve a goal (Williams et al., 2013; Henderson et al., 2014; Mrkˇsi´c et al., 2017; Young et al., 2013). However, these tasks are typically constrained to a fixed ontology (e.g. restaurant reservation), instead of a latent ontology specified via natural language documents. Dialogue systems. One traditional approach for designing dialogue systems divides the task into language understanding/state-tracking (Mrkˇsi´c et al., 2017; Zhong et al., 2018), reasoning/policy learning (Su et al., 2016), and response generation (Wen et al., 2015). The models for each of these subtasks are then combined to form a full dialogue system (Young et al., 2013; Wen et al"
P19-1223,D16-1011,0,0.0292855,"Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language documents. Finally, these rules provide rationalization for the model’s decision making, in the sense that the user can visualize what rules the model extracted and which rules are entailed by previous turns. 3 Entailment-driven Extract and Edit network In conversational machine reading, a system reads a document that contains a set of implicit decision"
P19-1223,D18-1362,0,0.0148241,"d, and does not require complex hand-designed features. Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language documents. Finally, these rules provide rationalization for the model’s decision making, in the sense that the user can visualize what rules the model extracted and which rules are entailed by previous turns. 3 Entailment-driven Extract and Edit network In conversational machine reading, a system re"
P19-1223,P14-5010,0,0.00245276,"acy and the BLEU4 score. show that these sub-models outperform neural models such as the entailment model by Parikh et al. (2016), and that the combined pipeline outperforms the attentive sequence-to-sequence model. In addition, we propose an extractive QA baseline based on BERT (BERTQA). Similar models achieved state-of-the-art on a variety of QA tasks (Rajpurkar et al., 2016; Reddy et al., 2019). We refer readers to Section A.1 of the appendices for implementation details BERTQA. 4.1 Experimental setup We tokenize using revtok1 and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased).2 We optimize using ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4. To supervise rule extraction, we reconstruct full dialogue trees from the ShARC training set and extract all follow-up questions as well as bullet points from each rule text and its corresponding dialogue tree. We then match these extracted clauses to spans in the rule text, and cons"
P19-1223,Q19-1016,0,0.0580565,"ry, the generated follow-up question is further evaluated using the BLEU score. In addition to official evaluation metrics, we also show a combined metric (“Comb.”), which is the product between the macro-averaged accuracy and the BLEU4 score. show that these sub-models outperform neural models such as the entailment model by Parikh et al. (2016), and that the combined pipeline outperforms the attentive sequence-to-sequence model. In addition, we propose an extractive QA baseline based on BERT (BERTQA). Similar models achieved state-of-the-art on a variety of QA tasks (Rajpurkar et al., 2016; Reddy et al., 2019). We refer readers to Section A.1 of the appendices for implementation details BERTQA. 4.1 Experimental setup We tokenize using revtok1 and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased).2 We optimize using ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4. To supervise rule extraction, we reconstruct full dialogue tree"
P19-1223,N13-1008,0,0.0197129,"system for ShARC (Saeidi et al., 2018) similarly breaks the CMR task into subtasks and combines handdesigned sub-models for decision classification, entailment, and follow-up generation. In contrast, the core reasoning (e.g. non-editor) components of E3 are jointly trained, and does not require complex hand-designed features. Extracting latent rules from text. There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992). Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013). Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015). These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016). Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis. In addition, the rules extracted by our model are used for inference over natural language documents. Finally, these rule"
P19-1223,D18-1233,0,0.145214,"Missing"
P19-1223,P16-1230,0,0.0288124,"Missing"
P19-1223,D15-1199,0,0.0203947,"Missing"
P19-1223,E17-1042,0,0.0448871,"Missing"
P19-1223,W13-4065,0,0.0113938,"ered (regulatory text vs Wikipedia). A consequence of this is that CMR requires the interpretation of complex decision rules in order to answer high-level questions, whereas dialogue QA typically contains questions whose answers are directly extractable from the text. In addition, CMR requires the formulation of free-form follow-up questions in order to identify whether the user satisfies decision rules, whereas dialogue QA does not. There has also been significant work on task-oriented dialogue, where the system must inquire about missing information in order to help the user achieve a goal (Williams et al., 2013; Henderson et al., 2014; Mrkˇsi´c et al., 2017; Young et al., 2013). However, these tasks are typically constrained to a fixed ontology (e.g. restaurant reservation), instead of a latent ontology specified via natural language documents. Dialogue systems. One traditional approach for designing dialogue systems divides the task into language understanding/state-tracking (Mrkˇsi´c et al., 2017; Zhong et al., 2018), reasoning/policy learning (Su et al., 2016), and response generation (Wen et al., 2015). The models for each of these subtasks are then combined to form a full dialogue system (Young"
P19-1223,N19-1241,0,0.0153276,"after removing the editor, entailment, and extraction modules, presents a strong baseline that exceeds previous results on all metrics except for BLEU1. This variant inquires about spans extracted from the text, which, while more relevant as indicated by the higher BLEU4 score, does not have the natural qualities of a question, hence it has a lower BLEU1. Nonetheless, the large gains of BERTQA over the attentive Seq2Seq model shows that retrieval is a more promising technique for asking follow-up questions than word-by-word generation. Similar findings were reported for question answering by Yatskar (2019). Extraction of document structure facilitates generalization. Adding explicit extraction of rules in the document (“-edit, entail”) forces the model to interpret all rules in the document versus only focusing on extracting the next inquiry. This results in better performance in both decision classification and inquiry relevance compared to the variant that is not forced to interpret all rules. Modeling entailment improves rule retrieval. The “-edit” model explicitly models whether an extracted rule is entailed by the user scenario and previous turns. Modeling entailment allows the model to be"
P19-1416,P17-1147,1,0.809277,"r most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), or us"
P19-1416,Q18-1023,0,0.0788822,"Missing"
P19-1416,D16-1264,0,0.0696953,"e distractors can recover most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and"
P19-1416,N18-1059,0,0.20532,"difficulty. However, since only one of the ten paragraphs is about an animal, one can immediately locate the answer in Paragraph 1 using one hop. The full example is provided in Appendix A. established to protect?”, and then answer “What is the former name of that animal?”. However, when considering the evidence paragraphs, the question is solvable in a single hop by finding the only paragraph that describes an animal. Introduction Multi-hop reading comprehension (RC) requires reading and aggregating information over multiple pieces of textual evidence (Welbl et al., 2017; Yang et al., 2018; Talmor and Berant, 2018). In this work, we argue that it can be difficult to construct large multi-hop RC datasets. This is because multi-hop reasoning is a characteristic of both the question and the provided evidence; even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. For example, the question in Figure 1 is compositional: a plausible solution is to find “What animal’s habitat was the R´eserve Naturelle Lomako Yokokala ∗ Equal Contribution. Our analysis is centered on H OTPOT QA (Yang et al., 2018), a dataset"
P19-1416,D13-1160,0,\N,Missing
P19-1416,D18-1453,0,\N,Missing
P19-1416,D18-1259,0,\N,Missing
P19-1416,P18-1078,1,\N,Missing
P19-1416,N19-1423,0,\N,Missing
P19-1416,Q18-1021,0,\N,Missing
P19-1593,D16-1053,0,0.0312085,"ison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a core"
P19-1593,P15-1136,0,0.179541,"Missing"
P19-1593,N19-1423,0,0.0266509,"ellelism+URL). Language model pretraining yields an absolute gain of 3.2 in F1 . This demonstrates the ability of RefReader to leverage unlabeled text, which is a distinctive feature in comparison with prior work. When training is carried out in the unsupervised setting (with the language modeling objective only), the model is still capable of learning the latent coreferential structure between pronouns and names to some extent, outperforming a supervised coreference system that gives competitive results on OntoNotes (Clark and Manning, 2015). We also test a combination of RefReader and BERT (Devlin et al., 2019), using BERT’s contextualized word embeddings as base features xt (concatenation of the top 4 layers), which yields substantial improvements in accuracy. While this model still resolves references incrementally, it cannot be said to be purely incremental, because BERT uses “future” information to build its contextualized embeddings.5 Note that the gender 5 Future work may explore the combination of RefReader bias increases slightly, possibly due to bias in the data used to train BERT. GAP examples are short, containing just a few entity mentions. To test the applicability of our method to long"
P19-1593,N18-2007,0,0.0650583,"is a learnable vector. Figure 2: Overview of the model architecture. (i) (i) value vt ∈ RDv , and a salience st ∈ [0, 1]. There are two components to the model: the memory unit, which stores and tracks the states of the entities in the text; and the recurrent unit, which controls the memory via a set of gates. An overview is presented in Figure 2. 2.1 Recurrent Unit The recurrent unit is inspired by the CoreferentialGRU, in which the current hidden state of a gated recurrent unit (GRU; Chung et al., 2014) is combined with the state at the time of the most recent mention of the current entity (Dhingra et al., 2018). However, instead of relying on the coreferential structure to construct a dynamic computational graph, we use an external memory unit to keep track of previously mentioned entities and let the model learn to decide what to store in each cell. The memory state is summarized by the P (i) (i) weighted sum over values: mt = N i=1 s vt . The current hidden state and the input are ˜t = combined into a pre-recurrent state h tanh(W ht−1 + U xt ), which is used to control the memory operations; the matrices W and U are trained parameters. To compute the next hidden state ht , we perform a recurrent u"
P19-1593,J95-2003,0,0.139821,"update (indicating coreference with Padbury), and a weaker update to m1. If the update to m0 is above the threshold, then the reader may receive credit for this coreference edge, which would otherwise be scored as a false negative. The reader ignores the names Braylon Edwards, Piers Haggard, and Cathy Vespers, leaving them out of the memory. Edwards and Vespers appear in prepositional phrases, while Haggard is a possessive determiner of the object of a prepositional phrase. Centering theory argues that these syntactic positions have low salience in comparison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to"
P19-1593,D17-1195,0,0.0335153,"t, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This paper demonstrates the viability of incremental reference resolution, using an end-to-end differentiable memory network. This enables semisupervised learning from a language modeling objective, which substantially improves performance. A key question for future work is the performance on longer texts, such as the full-length news articles encountered in"
P19-1593,I17-1048,0,0.0201279,"sive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This"
P19-1593,D17-1018,1,0.869104,"o3 Ismael told (2) u4 Captain Ahab (2) o7 he saw Moby-Dick self link coreferential not coreferential Figure 1: A referential reader with two memory cells. (i) (i) Overwrite and update are indicated by ot and ut ; in practice, these operations are continuous gates. Thickness and color intensity of edges between memory cells at neighboring steps indicate memory salience; 7 indicates an overwrite. Introduction Reference resolution is fundamental to language understanding. Current state-of-the-art systems employ the mention-pair model, in which a classifier is applied to all pairs of spans (e.g., Lee et al., 2017). This approach is expensive in both computation and labeled data, and it is also cognitively implausible: human readers interpret text in a nearly online fashion (Tanenhaus et al., 1995). We present a new method for reference resolution, which reads the text left-to-right while storing entities in a fixed-size working memory (Figure 1). As each token is encountered, the reader must decide whether to: (a) link the token to an existing memory, thereby creating a coreference link, (b) overwrite an existing memory and store a new entity, or (c) disregard the token and move ahead. As memories are"
P19-1593,D14-1162,0,0.0808162,"Missing"
P19-1593,W09-3905,0,0.0353313,"et of pronoun-name anaphora demonstrates strong performance with purely incremental text processing. Work carried out as an intern at Facebook AI Research ston et al., 2015), in which memory operations are differentiable, enabling end-to-end training from gold anaphora resolution data. Furthermore, the memory can be combined with a recurrent hidden state, enabling prediction of the next word. This makes it possible to train the model from unlabeled data using a language modeling objective. To summarize, we present a model that processes the text incrementally, resolving references on the fly (Schlangen et al., 2009). The model yields promising results on the GAP dataset of pronoun-name references.1 2 Model For a given document consisting of a sequence of tokens {wt }Tt=1 , we represent text at two levels: • Tokens: represented as {xt }Tt=1 , where the vector xt ∈ RDx is computed from any token-level encoder. • Entities: represented by a fixed-length mem(i) (i) (i) ory Mt = {(kt , vt , st )}N i=1 , where each (i) memory is a tuple of a key kt ∈ RDk , a 1 Code available at: liufly/refreader https://github.com/ 5918 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pag"
P19-1593,Q18-1042,0,0.267101,"f links). P The Pcoreference loss is then the cross-entropy Ti=1 Tj=i+1 H(ψˆi,j , yi,j ). Because the hidden state ht is computed recurrently from w1:t , the reader can also be trained from a language modeling objective, even when coreference annotations are unavailable. Word probabilities P (wt+1 |ht ) are computed by projecting the hidden state ht by a matrix of output embeddings, and applying the softmax operation. 3 Experiments As an evaluation of the ability of the referential reader to correctly track entity references in text, we evaluate against the GAP dataset, recently introduced by Webster et al. (2018). Each instance consists of: (1) a sequence of tokens w1 , . . . , wT extracted from Wikipedia biographical pages; (2) two person names (A and B, whose token index spans are denoted sA and sB ); (3) a single-token pronoun (P with the token index sP ); and (4) two binary labels (yA and yB ) indicating whether P is referring to A or B. Language modeling. Given the limited size of GAP, it is difficult to learn a strong recurrent model. We therefore consider the task of language modeling as a pre-training step. We make use of the page text of the original Wikipedia articles from GAP, the URLs to w"
P19-1593,D17-1197,0,0.0309738,"it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabele"
P19-1613,D13-1160,0,0.209872,"Missing"
P19-1613,N19-1240,0,0.07606,"Missing"
P19-1613,P17-1171,0,0.0428235,"tor setting contains the question and a collection of 10 paragraphs: 2 paragraphs are provided to crowd workers to write a multi-hop question, and 8 distractor paragraphs are collected separately via TF-IDF between the question and the paragraph. The train set contains easy, medium and hard examples, where easy examples are single-hop, and medium and hard examples are multi-hop. The dev and test sets are made up of only hard examples. Full wiki setting is an open-domain setting which contains the same questions as distractor setting but does not provide the collection of paragraphs. Following Chen et al. (2017), we retrieve 30 Wikipedia paragraphs based on TF-IDF similarity between the paragraph and the question (or subquestion). 6101 All D ECOMP RC 1hop train BERT 1hop train BiDAF 70.57 61.73 67.08 56.27 58.28 Distractor setting Bridge Comp Single Multi All 72.53 61.57 69.41 62.77 59.09 58.74 46.53 53.38 29.64 - 43.26 39.17 38.40 29.97 34.36 62.78 62.36 57.81 30.40 55.05 84.31 79.38 82.98 87.21 - Full wiki setting Bridge Comp Single Multi 40.30 35.30 34.77 32.15 30.42 35.64 29.83 31.74 15.18 - 55.04 54.57 52.85 21.29 50.70 52.11 50.03 46.14 47.14 - Table 3: F1 scores on the dev set of H OTPOT QA in"
P19-1613,D13-1020,0,0.0488979,"2, 2019. 2019 Association for Computational Linguistics bust than an end-to-end BERT baseline (Devlin et al., 2019). Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop dataset"
P19-1613,N18-1059,0,0.356645,"layer who won MVP from one paragraph, and then finding the team that player plays for from another paragraph. In this paper, we propose D ECOMP RC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions “Which player named 2015 Diamond Head Classics MVP?” and “Which team does ANS play for?”, Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms (Talmor and Berant, 2018), making it difficult to generalize to diverse natural language questions such as those on H OTPOT QA (Yang et al., 2018). In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning. Our experiments show that D ECOMP R"
P19-1613,P18-1078,0,0.037198,") q1c , q2c ← form subq(Q, ent1 , ent2 , op) q3c ← op (ent1 , ANS) (ent2 , ANS) Single-hop Reading Comprehension Given a decomposition, we use a single-hop RC model to answer each sub-question. Specifically, the goal is to obtain the answer and the evidence, given the sub-question and N paragraphs. Here, the answer is a span from one of paragraphs, yes or no. The evidence is one of N paragraphs on which the answer is based. Any off-the-shelf RC model can be used. In this work, we use the BERT reading comprehension model (Devlin et al., 2019) combined with the paragraph selection approach from Clark and Gardner (2018) to handle multiple paragraphs. Given N paragraphs S1 , . . . , SN , this approach independently computes answeri and yinone from each paragraph Si , where answeri and yinone denote the answer candidate from ith paragraph and the score indicating ith paragraph does not contain the answer. The final answer is selected from the paragraph with the lowest yinone . Although this approach takes a set of multiple paragraphs as an input, it is not capable of jointly reasoning across different paragraphs. For each paragraph Si , let Ui ∈ Rn×h be the BERT encoding of the sub-question concatenated with a"
P19-1613,N19-1423,0,0.603521,"n deciding on the decomposition in the beginning. Our experiments show that D ECOMP RC outperforms other published methods on H OTPOT QA (Yang et al., 2018), while providing explainable evidence in the form of sub-questions. In addition, we evaluate with alternative distrator paragraphs and questions and show that our decomposition-based approach is more ro6097 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097–6109 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics bust than an end-to-end BERT baseline (Devlin et al., 2019). Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al.,"
P19-1613,N18-2007,0,0.0429541,"-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yang et al., 2018), on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries. Prior methods on multi-hop reading comprehension focus on answering relational queries, and emphasize attention models that reason over coreference chains (Dhingra et al., 2018; Zhong et al., 2019; Cao et al., 2019). In contrast, our method focuses on answering natural language questions via question decomposition. By providing decomposed single-hop sub-questions, our method allows the model’s decisions to be explainable. Our work is most related to Talmor and Berant (2018), which answers questions over web snippets via decomposition. There are three key differences between our method and theirs. First, they decompose questions that are correspond to relational queries, whereas we focus on natural language questions. Next, they rely on an underlying relational query"
P19-1613,D18-1259,0,0.635327,"propose D ECOMP RC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions “Which player named 2015 Diamond Head Classics MVP?” and “Which team does ANS play for?”, Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms (Talmor and Berant, 2018), making it difficult to generalize to diverse natural language questions such as those on H OTPOT QA (Yang et al., 2018). In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning. Our experiments show that D ECOMP RC outperforms other published methods on H OTPOT QA (Yang et al., 2018), while providing explainable evidence in the form"
P19-1613,P17-1147,1,0.883857,"s show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yan"
P19-1613,P11-1060,0,0.0305598,"their model, while our method requires only 400 decomposition examples. Finally, they decide on a decomposition operation exclusively based on the question. In contrast, we decompose the question in multiple ways, obtain answers, and determine the best decomposition based on all given context, which we show is crucial to improving performance. Semantic Parsing. Semantic parsing is a larger area of work that involves producing logical forms from natural language utterances, which are then usually executed over structured knowledge graphs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Our work is inspired by the idea of compositionality from semantic parsing, however, we focus on answering natural language questions over unstructured text documents. 3 Model 3.1 Overview In multi-hop reading comprehension, a system answers a question over a collection of paragraphs by combining evidence from multiple paragraphs. In contrast to single-hop reading comprehension, in which a system can obtain good performance using a single sentence (Min et al., 2018), multi-hop reading comprehension typically requires more complex reasoning over how two pieces of evidence relate to each other"
P19-1613,P18-1160,1,0.923302,"teractive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yang et al., 2018), on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries. Prior methods on multi-hop reading comprehension focus on answering relational querie"
P19-1613,D17-1238,0,0.0134653,"of three types. In addition, these multi-hop reasoning types correspond to the types of compositional questions identified by Berant et al. (2013) and Talmor and Berant (2018). 3.2 Decomposition The goal of question decomposition is to convert a multi-hop question into simpler, single-hop subquestions. A key challenge of decomposition is that it is difficult to obtain annotations for how to decompose questions. Moreover, generating the question word-by-word is known to be a difficult task that requires substantial training data and is not straight-forward to evaluate (Gatt and Krahmer, 2018; Novikova et al., 2017). Instead, we propose a method to create subquestions using span prediction over the question. 6099 The key idea is that, in practice, each sub-question can be formed by copying and lightly editing a key span from the original question, with different span extraction and editing required for each reasoning type. For instance, the bridging question in Table 2 requires finding “the player named 2015 Diamond Head Classic MVP” which is easily extracted as a span. Similarly, the intersection question in Table 2 specifies the type of entity to find (“which actor and comedian”), with two conditions ("
P19-1613,Q18-1021,0,\N,Missing
Q13-1005,D11-1039,1,0.942206,"ntion, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al."
Q13-1005,P09-1010,1,0.903788,"s provide highly ambiguous supervision. Evaluation We evaluate task completion for single instructions on a test set {(xi , si , s0i ) : i = 1 . . . n}, where s0i is the final state of an oracle agent following the execution of xi starting at state si . We will also report accuracies for correctly interpreting instruction sequences ~x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel & Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the nav"
Q13-1005,P10-1129,1,0.535351,": i = 1 . . . n}, where s0i is the final state of an oracle agent following the execution of xi starting at state si . We will also report accuracies for correctly interpreting instruction sequences ~x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel & Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (201"
Q13-1005,P12-1045,0,0.359457,"al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we descr"
Q13-1005,J07-4004,0,0.00682264,"tkowski et al. (2011) introduced a factored CCG lexicon representation. Each lexical item is composed of a lexeme and a template. For example, the entry chair ` N : λx.chair(x) would be constructed by combining the lexeme chair ` [chair], which contains a word paired with logical constants, with the template λv.[N : λx.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark & Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector representation and θ ∈ Rd to be a parameter vector. The optimal parse for sentence x is y ∗ (x) = arg max y∈GEN(x;Λ) θ · φ(x, y) and the final output logical form z is the λ-calculus expression at the root of y ∗ (x). Section 7.2 describes how we efficiently"
Q13-1005,W10-2903,0,0.596633,"by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired wit"
Q13-1005,C92-4181,0,0.598419,"Missing"
Q13-1005,P11-1149,0,0.0290181,"Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We"
Q13-1005,P06-1115,0,0.558197,"navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use w"
Q13-1005,D12-1040,0,0.741896,"ollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently"
Q13-1005,D12-1069,0,0.534213,"pproaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972),"
Q13-1005,E12-1024,1,0.428562,"Missing"
Q13-1005,D10-1119,1,0.343154,"epresentations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011)"
Q13-1005,D11-1140,1,0.919549,"s. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic ana"
Q13-1005,P09-1011,0,0.0736477,"ive forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 51 Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1"
Q13-1005,P11-1060,0,0.696041,"tically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally"
Q13-1005,W04-3201,0,0.00623016,"x.chair(x) would be constructed by combining the lexeme chair ` [chair], which contains a word paired with logical constants, with the template λv.[N : λx.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark & Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; Λ) be the set of all possible CCG parses for x given the lexicon Λ. Define φ(x, y) ∈ Rd to be a d-dimensional feature–vector representation and θ ∈ Rd to be a parameter vector. The optimal parse for sentence x is y ∗ (x) = arg max y∈GEN(x;Λ) θ · φ(x, y) and the final output logical form z is the λ-calculus expression at the root of y ∗ (x). Section 7.2 describes how we efficiently compute an approximation to y ∗ (x) within the joint interpretation and execution model. Supervised learning with GENLEX Previous work (Zettlemoyer & Collins, 2005)"
Q13-1005,P10-1083,0,0.119461,"ation We evaluate task completion for single instructions on a test set {(xi , si , s0i ) : i = 1 . . . n}, where s0i is the final state of an oracle agent following the execution of xi starting at state si . We will also report accuracies for correctly interpreting instruction sequences ~x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel & Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted exp"
Q13-1005,H89-1033,0,0.385709,"d Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. 51 Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair ` N : λx.chair(x) for the word “chair”"
Q13-1005,P07-1121,0,0.415576,"r research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer & Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from con"
Q13-1005,D07-1071,1,0.926816,"Missing"
Q13-1030,P11-1040,0,0.046149,"Missing"
Q13-1030,P07-1073,0,0.0926997,"Missing"
Q13-1030,W99-0613,0,0.16261,"all curve (from 0.16 to 0.34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, s"
Q13-1030,W02-1001,0,0.0880796,"the sentences in our text corpus: θ∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this app"
Q13-1030,P11-1055,1,0.951987,"e. To address this challenge, we propose a joint model of extraction from text and the process by which propositions are observed or missing in both the database and text. Our approach provides a natural way to incorporate side information in the form of a missing data model. For instance, popular entities such as Barack Obama already have good coverage in Freebase, so new extractions are more likely to be errors than those involving rare entities with poor coverage. Our approach to missing data is general and can be combined with various IE solutions. As a proof of concept, we extend MultiR (Hoffmann et al., 2011), a recent model for distantly supervised information extraction, to explicitly model missing data. These extensions complicate the MAP inference problem which is used as a subroutine in learning. This motivated us to explore a variety of approaches to inference in the joint extraction and missing data model. We explore both exact inference based on A* search and efficient approximate inference using local search. Our experiments demonstrate that with a carefully designed set of search operators, local search produces optimal solutions in most cases. Experimental results demonstrate large perf"
Q13-1030,P06-1096,0,0.0137234,"Missing"
Q13-1030,N10-1082,0,0.0108074,"penalty for extracting a fact not in Freebase, and produce an overall higher score. To avoid the problem of getting stuck in local optima, we propose an additional search operator which considers changing all variables, zi , which are currently assigned to a specific relation r, to a new relation r0 , resulting in an additional (k − 1)2 possible neighbors, in addition to the n × (k − 1) neighbors which come from the standard search operator. This aggregate-level search operator allows for more global moves which help to avoid local optima, similar to the type-level sampling approach for MCMC (Liang et al., 2010). At each iteration, we consider all n × (k − 1) + (k−1)2 possible neighboring solutions generated by both search operators, and pick the one with biggest overall improvement, or terminate the algorithm if no improvements can be made over the current solution. 20 random restarts were used for each infer372 ence problem. We found this approach to almost always find an optimal solution. In over 100,000 problems with 200 or fewer variables from the New York Times dataset used in Section 7, an optimal solution was missed in only 3 cases which was verified by comparing against optimal solutions fou"
Q13-1030,N13-1095,0,0.0522778,"on, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approach assuming a fixed proportion of true positives for each entity pair. The Min et al. (2013) approach is perhaps the most closely related of the recent approaches for distant supervision. However, there are a number of key differences: (1) They impose a hard constraint on the proportion of true positive examples for each entity pair, whereas we jointly model relation extraction and missing data in the text and KB. (2) They only handle the case of missing information in the database and not in the text. (3) Their mo"
Q13-1030,P09-1113,0,0.938168,"Missing"
Q13-1030,W04-2407,0,0.0215227,"Times text, features and Freebase relations developed by Riedel et. al. (2010) which was also used by Hoffmann et. al. (2011). This dataset is constructed by extracting named entities from 1.8 million New York Times articles, which are then match against entities in Freebase. Sentences which contain pairs of entities participating in one or more relations are then used as training examples for those relations. The sentencelevel features include word sequences appearing in context with the pair of entities, in addition to part 373 of speech sequences, and dependency paths from the Malt parser (Nivre et al., 2004). 7.1.1 Baseline To evaluate the effect of modeling missing data in distant supervision, we compare against the MultiR model for distant supervision (Hoffmann et al., 2011), a state of the art approach for binary relation extraction which is the most similar previous work, and models facts in Freebase as hard constraints disallowing the possibility of missing information in either the text or the database. To make our experiment as controlled as possible and ruleout the possibility of differences in performance due to implementation details, we compare against our own re-implementation of Mult"
Q13-1030,N13-1008,0,0.0576422,"Missing"
Q13-1030,D11-1141,1,0.107054,"Missing"
Q13-1030,D12-1042,0,0.677887,"Missing"
Q13-1030,P12-1076,0,0.336988,"distant supervision to train event extractors from Twitter. Mintz et. al. (2009) used a set of relations from Freebase as a distant source of supervision to learn to extract information from Wikipedia. Ridel et. al. (2010), Hoffmann et. al. (2011), and Surdeanu et. al. (2012) presented a series of models casting distant supervision as a multiple-instance learning problem (Dietterich et al., 1997). Recent work has begun to address the challenge of noise in heuristically labeled training data generated by distant supervision, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approa"
Q13-1030,P10-1149,0,0.0213479,"34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, so we presented an approach ba"
Q13-1030,P13-2117,0,0.593462,"Missing"
Q13-1030,D07-1071,1,0.364681,"∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this approximation to the gradient requires solving two infe"
S14-1015,W13-1302,0,0.00932687,"n. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermedia"
S14-1015,D10-1049,0,0.040539,"been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations,"
S14-1015,D09-1030,0,0.020787,"bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collected later in Stage 3. This list also included groups, such as crowds of people. Stage 2: For each unique object label from Stage 1, we asked two turkers to draw a polygon around the object identified.3 In cases where the object is a group, we also asked for the number of objects present (1-6 or many). Finally, we created a list of all references to the object from the firs"
S14-1015,P12-1038,0,0.0632436,"halving the difference from human performance to two baselines on 4-gram BLEU. In ablations, we measure the importance of different annotations and visual cues, showing that annotation of activities and relative bounding box information between objects are crucial to generating human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triple"
S14-1015,de-marneffe-etal-2006-generating,0,0.104217,"Missing"
S14-1015,D13-1128,0,0.219833,"Missing"
S14-1015,E12-1076,0,0.27079,"t surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermediate representation for categorization (Farhadi"
S14-1015,N13-1137,0,0.0823812,"Missing"
S14-1015,W99-0604,0,0.0902418,"ring something true in the image; and salience, which sentence is capturing important things in the image while still being concise. Two annotators annotated all test pairs for all criteria for a given pair of systems. Six annotators were used (none authors) and agreement was high (Cohen’s kappa = 0.963, 0.823 and 0.703 for grammar, truth and salience). Machine Translation Baseline The first baseline is designed to see if it is possible to generate good sentences from the facet string labels alone, with no visual information. We use an extension of phrase-based machine translation techniques (Och et al., 1999). We created a virtual bitext by pairing each image description (the target sentence) with a sequence10 of visual identifiers (the source “sentence”) listing strings from the facet labels. Since phrases produced by turkers lack many of the functions words needed to create fluent sentences, we added one of 47 function words either at the start or the end of each output phrase. The translation model included standard features such as language model score (using our caption language model described previously), word count, phrase count, linear distortion, and the count of deleted source words. We"
S14-1015,P10-1126,0,0.0977587,"Missing"
S14-1015,P03-1021,0,0.0393045,"s computation is intractable because we need to consider all possible sentences, so we use beam search for strings up to a fixed length. Reranking Generating directly from the process in Figure 3 results in sentences that may be short and repetitive because the model score is a product of locally normalized distributions. The reranker takes as input a candidate list c, for an image I, as decoded from the generative model. The candidate list includes the top-k scoring hypotheses for each sentence length up to a fixed maximum. A linear scoring function is used for reranking optimized with MERT (Och, 2003) to maximize BLEU-2. 5 Features We construct indicator features to capture variation in usage in different parts of the sentence, types of objects that are mentioned, visual salience, and semantic and visual coordination between objects. The features are included in the maximum entropy models used to parameterize the distributions described in Figure 3. Whenever possible, we use WordNet Synsets (Miller, 1995) instead of lexical features to limit over-fitting. Features in the generative model use tests for local properties, such as the identity of a synset of a word in WordNet, conjoined with a"
S14-1015,D13-1197,1,0.58205,"adi et al., 2009). Activity recognition is an emerging area in images (Li and Fei-Fei, 2007; Yao et al., 2011; Sharma et al., 2013) and video (Weinland et al., 2011), although less studied than object recognition. Also, parts have been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have b"
S14-1015,2001.mtsummit-papers.68,0,0.0257801,"Missing"
S14-1015,P12-1039,0,0.0297731,"work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms"
S14-1015,W10-0721,0,0.428906,"richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collecte"
S14-1015,J12-1006,0,0.0506618,"Missing"
S14-1015,D12-1130,0,0.153821,"ndaries and descriptive text, here including the facts that the children are “riding” and “walking” and that they are “young.” Our goal is to be as exhaustive as possible, giving equal treatment to all objects. For example, the annotations in Figure 1 contain enough information to generate the first three sentences and most of the content in the remaining two. Labels gathered in this way are a type of feature norms (McRae et al., 2005), which have been used in the cognitive science literature to approximate human perception and were recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012). We present the first effort, that we are aware of, for using feature norms to study image description generation. Such rich data allows us to develop significantly more comprehensive generation models. We divide generation into choices about which visual content to select and how to realize a sentence that describes that content. Our approach is grammarbased, feature-rich, and jointly models both decisions. The content selection model includes latent variables that align phrases to visual objects and features that, for example, measure how visual salience and spatial relationships influence"
S14-1015,P13-1056,0,0.0159995,"a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms (McRae et al., 2005), which have recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012; Silberer et al., 2013) but have not been previous studied for generation. This annotation of 500 images (2500 sentences) yielded over 4000 object instances and 100,000 textual labels. Dataset We collected a dataset of richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ percept"
S14-1015,P13-1006,0,0.0292359,"human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. At"
S14-1015,W04-0216,0,0.0134982,"Missing"
S14-1015,D11-1041,0,\N,Missing
S14-1015,P02-1040,0,\N,Missing
S14-1015,W10-0707,0,\N,Missing
W18-2501,D17-1160,1,0.116332,"loning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1–6 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics with v"
W18-2501,D17-1018,1,0.892756,"and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1–6 c Melbourne, Australia, July 20, 2018."
W18-2501,D15-1075,0,0.0275455,"Missing"
W18-2501,P14-5010,0,0.0133354,"models. 4 5 The design of AllenNLP allows researchers to focus on the high-level summary of their models rather than the details, and to do careful, reproducible research. Internally at the Allen Institute for Artificial Intelligence the library is widely adopted and has improved the quality of our research code, spread knowledge about deep learning, and made it easier to share discoveries between teams. AllenNLP is gaining traction externally and is growing an open-source community of contributors 7 . The AllenNLP team is comRelated Work Many existing NLP pipelines, such as Stanford CoreNLP (Manning et al., 2014) and spaCy6 , focus on predicting linguistic structures rather than modeling NLP architectures. While AllenNLP supports making predictions using pre-trained 6 Conclusion 7 See GitHub stars and issues on https://github.com/allenai/allennlp and mentions from publications at https://www.semanticscholar.org/search?q=allennlp. https://spacy.io/ 4 mitted to continuing work on this library in order to enable better research practices throughout the NLP community and to build a community of researchers who maintain a collection of the best models in natural language processing. Sepp Hochreiter and J¨u"
W18-2501,D17-2014,0,0.0500079,"Missing"
W18-2501,D16-1053,0,0.0107399,"n choose between pre-trained word embeddings, word embeddings concatenated with a character-level CNN encoding, or even pre-trained model token-incontext embeddings (Peters et al., 2017), which allows for easy controlled experimentation. Seq2SeqEncoder: A common operation in deep NLP models is to take a sequence of word vectors and pass them through a recurrent network to encode contextual information, producing a new sequence of vectors as output. There is a large number of ways to do this, including LSTMs (Hochreiter and Schmidhuber, 1997), GRUs (Cho et al., 2014), intra-sentence attention (Cheng et al., 2016), recurrent additive networks (Lee et al., 2017b), and many more. AllenNLP’s Seq2SeqEncoder abstracts away the decision of which particular encoder to use, allowLibrary Design AllenNLP is a platform designed specifically for deep learning and NLP research. AllenNLP is built on PyTorch (Paszke et al., 2017), which provides many attractive features for NLP research. PyTorch supports dynamic networks, has a clean “Pythonic” syntax, and is easy to use. The AllenNLP library provides (1) a flexible data API that handles intelligent batching and padding, (2) high-level abstractions for common operati"
W18-2501,D14-1179,0,0.00490643,"Missing"
W18-2501,J05-1004,0,0.0517239,"Missing"
W18-2501,P17-1044,1,0.288803,"Apache 2.0 license and is easy to download and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1"
W18-2501,P17-1161,1,0.174704,"ector sequences, (3) how vector sequences are merged into a single vector. TokenEmbedder: This abstraction takes input arrays generated by e.g. a TextField and returns a sequence of vector embeddings. Through the use of polymorphism and AllenNLP’s experiment framework (see Section 2.3), researchers can easily switch between a wide variety of possible word representations. Simply by changing a configuration file, an experimenter can choose between pre-trained word embeddings, word embeddings concatenated with a character-level CNN encoding, or even pre-trained model token-incontext embeddings (Peters et al., 2017), which allows for easy controlled experimentation. Seq2SeqEncoder: A common operation in deep NLP models is to take a sequence of word vectors and pass them through a recurrent network to encode contextual information, producing a new sequence of vectors as output. There is a large number of ways to do this, including LSTMs (Hochreiter and Schmidhuber, 1997), GRUs (Cho et al., 2014), intra-sentence attention (Cheng et al., 2016), recurrent additive networks (Lee et al., 2017b), and many more. AllenNLP’s Seq2SeqEncoder abstracts away the decision of which particular encoder to use, allowLibrar"
W18-2501,N18-1202,1,0.106576,"Missing"
W18-2501,D16-1264,0,0.0112895,"nd models are working as intended. Library features are built with testability in mind so new components can maintain a similar test coverage. 2.1 Text Data Processing AllenNLP’s data processing API is built around the notion of Fields. Each Field represents a single input array to a model. Fields are grouped together in Instances that represent the examples for training or prediction. The Field API is flexible and easy to extend, allowing for a unified data API for tasks as diverse as tagging, semantic role labeling, question answering, and textual entailment. To represent the SQuAD dataset (Rajpurkar et al., 2016), for example, which has a question and a passage as inputs and a span from the passage as output, each 4 NLP-Focused Abstractions https://codecov.io/gh/allenai/allennlp 2 ing the user to build an encoder-agnostic model and specify the encoder via configuration. In this way, a researcher can easily explore new recurrent architectures; for example, they can replace the LSTMs in any model that uses this abstraction with any other encoder, measuring the impact across a wide range of models and tasks. Seq2VecEncoder: Another common operation in NLP models is to merge a sequence of vectors into a s"
W18-2501,P17-1076,0,0.0132037,"a sequence of vectors into a single vector, using either a recurrent network with some kind of averaging or pooling, or using a convolutional network. This operation is encapsulated in AllenNLP by a Seq2VecEncoder. This abstraction again allows the model code to only describe a class of similar models, with particular instantiations of that model class being determined by a configuration file. SpanExtractor: A recent trend in NLP is to build models that operate on spans of text, instead of on tokens. State-of-the-art models for coreference resolution (Lee et al., 2017a), constituency parsing (Stern et al., 2017), and semantic role labeling (He et al., 2017) all operate in this way. Support for building this kind of model is built into AllenNLP, including a SpanExtractor abstraction that determines how span vectors get computed from sequences of token vectors. 2.3 or even to create their own new abstractions. While some entries in the configuration file are optional, many are required and if unspecified AllenNLP will raise a ConfigurationError when reading the configuration. Additionally, when a configuration file is loaded, AllenNLP logs the configuration values, providing a record of both specified"
W18-2501,P15-1109,0,0.0115804,"m has a permissive Apache 2.0 license and is easy to download and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source"
W18-2501,D16-1244,0,\N,Missing
W18-2501,P17-4012,0,\N,Missing
