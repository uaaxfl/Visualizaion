2010.iwslt-evaluation.22,W06-3103,1,0.835479,"Workshop on Spoken Language Translation (IWSLT 2010). We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Sectio"
2010.iwslt-evaluation.22,N04-4015,0,0.0675793,"We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present"
2010.iwslt-evaluation.22,N06-2013,0,0.0454777,"it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present the sol"
2010.iwslt-evaluation.22,N04-4038,0,0.0482616,"asily captured by the IBM alignment models. In this work, we experimented with the following segmenters: • FST - A Finite State Transducer-based approach introduced and implemented by [1]. The FST is used as a framework to implement a set of rules for segmentation of Arabic. The prefixes that are split include w,f,k,l,b,Al and s. Suffixes which are segmented are pronouns (objective and possessive). The method is characterized by fast processing speed but suffers from the lack of context in the decision procedure leading to erroneous output. • SVM - we reimplemented the classifier suggested by [4]. In their method, each character is classified by its segment rule (prefix, stem and suffix) and position (beginning and inside segment). Arabic words are segmented according to the ATB scheme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 ."
2010.iwslt-evaluation.22,W07-0813,0,0.052985,"eme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA versi"
2010.iwslt-evaluation.22,P05-1071,0,0.0466539,"lementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-tar"
2010.iwslt-evaluation.22,2010.amta-papers.8,1,0.836307,"WTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-terminals is then used to compute a new prob"
2010.iwslt-evaluation.22,2010.iwslt-papers.18,1,0.874946,"h respect to the syntactic constructs. 5.3. Poor-man syntax (POMS) In this approach we apply the same model as described in the previous section, but the method for producing the new 6 http://nlp.stanford.edu/software/lex-parser.shtml JANE BLEU TER 55.4 30.6 55.3 30.2 55.2 29.4 53.9 31.2 54.6 30.2 56.6 28.8| 57.1| 29.4 56.6− 28.9− 53.0 32.4 SYN BLEU TER 55.7 30.8 54.4 31.2 55.7 29.4 54.5 30.9 54.8 31.2 56.5 28.5+ 56.6| 29.2 55.4 30.3 52.7 32.5 POMS BLEU TER 56.1 30.2 56.0− 29.4− 55.2 29.9 54.8 30.8 55.5− 29.7− 56.8− 28.7 57.5∗ 28.5∗ 54.9 29.5 53.4 32.3 non-terminals is altered as described in [16]. Instead of relying on parse trees based on linguistic knowledge we rely on automatic clustering methods. This makes this approach applicable also for underresourced languages for which no linguistic tools may be available. 6. Results The results of the different segmentation methods and schemes are summarized in Table 1. In this table, the best result in a column is marked with |, thus comparing different segmentations for the same decoder setup. We mark with − the best (in a row) performing decoder over a specific segmentation method. ∗ marks the best result overall in the table. For compar"
2010.iwslt-evaluation.22,W10-1747,1,0.898303,"Missing"
2010.iwslt-evaluation.22,2008.iwslt-papers.8,1,0.0907932,"MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which i"
2010.iwslt-evaluation.22,W99-0604,1,0.294484,"m 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th Internati"
2010.iwslt-evaluation.22,P10-1049,1,0.304281,"ystem used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd"
2010.iwslt-evaluation.22,W10-1738,1,0.0939642,"010: IWSLT08 results summary (nocase+punc) System CRF FST MADA ATB MADA D1 MADA D2 MADA D3 MorphTagger SVM TOK PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of consider"
2010.iwslt-evaluation.22,J07-2003,0,0.142759,"K PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syn"
2010.iwslt-evaluation.22,N09-1027,0,0.0119972,"phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-ter"
2010.iwslt-evaluation.22,popovic-ney-2006-pos,1,\N,Missing
2010.iwslt-evaluation.22,D11-1033,0,\N,Missing
2010.iwslt-evaluation.22,E09-1044,0,\N,Missing
2010.iwslt-evaluation.22,D09-1022,1,\N,Missing
2010.iwslt-evaluation.22,J93-2003,0,\N,Missing
2010.iwslt-evaluation.22,E06-1005,1,\N,Missing
2010.iwslt-evaluation.22,E03-1076,0,\N,Missing
2010.iwslt-evaluation.22,D08-1089,0,\N,Missing
2010.iwslt-evaluation.22,P03-1054,0,\N,Missing
2010.iwslt-evaluation.22,P02-1040,0,\N,Missing
2010.iwslt-evaluation.22,W06-3110,1,\N,Missing
2010.iwslt-evaluation.22,J10-3008,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-keynotes.2,0,\N,Missing
2010.iwslt-evaluation.22,P10-2041,0,\N,Missing
2010.iwslt-evaluation.22,P08-2030,0,\N,Missing
2010.iwslt-evaluation.22,W07-0734,0,\N,Missing
2010.iwslt-evaluation.22,W06-3105,0,\N,Missing
2010.iwslt-evaluation.22,N03-1017,0,\N,Missing
2010.iwslt-evaluation.22,2008.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,J03-1002,1,\N,Missing
2010.iwslt-evaluation.22,W06-3108,1,\N,Missing
2010.iwslt-evaluation.22,P07-1019,0,\N,Missing
2010.iwslt-evaluation.22,D08-1039,1,\N,Missing
2010.iwslt-evaluation.22,P08-1066,0,\N,Missing
2010.iwslt-evaluation.22,2009.mtsummit-posters.17,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-papers.15,1,\N,Missing
2010.iwslt-evaluation.22,2006.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.8,1,\N,Missing
2010.iwslt-evaluation.22,N04-1033,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-evaluation.1,0,\N,Missing
2010.iwslt-evaluation.22,D08-1076,0,\N,Missing
2010.iwslt-evaluation.22,P03-1021,0,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.5,1,\N,Missing
2010.iwslt-evaluation.22,P08-1000,0,\N,Missing
2010.iwslt-papers.11,W06-3105,0,0.0618427,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,P06-1096,0,0.0221215,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,2009.eamt-1.23,0,0.0260374,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,P10-1049,1,0.931056,"ed. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and alignment of each sentence pair of the training data using a modification of the"
2010.iwslt-papers.11,P08-1024,0,0.0416513,"phrase alignment between source and target sentences. Phrase translation probabilities are then updated based on this alignment. By applying leaving-one-out in the training procedure, overfitting effects can be diminished. The phrase table which is learnt from forced alignments can be used as phrase table itself in the translation system or can be combined with the original phrase-based system. Experiments in [4] show that the latter gives better results. In recent years, conventional phrase-based systems have been outerperformed by hierarchical phrase-based or syntaxbased systems. The papers [5, 6] describe first training approaches with forced alignment techniques with hierarchical translation systems. However, they report difficulties when aligning training sentences because of the restrictions in the phrase extraction process. Our work is intended to neither solve this problem nor propose any forced alignment training method on hierarchical systems. Instead, we want to see the effects of combining a hierarchical system with forced alignments from a phrase-based decoder. The next section will recall phrase-based and hierarchical phrase-based translation models. In Section 3 we describ"
2010.iwslt-papers.11,2008.iwslt-papers.7,1,0.851625,"forced alignments as described in Section 3. The hierarchical system is used as baseline and combined with the retrained phrase table from the forced alignment training using the phrase-based system. 5.1. Results First, we present our results on the IWSLT 2010 BTEC task for Arabic-to-English for which corpus statistics are given in Table 1. We give BLEU scores in Table 2 as well as TER scores in Table 3. We chose test04 as development set and test08 and test05 as blind test sets. We experimented with a hierarchical baseline system and a hierarchical system enriched with soft syntactic labels [15]. The second will be simply denoted by the label syntax in our tables of results. The filtering method from Section 4 improves both the baseline and the system with syntax information, though improvements can be found only on one of the test sets each. The intersection method was only tested on the simple baseline and performed worse on all test sets. In order to investigate our method on a larger corpus, we experimented on the English-to-German Quaero project corpus from 2010. This corpus mainly contains the data from the Workshop of Machine Translation (WMT) 2010, namely Europarl and news-co"
2010.iwslt-papers.11,2009.iwslt-papers.2,0,0.123072,"phrase alignment between source and target sentences. Phrase translation probabilities are then updated based on this alignment. By applying leaving-one-out in the training procedure, overfitting effects can be diminished. The phrase table which is learnt from forced alignments can be used as phrase table itself in the translation system or can be combined with the original phrase-based system. Experiments in [4] show that the latter gives better results. In recent years, conventional phrase-based systems have been outerperformed by hierarchical phrase-based or syntaxbased systems. The papers [5, 6] describe first training approaches with forced alignment techniques with hierarchical translation systems. However, they report difficulties when aligning training sentences because of the restrictions in the phrase extraction process. Our work is intended to neither solve this problem nor propose any forced alignment training method on hierarchical systems. Instead, we want to see the effects of combining a hierarchical system with forced alignments from a phrase-based decoder. The next section will recall phrase-based and hierarchical phrase-based translation models. In Section 3 we describ"
2010.iwslt-papers.11,W99-0604,1,0.633447,"how we combine these forced alignments with hierarchical translation models. An empirical evaluation on two different tasks is done in Section 5. Finally, Section 6 concludes the paper. 2. Translation Models In this work we will study the combination of two widely used approaches to statistical machine translation. The main difference between the two models lies in the basic units that are used for the translation. 2.1. Phrase-based Translation Model The phrase based translation model is based on the concept of phrase, a bilingual pair of sequences of words that are translations of each other [7]. Given a word-aligned training corpus, we extract those phrases for which the source words are aligned only to target words within the phrase and vice-versa. This set can be formalized for a sentence pair (f1J , eI1 ) as P(f1J ,eI1 , A) =  hfjj12 , eii21 i |j1 , j2 , i1 , i2 s.t. ∀(j, i) ∈ A : j1 ≤ j ≤ j2 ⇔ i1 ≤ i ≤ i2 ∧ ∃(j, i) ∈ A : (j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 ) , (1) where A is the alignment between the source and target sentences expressed as a set of position pairs. 291 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 2.2. Hie"
2010.iwslt-papers.11,P05-1033,0,0.169168,"re aligned only to target words within the phrase and vice-versa. This set can be formalized for a sentence pair (f1J , eI1 ) as P(f1J ,eI1 , A) =  hfjj12 , eii21 i |j1 , j2 , i1 , i2 s.t. ∀(j, i) ∈ A : j1 ≤ j ≤ j2 ⇔ i1 ≤ i ≤ i2 ∧ ∃(j, i) ∈ A : (j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 ) , (1) where A is the alignment between the source and target sentences expressed as a set of position pairs. 291 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 2.2. Hierarchical Phrase-based Translation Model set of hierarchical rules The hierarchical approach [8] to machine translation is a generalization of the above model where the phrases are allowed to have “gaps”. Those gaps are linked in the source and target language such that a translation rule specifies the location of the translation of the text filling a gap in the source side. The model is formalized as a synchronous context-free grammar. The set of rules extracted from an aligned bilingual sentence pair is best described in a recursive way. Given a source sentence f1J , a target sentence eI1 , an alignment A between them and N the maximum number of gaps allowed (usually N = 2), we can def"
2010.iwslt-papers.11,W06-3119,0,0.0206708,"s relative frequencies. In this work we will study alternative ways to compute these probabilities. α, β ∈ (F ∪ N )? , δ, γ ∈ (E ∪ N )? ∧ ∃j1 , j2 , i1 , i2 : j1 < j2 , i1 < i2 :  2 X → hαfjj12 β, δeii1 γ i ∈ Hn−1 (f1J , eI1 , A)  ∧X → hfjj12 , eii21 i ∈ H0 (f1J , eI1 , A) , The total set of hierarchical phrases extracted from a parallel corpus is the union of the hierarchical phrases extracted from each of its sentences. As can be seen from Equation 4, in the standard approach only one generic non-terminal is used. There are works which propose to extend the set of non-terminals, see e.g. [9]. It is common practice to include two additional rules to the • IBM1-like word-based probabilities computed at the phrase level, also in source-to-target and target-tosource directions. These probabilities can be seen as a smoothing of the afore mentioned phrase-based probabilities. In the case of the hierarchical model, the nonterminals in the rules are simply ignored in the computation. 292 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 • Language model probabilities of the produced translation. • Different penalties. These heu"
2010.iwslt-papers.11,P03-1021,0,0.00786503,"s help in controlling different aspects of the translation. A word penalty can guide the translation process into choosing longer or shorter translations, a penalty for rules not in the set H0 can favour hierarchical rules over lexical phrases, etc. The set of chosen features is dependent of the model used, but they are similar in spirit. • Phrase count features which penalize phrases with low counts. The values for the scaling factors λm are estimated by minimum error rate training, a numerical method that optimizes a measure of translation quality (usually BLEU) on a heldout development set [10]. 3. Forced Alignments The standard phrase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatc"
2010.iwslt-papers.11,J93-2003,0,0.02008,"s, etc. The set of chosen features is dependent of the model used, but they are similar in spirit. • Phrase count features which penalize phrases with low counts. The values for the scaling factors λm are estimated by minimum error rate training, a numerical method that optimizes a measure of translation quality (usually BLEU) on a heldout development set [10]. 3. Forced Alignments The standard phrase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical ph"
2010.iwslt-papers.11,2008.iwslt-papers.8,1,0.904736,"Missing"
2010.iwslt-papers.11,W10-1738,1,0.892031,"Missing"
2010.iwslt-papers.11,J03-1002,1,\N,Missing
2010.iwslt-papers.11,D08-1076,0,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.iwslt-papers.5,D10-1044,0,\N,Missing
2011.iwslt-papers.5,D11-1033,0,\N,Missing
2011.iwslt-papers.5,J93-2003,0,\N,Missing
2011.iwslt-papers.5,P02-1040,0,\N,Missing
2011.iwslt-papers.5,D09-1074,0,\N,Missing
2011.iwslt-papers.5,P10-2041,0,\N,Missing
2011.iwslt-papers.5,P07-1004,0,\N,Missing
2011.iwslt-papers.5,P07-2045,0,\N,Missing
2011.iwslt-papers.5,P08-2030,0,\N,Missing
2011.iwslt-papers.5,D07-1036,0,\N,Missing
2011.iwslt-papers.5,W07-0733,0,\N,Missing
2011.iwslt-papers.5,P03-1021,0,\N,Missing
2011.iwslt-papers.5,J03-1002,1,\N,Missing
2011.iwslt-papers.5,W04-3250,0,\N,Missing
2011.iwslt-papers.5,N04-1021,0,\N,Missing
2012.iwslt-evaluation.7,popovic-ney-2006-pos,1,\N,Missing
2012.iwslt-evaluation.7,N04-4026,0,\N,Missing
2012.iwslt-evaluation.7,D09-1022,1,\N,Missing
2012.iwslt-evaluation.7,J93-2003,0,\N,Missing
2012.iwslt-evaluation.7,E03-1076,0,\N,Missing
2012.iwslt-evaluation.7,N04-4038,0,\N,Missing
2012.iwslt-evaluation.7,C02-1050,0,\N,Missing
2012.iwslt-evaluation.7,P02-1040,0,\N,Missing
2012.iwslt-evaluation.7,W10-1738,1,\N,Missing
2012.iwslt-evaluation.7,P12-3029,0,\N,Missing
2012.iwslt-evaluation.7,J10-3008,0,\N,Missing
2012.iwslt-evaluation.7,2010.iwslt-keynotes.2,0,\N,Missing
2012.iwslt-evaluation.7,P10-2041,0,\N,Missing
2012.iwslt-evaluation.7,P10-1049,1,\N,Missing
2012.iwslt-evaluation.7,P07-2045,0,\N,Missing
2012.iwslt-evaluation.7,P08-2030,0,\N,Missing
2012.iwslt-evaluation.7,W07-0734,0,\N,Missing
2012.iwslt-evaluation.7,2008.iwslt-papers.8,1,\N,Missing
2012.iwslt-evaluation.7,J03-1002,1,\N,Missing
2012.iwslt-evaluation.7,W06-3108,1,\N,Missing
2012.iwslt-evaluation.7,D09-1117,0,\N,Missing
2012.iwslt-evaluation.7,P07-1019,0,\N,Missing
2012.iwslt-evaluation.7,C12-3061,1,\N,Missing
2012.iwslt-evaluation.7,W06-3103,1,\N,Missing
2012.iwslt-evaluation.7,2012.iwslt-papers.18,1,\N,Missing
2012.iwslt-evaluation.7,C12-2091,1,\N,Missing
2012.iwslt-evaluation.7,2010.iwslt-papers.15,1,\N,Missing
2012.iwslt-evaluation.7,2006.iwslt-papers.1,1,\N,Missing
2012.iwslt-evaluation.7,J07-2003,0,\N,Missing
2012.iwslt-evaluation.7,2002.tmi-tutorials.2,0,\N,Missing
2012.iwslt-evaluation.7,P03-1021,0,\N,Missing
2012.iwslt-evaluation.7,2012.eamt-1.60,0,\N,Missing
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2014.amta-researchers.15,D11-1033,0,0.235668,"om itself. Now, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 195 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012"
2014.amta-researchers.15,2011.iwslt-evaluation.18,0,0.0595347,"og-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT system towards a target domain using small amounts of parallel in-domain data. The main outcome of this paper is that all the approaches successfully improve the generic model and none of them is better than the others. The performances of the approaches mainly depend on their match to the specific data. 2.3 Data selection The main idea of data selection is to try to take advantage of a generic corpus by picking out a subset of training data that is most relevant to the domain of interest. Two main approaches are used to perform domain adapta"
2014.amta-researchers.15,J93-2003,0,0.0861508,"both source and target side: ¯ LM (s, t) = HLM (s) − HLM H (s) + HLMItrg (t) − HLMOˆ Isrc ˆ src O Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC trg (t) (3) © The Authors 196 Note that since the scores in Equation 3 are computed for the source and target separately, any target sentence t0 whose cross-entropy score is similar to that of t can exchange t and have a similar score assigned to it by this method. As a result, poorly aligned data can not be detected by LM cross-entropy scoring only. 3.2 Translation Model Cross-entropy The IBM-Model 1 (M1) (Brown et al., 1993) is a model used in state-of-the-art SMT systems for a variety of applications. In this work, we apply M1 scores to achieve adaptation to some domain specific data. Mansour et al. (2011) extend the formulation by Axelrod et al. (2011), which is described in Equation (3), by adding the M1 cross-entropy score to the LM cross entropy score. The M1 cross-entropy for a sentence pair (s, t) = s1 , ..., s|s |, t1 , ..., t|t |is defined as: where ¯ M 1 (s, t) = HM 1 (t|s) − HM 1 (t|s) + HM 1 (s|t) − HM 1 (s|t) H I I ˆ ˆ O O (4)   |s| |t| X X 1 1 log  pM 1 (ti |sj ) HM 1 (t|s) = − |t| |s| j=1 i="
2014.amta-researchers.15,W07-0722,1,0.799684,"e amounts of monolingual training data translated using a completely new model. Lambert et al. (2011) enhanced this approach by using the translations of monolingual data in the target language. 2.2 Model combination and update One way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT syst"
2014.amta-researchers.15,W07-0717,0,0.0282688,"it proposes to add large amounts of monolingual training data translated using a completely new model. Lambert et al. (2011) enhanced this approach by using the translations of monolingual data in the target language. 2.2 Model combination and update One way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches ai"
2014.amta-researchers.15,D08-1089,0,0.0143253,"searches for the best translation eˆI1 as defined by the I K J models hm (e1 , s1 , f1 ). It can be written as (Och and Ney, 2004) ( ˆ eˆI1 = arg max I,eI1 M X ) J λm hm (eI1 , sK 1 , f1 ) , (9) m=1 where f1J = f1 . . . fJ is the source sentence, eI1 = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. The feature functions hm include translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model, phrase and word penalty, a jump-distance-based distortion model, a hierarchical orientation model (Galley and Manning, 2008) and an n-gram cluster language model (Wuebker et al., 2013). The log-linear feature weights λm are optimized on a development data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 199 5 Experiments In this section we describe the different experiments we made in order to compare between the approaches. 5.1 The VideoLectures.NET Repository VideoLectures.NET1 is a free and open access repository of video lectures mostly f"
2014.amta-researchers.15,P02-1023,0,0.656356,"nd, such approaches use information retrieval techniques and similarity scores. On the other hand, language models are used associated to perplexity and cross-entropy. Intuitively, seeking the data closest to the test set is related to information retrieval techniques. Lü et al. (2007) present this approach using the standard measure T F.IDF (Term Frequency – Inverse Document Frequency) to measure the similarity between the test sentences and the training sentences. This approach is based on a bag-of-words scheme. The second approach, based on language models (LMs), was originally proposed by Gao and Zhang (2002). Here, the generic corpus is scored against an LM trained on a seed of domain-specific data, and the cross-entropy is computed for each sentence. Then, the same generic corpus is scored against an LM trained on a random sample taken from itself. Now, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 195 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing"
2014.amta-researchers.15,E12-1016,0,0.540022,"elrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012), is explored. 3.1 Language Model Cross-entropy The LM cross-entropy difference can be used for both monolingual data selection for LM training as described by Moore and Lewis (2010), or bilingual selection for translation model training (Axelrod et al., 2011). Given an in-domain corpus I and an out-of-domain or general-domain corpus O, first we ˆ ⊆ O of approximately the same size as I, and train the LMs LMI generate a random subset O and LMOˆ using the corresponding training data. Afterwards, each sentence o ∈ O is scored according to: HLMI (o) − HLMOˆ (o) (1) where H is the length-normalise"
2014.amta-researchers.15,N03-1017,0,0.0353971,"cross-entropy achieves the most stable results. As another important criterion for measuring translation quality in our application, we identify the number of out-ofvocabulary words. Here, infrequent n-gram recovery shows superior performance. Finally, we combine the two selection techniques in order to benefit from both their strengths. 1 Introduction With the continuous growth of available bitexts and research advances of the underlying technology, statistical machine translation (SMT) has become popular for many real world tasks. The most common approach is still the phrase-based paradigm (Koehn et al., 2003), that provides an efficient framework with good translation quality for many language pairs. This work focuses on the application of SMT to the task of translating scientific video lectures. Online scientific video lectures are becoming increasingly popular, e.g. in the context of massive open online courses (MOOCs). Being able to provide high quality automatic translations for this kind of technical talks could, e.g., prove beneficial to education at universities, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 193 sharing technical kno"
2014.amta-researchers.15,W07-0733,0,0.0273928,"anced this approach by using the translations of monolingual data in the target language. 2.2 Model combination and update One way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT system towards a target domain using small amounts of parallel in-domain data. The main outcome of this paper is"
2014.amta-researchers.15,W11-2132,1,0.895614,"involves a confidence measure in order to select the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 194 most reliable data to train a small additional phrase table (PT). The generic and the new phrase tables are used jointly for translation, which can be seen as a mixture model with one specific PT built for each test set. The lightly-supervised training approach proposed by Schwenk (2008) does not adapt the model to the test data, but it proposes to add large amounts of monolingual training data translated using a completely new model. Lambert et al. (2011) enhanced this approach by using the translations of monolingual data in the target language. 2.2 Model combination and update One way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific mode"
2014.amta-researchers.15,E12-2003,0,0.0201164,"lly transcribed and translated into several languages. In particular, 23 of these 27 lectures (16 hours) were translated into French by professional translators. 5.2 Data Our experiments are performed on the task of translating manually transcribed English video lectures into French. In addition to around 5000 sentence pairs from VideoLectures.NET, we use the parallel TED talk data provided for the shared translation task of the International Workshop on Spoken Language Translation4 as in-domain data. The general domain data consists of several corpora. The COSMAT scientific thesis abstracts (Lambert et al., 2012) and the news-commentary-v8 corpus, provided by the ACL 2013 8th Workshop on Statistical Machine Translation5 (WMT), are directly added to the baseline without instance selection due to their small size. The large corpora on which data selection is performed, are the Europarl-v7 corpus (also provided by WMT), the JRC-Acquis corpus (Steinberger et al., 2006) and the Open Subtitles corpus6 (Tiedemann, 2012). Data statistics for the complete in-domain and out-of-domain data are given in Table 1. For the development and test sets we selected four video lectures each, that were manually transcribed"
2014.amta-researchers.15,D07-1036,0,0.650913,"Missing"
2014.amta-researchers.15,2012.iwslt-papers.7,1,0.80783,"ric corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012), is explored. 3.1 Language Model Cross-entropy The LM cross-entropy difference can be used for both monolingual data selection for LM training as"
2014.amta-researchers.15,2011.iwslt-papers.5,1,0.90042,"izan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 195 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012), is explored. 3.1 Lan"
2014.amta-researchers.15,D09-1074,0,0.0228901,"95 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012), is explored. 3.1 Language Model Cross-entropy The LM cross-entropy difference can be used for both monolingual data sele"
2014.amta-researchers.15,P10-2041,0,0.494601,"cific data, and the cross-entropy is computed for each sentence. Then, the same generic corpus is scored against an LM trained on a random sample taken from itself. Now, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 195 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we proc"
2014.amta-researchers.15,2012.amta-papers.19,0,0.0272372,"s. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT system towards a target domain using small amounts of parallel in-domain data. The main outcome of this paper is that all the approaches successfully improve the generic model and none of them is better than the others. The performances of the approaches mainly depend on their match to the specific data. 2.3 Data selection The main idea of data selection is to try to take advantage of a generic corpus by picking out"
2014.amta-researchers.15,P03-1021,0,0.035916,"ere f1J = f1 . . . fJ is the source sentence, eI1 = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. The feature functions hm include translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model, phrase and word penalty, a jump-distance-based distortion model, a hierarchical orientation model (Galley and Manning, 2008) and an n-gram cluster language model (Wuebker et al., 2013). The log-linear feature weights λm are optimized on a development data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 199 5 Experiments In this section we describe the different experiments we made in order to compare between the approaches. 5.1 The VideoLectures.NET Repository VideoLectures.NET1 is a free and open access repository of video lectures mostly filmed by people from the Jožef Stefan Institute (JSI, Slovenia) at major conferences, summer schools, workshops and science promotional events from many fields of science. VideoLectures.N"
2014.amta-researchers.15,J04-4002,1,0.68254,"Missing"
2014.amta-researchers.15,2001.mtsummit-papers.68,0,0.0282297,"e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. The feature functions hm include translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model, phrase and word penalty, a jump-distance-based distortion model, a hierarchical orientation model (Galley and Manning, 2008) and an n-gram cluster language model (Wuebker et al., 2013). The log-linear feature weights λm are optimized on a development data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 199 5 Experiments In this section we describe the different experiments we made in order to compare between the approaches. 5.1 The VideoLectures.NET Repository VideoLectures.NET1 is a free and open access repository of video lectures mostly filmed by people from the Jožef Stefan Institute (JSI, Slovenia) at major conferences, summer schools, workshops and science promotional events from many fields of science. VideoLectures.NET has so far published more than 15K lectures, all of them reco"
2014.amta-researchers.15,2008.iwslt-papers.6,0,0.0272986,"s was first proposed by Ueffing (2006) and refined by Ueffing et al. (2007). The main idea is to filter the translations with the translated test data. This process involves a confidence measure in order to select the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 194 most reliable data to train a small additional phrase table (PT). The generic and the new phrase tables are used jointly for translation, which can be seen as a mixture model with one specific PT built for each test set. The lightly-supervised training approach proposed by Schwenk (2008) does not adapt the model to the test data, but it proposes to add large amounts of monolingual training data translated using a completely new model. Lambert et al. (2011) enhanced this approach by using the translations of monolingual data in the target language. 2.2 Model combination and update One way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be co"
2014.amta-researchers.15,I08-2089,0,0.0254036,"way to adapt MT models is to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT system towards a target domain using small amounts of parallel in-domain data. The main outcome of this paper is that all the approaches successfully improve the generic model and none of them is better than the others. The performances o"
2014.amta-researchers.15,P13-1082,0,0.0129827,"to combine translation models. Models can be combined using the mixture-model approach, a log-linear combination or through incremental learning approaches. To obtain a mixture of domain-specific models trained on several different domain-specific corpora, they can be combined using a log-linear or a linear approach (Foster and Kuhn, 2007; Civera and Juan, 2007). The standard log-linear model may be used to combine some domainspecific models (Koehn and Schroeder, 2007). In the same way target language models may be combined using a log-linear or a linear combination (Schwenk and Koehn, 2008). Sennrich et al. (2013) proposed to combine different specific parts of the phrase-table during translation leading to a multi-domain adaptation approach. Niehues and Waibel (2012) compared several incremental approaches, namely the backoff, the factored, the log-linear and the fill-up (Bisazza et al., 2011) techniques. These approaches aim at adapting an MT system towards a target domain using small amounts of parallel in-domain data. The main outcome of this paper is that all the approaches successfully improve the generic model and none of them is better than the others. The performances of the approaches mainly"
2014.amta-researchers.15,W10-1759,0,0.335217,"Researchers Vancouver, BC © The Authors 195 sentences of the generic corpus are sorted regarding the computation of the difference between domain-specific score and generic score. At last, the best amount of the sorted data has to be determined. This best point is found by minimizing the perplexity of a development set on growing percentages of the sorted corpus. Moore and Lewis (2010) reported that the perplexity decreases when less, but more appropriate data is used. Recent works expand this approach to bitexts (Axelrod et al., 2011; Mansour et al., 2011). Approaches like corpus weighting (Shah et al., 2010) or sentence weighting (Matsoukas et al., 2009; Mansour and Ney, 2012) are not suitable to our translation task because these approaches can produce huge models by considering the whole data. 3 Cross-entropy based Data Selection versus Infrequent n-gram Recovery In this section we detail the different approaches experimented with for data selection. On one hand we process the data selection for both LM and translation model (TM) using cross-entropy. On the other hand, the infrequent n-gram recovery (Gascó et al., 2012), is explored. 3.1 Language Model Cross-entropy The LM cross-entropy differe"
2014.amta-researchers.15,steinberger-etal-2006-jrc,0,0.286062,"Missing"
2014.amta-researchers.15,tiedemann-2012-parallel,0,0.0508334,"slation task of the International Workshop on Spoken Language Translation4 as in-domain data. The general domain data consists of several corpora. The COSMAT scientific thesis abstracts (Lambert et al., 2012) and the news-commentary-v8 corpus, provided by the ACL 2013 8th Workshop on Statistical Machine Translation5 (WMT), are directly added to the baseline without instance selection due to their small size. The large corpora on which data selection is performed, are the Europarl-v7 corpus (also provided by WMT), the JRC-Acquis corpus (Steinberger et al., 2006) and the Open Subtitles corpus6 (Tiedemann, 2012). Data statistics for the complete in-domain and out-of-domain data are given in Table 1. For the development and test sets we selected four video lectures each, that were manually transcribed and professionally translated, resulting in a total of 1013 and 1360 sentences for development and test, respectively. In addition to the target side of the bilingual data, we leverage large amounts of monolingual resources for language model training. These include the Common Crawl Corpus, the 109 French-English corpus, the UN corpus and the News Crawl articles, available from the WMT 1 http://videolect"
2014.amta-researchers.15,2006.iwslt-papers.3,0,0.0243411,"on techniques in Section 3. Section 4 gives an account of the statistical translation system used in our experiments. Finally, the experimental setup and results are discussed in Section 5 and we conclude with Section 6. 2 Domain Adaptation Domain adaptation can be performed in different ways: using lightly-supervised approaches, model combination/update or data selection. 2.1 Lightly-supervised approaches A common way to adapt a statistical machine translation model is to use lightly-supervised approaches. These approaches aim to self-enhance the translation model. This was first proposed by Ueffing (2006) and refined by Ueffing et al. (2007). The main idea is to filter the translations with the translated test data. This process involves a confidence measure in order to select the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 194 most reliable data to train a small additional phrase table (PT). The generic and the new phrase tables are used jointly for translation, which can be seen as a mixture model with one specific PT built for each test set. The lightly-supervised training approach proposed by Schwenk (2008) does not adapt the mode"
2014.amta-researchers.15,P07-1004,0,0.0256273,"ction 4 gives an account of the statistical translation system used in our experiments. Finally, the experimental setup and results are discussed in Section 5 and we conclude with Section 6. 2 Domain Adaptation Domain adaptation can be performed in different ways: using lightly-supervised approaches, model combination/update or data selection. 2.1 Lightly-supervised approaches A common way to adapt a statistical machine translation model is to use lightly-supervised approaches. These approaches aim to self-enhance the translation model. This was first proposed by Ueffing (2006) and refined by Ueffing et al. (2007). The main idea is to filter the translations with the translated test data. This process involves a confidence measure in order to select the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 194 most reliable data to train a small additional phrase table (PT). The generic and the new phrase tables are used jointly for translation, which can be seen as a mixture model with one specific PT built for each test set. The lightly-supervised training approach proposed by Schwenk (2008) does not adapt the model to the test data, but it proposes t"
2014.amta-researchers.15,C12-3061,1,0.813971,"a is selected based on infrequent n-gram recovery and part is selected with TM model cross-entropy. This way, we hope to benefit from the new information introduced by the first while reinforcing a domain-specific distribution at the same time. In practice we start with the maximum amount of data selected by infrequent n-gram recovery. On top of this, we now add increasing amounts of data selected by TM model cross-entropy, until the full general domain data has been added. 4 Statistical Translation System We use the standard phrase-based translation decoder from the open source toolkit Jane (Wuebker et al., 2012) for all translation experiments. The translation process is framed as a loglinear combination of models, which is a generalization of the source-channel paradigm introˆ duced by Brown et al. (1993). The decoder searches for the best translation eˆI1 as defined by the I K J models hm (e1 , s1 , f1 ). It can be written as (Och and Ney, 2004) ( ˆ eˆI1 = arg max I,eI1 M X ) J λm hm (eI1 , sK 1 , f1 ) , (9) m=1 where f1J = f1 . . . fJ is the source sentence, eI1 = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. The feature functions hm include transl"
2014.amta-researchers.15,D13-1138,1,0.836368,"models hm (e1 , s1 , f1 ). It can be written as (Och and Ney, 2004) ( ˆ eˆI1 = arg max I,eI1 M X ) J λm hm (eI1 , sK 1 , f1 ) , (9) m=1 where f1J = f1 . . . fJ is the source sentence, eI1 = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. The feature functions hm include translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model, phrase and word penalty, a jump-distance-based distortion model, a hierarchical orientation model (Galley and Manning, 2008) and an n-gram cluster language model (Wuebker et al., 2013). The log-linear feature weights λm are optimized on a development data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 199 5 Experiments In this section we describe the different experiments we made in order to compare between the approaches. 5.1 The VideoLectures.NET Repository VideoLectures.NET1 is a free and open access repository of video lectures mostly filmed by people from the Jožef Stefan Institute (JSI, Sloven"
2014.amta-researchers.15,P02-1040,0,\N,Missing
2014.amta-researchers.15,D08-1076,0,\N,Missing
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2020.acl-main.146,W18-6318,0,0.0443468,"onditioned on the position of the previous alignment link. While simpler and faster tools exist such as FastAlign (Dyer et al., 2013), which is based on a reparametrization of IBM Model 2, the GIZA++ implementation of Model 4 is still used today in applications where alignment quality is important. In contrast to GIZA++, our neural approach is easy to integrate on top of an attention-based translation network, has a training pipeline with fewer steps, and leads to superior alignment quality. 2.2 Neural Models Most neural alignment approaches in the literature, such as Tamura et al. (2014) and Alkhouli et al. (2018), rely on alignments generated by statistical systems that are used as supervision for training the neural systems. These approaches tend to learn to copy the alignment errors from the supervising statistical models. Zenkel et al. (2019) use attention to extract alignments from a dedicated alignment layer of a neural model without using any output from a statistical aligner, but fail to match the quality of GIZA++. Garg et al. (2019) represents the current state of the art in word alignment, outperforming GIZA++ by training a single model that is able to both translate and align. This model is"
2020.acl-main.146,D16-1162,0,0.0335281,"on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typica"
2020.acl-main.146,J93-2003,0,0.137104,"sferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is"
2020.acl-main.146,P11-1043,1,0.765311,"approach that shares most parameters with a neural translation model can potentially take advantage of improvements to the underlying translation model, for example from domain adaptation via fine-tuning. Figure 1: Word alignment generated by a human annotator. tions, we infer a symmetrized attention matrix that jointly optimizes the likelihood of the correct output words under both models in both languages. Ablation experiments highlight the effectiveness of this novel extension, which is reminiscent of agreement-based methods for statistical models (Liang et al., 2006; Grac¸a et al., 2008; DeNero and Macherey, 2011). End-to-end experiments show that our system is the first to consistently yield higher alignment quality than GIZA++ using a fully unsupervised neural model that does not use the output of a statistical alignment model in any way. 2 2.1 Related Work Statistical Models Statistical alignment models directly build on the lexical translation models of Brown et al. (1993), known as the IBM models. The most popular statistical alignment tool is GIZA++ (Och and Ney, 2000b, 2003; Gao and Vogel, 2008). For optimal performance, the training pipeline of GIZA++ relies on multiple iterations of IBM Model"
2020.acl-main.146,N13-1073,0,0.834775,"annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias"
2020.acl-main.146,2015.mtsummit-papers.11,0,0.0250753,"not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several"
2020.acl-main.146,D19-1453,0,0.765707,"cult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage cont"
2020.acl-main.146,2013.mtsummit-wptp.9,0,0.0506155,"y integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has mult"
2020.acl-main.146,2005.iwslt-1.8,0,0.626432,"IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural translation model can poten"
2020.acl-main.146,P19-1124,0,0.0615851,"n are both salient in the input to the attention component. 3.4 A state-of-the-art Transformer includes multiple attention heads whose context vectors are stacked to form the context activation for a layer, and the encoder and decoder have multiple layers. For all experiments, we use a downscaled Transformer model trained for translation with a 6-layer encoder, a 3-layer decoder, and 256-dimensional hidden states and embedding vectors. Alignment Layer Attention Optimization Extracting alignments with attention-based models works well when used in combination with greedy translation inference (Li et al., 2019). However, the alignment task involves predicting an alignment between a sentence and an observed translation, which requires forced decoding. When a token in the target sentence is unexpected given the preceding target prefix, attention activations computed 1607 Attention Optimization Word Alignment Layer Alignment Layer Word Softmax A Guided Loss Softmax Linear Linear K ApplyAtt A V Linear Q Self Attention Linear Softmax AL Linear Softmax AL CalcAttLogits CalcAttLogits Q K Linear Linear Encoder Decoder Input Emb. Output Emb. E Encoder Decoder Input Emb. Output Emb. Figure 3: Alignment layer"
2020.acl-main.146,N06-1014,0,0.166445,"al Linguistics Moreover, our fully neural approach that shares most parameters with a neural translation model can potentially take advantage of improvements to the underlying translation model, for example from domain adaptation via fine-tuning. Figure 1: Word alignment generated by a human annotator. tions, we infer a symmetrized attention matrix that jointly optimizes the likelihood of the correct output words under both models in both languages. Ablation experiments highlight the effectiveness of this novel extension, which is reminiscent of agreement-based methods for statistical models (Liang et al., 2006; Grac¸a et al., 2008; DeNero and Macherey, 2011). End-to-end experiments show that our system is the first to consistently yield higher alignment quality than GIZA++ using a fully unsupervised neural model that does not use the output of a statistical alignment model in any way. 2 2.1 Related Work Statistical Models Statistical alignment models directly build on the lexical translation models of Brown et al. (1993), known as the IBM models. The most popular statistical alignment tool is GIZA++ (Och and Ney, 2000b, 2003; Gao and Vogel, 2008). For optimal performance, the training pipeline of G"
2020.acl-main.146,W03-0301,0,0.455216,"Missing"
2020.acl-main.146,W17-4804,0,0.522781,"Missing"
2020.acl-main.146,C00-2163,0,0.624383,"her underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural trans"
2020.acl-main.146,P00-1056,0,0.900563,"her underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural trans"
2020.acl-main.146,J03-1002,0,0.151023,"for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment"
2020.acl-main.146,P16-1162,0,0.192525,"Missing"
2020.acl-main.146,D19-1084,0,0.12189,"Missing"
2020.acl-main.146,P14-1138,0,0.46193,"each alignment link is conditioned on the position of the previous alignment link. While simpler and faster tools exist such as FastAlign (Dyer et al., 2013), which is based on a reparametrization of IBM Model 2, the GIZA++ implementation of Model 4 is still used today in applications where alignment quality is important. In contrast to GIZA++, our neural approach is easy to integrate on top of an attention-based translation network, has a training pipeline with fewer steps, and leads to superior alignment quality. 2.2 Neural Models Most neural alignment approaches in the literature, such as Tamura et al. (2014) and Alkhouli et al. (2018), rely on alignments generated by statistical systems that are used as supervision for training the neural systems. These approaches tend to learn to copy the alignment errors from the supervising statistical models. Zenkel et al. (2019) use attention to extract alignments from a dedicated alignment layer of a neural model without using any output from a statistical aligner, but fail to match the quality of GIZA++. Garg et al. (2019) represents the current state of the art in word alignment, outperforming GIZA++ by training a single model that is able to both transla"
2020.acl-main.146,2011.eamt-1.10,0,0.0337482,"odel in a manner that is tightly integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017)."
2020.acl-main.146,C96-2141,0,0.944322,"ance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational L"
2020.acl-main.146,W08-0509,0,\N,Missing
2021.findings-emnlp.299,J93-2003,0,0.136384,"Missing"
2021.findings-emnlp.299,2020.emnlp-main.42,0,0.011746,"slation system in the forward and backward direction. We then extract alignments using bidirectional attention optimization. We follow the hyperparameter settings of Zenkel et al. (2020): 6 encoder and 3 decoder layers with a layer dimension of 256. Finally, we train a guided alignment layer on top of the existing translation model in the forward direction. In contrast to Zenkel et al. (2020), we additionally shift the attention by one 5 Scripts to reproduce this setup are available at https://github.com/lilt/markup-transfer-scripts. unit to the right using the “SHIFT-ATT” method described by Chen et al. (2020), which resulted in higher quality alignments. We finally generate attention distributions from the guided alignment layer and extract alignments based on the attention. To extract alignments, for each target token we select the source token with the highest attention value as its alignment link. This method, which is commonly used across neural alignment systems (Garg et al., 2019; Zenkel et al., 2019), does not produce any unaligned target tokens and produces more alignment links than FastAlign. 6.4 Supervised Markup Transfer: Sequence-to-Sequence Model The sequence-to-sequence markup transf"
2021.findings-emnlp.299,N13-1073,0,0.0340796,"expresses the token-level correspondence between a source sentence and its target translation. Tokens can be words, individual characters or subwords. Our experiments align subwords to minimize alignment error rate (Zenkel et al., 2020). Let si and tj represent the ith token in the source sentence and the jth token in its translation, respectively. The number of tokens of the source sentence and its translation are I and J. Additionally, let A(si ) ⊆ {1, . . . , J} define the alignments of the ith source token to a set of target tokens. In this work, we compare the popular FastAlign toolkit (Dyer et al., 2013), a statistical aligner, to a state-of-the-art neural alignment approach described by Zenkel et al. (2020) based on the Transformer architecture. 4.2 Min-Max Tag Pair Projection As a baseline markup transfer algorithm we implement the approach described by Hanneman and https://github.com/lilt/markup-tag-evaluation. 2 Dinu (2020), which we call the Min-Max algorithm. In case of ambiguity due to multiple tags with the same label, each reference tag is matched with a unique hypothesis Each tag pair in the source sentence spans multiple tag in a way that maximizes accuracy. contiguous source token"
2021.findings-emnlp.299,D19-1453,0,0.0991263,"sfer markup into this text. This paper describes approaches to automating the sec- 2 Related Work ond step in this workflow by automatically transferring source markup into a fixed reference trans- Recent work in neural word alignment has indilation. This fixed reference may not be preferred cated that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation mod"
2021.findings-emnlp.299,2020.wmt-1.138,0,0.0260092,"o approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as input a source sentence with XML markup and directly generates a translation that includes XML tags. A translation training set that includes markup can either be created by human translators (Hashimoto et al., 2019) or synthesized by adding markup to an existing unformatted bitext (Hanneman and Dinu, 2020). A translation model that generates both text and markup may prefer an output sequence for which the XML markup is invalid (e.g. there might be an opening tag that is not closed). This problem can be addressed through XML-constrained beam search (Hashimoto et al., 2019). This approach requires training data that contains XML markup. The second approach is to train the translation model without markup, separately train a word aligner, and then transfer format using an inference pipeline. After the translation model has generated a text translation, the alignment model aligns the tokens of the"
2021.findings-emnlp.299,W19-5212,0,0.323417,"ted that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as input a source sentence with XML markup and directly generates a translation that includes XML tags. A translation training set that includes markup can either be created by human translators (Hashimoto et al., 2019) or synthesized by adding markup to an e"
2021.findings-emnlp.299,2020.findings-emnlp.147,0,0.0168766,"s approaches to automating the sec- 2 Related Work ond step in this workflow by automatically transferring source markup into a fixed reference trans- Recent work in neural word alignment has indilation. This fixed reference may not be preferred cated that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as inpu"
2021.findings-emnlp.299,P18-1007,0,0.0132837,"onsisting of approximately 100k segments, a validation set of 2k segments and an unreleased test set. One fourth of the segments in both the training and validation set contain at least one markup tag. We hold out 1k segments of the training set for early stopping, use the remaining segments for training and the validation set for testing. Only a fixed set of 14 different opening and closing markup tags appear in the dataset, each of these tag pairs spanning one or more characters. 6.2 Tokenization We use byte pair encoding (BPE) (Sennrich et al., 2016) computed via the SentencePiece toolkit (Kudo, 2018), and follow the setup described by Hashimoto et al. (2019) for subword tokenization. We add all tags and the separator token used for the input of the sequence-to-sequence model as user-defined symbols. In contrast to Hashimoto et al. (2019), we also add all punctuation marks to this set. These symbols will not be split or merged by the SentencePiece toolkit and are always represented as a single token. We learn a joint subword vocabulary of 10k tokens for each language pair and use this tokenization for both the supervised sequence-to-sequence model and the unsupervised alignment systems. Ze"
2021.findings-emnlp.299,W17-4804,0,0.0172484,"generated a text translation, the alignment model aligns the tokens of the source segment to the generated translation. Finally, a deterministic algorithm (labeled Min-Max in Section 4.2) transfers the markup from the source segment into the translation via the word alignments (Hanneman and Dinu, 2020). This approach does not require training data that contains XML markup. Figure 1: Two nested tag pairs that have similar tag positions. Tag pair 1 is the parent of tag pair 2. segments (Hashimoto et al., 2019). Past work has also included manual evaluation of the transferred markup information (Müller, 2017; Hanneman and Dinu, 2020), since transfer accuracy could not be assessed directly. In contrast, our goal is to transfer markup directly into the reference translation. Evaluation of markup accuracy is therefore straightforward: a tag is placed correctly if it appears at the correct character position within the reference translation. 3 Bilingual Markup Transfer In this section we introduce tag pairs, the data structure with which we represent markup information, and define two evaluation metrics. 3.1 Definition We represent all markup information as tag pairs. A tag pair contains an opening a"
2021.findings-emnlp.299,2020.emnlp-main.41,0,0.0223022,"Missing"
2021.findings-emnlp.299,P02-1040,0,0.110985,"ers from the reference by more than just markup. characters that appear before the tag in the sentence, Instead, automatic metrics such as XML accuracy not including any other tags. In contrast to the check that all source tags appear in the target and opening tag, the label of a closing tag contains are properly nested. XML-based BLEU splits the a forward slash (e.g. &lt;/b&gt;). There are no selftranslation at every formatting tag both for the ref- closing tags in this representation. A TagPair erence and the translation and calculates the BLEU has a parent if there is another TagPair that score (Papineni et al., 2002) on the resulting sub- encloses it. 3525 3.2 Metrics The following two metrics1 score a proposed set of tags that are well-formed XML (properly nested with each opening tag closed) in which every source tag pair appears exactly once in the target. Let L be the character length of the reference translation. In the following we denote the character position of a tag as p ∈ {0, ..., L}. We start by matching the reference and hypothesis tags by their label. Therefore, let T = {(pr , ph )} ∈ {0, ..., L} × {0, ..., L} be the set of tuples of all reference and hypothesis character-level positions, an"
2021.findings-emnlp.299,P16-1162,0,0.0195699,"s aligned segments. The data set is split into a training set consisting of approximately 100k segments, a validation set of 2k segments and an unreleased test set. One fourth of the segments in both the training and validation set contain at least one markup tag. We hold out 1k segments of the training set for early stopping, use the remaining segments for training and the validation set for testing. Only a fixed set of 14 different opening and closing markup tags appear in the dataset, each of these tag pairs spanning one or more characters. 6.2 Tokenization We use byte pair encoding (BPE) (Sennrich et al., 2016) computed via the SentencePiece toolkit (Kudo, 2018), and follow the setup described by Hashimoto et al. (2019) for subword tokenization. We add all tags and the separator token used for the input of the sequence-to-sequence model as user-defined symbols. In contrast to Hashimoto et al. (2019), we also add all punctuation marks to this set. These symbols will not be split or merged by the SentencePiece toolkit and are always represented as a single token. We learn a joint subword vocabulary of 10k tokens for each language pair and use this tokenization for both the supervised sequence-to-seque"
2021.findings-emnlp.299,2020.acl-main.146,1,0.916657,"e apply a twostep process. First we use an unsupervised aligner to infer the alignments between source and target subwords. The second step uses a deterministic algorithm to place tag pairs based on these alignments. Two advantages of this unsupervised approach are that it does not require training data with markup, and it can leverage any word aligner. 4.1 Alignments An alignment expresses the token-level correspondence between a source sentence and its target translation. Tokens can be words, individual characters or subwords. Our experiments align subwords to minimize alignment error rate (Zenkel et al., 2020). Let si and tj represent the ith token in the source sentence and the jth token in its translation, respectively. The number of tokens of the source sentence and its translation are I and J. Additionally, let A(si ) ⊆ {1, . . . , J} define the alignments of the ith source token to a set of target tokens. In this work, we compare the popular FastAlign toolkit (Dyer et al., 2013), a statistical aligner, to a state-of-the-art neural alignment approach described by Zenkel et al. (2020) based on the Transformer architecture. 4.2 Min-Max Tag Pair Projection As a baseline markup transfer algorithm w"
C12-2091,N10-1033,0,\N,Missing
C12-2091,D12-1041,0,\N,Missing
C12-2091,E03-1076,0,\N,Missing
C12-2091,C08-1064,0,\N,Missing
C12-2091,P02-1040,0,\N,Missing
C12-2091,W10-1738,1,\N,Missing
C12-2091,P10-1049,1,\N,Missing
C12-2091,W06-3123,0,\N,Missing
C12-2091,2006.amta-papers.2,0,\N,Missing
C12-2091,D12-1089,0,\N,Missing
C12-2091,C10-2021,0,\N,Missing
C12-2091,W06-3105,0,\N,Missing
C12-2091,P05-1033,0,\N,Missing
C12-2091,J03-1002,1,\N,Missing
C12-2091,2009.iwslt-papers.2,0,\N,Missing
C12-2091,P07-1019,0,\N,Missing
C12-2091,2010.iwslt-papers.11,1,\N,Missing
C12-2091,D08-1076,0,\N,Missing
C12-2091,P08-1024,0,\N,Missing
C12-3061,J99-4005,0,\N,Missing
C12-3061,N10-1140,0,\N,Missing
C12-3061,D09-1022,1,\N,Missing
C12-3061,C04-1030,1,\N,Missing
C12-3061,W10-1738,1,\N,Missing
C12-3061,D08-1024,0,\N,Missing
C12-3061,P12-3004,0,\N,Missing
C12-3061,N09-1027,0,\N,Missing
C12-3061,P10-1049,1,\N,Missing
C12-3061,P07-2045,0,\N,Missing
C12-3061,N09-1025,0,\N,Missing
C12-3061,J06-4004,0,\N,Missing
C12-3061,N03-1017,0,\N,Missing
C12-3061,P02-1038,1,\N,Missing
C12-3061,2008.iwslt-papers.8,1,\N,Missing
C12-3061,P10-4002,0,\N,Missing
C12-3061,2008.iwslt-papers.7,1,\N,Missing
C12-3061,P07-1019,0,\N,Missing
C12-3061,P12-2006,1,\N,Missing
C12-3061,W06-3119,0,\N,Missing
C12-3061,2010.iwslt-papers.18,1,\N,Missing
C12-3061,2011.iwslt-papers.8,1,\N,Missing
C12-3061,J07-2003,0,\N,Missing
C12-3061,N10-2003,0,\N,Missing
C12-3061,D07-1080,0,\N,Missing
C12-3061,2009.eamt-1.33,1,\N,Missing
C12-3061,P03-1021,0,\N,Missing
C12-3061,2012.eamt-1.66,1,\N,Missing
D13-1138,E99-1010,0,\N,Missing
D13-1138,D08-1089,0,\N,Missing
D13-1138,P02-1040,0,\N,Missing
D13-1138,W10-1738,1,\N,Missing
D13-1138,D07-1091,0,\N,Missing
D13-1138,W12-3125,0,\N,Missing
D13-1138,J04-4002,1,\N,Missing
D13-1138,N03-1017,0,\N,Missing
D13-1138,J03-1002,1,\N,Missing
D13-1138,C12-3061,1,\N,Missing
D13-1138,2010.iwslt-evaluation.11,0,\N,Missing
D13-1138,W04-3250,0,\N,Missing
D13-1138,P03-1021,0,\N,Missing
D13-1138,N13-1003,0,\N,Missing
D14-1003,D13-1106,0,0.319897,"ks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into the hidden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. Auli et al. (2013) also include source sentence representations built either using Latent Semantic Analysis or by concatenating word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our proposed bidirectional models include the full source sentence relying on recurrency, yielding improvements over competitive baselines already including a recurrent language model. RNNs were also used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases"
D14-1003,1997.mtsummit-plenaries.5,0,0.404519,"Missing"
D14-1003,E14-1003,0,0.631886,"Translation Modeling with Bidirectional Recurrent Neural Networks Martin Sundermeyer1 , Tamer Alkhouli1 , Joern Wuebker1 , and Hermann Ney1,2 1 Human Language Technology and Pattern Recognition Group RWTH Aachen University, Aachen, Germany 2 Spoken Language Processing Group Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France {surname}@cs.rwth-aachen.de Abstract on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel approaches to recurrent neural translation modeling: wordbased and phrase-based. The word-based approach assumes one-to-one aligned source and target sentences. We evaluate different ways of resolving alignment ambiguities to obtain such alignments. The phrase-based RNN approach is more closely tied to the underlying translation paradigm. It models actual phrasal translation probabilities while avoiding sparsity issues by using single words as input and output units. Furthermore, in addition to the unidirectional formulation, we are the first to propose a b"
D14-1003,1997.tmi-1.19,0,0.186958,"Missing"
D14-1003,W13-2258,1,0.865603,"Missing"
D14-1003,D13-1176,0,0.131366,"al networks in machine translation (MT) are presented in (Casta˜no et al., 1997; Casta˜no and Casacuberta, 1997; Casta˜no and Casacuberta, 1999), where they were used for example-based MT. Recently, Le et al. (2012) presented translation models using an output layer with classes and a shortlist for rescoring using feedforward networks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into the hidden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. Auli et al. (2013) also include source sentence representations built either using Latent Semantic Analysis or by concatenating word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our proposed bidirectional models include the f"
D14-1003,P14-1129,0,0.656153,"−1 1 , f1 class layer  I p ei |c(ei ), ei−1 1 , f1  data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists. To confirm our results, we run additional experiments on the Arabic→English and Chinese→English tasks of the DARPA BOLT project. In both cases, the neural network models are added on top of our most competitive evaluation system"
D14-1003,N03-1017,0,0.150988,"Missing"
D14-1003,D08-1089,0,0.200815,"Missing"
D14-1003,N12-1005,0,0.204244,"large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. In this Section we contrast previous work to ours, where we design RNNs to model bilingual dependencies, which are applied to rerank n-best lists after decoding. To the best of our knowledge, the earliest attempts to apply neural networks in machine translation (MT) are presented in (Casta˜no et al., 1997; Casta˜no and Casacuberta, 1997; Casta˜no and Casacuberta, 1999), where they were used for example-based MT. Recently, Le et al. (2012) presented translation models using an output layer with classes and a shortlist for rescoring using feedforward networks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into"
D14-1003,P10-2041,0,0.188549,"Missing"
D14-1003,D13-1140,0,0.138454,"Missing"
D14-1003,P03-1021,0,0.552689,"Missing"
D14-1003,W10-1738,1,0.474078,"Missing"
D14-1003,P02-1040,0,0.0930684,"Missing"
D14-1003,P10-1049,1,0.701202,"from the example sentence. For brevity, we omit the precise handling of sentence begin and end tokens. the last word of the source phrase “Beispiel” being presented to the network, the full source phrase is stored in the hidden layer, and the neural network is then trained to predict the target phrase words at the output layer. Subsequently, the source input is ε, and the target input is the most recent target side history word. To obtain a phrase-aligned training sequence for the phrase-based RNN models, we force-align the training data with the application of leave-one-out as described in (Wuebker et al., 2010). presentation of the source side is finished we start estimating probabilities for the target side. Therefore, we do not let the neural network learn a target distribution until the very last source word is considered. In this way, we break up the conventional RNN training scheme where an input sample is directly followed by its corresponding teacher signal. Similarly, the presentation of the source side of the next phrase only starts after the prediction of the current target side is completed. To this end, we introduce a no-operation token, denoted by ε, which is not part of the vocabulary"
D14-1003,P06-2093,0,0.0838438,"Missing"
D14-1003,C12-3061,1,0.409628,"Missing"
D14-1003,C12-2104,0,0.155737,"n units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our proposed phrase-based models maintain word order within phrases, but the phrases are processed in a wordpair manner, while the phrase boundaries remain implicitly encoded in the way the words are presented to the network. Schwenk (2012) proposed a feedforward network that predicts phrases of a 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart"
D14-1003,D13-1138,1,0.896862,"Missing"
D14-1003,2006.amta-papers.25,0,0.176983,"Missing"
D14-1003,W12-2703,0,\N,Missing
D14-1003,P04-1013,0,\N,Missing
D15-1123,2010.amta-papers.16,0,0.0507827,"Missing"
D15-1123,2013.mtsummit-papers.5,0,0.089191,"177.6 58.5 + genre TM + doc. weights + sparse features 14.1 9.8 5.8 Table 2: Decoding speed on the lecture data. 2 0 -2 2 4 6 8 10 12 14 i-th sentence in document 16 18 20 Figure 2: Bleu difference between baseline + genre weights and our incremental adaptation approach, computed on a single segment from each document according to their order, i.e. the first segment from each document, then the second segment from each document, etc. large improvement in this genre would only be expected to arise in similarly structured domains. This property is quantified by the repetition rate measure (RR) (Bertoldi et al., 2013) reported in Table 1, which confirms the finding by Cettolo et al. (2014) that RR correlates with the effectiveness of adaptation. Analysis. Figure 2 shows Bleu score differences to the baseline + genre weights system for different subsets of the news and patent test sets. Each point is computed by document slicing, i.e. on a single segment from each document. The rightmost data point is the Bleu score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus. Note that this group does not correspond to any number in Table 1, which reports Bleu on the entire te"
D15-1123,2011.eamt-1.5,0,0.0230876,"lue phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For"
D15-1123,2014.amta-researchers.13,0,0.066112,"2: Decoding speed on the lecture data. 2 0 -2 2 4 6 8 10 12 14 i-th sentence in document 16 18 20 Figure 2: Bleu difference between baseline + genre weights and our incremental adaptation approach, computed on a single segment from each document according to their order, i.e. the first segment from each document, then the second segment from each document, etc. large improvement in this genre would only be expected to arise in similarly structured domains. This property is quantified by the repetition rate measure (RR) (Bertoldi et al., 2013) reported in Table 1, which confirms the finding by Cettolo et al. (2014) that RR correlates with the effectiveness of adaptation. Analysis. Figure 2 shows Bleu score differences to the baseline + genre weights system for different subsets of the news and patent test sets. Each point is computed by document slicing, i.e. on a single segment from each document. The rightmost data point is the Bleu score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus. Note that this group does not correspond to any number in Table 1, which reports Bleu on the entire test sets. Thus, we evaluate on all sentences that have learned from exactl"
D15-1123,D13-1107,0,0.0179591,"cators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre,"
D15-1123,P07-1033,0,0.305837,"Missing"
D15-1123,E14-1042,0,0.352159,"o levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example, our extension adjusts off"
D15-1123,W10-1757,0,0.137447,"lation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and den"
D15-1123,N09-1068,0,0.083889,"Missing"
D15-1123,P13-1031,1,0.904175,"ible hierarchical domain structure within a single consistent model. Both weights and rules are updated incrementally on a stream of post-edits. Our multi-level domain hierarchy allows the system to adapt simultaneously towards local context at different levels of granularity, including genres and individual documents. Our experiments show consistent improvements in translation quality from all components of our approach. 1 Introduction Suggestions from a machine translation system can increase the speed and quality of professional human translators (Guerberof, 2009; Plitt and Masselot, 2010; Green et al., 2013a, inter alia). However, querying a single fixed model for all different documents fails to incorporate contextual information that can potentially improve suggestion quality. We describe a model architecture that adapts simultaneously to multiple genres and individual documents, so that translation suggestions are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of"
D15-1123,W14-3311,1,0.910621,"Missing"
D15-1123,W14-3360,1,0.877624,"m rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example, our extension adjusts offsets to a fixed baseline system. In this way, the system can adapt to multiple genres while preventing cross-genre contamination. In large-scale experiments, we adapt a multigenre baseline system to patents, lectures, and news articles. Our experiments show that sparse models, hierarchical updates, and rule adaptation all contribute consistent improvements. We observe quality gains in all genres, validating our hypothesis that document and genre context are important additional inputs to a machine translation"
D15-1123,2010.amta-papers.21,0,0.55906,"ection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and dense feature estimation.1 To translate the ith sentence fi , the system uses weights wi−1 and corpus ci−1 . The new corpus ci results from adding (fi , e∗i ) to ci−1"
D15-1123,P13-2121,0,0.0566368,"Missing"
D15-1123,N03-1017,0,0.00683929,". where f ∈ F is a string in the set of all source language strings F, e ∈ E is a string in the set of all target language strings E, r is a phrasal derivation with source and target projections src(r) and tgt(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropriate normalizing constant. During search, the maximum approximation is applied rather than summing over the derivations r. Model. We extend a phrase-based system for which φ(r; c) includes 16 dense features: • Two phrasal channel models and two lexical channel models (Koehn et al., 2003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Lian"
D15-1123,P07-2045,0,0.00955131,"t(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropriate normalizing constant. During search, the maximum approximation is applied rather than summing over the derivations r. Model. We extend a phrase-based system for which φ(r; c) includes 16 dense features: • Two phrasal channel models and two lexical channel models (Koehn et al., 2003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The"
D15-1123,W04-3250,0,0.0871522,"Missing"
D15-1123,N10-1062,0,0.064019,"ific corpus that includes all sentence pairs from the tuning corpus as well as from the adaptation corpus (fj , e∗j ) with j < i sharing fi ’s genre. We refer to this combined corpus as ci . The tuning corpus is the same that is used for parameter tuning in the baseline system. The adaptation corpus is our test set. Note that in our evaluation, each sentence is translated before it is used for adaptation, so that there is no contamination of results. In order to extend the model efficiently within a streaming data environment, we make use of a suffix-array implementation for our phrase table (Levenberg et al., 2010). Rather than combining corpus counts across these different sources, separate rules extracted from the baseline corpus and the genre-specific corpus exist independently in the derivation space, and features of each are computed only with one corpus. In this configuration, a large amount of outof-domain evidence from the baseline model will not dampen the feature value adaptation effects of adding new sentence pairs from the adaptation corpus. The genre-specific phrases are distinguished by an additional binary provenance feature. In order to extract features from the genrespecific corpus, a w"
D15-1123,P06-1096,0,0.0385464,"003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also"
D15-1123,W13-2237,0,0.501967,"ns are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example,"
D15-1123,J03-1002,0,0.00999119,"t ideal for observing the effects of incremental adaptation techniques. To train the language and translation model we additionally leveraged all available bilingual and monolingual data provided for the EMNLP 2015 Tenth Workshop on Machine Translation3 . The total size of the bitext used for rule extraction and feature estimation was 6.4M sentence pairs. We trained a standard 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) using the KenLM toolkit (Heafield et al., 2013) on 4 billion running words. The bitext was word-aligned with mgiza (Och and Ney, 2003), and we used the phrasal decoder (Green et al., 2014a) with standard GermanEnglish settings for experimentation. Our second set of experiments was performed on a mixed-genre corpus containing lectures, patents, and news articles. The standard dev and test sets of the IWSLT 2014 shared task4 were used for the lecture genre. Each document corresponded to an entire lecture. For the news genre, we used newstest2012 for tuning, newstest2013 for metaparameter optimization, and newstest2014 for testing. The tune set for the patent genre is identical to the first set of experiments, while the test se"
D15-1123,J04-4002,0,0.107292,"cross-genre contamination. In large-scale experiments, we adapt a multigenre baseline system to patents, lectures, and news articles. Our experiments show that sparse models, hierarchical updates, and rule adaptation all contribute consistent improvements. We observe quality gains in all genres, validating our hypothesis that document and genre context are important additional inputs to a machine translation system used for post-editing. 2 Background The log-linear appoach to statistical machine translation models the predictive translation distribution p(e|f ; w) directly in log-linear form (Och and Ney, 2004): h i X 1 p(e|f ; w) = exp w&gt; φ(r; c) (1) Z(f ) r: src(r)=f tgt(r)=e 1059 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1059–1065, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. where f ∈ F is a string in the set of all source language strings F, e ∈ E is a string in the set of all target language strings E, r is a phrasal derivation with source and target projections src(r) and tgt(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropria"
D15-1123,N10-1079,0,0.318148,"single fixed model for all different documents fails to incorporate contextual information that can potentially improve suggestion quality. We describe a model architecture that adapts simultaneously to multiple genres and individual documents, so that translation suggestions are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et"
D15-1123,P13-1082,0,0.0188518,"rse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains:"
D15-1123,W13-2236,0,0.155725,"tive lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure"
D15-1123,P12-1002,0,0.0376166,"c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothes"
D15-1123,2011.mtsummit-papers.33,0,0.0284313,"a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and dense feature estimatio"
D15-1123,2012.amta-papers.18,0,0.0182524,"depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root"
D15-1123,2013.mtsummit-papers.2,0,0.0257576,"ntion in the experiment cycle, simulated post-editing (Hardt and Elming, 2010; Denkowski et al., 2014) replaces each e∗ with a reference that is not a corrected variant of e. Thus, a standard test corpus can be used as an adaptation corpus. Prior work on online learning from post-edits has demonstrated the benefit of adjusting only c (Ortiz-Martínez et al., 2010; Hardt and Elming, 2010) and further benefit from adjusting both c and w (Mathur et al., 2013; Denkowski et al., 2014). Incremental adaptation of both c and the weights w for sparse features is reported to yield large quality gains by Wäschle et al. (2013).2 3 Hierarchical Incremental Adaptation Our hierachical approach to incremental adaptation uses document and genre information to adapt appropriately to multiple contexts. We assume that each sentence fi has a known set Di of domains, which identify the genre and individual document origin of the sentence. This set could be extended to include topics, individual translators, etc. Figure 1 shows the domains that we apply in experiments. All sentences in the baseline training corpus, the tuning corpus, and the adaptation corpus share a root domain. 1 For the purpose of our description, the corp"
D15-1123,2007.mtsummit-papers.68,0,0.0533315,"a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the mac"
D15-1165,W15-3034,1,0.842973,"rd reordering. In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words. As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the opposite side. The integration of the NNs into the decoder is not trivial, due to the dependence on the target context. In the case of RNNs, the context is unbounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space. Therefore, the RNN scores are computed using approximations instead (Auli et al., 2013; Alkhouli et al., 2015). In (Alkhouli et al., 2015), it is shown that approximate RNN integration into the phrase-based decoder has a slight advantage over n-best rescoring. Therefore, we apply RNNs in rescoring in this work, and to allow for a direct comparison between FFNNs and RNNs, we apply FFNNs in rescoring as well. 5 Evaluation We perform experiments on the largescale IWSLT 20132 (Cettolo et al., 2014) German→English, WMT 20153 German→English and the DARPA BOLT Chinese→English tasks. The statistics for the bilingual corpora are shown in Table 2. Word alignments are generated with the GIZA++ toolkit 1406 2 htt"
D15-1165,D13-1106,0,0.268023,"ot have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 ( f1J , eI1 , bI1 ) is a uniquely defined interpretation of a given source sentence f1J , its translation eI1 and the inverted alignment bI1 , where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mentio"
D15-1165,J90-2002,0,0.838221,"Missing"
D15-1165,J93-2003,0,0.0526068,"Missing"
D15-1165,2014.iwslt-evaluation.1,0,0.154546,"Missing"
D15-1165,2011.mtsummit-papers.30,0,0.0297545,"T German English BOLT Chinese English 4.32M 108M 109M 836K 792K 4.22M 106M 108M 814K 773K 4.08M 78M 86M 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German→English, WMT 2015 German→English, and the DARPA BOLT Chinese→English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013)."
D15-1165,P11-2031,0,0.0703101,"ch. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 × 100 nodes, the first hidden layer 1000 and the second 500. The RNNs have LSTM architectures. The URNN has 2 hidden"
D15-1165,C10-2023,0,0.0197048,"gnments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translati"
D15-1165,H05-1022,0,0.145439,"this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoot"
D15-1165,P14-1129,0,0.170502,"Missing"
D15-1165,P11-1105,0,0.0493314,"odel (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durra"
D15-1165,N13-1001,0,0.427665,"ng word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each ot"
D15-1165,P13-2071,0,0.556345,"ng word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each ot"
D15-1165,C14-1041,0,0.116443,"2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ET"
D15-1165,P13-1033,0,0.509592,"guage models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was ful"
D15-1165,P13-1032,1,0.767593,"s of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014)"
D15-1165,W14-1616,0,0.324511,"Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments i"
D15-1165,D08-1089,0,0.193694,"792K 4.22M 106M 108M 814K 773K 4.08M 78M 86M 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German→English, WMT 2015 German→English, and the DARPA BOLT Chinese→English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phra"
D15-1165,W15-3033,1,0.349271,"Missing"
D15-1165,P13-2121,0,0.0813648,"Missing"
D15-1165,E14-1003,0,0.300348,"t treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 ( f1J , eI1 , bI1 ) is a uniquely defined interpretation of a given source sentence f1J , its translation eI1 and the inverted alignment bI1 , where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mention of ( f1J , eI1 ,"
D15-1165,N03-1017,0,0.353414,"ignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). Although JTR n-gram models are closely related to the op"
D15-1165,P07-2045,0,0.00776249,"ource words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the trans"
D15-1165,N12-1005,0,0.109906,"The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 ( f1J , eI1 , bI1 ) is a uniquely defined interpretation of a given source sentence f1J , its translation eI1 and the inverted alignment bI1 , where bi denotes the ordered sequence of source positions j aligned to target position i. We drop"
D15-1165,J06-4004,0,0.152922,"Missing"
D15-1165,P10-2041,0,0.053486,"ain data, smaller n-gram sizes were used. All rescoring experiments used 1000best lists without duplicates. 5.1 Tasks description The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online4 . All systems are optimized on the dev2010 corpus, named dev here. Some of the OSM and JTR systems are trained on the TED portions of the data containing 138K sentences. To estimate the 4-gram LM, we additionally make use of parts of the Shuffled News, LDC English Gigaword and 109 -French-English corpora, selected by a cross-entropy difference criterion (Moore and Lewis, 2010). In total, 1.7 billion running words are taken for LM training. The BOLT Chinese→English task is evaluated on the “discussion forum” domain. The 5-gram LM is trained on 2.9 billion running words in total. The in-domain data consists of a subset of 67.8K sentences and we used a set of 1845 sentences for tuning. The evaluation set test1 contains 1844 and test2 1124 sentences. For the WMT task, we used the target side of the bilingual data and all monolingual data to train a pruned 5-gram LM on a total of 4.4 billion running words. We concatenated the newstest2011 and newstest2012 corpora for tu"
D15-1165,J03-1002,1,0.0842537,"neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). Although JTR n-gram models are closely related to the operation sequence model (OSM) (Durrani et al., 2013b), there are three main differences. To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases. A"
D15-1165,W99-0604,1,0.577079,"le manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). Although JTR n-g"
D15-1165,P03-1021,0,0.0614101,"lation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 × 100 nodes, the first"
D15-1165,2001.mtsummit-papers.68,0,0.0140599,"et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 × 100 nodes, the first hidden layer 1000 and the second"
D15-1165,2014.amta-researchers.3,0,0.222678,"ering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reo"
D15-1165,D14-1003,1,0.862777,"rrani et al., 2013b). Experimental results confirm that this simplification does not make JTR models less expressive, as their performance is on par with the OSM. Due to data sparsity, increasing the n-gram order of count-based models beyond a certain point becomes useless. To address this, we resort to neu1401 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1401–1411, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. ral networks (NNs), as they have been successfully applied to machine translation recently (Sundermeyer et al., 2014; Devlin et al., 2014). They are able to score any word combination without requiring additional smoothing techniques. We experiment with feed-forward and recurrent translation networks, benefiting from their smoothing capabilities. To this end, we split the linear sequence into two sequences for the neural translation models to operate on. This is possible due to the simplicity of the JTR sequence. We show that the count and NN models perform well on their own, and that combining them yields even better results. In this work, we apply n-gram models with modified Kneser-Ney smoothing during ph"
D15-1165,N04-4026,0,0.0982156,"ved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decode"
D15-1165,D13-1138,1,0.861827,"384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German→English, WMT 2015 German→English, and the DARPA BOLT Chinese→English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on B LEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported B LEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescor"
D15-1165,2002.tmi-tutorials.2,0,0.0611081,"pturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). Although JTR n-gram models are clos"
D15-1165,N13-1002,0,0.0782986,"operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurren"
D15-1165,P02-1040,0,\N,Missing
D18-1104,2014.amta-researchers.13,0,0.036152,"Missing"
D18-1104,E14-1042,0,0.165274,"Missing"
D18-1104,W14-0311,0,0.0605659,"Missing"
D18-1104,W10-1757,0,0.0280409,"ova et al. (2017) apply vanilla fine-tuning algorithms. In addition to fine-tuning towards user corrections, the former applies a priori adaptation to retrieved data that is similar to the incoming source sentences. Peris et al. (2017) propose a variant of fine-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for pers"
D18-1104,D16-1139,0,0.0285815,"et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are applied to each sub-layer. A series of stacked decoder layers produces a sequence of target word vectors y1 , . . . , yn . Each decoder layer has three sub-layers: self-attention, encoder-attention, and a filter. For target position j, the self-attention layer can attend to any previous target position j 0 ∈ [1, j],"
D18-1104,2015.iwslt-evaluation.11,0,0.111581,"ers generate intermediate representations z1 , . . . , zm . Each layer of the encoder consists of two sub-layers: a multi-head selfattention layer that uses scaled dot-product attenCompact Adaptation 4.1 Fine Tuning Personalized machine translation requires batch adaptation to a domain-relevant bitext (such as a user provided translation memory) as well as incremental adaptation to the test set. We apply finetuning, which involves continuing to train model parameters with a gradient-based method on domainrelevant data, as a simple and effective method for neural translation domain adaptation (Luong and Manning, 2015). The fine-tuned model without regularization and clipping is denoted as the Full Model. Confirming previous work, we found that 882 1 https://www.tensorflow.org/ stochastic gradient descent (SGD) is the most effective optimizer for fine tuning (Turchi et al., 2017). In our experiments, batch adaptation uses a batch size of 7000 words for 10 Epochs and a fixed learning rate of 0.1, dropout of 0.1, and label smoothing with ls = 0.1 (Szegedy et al., 2016). Incremental adaptation uses a batch size of one and a learning rate of 0.01. To ensure a strong adaptation effect within a single document,"
D18-1104,P18-2050,0,0.0522982,"arning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are applied to each sub-layer. A series of stacked decoder layers produces a sequence of target word vectors y1 , . ."
D18-1104,P02-1040,0,0.103085,"ting 4 http://workshop2017.iwslt.org/ 3 and the dev2010 set (888 sentences) for testing. The overall best performing compact adaptation technique, group lasso regularization, is further evaluated on six other language pairs trained using production data sets collected from Lilt’s user base: English↔French, English↔Russian and English↔Chinese. Adaptation is performed on user data from various domains (technical manuals, finance, legal), each with 8k-10k segments for batch adaptation and 2000 segments for testing and incremental adaptation. Translation quality is evaluated using the cased Bleu (Papineni et al., 2002) measure. 5.2 Results Table 1 shows English→German results. Full model adaptation, where all offsets are stored, improves over the baseline in all cases to various degrees. This full model contains only 25.8M parameters, as offsets for both embedding matrices are stored as sparse collections of columns for the vocabulary present in the adaptation data. Next, we evaluate the impact of storing offsets only for one region at a time. We observe that among the three vocabulary matrices, the output projection Yo has the strongest impact on quality, which is not dimin884 Baseline Full Model Batch+Inc"
D18-1104,P16-1162,0,0.281616,"Missing"
D18-1104,W13-2236,1,0.845457,"essive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowl"
D18-1104,P12-1002,1,0.839561,"ne-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush,"
D18-1104,P09-1054,0,0.0419851,"d in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are a"
D18-1104,D15-1123,1,0.865103,"Missing"
N15-1175,D14-1132,0,0.555118,"being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North A"
N15-1175,P08-1024,0,0.0212667,"all development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weig"
N15-1175,J93-2003,0,0.0601724,"performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature. 1 Introduction The main advantage of learning parameters in a discriminative fashion is the possibility to directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its"
N15-1175,J04-2004,0,0.0219315,"errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weights. Model updates are carried out by the RPROP algorithm (Riedmiller and Braun, 1993). However, both approaches only improve over constrained baselines. Our work is inspired by (He and Deng, 2012; Setiawan and Zhou, 2013), where the authors propose to train the standard phrasal and lexical channel models with the growth transformation (GT) algorithm. They use n-best lists on the training data and optimize a maximum expected B LEU objective, that provides a cle"
N15-1175,D08-1024,0,0.0301702,"m expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying trans"
N15-1175,P05-1033,0,0.0663444,"training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear feature weights λm , the translation decoder ˆ searches for the b"
N15-1175,P11-2031,0,0.0801903,"s for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). The 5-gram backoff LM is in total trained on 2.9 billion running words. We use the same data for tuning and testing as Setiawan and Zhou (2013), na"
N15-1175,D08-1089,0,0.444835,"apply leave-one-out with all update strategies. 5.3 Features Maximum expected B LEU training facilitates training of arbitrary features. In this work we apply four types of features. (a) A discriminative phrase table, i.e. one feature for each phrase pair. (b) Lexical features, i.e. one feature for each source-target word pair that appear within the same phrase. (c) Source and target triplet features (Hasan et al., 2008), i.e. triples of one source and two target words or one target and two source words appearing within a single phrase pair. (d) The hierarchical lexicalized reordering model (Galley and Manning, 2008), i.e. one feature for each combination of phrase pair, orientation (monotone (M), swap (S) or discontinuous (D)) and orientation direction (forward or backward). GT is only applied with feature set (a), where we reestimate the two phrasal channel models as was done in (He and Deng, 2012). With the other update algorithms we follow the approach taken in (Auli et al., 2014) and condense each feature type into a small number of models for the log-linear combination, which is afterwards tuned with MERT. (a) and (b) result in a single additional model, (c) in two models (source and target triplets"
N15-1175,N13-1048,0,0.0164076,"ization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the standard phrasebased model. Gao and He (2013) use gradient ascent to train Markov random field models for phrase translation. These models are interpreted as undirected phrase compatibility scores rather than translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training with joint feature selection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just"
N15-1175,P13-1031,0,0.163139,"contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages"
N15-1175,W14-3360,0,0.0118293,"translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training with joint feature selection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just using a small development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used"
N15-1175,D08-1039,1,0.869947,"Missing"
N15-1175,P12-1031,0,0.0870934,"o directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence"
N15-1175,P13-2121,0,0.0909589,"Missing"
N15-1175,D11-1125,0,0.0240956,"rithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the"
N15-1175,W13-2258,1,0.854976,"Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). The 5-gram backoff LM is in total trained on 2.9 billion running words. We use the same data for tuning and testing as Setiawan and Zhou (2013), namely 1275 (tune) and 12393 sentences of web data taken from LDC2010E30, the NIST MT06 evaluation set and an additional single-reference test set from the discussion forum (df) domain containing 1124 sentence pairs. Maximum expected B LEU training is performed on the discussion forum portion of the training data, consisting of 67.8K sentence pairs. On the German→English task of the 9th Workshop on Statistical Machine Translation4 , both translation"
N15-1175,N07-1008,0,0.0157828,"ll development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the standard phrasebased model. Gao and He (2013) use gradient ascent to train Markov random field models for phrase translation. These models are interpreted as undirected phrase compatibility scores rather than translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training"
N15-1175,N03-1017,0,0.0329409,"tal comparison. (ii) For the first time, we apply maximum expected B LEU training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear"
N15-1175,W11-2168,0,0.073415,"le training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weights. Model updates are carried out by the RPROP algorithm (Riedmiller and Braun, 1993). However, both approaches only improve over constrained baselines. Our work is inspired by (He and Deng, 2012; Setiawan and Zhou, 2013), where the authors propose to train the standard phrasal and lexical channel models with the growth transformation (GT) algorithm. They use n-best lists on the training data and optim"
N15-1175,P06-1096,0,0.0900743,"Missing"
N15-1175,J06-4004,0,0.067937,"Missing"
N15-1175,P10-2041,0,0.0520213,"rpora as well as selected parts of the Shuffled News 1 Note that we keep the λ weights fixed throughout all iterations of maximum expected B LEU training. 2 http://www.iwslt2013.org Sentences Run. Words Vocabulary IWSLT German English BOLT Chinese English WMT German English 138K 2.63M 2.70M 75.4K 50.2K 4.08M 78.3M 85.9M 384K 817K 4.09M 105M 104M 659K 649K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system a"
N15-1175,J04-4002,1,0.470015,"paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear feature weights λm , the translation decoder ˆ searches for the best scoring translation E: ˆ = arg max {fΘ (E, F )} E E X fΘ (E, F ) = λm hm,Θ (E, F ) (1) (2) m∈M plied and for simplicity, in the following we will assume the particular derivation for a translation hypothesis to be included in the variable E. The loglinear feature weights are optimized with minimum error rate training (MERT) (Och, 2003). 4 4.1 Update Strategies Previously Proposed Algorithms The Growth Transformation (GT) or Extended Baum-Wel"
N15-1175,P03-1021,0,0.417489,"ur experiments also prove that leave-one-out impacts translation quality. This paper is organized as follows. We review related work in Section 2 and present the translation system in Section 3. In Section 4 we describe the different discriminative update strategies applied in this work and Section 5 derives the complete maximum expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one m"
N15-1175,P02-1040,0,0.0967528,"Missing"
N15-1175,N13-1034,0,0.194761,"exible and efficient discriminative training approach for statistical machine translation. We propose to use the RPROP algorithm for optimizing a maximum expected B LEU objective and experimentally compare it to several other updating schemes. It proves to be more efficient and effective than the previously proposed growth transformation technique and also yields better results than stochastic gradient descent and AdaGrad. We also report strong empirical results on two large scale tasks, namely BOLT Chinese→English and WMT German→English, where our final systems outperform results reported by Setiawan and Zhou (2013) and on matrix.statmt.org. On the WMT task, discriminative training is performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature. 1 Introduction The main advantage of learning parameters in a discriminative fashion is the possibility to directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine"
N15-1175,P12-1002,0,0.078287,"ate a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1516–1526, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics pairs, which, to the best of our knowledge, is 2.4 times the size of the largest training set reported in previous work (1.66M sentences in (Simianer et al., 2012)). This proves the scalability of our approach. 4. On two large scale tasks our experiments show good improvements over strong baselines which include recurrent language modeling components. On the Chinese→English DARPA BOLT task, we achieve nearly twice the improvement reported in (Setiawan and Zhou, 2013) on the same test sets which results in a superior final system. Finally, the best single system reported on matrix.statmt.org is outperformed by 0.8 B LEU points on the WMT German→English newstest2013 set. Our experiments also prove that leave-one-out impacts translation quality. This paper"
N15-1175,W10-1738,1,0.892163,"Missing"
N15-1175,D07-1080,0,0.0269347,"ves the complete maximum expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model an"
N15-1175,P10-1049,1,0.953347,"ted B LEU objective, that provides a clear training criterion, which is missing e.g. in MIRA estimation. Auli et al. (2014) report good results by applying the same objective function to reordering features, which are trained with stochastic gradient descent (SGD). Our work differs in several key aspects: (i) We propose to apply the RPROP algorithm, which yields superior results to GT, SGD and AdaGrad in our experimental comparison. (ii) For the first time, we apply maximum expected B LEU training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where"
N15-1175,C12-3061,1,0.935254,"Missing"
N15-1175,D13-1138,1,0.867616,"nglish BOLT Chinese English WMT German English 138K 2.63M 2.70M 75.4K 50.2K 4.08M 78.3M 85.9M 384K 817K 4.09M 105M 104M 659K 649K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). T"
N15-1175,D13-1112,0,0.0146615,"lection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just using a small development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of"
P10-1049,W99-0604,1,\N,Missing
P10-1049,J93-2003,0,\N,Missing
P10-1049,W02-1021,1,\N,Missing
P10-1049,P02-1040,0,\N,Missing
P10-1049,D08-1033,0,\N,Missing
P10-1049,J04-4002,1,\N,Missing
P10-1049,W06-3123,0,\N,Missing
P10-1049,2006.amta-papers.2,0,\N,Missing
P10-1049,P06-1096,0,\N,Missing
P10-1049,W06-3105,0,\N,Missing
P10-1049,P07-2026,1,\N,Missing
P10-1049,N03-1017,0,\N,Missing
P10-1049,J03-1002,1,\N,Missing
P10-1049,D08-1065,0,\N,Missing
P10-1049,P08-1024,0,\N,Missing
P10-1049,P08-2007,0,\N,Missing
P10-1049,2008.iwslt-evaluation.10,0,\N,Missing
P12-2006,2006.iwslt-papers.6,0,0.0588331,"Missing"
P12-2006,W11-2123,0,0.0949861,"Missing"
P12-2006,N03-1017,0,0.0610843,"Missing"
P12-2006,P07-2045,1,0.0181414,"Missing"
P12-2006,2007.mtsummit-papers.43,0,0.0352097,"Missing"
P12-2006,W06-3109,0,0.267098,"Missing"
P12-2006,P02-1040,0,0.0844948,"Missing"
P12-2006,2006.amta-papers.25,0,0.151541,"Missing"
P12-2006,2008.iwslt-papers.8,1,0.94228,"Missing"
P16-1007,2005.eamt-1.6,1,0.820554,"ase-based engine, like , die in Ex. 6. Table 7 summarizes how many times each of the systems produced a better output than the other, broken down by category. 8 within a word graph, which is generated by the translation decoder without constraints at the beginning of the workflow. A given prefix is then matched to the paths within the word graph. This approach was recently refined with more permissive matching criteria by Koehn et al. (2014), who report strong improvements in prediction accuracy. Instead of using a word graph, it is also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several"
P16-1007,bernth-mccord-2000-effect,0,0.0631123,"haracters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-Trilles et al., 2008), or source difficulty (Bernth and McCord, 2000) would certainly be relevant for evaluating a full interactive system, but are beyond the scope of this work. 3 Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation candidates are bundled and sorted with respect to each target phrase rather than each source phrase. Crucially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned, each hypothesis from the final target beam is c"
P16-1007,J93-2003,0,0.105246,"Missing"
P16-1007,N10-1062,0,0.168941,"o avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics. We first find the intersection of two directional word alignments. The directional alignments are obtained similar to IBM Model 2 (Brown et al., 1993) by aligning the most likely source word to each target word. Given a source sequence f = f1 . . . f|f | and a target sequence e = e1 . . . e|e |, we define the alignment a = a1 . . . a|e |, where ai = j means th"
P16-1007,buck-etal-2014-n,0,0.0502999,"Missing"
P16-1007,D15-1166,0,0.0345425,"Missing"
P16-1007,D08-1076,0,0.0481463,"se replicas can be interpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix ep and suffix es , then we can generate diverse alternatives in precisely the right location in the target. Let G = (V, E) be the search lattice created by decodin"
P16-1007,P07-1033,0,0.0839266,"Missing"
P16-1007,W15-5003,0,0.0374497,"Missing"
P16-1007,J04-4002,0,0.161353,"d correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned, each hypothesis from the final target beam is copied to an appropriate source beam. Search starts with the lowest-count source beam that contains at least one hypothesis. Here, we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical"
P16-1007,P04-3001,0,0.032026,"also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several different approaches were evaluated. Barrachina et al. (2008) reports experimental results that show the superiority of phrase-based models over stochastic finite state transducers and alignment templates, which were extended for the interactive translation paradigm by Och et al. (2003). Ortiz-Martínez et al. (2009) confirm this observation, and find that their own suggested method using partial statistical phrase-based alignments performs on a similar level on most tasks. The approach using phrase-based models is used as the baseline"
P16-1007,W99-0604,0,0.136312,"arget is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3) (4) (5) (6) 2 We choose cube pruning (Huang and Chiang, 2007) as the beam-filling strategy. 68 Here, cnt(ei , fj ) is the count of all word alignments between ei and fj in the training bitext, and cnt(fj ) the monolingual occurrence count of fj . We perform standard phrase extraction (Och et al., 1999; Koehn et al., 2003) to obtain our synthetic phrases, whose translation probabilities are again estimated based on the single-word probabilities p(ei |fj ) from our translation model. Given a synthetic phrase pair (e, f ), the phrase translation probability is computed as Y p(e|f ) = max p(ei |fj ) (7) 1≤i≤|e| f ∈ φ, one for each d ∈ D. ( f, d ∈ D fd = 0, otherwise. The weights of the replicated feature space are initialized with 0 except for the root domain, where we copy the baseline weights w. ( w, d is root wd = (9) 0, otherwise. All our phrase-based systems are first tuned without prefix"
P16-1007,1997.mtsummit-papers.1,0,0.895889,"n et al. (2014), who report strong improvements in prediction accuracy. Instead of using a word graph, it is also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several different approaches were evaluated. Barrachina et al. (2008) reports experimental results that show the superiority of phrase-based models over stochastic finite state transducers and alignment templates, which were extended for the interactive translation paradigm by Och et al. (2003). Ortiz-Martínez et al. (2009) confirm this observation, and find that their own suggested method using partial statistical phrase-based alignme"
P16-1007,E03-1032,0,0.449493,"ric is 0 if the first word of es is not also the first word of e∗s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full reference suffix, most reference information is unused in practice. 2.2 Prefix-Bleu (pxBleu): Bleu (Papineni et al., 2002) is computed from the geometric mean of clipped n-gram precisions precn (·, ·) and a brevity Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-T"
P16-1007,W08-0509,0,0.0398275,"ength(v) ≤ P and length(w) > P then 5: Add w to M . Mark node 6: end if 7: v.child = v.child ⊕ w . Child pointer update 8: end for 9: N = [] . n-best target strings 10: for m ∈ M do 11: Add target(m) to N 12: end for 13: return N 7 We evaluate our models and methods for EnglishFrench and English-German on two domains: software and news. The phrase-based systems are built with Phrasal (Green et al., 2014), an open source toolkit. We use a dynamic phrase table (Levenberg et al., 2010) and tune parameters with AdaGrad. All systems have 42 dense baseline features. We align the bitexts with mgiza (Gao and Vogel, 2008) and estimate 5-gram language models (LMs) with KenLM (Heafield et al., 2013). The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 (Bojar et al., 2015). The LM was estimated from the target side of the bitext. For English-German we run large-scale experiments. The bitext contains 19.9M parallel segments collected from WMT 2015 and the OPUS collection (Skadin¸š et al., 2014). The LM was estimated from the target side of the bitext and the monolingual Common Crawl corpus (Buck et al., 2014), altogether 37.2B running"
P16-1007,R09-1060,0,0.122015,"∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. For the same model, the distribution over suffixes es ∈ E must also condition on a prefix ep ∈ E: h i X 1 p(es |ep , f ; w) = exp w> φ(r) (2) Z(f ) r: src(r)=f tgt(r)=ep es In phrase-based decoding, the best scoring derivation r given a source sentence f and weights w is found efficiently by beam search, with one beam for every count of source words covered by a partial derivation (known as the source coverage cardinality). To predict a suffix conditioned on a prefix by constrained decoding, Barrachina et al. (2008) and Ortiz-Martínez et al. (2009) modify the beam search by discarding hypotheses (partial derivations) that do not match the prefix ep . We propose target beam search, a two-step inference procedure. The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3)"
P16-1007,D13-1111,0,0.039975,"rpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix ep and suffix es , then we can generate diverse alternatives in precisely the right location in the target. Let G = (V, E) be the search lattice created by decoding, where V are nodes a"
P16-1007,N10-1079,0,0.0838326,"y of the suffix. Each metric takes example triples (f, ep , e∗ ) produced during an interactive MT session in which ep was generated in the process of constructing e∗ . A simulated corpus of examples can be produced from a parallel corpus of (f, e∗ ) pairs by selecting prefixes of each e∗ . An exhaustive simulation selects all possible prefixes, while a sampled simulation selects only k prefixes uniformly at random for each e∗ . Computing metrics for exhaustive simulations is expensive because it requires performing suffix prediction inference for every prefix: |e∗ | times for each reference. Ortiz-Martínez et al. (2010) use BLEU directly for training an interactive system, but we propose a variant that only scores the predicted suffix and not the input prefix. The pxBleu metric comˆ E ˆ ∗ ) for the following constructed putes Bleu(E, ˆ ˆ ∗: sequences E and E • For each (f, ep , e∗ ) and suffix prediction es , ˆ includes the full sentence e = ep es . E ˆ ∗ is a masked copy of • For each (f, ep , e∗ ), E e∗ in which all prefix words that do not match any word in e are replaced by null tokens. This construction maintains the original computation of the brevity penalty, but does not include the prefix in the pre"
P16-1007,P13-1031,1,0.856739,"baseline translation quality and only update the weights corresponding to the prefix, overlap and suffix domains. 1≤j≤|f | Additionally, we introduce three indicator features that count the number of synthetic phrase pairs, source words and target words, respectively. 4 (8) Tuning 5 In order to tune the model for suffix prediction, we optimize the weights w in Equation 2 to maximize the metrics introduced in Section 2. Model tuning is performed with AdaGrad (Duchi et al., 2011), an online subgradient method. It features an adaptive learning rate and comes with good theoretical guarantees. See Green et al. (2013) for the details of applying AdaGrad to phrase-based translation. The same model scores both alignment of the prefix and translation of the suffix. However, different feature weights may be appropriate for scoring each step of the inference process. In order to learn different weights for alignment and translation within a unified joint model, we apply the hierarchical adaptation method of Wuebker et al. (2015), which is based on frustratingly easy domain adaptation (FEDA) (Daumé III, 2007). We define three sub-segment domains: prefix, overlap and suffix. The prefix domain contains all phrases"
P16-1007,P02-1040,0,0.0971621,"ic, learning can focus on these words while using all available information in the references. Number of Predicted Words (#prd) is the maximum number of contiguous words at the start of the predicted suffix that match the reference. Like WPA, this metric is 0 if the first word of es is not also the first word of e∗s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full reference suffix, most reference information is unused in practice. 2.2 Prefix-Bleu (pxBleu): Bleu (Papineni et al., 2002) is computed from the geometric mean of clipped n-gram precisions precn (·, ·) and a brevity Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quan"
P16-1007,W14-3311,1,0.844197,"e, the most probable word ei is passed to the next time step. Require: Lattice G = (V, E), prefix length P 1: M = [] . Marked nodes 2: for w ∈ V in reverse topological order do 3: v = parent(w) . v, w ∈ E 4: if length(v) ≤ P and length(w) > P then 5: Add w to M . Mark node 6: end if 7: v.child = v.child ⊕ w . Child pointer update 8: end for 9: N = [] . n-best target strings 10: for m ∈ M do 11: Add target(m) to N 12: end for 13: return N 7 We evaluate our models and methods for EnglishFrench and English-German on two domains: software and news. The phrase-based systems are built with Phrasal (Green et al., 2014), an open source toolkit. We use a dynamic phrase table (Levenberg et al., 2010) and tune parameters with AdaGrad. All systems have 42 dense baseline features. We align the bitexts with mgiza (Gao and Vogel, 2008) and estimate 5-gram language models (LMs) with KenLM (Heafield et al., 2013). The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 (Bojar et al., 2015). The LM was estimated from the target side of the bitext. For English-German we run large-scale experiments. The bitext contains 19.9M parallel segments co"
P16-1007,D08-1051,0,0.0885272,"l., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-Trilles et al., 2008), or source difficulty (Bernth and McCord, 2000) would certainly be relevant for evaluating a full interactive system, but are beyond the scope of this work. 3 Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation candidates are bundled and sorted with respect to each target phrase rather than each source phrase. Crucially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned,"
P16-1007,P13-2121,0,0.0528998,"Missing"
P16-1007,P07-1019,0,0.0476989,"ypotheses (partial derivations) that do not match the prefix ep . We propose target beam search, a two-step inference procedure. The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3) (4) (5) (6) 2 We choose cube pruning (Huang and Chiang, 2007) as the beam-filling strategy. 68 Here, cnt(ei , fj ) is the count of all word alignments between ei and fj in the training bitext, and cnt(fj ) the monolingual occurrence count of fj . We perform standard phrase extraction (Och et al., 1999; Koehn et al., 2003) to obtain our synthetic phrases, whose translation probabilities are again estimated based on the single-word probabilities p(ei |fj ) from our translation model. Given a synthetic phrase pair (e, f ), the phrase translation probability is computed as Y p(e|f ) = max p(ei |fj ) (7) 1≤i≤|e| f ∈ φ, one for each d ∈ D. ( f, d ∈ D fd = 0,"
P16-1007,skadins-etal-2014-billions,0,0.0423715,"Missing"
P16-1007,N03-1017,0,0.391558,"we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics. We first find the intersection of two directional word alignments. The directional alignments are obtained similar to IBM Model 2 (Brown et al., 1993) by aligning the most likely source word to each target word. Given a source sequence f = f1 . . . f|f | and a target sequence e = e1 . . ."
P16-1007,P14-2094,0,0.541798,"r boundary conditions, the reference e∗ is masked by the prefix ep as follows: we replace each of the first |ep − 3 |words with a null token enull , unless the word also appears in the suffix e∗s . Masking retains the last three words of the prefix so that the first words after the prefix can contribute to the precision of all n-grams that overlap with the prefix, up to n = 4. Words that also appear in the suffix are retained so that their correct prediction in the suffix can contribute to those precisions, which would otherwise be clipped. Word Prediction Accuracy (WPA) or nextword accuracy (Koehn et al., 2014) is 1 if the first word of the predicted suffix es is also the first word of reference suffix e∗s , and 0 otherwise. Averaging over examples gives the frequency that the word following the prefix was predicted correctly. In a sampled simulation, all reference words that follow the first word of a sampled suffix are ignored by the metric, so most reference information is unused. 2.1 Loss Functions for Learning All of these metrics can be used as the tuning objective of a phrase-based machine translation system. Tuning toward a sampled simulation that includes one or two prefixes per reference i"
P16-1007,W00-0507,0,0.507119,"sed to translate the remainder of the sentence. The root domain spans the entire phrasal derivation. Formally, given a set of domains D = {root, prefix, overlap, suffix}, each feature is replicated for each domain d ∈ D. These replicas can be interpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can"
P16-1007,D15-1123,1,0.82902,"Section 2. Model tuning is performed with AdaGrad (Duchi et al., 2011), an online subgradient method. It features an adaptive learning rate and comes with good theoretical guarantees. See Green et al. (2013) for the details of applying AdaGrad to phrase-based translation. The same model scores both alignment of the prefix and translation of the suffix. However, different feature weights may be appropriate for scoring each step of the inference process. In order to learn different weights for alignment and translation within a unified joint model, we apply the hierarchical adaptation method of Wuebker et al. (2015), which is based on frustratingly easy domain adaptation (FEDA) (Daumé III, 2007). We define three sub-segment domains: prefix, overlap and suffix. The prefix domain contains all phrases that are used for aligning the prefix with the source sentence. Phrases that span both prefix and suffix additionally belong to the overlap domain. Finally, once the prefix has been completely covered, the suffix domain applies to all phrases that are used to translate the remainder of the sentence. The root domain spans the entire phrasal derivation. Formally, given a set of domains D = {root, prefix, overlap"
W10-1711,E03-1076,0,0.129532,"Missing"
W10-1711,E06-1005,1,0.838332,"Association for Computational Linguistics Standard FA German→English BLEU # Phrases 19.7 128M 20.0 12M French→English BLEU # Phrases 25.5 225M 25.9 35M English→French BLEU # Phrases 23.7 261M 24.0 33M Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For German→English and English→French phrase table interpolation was applied. tains flexibility on the input systems. additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. Alignments between the systems are learned by GIZA++, a one-to-one alignment is generated from the learned state occupation probabilities. From these alignments, a confusion network (CN) is then built using one of the hypotheses as “skeleton” or “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible CNs into a single lattice. Majority voting on the generated lattice is performed usi"
W10-1711,D09-1022,1,0.239401,"Missing"
W10-1711,popovic-ney-2006-pos,1,0.575104,"Missing"
W10-1711,2009.mtsummit-posters.17,0,0.036503,"Missing"
W10-1711,J07-2003,0,0.0508773,"etraining method and additional models were tested and investigated with respect to their effect for the different language pairs. For two of the language pairs we could improve performance by system combination. An overview of the systems and models will follow in Section 2 and 3, which describe the baseline architecture, followed by descriptions of the additional system components. Morpho-syntactic analysis and other preprocessing issues are covered by Section 4. Finally, translation results for 2.2 Hierarchical System Our hierarchical phrase-based system is similar to the one described in (Chiang, 2007). It allows for gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step. It has similar features as the phrasebased system mentioned above. For some systems, we only allowed the non-terminals in hierarchical phrases to be substituted with initial phrases as in (Iglesias et al., 2009), which gave better results on some language pairs. We will refer to this as “shallow rules”. 2.3 System Combination The RWTH approach to MT system combination of the French→English systems as well as the German→English systems is a refined version of the ROVER approa"
W10-1711,P10-1049,1,0.176425,"Missing"
W10-1711,W06-3105,0,0.0459067,"Missing"
W10-1711,W06-3108,1,0.318609,"with a variant of GIZA++. Target language models are 4-gram language models trained with the SRI toolkit, using Kneser-Ney discounting with interpolation. 2.1 Phrase-Based System Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (Zens and Ney, 2006). Introduction This paper describes the statistical MT system used for our participation in the WMT 2010 shared translation task. We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. For all tasks we used standard alignment and training tools as well as our in-house phrasebased and hierarchical statistical MT decoders. When German was involved, morpho-syntactic preprocessing was applied. An alternative phrasetraining method and additional models were tested and investigated"
W10-1711,2008.iwslt-papers.8,1,0.0627609,"ome tasks, a system combination of the best systems was used to generate a final hypothesis. We participated in the constrained condition of GermanEnglish and French-English in each translation direction. 1 2 Translation Systems For the WMT 2010 Evaluation we used standard phrase-based and hierarchical translation systems. Alignments were trained with a variant of GIZA++. Target language models are 4-gram language models trained with the SRI toolkit, using Kneser-Ney discounting with interpolation. 2.1 Phrase-Based System Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (Zens and Ney, 2006). Introduction This paper describes the statistical MT system used for our participation in the WMT 2010 shared translation task. We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and whic"
W10-1711,D08-1039,1,0.678444,"Missing"
W10-1711,W99-0604,1,\N,Missing
W10-1711,D11-1033,0,\N,Missing
W10-1711,E09-1044,0,\N,Missing
W10-1711,J93-2003,0,\N,Missing
W10-1711,N04-4015,0,\N,Missing
W10-1711,W07-0813,0,\N,Missing
W10-1711,N04-4038,0,\N,Missing
W10-1711,D08-1089,0,\N,Missing
W10-1711,P03-1054,0,\N,Missing
W10-1711,P02-1040,0,\N,Missing
W10-1711,W10-1738,1,\N,Missing
W10-1711,W06-3110,1,\N,Missing
W10-1711,J10-3008,0,\N,Missing
W10-1711,2010.iwslt-keynotes.2,0,\N,Missing
W10-1711,P10-2041,0,\N,Missing
W10-1711,N09-1027,0,\N,Missing
W10-1711,P08-2030,0,\N,Missing
W10-1711,W07-0734,0,\N,Missing
W10-1711,N06-2013,0,\N,Missing
W10-1711,N03-1017,0,\N,Missing
W10-1711,2008.iwslt-papers.7,1,\N,Missing
W10-1711,J03-1002,1,\N,Missing
W10-1711,P07-1019,0,\N,Missing
W10-1711,W06-3103,1,\N,Missing
W10-1711,P08-1066,0,\N,Missing
W10-1711,2010.iwslt-papers.15,1,\N,Missing
W10-1711,2006.iwslt-papers.1,1,\N,Missing
W10-1711,2011.iwslt-papers.1,1,\N,Missing
W10-1711,2011.iwslt-papers.7,1,\N,Missing
W10-1711,2011.iwslt-papers.8,1,\N,Missing
W10-1711,N04-1033,1,\N,Missing
W10-1711,2011.iwslt-evaluation.1,0,\N,Missing
W10-1711,W10-1747,1,\N,Missing
W10-1711,D08-1076,0,\N,Missing
W10-1711,P03-1021,0,\N,Missing
W10-1711,2011.iwslt-papers.5,1,\N,Missing
W10-1711,P08-1000,0,\N,Missing
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2149,P07-1019,0,0.0299407,"k. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The standard models integrated into our Jane systems are: phrase translation probabilities and lexical translation probabilities on phrase level, each for both translation directions, length 405 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405–412, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics penalties on word and phrase level, three binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count feat"
W11-2149,E09-1044,0,0.039482,"Missing"
W11-2149,P03-1054,0,0.00307413,"The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases that meet certain restrictions. The first possibility is what the authors call a fixed dependency structure. With the exception of one word within this phrase, called the head, no outside word may have a dependency within this phrase. Also, all inner words may only depend on each other or on t"
W11-2149,E03-1076,0,0.0408333,"pora were used additionally. For the 109 French-English and LDC Gigaword corpora RWTH applied the data selection technique described in Section 3.1. We examined two different language models, one with LDC data and one without. Systems were optimized on the newstest2009 data set, newstest2008 was used as test set. The scores for newstest2010 are included for completeness. 5.1 Morpho-Syntactic Analysis In order to reduce the source vocabulary size for the German→English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). To further reduce translation complexity, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). For additional experiments we used the TreeTagger (Schmid, 1995) to produce a lemmatized version of the German source. 5.2 Optimization Criterion We studied the impact of different optimization criteria on tranlsation performance. The usual practice is to optimize the scaling factors to maximize BLEU. We also experimented with two different combinations of BLEU and Translation Edit Rate (TER): TER−BLEU and TER−4BLEU. The first denotes the equally we"
W11-2149,E06-1005,1,0.834047,"rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count features and an n-gram language model. The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.3 System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (Matusov et al., 2006; Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 3 Translation Modeling We incorporated several novel methods into our systems for the WMT 2011 evaluation. This section provides a short survey of three of the methods which we suppose to be of particular interest. 3.1 Language Model Data Selection For the English and German language models, we applied the data selectio"
W11-2149,D09-1022,1,0.84817,"on automatically selected English data (cf. Section 3.1) from the provided resources including the 109 corpus and LDC Gigaword. The scaling factors of the log-linear model combination are optimized towards BLEU on newstest2009, newstest2010 is used as an unseen test set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment"
W11-2149,P10-2041,0,0.0393086,"). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 3 Translation Modeling We incorporated several novel methods into our systems for the WMT 2011 evaluation. This section provides a short survey of three of the methods which we suppose to be of particular interest. 3.1 Language Model Data Selection For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). Each sentence is scored by the difference in cross-entropy between a language model trained from in-domain data and a language model trained from a similar-sized sample of the out-of-domain data. As in-domain data we used the news-commentary corpus. The out-of-domain data from which the data was selected are the news crawl corpus for both languages and for English the 109 corpus and the LDC Gigaword data. We used a 3-gram trained with the SRI toolkit to compute the cross-entropy. For the news crawl corpus, only 1/8 of the sentences were discarded. Of the 109 corpus we retained 1/2 and of the"
W11-2149,J03-1002,1,0.00864889,"overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the paper in Section 6. 2 Phrase-Based System Translation Systems For the WMT 2011 evaluation we utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The"
W11-2149,P03-1021,0,0.00696987,"ities and lexical translation probabilities on phrase level, each for both translation directions, length 405 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405–412, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics penalties on word and phrase level, three binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count features and an n-gram language model. The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.3 System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (Matusov et al., 2006; Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accord"
W11-2149,P08-1066,0,0.0240437,"are estimated from their relative frequencies in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases that meet certain restrictions. The first possibility is what the authors call a fixed dependency structure. With the exception of one word within this phrase, called the head, no outside word may have a d"
W11-2149,2010.amta-papers.8,1,0.791938,"t set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment (Wuebker et al., 408 2010) on WMT 2010 data only. 4.2 Experimental Results English→French The results for the English→French task are given in Table 4. We likewise submitted two contrastive systems for this translation direction. The first contrastive submission"
W11-2149,2008.iwslt-papers.7,1,0.86264,"t2009, newstest2010 is used as an unseen test set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment (Wuebker et al., 408 2010) on WMT 2010 data only. 4.2 Experimental Results English→French The results for the English→French task are given in Table 4. We likewise submitted two contrastive systems for this translation"
W11-2149,W10-1738,1,0.820443,"h will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the paper in Section 6. 2 Phrase-Based System Translation Systems For the WMT 2011 evaluation we utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The standard models integrated into our Jane systems are: phrase translation probabilities and lexical translation probabilities on"
W11-2149,P10-1049,1,0.92613,"tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm, similar to the one described in (DeNero et al., 2006). Here, the phrase translation probabilities are estimated from their relative frequencies in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases tha"
W11-2149,W06-3108,1,0.91274,"rdering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation task by providing an overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the pap"
W11-2149,W06-3110,1,0.909489,"rdering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation task by providing an overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the pap"
W11-2149,2008.iwslt-papers.8,1,0.857725,"f the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. Both phrasebased and hierarchical SMT systems were trained for the constrained German-English and French-English tasks in all directions. Experiments were conducted to compare different training data sets, training methods and optimization criteria, as well as additional models on dependency structure and phrase reordering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation ta"
W11-2149,D07-1103,0,\N,Missing
W11-2149,W10-1723,0,\N,Missing
W11-2149,W08-0509,0,\N,Missing
W11-2149,P07-2045,0,\N,Missing
W11-2149,W10-1713,0,\N,Missing
W11-2149,2005.eamt-1.19,0,\N,Missing
W12-3157,W06-3123,0,0.0468995,"Missing"
W12-3157,J90-2002,0,0.717089,"v model. In this work we apply the phrase training method introduced by Wuebker et al. (2010), where the phrase translation model of a fully competitive SMT system is trained in a generative way. The key to avoiding the over-fitting effects described by DeNero et al. (2006) is their novel leave-one-out procedure. 3 Decoding 3.1 Phrase-based translation We use a standard phrase-based decoder which ˆ searches for the best translation eˆI1 for a given input 452 sentence f1J by maximizing the posterior probability ˆ eˆI1 = arg max P r(eI1 |f1J ). (1) I,eI1 Generalizing the noisy channel approach (Brown et al., 1990) and making use of the maximum approximation (Viterbi), the decoder directly models the posterior probability by a log-linear combiJ nation of several feature functions hm (eI1 , sK 1 , f1 ) weighted with scaling factors λm , which results in the decision rule (Och and Ney, 2004) ( ˆ eˆI1 = arg max I,eI1 ,K,sK 1 M X ) J λm hm (eI1 , sK 1 , f1 ) . (2) m=1 I J Here, sK 1 denotes the segmentation of e1 and f1 into K phrase-pairs and their alignment. The features used are the language model, phrase translation and lexical smoothing models in both directions, word and phrase penalty and a simple di"
W12-3157,W06-3105,0,0.0588457,"Missing"
W12-3157,D08-1033,0,0.0378588,"Missing"
W12-3157,P08-1115,0,0.574401,"dapted this approach to model ambiguities in representing the source language with lattices and were able to report improvements over their respective baselines. The probabilities for different paths through the lattice are usually modeled by assigning probabilities to arcs as a byproduct of the lattice generation or by defining binary indicator features. Applying the first method only makes sense if the lattice construction is based on a single, comprehensive probabilistic method, like a Chinese word segmentation model as is used by Xu et al. (2005). In applications like the one described by Dyer et al. (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. Also, our intuition suggests that simply defining indicator features for each of the segmenters may not be ideal, if we assume that there is not a single best segmenter, but rather that for different data instances a different one works best. In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to"
W12-3157,N09-1046,0,0.0824065,"Missing"
W12-3157,2009.eamt-1.23,0,0.0600468,"Missing"
W12-3157,W10-1710,0,0.311321,"Missing"
W12-3157,D07-1091,0,0.352491,"Missing"
W12-3157,E03-1076,0,0.417916,"standard distance-based reordering model needs to be redefined for lattice input. We define the distortion penalty as the difference in slot number. Using the shortest path within the lattice is reported to have better performance in (Dyer et al., 2008), however we did not implement it due to time constraints. 4 Lattice design We construct lattices from three different preprocessing variants of the German source side of the data. The surface form is the standard tokenization of the source sentence. The word compounds are produced by the frequency-based compound splitting method described in (Koehn and Knight, 2003), applied to the tokenized sentence. From the compound split sentence we produce the lemma of the 453 German words by applying the TreeTagger toolkit (Schmid, 1995). Each of the different preprocessing variants is assigned a separate layer within the lattice. For the phrase model, word identities are defined by both the word and its layer. In this way, the phrase model can assign different scores to phrases in different layers, allowing it to guide the search towards a specific layer for each word. In practice, this is done by annotating words with a unique identifier for each layer. For examp"
W12-3157,N03-1017,0,0.0253648,"Missing"
W12-3157,W04-3250,0,0.300902,"Missing"
W12-3157,W02-1018,0,0.0914856,"Missing"
W12-3157,W07-0715,0,0.037709,"Missing"
W12-3157,D08-1066,0,0.0297649,"Missing"
W12-3157,W09-0435,0,0.223241,"Missing"
W12-3157,J03-1002,1,0.00897445,"-gram count on the other. In this way the model learns to prefer lattice paths which are taken more often in training. For example, if the phrase (LEM.Streit LEM.Kraft) is used to align the sentence from Figure 1, Cmon (f˜) will 454 Experimental evaluation 6.1 Experimental setup Our experiments are carried out on the newscommentary portion of the German→English data provided for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT 2011).∗ We use newstest2008 as development set and newstest2009 and newstest2010 as unseen test sets. The word alignments are produced with GIZA++ (Och and Ney, 2003). To optimize the loglinear parameters, the Downhill-Simplex algorithm (Nelder and Mead, 1965) is applied with B LEU (Papineni et al., 2002) as optimization criterion. The ∗ http://www.statmt.org/wmt11 Surface Train newstest2008 newstest2009 newstest2010 Sentences Running Words Vocabulary Size Sentences Running Words Vocabulary Size OOVs (Running Words) Sentences Running Words Vocabulary Size OOVs (Running Words) Sentences Running Words Vocabulary Size OOVs (Running Words) 3.4M 118K 48K 10.3K 3041 63K 12.2K 4058 62K 12.3K 4357 German Compound Lemma 136K 3.5M 81K 52K 2051 50K 9.7K 7.3K 2092 174"
W12-3157,J04-4002,1,0.652426,"ir novel leave-one-out procedure. 3 Decoding 3.1 Phrase-based translation We use a standard phrase-based decoder which ˆ searches for the best translation eˆI1 for a given input 452 sentence f1J by maximizing the posterior probability ˆ eˆI1 = arg max P r(eI1 |f1J ). (1) I,eI1 Generalizing the noisy channel approach (Brown et al., 1990) and making use of the maximum approximation (Viterbi), the decoder directly models the posterior probability by a log-linear combiJ nation of several feature functions hm (eI1 , sK 1 , f1 ) weighted with scaling factors λm , which results in the decision rule (Och and Ney, 2004) ( ˆ eˆI1 = arg max I,eI1 ,K,sK 1 M X ) J λm hm (eI1 , sK 1 , f1 ) . (2) m=1 I J Here, sK 1 denotes the segmentation of e1 and f1 into K phrase-pairs and their alignment. The features used are the language model, phrase translation and lexical smoothing models in both directions, word and phrase penalty and a simple distancebased reordering penalty. 3.2 Lattice translation For lattice input we generalize Equation 2 to also maximize over the set of sentences F(L) encoded by a given source word lattice L: ˆ eˆI1 = ( arg max J I,eI1 ,K,sK 1 ,f1 ∈F (L) M X ) J λm hm (eI1 , sK 1 , f1 ) (3) m=1 Note"
W12-3157,P10-2001,0,0.0587469,"Missing"
W12-3157,P02-1040,0,0.0833125,"Missing"
W12-3157,popovic-ney-2006-pos,1,0.92095,"Missing"
W12-3157,E09-1082,0,0.0428343,"Missing"
W12-3157,2006.amta-papers.25,0,0.0855428,"Missing"
W12-3157,P10-1049,1,0.853799,"raining data sentence. For each layer, we add two indicator features to the phrase table: One binary feature which is set to 1 if the phrase is taken from this layer, and one feature which is equal to the number of words from this layer. This results in six additional feature functions, whose weights are optimized jointly with the standard features described in Section 3.1. We will denote them as layer features. 5 Phrase translation model training To train the phrase model, we use a modified version of the translation decoder to force-align the training data. We apply the method described in (Wuebker et al., 2010), but with word lattices on the source side. To avoid over-fitting, we use their cross-validation technique, which is described as a low-cost alternative to leave-one-out. For cross-validation we segment the training data into batches containing 5000 sentences. For each batch, the phrase table is updated by reducing the phrase counts by the local counts produced by the current batch in the previous training iteration. For the first iteration, we perform the standard phrase extraction separately for each batch to produce the local counts. Singleton ˜ phrases are assigned the probability β (|f |"
W12-3157,2005.iwslt-1.18,1,0.874249,"Missing"
W12-3157,2006.amta-papers.2,0,\N,Missing
W12-3158,P08-1024,0,0.0424276,"Missing"
W12-3158,2009.iwslt-papers.2,0,0.0484274,"Missing"
W12-3158,D10-1053,0,0.0325832,"Missing"
W12-3158,W06-3105,0,0.14959,"Missing"
W12-3158,W06-1607,0,0.0608308,"Missing"
W12-3158,P12-1031,0,0.0591497,"Missing"
W12-3158,D07-1103,0,0.0478539,"Missing"
W12-3158,P06-1096,0,0.122379,"Missing"
W12-3158,W10-2915,0,0.025654,"Missing"
W12-3158,P03-1021,0,0.198453,"Missing"
W12-3158,P03-1041,0,0.0854189,"Missing"
W12-3158,P10-1049,1,0.843105,"Missing"
W12-3158,P07-2045,0,\N,Missing
W12-3158,N03-1017,0,\N,Missing
W12-3158,2008.iwslt-evaluation.10,0,\N,Missing
W13-2238,D11-1033,0,0.111872,"Missing"
W13-2238,W06-3123,0,0.0218206,"training procedure in Section 4. The complete algorithm is described in Section 5 and experiments are presented in Section 6. We conclude with Section 7. 2 Related Work Marcu and Wong (2002) present a joint probability model, which is trained with a hill-climbing technique based on break, merge, swap and move operations. Due to the computational complexity they are only able to consider phrases, which appear at least five times in the data. The model is shown to slightly underperform heuristic extraction in (Koehn et al., 2003). For higher efficiency, it is constrained by a word alignment in (Birch et al., 2006). DeNero et al. (2008) introduce a different training procedure for this model based on a Gibbs sampler. They make use of the word alignment for initialization. A generative phrase model trained with the Expectation-Maximization (EM) algorithm is shown in (DeNero et al., 2006). It also does not reach the same top performance as heuristic extraction. The authors identify the hidden segmentation variable, which results in over-fitting, as the main problem. 310 data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). quence mode"
W13-2238,P09-1088,0,0.172033,"Missing"
W13-2238,2007.mtsummit-tutorials.1,0,0.0622985,"Missing"
W13-2238,J93-2003,0,0.03583,"Missing"
W13-2238,W06-3105,0,0.252908,"Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 309–319, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Liang et al. (2006) present a discriminative translation system. One of the proposed strategies for training, which the authors call bold updating, is similar to our training scheme. They use heuristically extracted phrase translation probabilities as blanket features in all setups. Another iteratively-trained phrase model is described by Moore and Quirk (2007). Their model is segmentation-free and, confirming the findings in (DeNero et al., 2006), can close the gap to phrase tables induced from surface heuristics. It relies on word alignment for phrase selection. Mylonakis and Sima’an (2008) present a phrase model, whose training procedure uses prior probabilities based on Inversion Transduction Grammar and smoothing as learning objective to prevent over-fitting. They also rely on the word alignment to select phrase pairs. Blunsom et al. (2009) perform inference over latent synchronous derivation trees under a nonparametric Bayesian model with a Gibbs sampler. Training is also initialized by extracting rules from a word alignment, but"
W13-2238,D08-1066,0,0.0379286,"Missing"
W13-2238,D08-1033,0,0.170076,"Missing"
W13-2238,P11-1064,0,0.117924,"Missing"
W13-2238,D12-1041,0,0.0399692,"Missing"
W13-2238,J03-1002,1,0.0188411,"Missing"
W13-2238,P12-1031,0,0.018182,"generative phrase model trained with the Expectation-Maximization (EM) algorithm is shown in (DeNero et al., 2006). It also does not reach the same top performance as heuristic extraction. The authors identify the hidden segmentation variable, which results in over-fitting, as the main problem. 310 data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). quence model. For inference, they optimize their parameters towards alignment F-score. The forced derivations are initialized with the standard heuristic extraction scheme. He and Deng (2012) describe a discriminative phrase training procedure, where n-best translations are produced by the decoder on the whole training data. The heuristically extracted relative frequencies serve as a prior, and the probabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based translation decoder from the open source toolkit Jane 2 (Wuebker et al., 2012a) for both the training procedure and the translation experiments. It makes use of the usual features: Translation channel models in both directions, lexical smoothing models in both dir"
W13-2238,J04-4002,1,0.524096,"obabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based translation decoder from the open source toolkit Jane 2 (Wuebker et al., 2012a) for both the training procedure and the translation experiments. It makes use of the usual features: Translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model (LM), phrase and word penalty and a jump-distancebased distortion model. Formally, the best transˆ J lation eˆI1 as defined by the models hm (eI1 , sK 1 , f1 ) can be written as (Och and Ney, 2004) ) ( M X I K J Iˆ λm hm (e1 , s1 , f1 ) , (1) eˆ1 = arg max ˆ sˆK 1 where f1J = f1 . . . fJ is the source sentence, = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. We define sk := (ik , bk , jk ), where ik is the last position of kth target phrase, and (bk , jk ) are the start and end positions of the source phrase aligned to the kth target phrase. Different from many standard systems, the lexical smoothing scores are not estimated by extracting counts from a word alignment, but with IBM-1 model scores trained on the bilingual data with GIZA++."
W13-2238,P03-1021,0,0.109302,"). For higher efficiency, it is constrained by a word alignment in (Birch et al., 2006). DeNero et al. (2008) introduce a different training procedure for this model based on a Gibbs sampler. They make use of the word alignment for initialization. A generative phrase model trained with the Expectation-Maximization (EM) algorithm is shown in (DeNero et al., 2006). It also does not reach the same top performance as heuristic extraction. The authors identify the hidden segmentation variable, which results in over-fitting, as the main problem. 310 data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). quence model. For inference, they optimize their parameters towards alignment F-score. The forced derivations are initialized with the standard heuristic extraction scheme. He and Deng (2012) describe a discriminative phrase training procedure, where n-best translations are produced by the decoder on the whole training data. The heuristically extracted relative frequencies serve as a prior, and the probabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based transla"
W13-2238,N03-1017,0,0.0496928,"he decoder and its features are described in Section 3 and we give an overview of the training procedure in Section 4. The complete algorithm is described in Section 5 and experiments are presented in Section 6. We conclude with Section 7. 2 Related Work Marcu and Wong (2002) present a joint probability model, which is trained with a hill-climbing technique based on break, merge, swap and move operations. Due to the computational complexity they are only able to consider phrases, which appear at least five times in the data. The model is shown to slightly underperform heuristic extraction in (Koehn et al., 2003). For higher efficiency, it is constrained by a word alignment in (Birch et al., 2006). DeNero et al. (2008) introduce a different training procedure for this model based on a Gibbs sampler. They make use of the word alignment for initialization. A generative phrase model trained with the Expectation-Maximization (EM) algorithm is shown in (DeNero et al., 2006). It also does not reach the same top performance as heuristic extraction. The authors identify the hidden segmentation variable, which results in over-fitting, as the main problem. 310 data set with minimum error rate training (MERT) (O"
W13-2238,2001.mtsummit-papers.68,0,0.0296618,"d alignment in (Birch et al., 2006). DeNero et al. (2008) introduce a different training procedure for this model based on a Gibbs sampler. They make use of the word alignment for initialization. A generative phrase model trained with the Expectation-Maximization (EM) algorithm is shown in (DeNero et al., 2006). It also does not reach the same top performance as heuristic extraction. The authors identify the hidden segmentation variable, which results in over-fitting, as the main problem. 310 data set with minimum error rate training (MERT) (Och, 2003). As optimization criterion we use B LEU (Papineni et al., 2001). quence model. For inference, they optimize their parameters towards alignment F-score. The forced derivations are initialized with the standard heuristic extraction scheme. He and Deng (2012) describe a discriminative phrase training procedure, where n-best translations are produced by the decoder on the whole training data. The heuristically extracted relative frequencies serve as a prior, and the probabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based translation decoder from the open source toolkit Jane 2 (Wuebker et al."
W13-2238,W04-3250,0,0.341203,"Missing"
W13-2238,P06-1096,0,0.136883,"Missing"
W13-2238,2011.eamt-1.42,0,0.0452054,"Missing"
W13-2238,2006.amta-papers.25,0,0.141837,"Missing"
W13-2238,W02-1018,0,0.0938968,"a disadvantage for translation quality. Although we use a phrase-based decoder here, the principles of our work can be applied to any statistical machine translation paradigm. The software used for our experiments is available under a non-commercial open source licence. The paper is organized as follows. We review related work in Section 2. The decoder and its features are described in Section 3 and we give an overview of the training procedure in Section 4. The complete algorithm is described in Section 5 and experiments are presented in Section 6. We conclude with Section 7. 2 Related Work Marcu and Wong (2002) present a joint probability model, which is trained with a hill-climbing technique based on break, merge, swap and move operations. Due to the computational complexity they are only able to consider phrases, which appear at least five times in the data. The model is shown to slightly underperform heuristic extraction in (Koehn et al., 2003). For higher efficiency, it is constrained by a word alignment in (Birch et al., 2006). DeNero et al. (2008) introduce a different training procedure for this model based on a Gibbs sampler. They make use of the word alignment for initialization. A generati"
W13-2238,P10-1049,1,0.941268,"unified framework induces the phrases based on the same models as in decoding. We train the phrase table without using a word alignment or the extraction heuristics. Different from previous work, we are able to generate all possible phrase pairs on-the-fly during the training procedure. A further advantage of our proposed algorithm is that we use basically the same beam search as in translation. This makes it easy to re-implement by modifying any translation decoder, and makes sure that training and translation are consistent. In principle, we apply the forced decoding approach described in (Wuebker et al., 2010) with cross-validation to prevent over-fitting, but we initialize the phrase table with IBM-1 lexical probabilities (Brown et al., 1993) instead of heuristically extracted relative frequencies. The algorithm is extended with the concept of backoff phrases, so that new phrase pairs can be generated at training time. The size of the newly generated phrases is incremented over the training iterations. By introducing fallback decoding runs, we are able to successfully align the complete training data. Local language models are used for better phrase pair pre-selection. We present an iterative tech"
W13-2238,C12-3061,1,0.931403,"et al., 2001). quence model. For inference, they optimize their parameters towards alignment F-score. The forced derivations are initialized with the standard heuristic extraction scheme. He and Deng (2012) describe a discriminative phrase training procedure, where n-best translations are produced by the decoder on the whole training data. The heuristically extracted relative frequencies serve as a prior, and the probabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based translation decoder from the open source toolkit Jane 2 (Wuebker et al., 2012a) for both the training procedure and the translation experiments. It makes use of the usual features: Translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model (LM), phrase and word penalty and a jump-distancebased distortion model. Formally, the best transˆ J lation eˆI1 as defined by the models hm (eI1 , sK 1 , f1 ) can be written as (Och and Ney, 2004) ) ( M X I K J Iˆ λm hm (e1 , s1 , f1 ) , (1) eˆ1 = arg max ˆ sˆK 1 where f1J = f1 . . . fJ is the source sentence, = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phr"
W13-2238,W12-3158,1,0.874049,"et al., 2001). quence model. For inference, they optimize their parameters towards alignment F-score. The forced derivations are initialized with the standard heuristic extraction scheme. He and Deng (2012) describe a discriminative phrase training procedure, where n-best translations are produced by the decoder on the whole training data. The heuristically extracted relative frequencies serve as a prior, and the probabilities are updated with a maximum B LEU criterion based on the n-best lists. 3 4 4.1 We use the standard phrase-based translation decoder from the open source toolkit Jane 2 (Wuebker et al., 2012a) for both the training procedure and the translation experiments. It makes use of the usual features: Translation channel models in both directions, lexical smoothing models in both directions, an n-gram language model (LM), phrase and word penalty and a jump-distancebased distortion model. Formally, the best transˆ J lation eˆI1 as defined by the models hm (eI1 , sK 1 , f1 ) can be written as (Och and Ney, 2004) ) ( M X I K J Iˆ λm hm (e1 , s1 , f1 ) , (1) eˆ1 = arg max ˆ sˆK 1 where f1J = f1 . . . fJ is the source sentence, = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phr"
W13-2238,I08-1068,0,0.02792,"(e1 , s1 , f1 ) , (1) eˆ1 = arg max ˆ sˆK 1 where f1J = f1 . . . fJ is the source sentence, = e1 . . . eI the target sentence and sK 1 = s1 . . . sK their phrase segmentation and alignment. We define sk := (ik , bk , jk ), where ik is the last position of kth target phrase, and (bk , jk ) are the start and end positions of the source phrase aligned to the kth target phrase. Different from many standard systems, the lexical smoothing scores are not estimated by extracting counts from a word alignment, but with IBM-1 model scores trained on the bilingual data with GIZA++. They are computed as (Zens, 2008) eI1 k=1 j=bk ik X i=ik−1 +1 = arg max K,sK 1 ( M X ) J λm hm (eI1 , sK 1 , f1 ) m=1 (3) To force-align the training data, the translation decoder is constrained to the given target sentence. The translation candidates applicable for each sentence pair are selected through a bilingual phrase matching before the actual search. In the M-step, we re-estimate the phrase table from the phrase alignments. The translation probability of a phrase pair (f˜, e˜) is estimated as m=1 J hlex (eI1 , sK 1 , f1 ) =  jk K X X log p(fj |e0 ) + Overview In this work we employ a training procedure inspired by t"
W13-2238,P02-1040,0,\N,Missing
W13-2238,P10-2041,0,\N,Missing
W13-2238,2006.amta-papers.2,0,\N,Missing
W13-2238,W07-0715,0,\N,Missing
W13-2238,D08-1076,0,\N,Missing
W13-2258,W12-3125,0,0.0598878,"ases. The only difference is that length constraints are applied to phrases, but not to blocks. Figure 1 illustrates the extraction of monotone, swap, and discontinuous orientation classes in left-to-right direction from word-aligned bilingual training samples. The right-to-left direction works analogously. p(O) = P N (O) O0 ∈{M,S,D} N (O We found that this concept can be neatly plugged into the hierarchical phrase-based translation paradigm, without having to resort to approximations in decoding, which is necessary to determine the right-to-left orientation in a standard phrase-based system (Cherry et al., 2012). To train the orientations, the extraction procedure from the standard phrase-based version of the reordering model can be used with a minor extension. The model is trained on the same word-aligned data from which the translation rules are extracted. For each training sentence, we extract all phrases of unlimited length that are consistent with the word alignment, and store their corners in a matrix. The corners are distinguished by their location: topleft, top-right, bottom-left, and bottom-right. For each bilingual phrase, we determine its left-toright and right-to-left orientation by check"
W13-2258,P05-1033,0,0.878427,"sequence and a distortion cost that is computed from the sourceside jump distances. Though the space of admissible reorderings is in most cases contrained by a maximum jump width or coverage-based restrictions (Zens et al., 2004) for efficiency reasons, the basic approach of arbitrarily jumping to uncovered positions on source side is still very permissive. Lexicalized reordering models assist the decoder in taking a good decision. Phrase-based decoding allows for a straightforward integration of lexicalized reordering models which assign Introduction In hierarchical phrase-based translation (Chiang, 2005), a probabilistic synchronous context-free grammar (SCFG) is induced from bilingual training corpora. In addition to continuous lexical phrases as in standard phrase-based translation, hierarchical phrases with usually up to two nonterminals are extracted from the word-aligned parallel training data. Hierarchical decoding is typically carried out with a parsing-based procedure. The parsing algorithm is extended to handle translation candi452 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452–463, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computation"
W13-2258,J07-2003,0,0.404938,"(with 1 ≤ j1 ≤ j2 ≤ J and 1 ≤ i1 ≤ i2 ≤ I) is consistent with respect to the word alignment A ⊆ {1, ..., I} × {1, ..., J} iff (1) is engrafted into the hierarchical grammar, as well as a special glue rule S → hS ∼0 X ∼1 , S ∼0 X ∼1 i Modeling Phrase Orientation for Hierarchical Machine Translation (2) that the system can use for serial concatenation of phrases as in monotonic phrase-based translation. The initial symbol S is the start symbol of the grammar. Hierarchical search is conducted with a customized version of the CYK+ parsing algorithm (Chappelier and Rajman, 1998) and cube pruning (Chiang, 2007). A hypergraph which represents the whole parsing space is built employing CYK+. Cube pruning operates in bottom-up topological order on this hypergraph and expands at most k derivations at each hypernode. ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 ∧ ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 . (3) Consistency is based upon two conditions in this definition: (1.) At least one source and target position within the block must be aligned, and (2.) words from inside the source interval may only be aligned to words from inside the target interval and vice versa. These are the same conditions as those tha"
W13-2258,D08-1089,0,0.729017,"archical decoding is typically carried out with a parsing-based procedure. The parsing algorithm is extended to handle translation candi452 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452–463, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 different scores depending on how a currently translated phrase has been reordered with respect to its context. Popular lexicalized reordering models for phrase-based translation distinguish three orientation classes: monotone, swap, and discontinuous (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008). To obtain such a model, scores for the three classes are calculated from the counts of the respective orientation occurrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical phrase-based translation,"
W13-2258,D08-1039,1,0.911341,"Missing"
W13-2258,C10-1050,0,0.464037,"urrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently (He et al., 2010a; He et al., 2010b; Hayashi et al., 2010; Huck et al., 2012a), but in none of them are the orientation scores conditioned on the lexical identity of each phrase individually. These models are rather word-based and applied on block boundaries. Experimental results obtained with these other types of lexicalized reordering models have been very encouraging, though. There are certain reasons why assessing the adequacy of phrase reordering should be useful in hierarchical search: Outline The remainder of the paper is structured as follows: We briefly outline important related publications in the following section. We subsequently give a"
W13-2258,2007.mtsummit-tutorials.1,0,0.054362,"Missing"
W13-2258,2010.amta-papers.25,0,0.16219,"ts of the respective orientation occurrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently (He et al., 2010a; He et al., 2010b; Hayashi et al., 2010; Huck et al., 2012a), but in none of them are the orientation scores conditioned on the lexical identity of each phrase individually. These models are rather word-based and applied on block boundaries. Experimental results obtained with these other types of lexicalized reordering models have been very encouraging, though. There are certain reasons why assessing the adequacy of phrase reordering should be useful in hierarchical search: Outline The remainder of the paper is structured as follows: We briefly outline important related publications in the f"
W13-2258,D10-1054,0,0.0770109,"ts of the respective orientation occurrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently (He et al., 2010a; He et al., 2010b; Hayashi et al., 2010; Huck et al., 2012a), but in none of them are the orientation scores conditioned on the lexical identity of each phrase individually. These models are rather word-based and applied on block boundaries. Experimental results obtained with these other types of lexicalized reordering models have been very encouraging, though. There are certain reasons why assessing the adequacy of phrase reordering should be useful in hierarchical search: Outline The remainder of the paper is structured as follows: We briefly outline important related publications in the f"
W13-2258,P02-1038,1,0.659361,"checking for adjacent corners. 0) (4) σ · p(O) + N (O|α, β) . P σ + O0 ∈{M,S,D} N (O0 |f˜, e˜) (5) Here, N (O) denotes the global count and N (O|α, β) the lexicalized count for the orientation O. σ is a smoothing constant. To determine the orientation frequency for a hierarchical phrase with non-terminal symbols, the orientation counts of all those phrases are accumulated from which a sub-phrase is cut out and replaced by a non-terminal symbol to obtain this hierarchical phrase. Figure 2 gives an example. Negative logarithms of the values are used as costs in the log-linear model combination (Och and Ney, 2002). Cost 0 for all orientations is assigned to the special rules which are not extracted from the training data (initial and glue rule). p(O|α, β) = 455 source source source f4 X~0 f3 f2 f1 f4 f3 f2 X~0 f1 f4 f3 X~0 f2 f1 e1 e2 e3 X~0 e4 e1 e2 e3 X~0 e4 e1 e2 X~0 e3 e4 target target target (a) Monotone non-terminal orientation. (b) Swap non-terminal orientation. (c) Discontinuous non-terminal orientation. Figure 3: Scoring with the orientation classes monotone, swap, and discontinuous. Each picture shows exactly one hierarchical phrase. The block which replaces the non-terminal X during decoding"
W13-2258,2011.iwslt-papers.1,1,0.881651,"Missing"
W13-2258,J03-1002,1,0.0154534,"an language pair using the standard WMT3 newstest sets for development and testing. 7.1 Experimental Setup We work with a Chinese–English parallel training corpus of 3.0 M sentence pairs (77.5 M Chinese / 81.0 M English running words). To train the German→French baseline system, we use 2.0 M sentence pairs (53.1 M French / 45.8 M German running words) that are partly taken from the Europarl corpus (Koehn, 2005) and have partly been collected within the Quaero project.4 Word alignments are created by aligning the data in both directions with GIZA++5 and symmetrizing the two trained alignments (Och and Ney, 2003). When extracting phrases, we apply several restrictions, in particular a maximum length of ten on source and target side for lexical phrases, a length limit of five on source and ten on target side for hierarchical phrases (including non-terminal symbols), and no more than two non-terminals per phrase. A standard set of models is used in the baselines, comprising phrase translation probabilities and lexical translation probabilities in both directions, word and phrase penalty, binary features marking hierarchical rules, glue rule, and rules 7.2 Chinese→English Experimental Results Table 1 com"
W13-2258,P03-1021,0,0.319883,"Delayed scoring can lead to search errors; in order to keep them confined, the delayed scoring needs to be done separately for all derivations, not just for the first-best subderivations along the incoming hyperedges. 7 with non-terminals at the boundaries, three simple count-based binary features, phrase length ratios, and a language model. The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002). Model weights are optimized against B LEU (Papineni et al., 2002) with MERT (Och, 2003) on 100-best lists. For Chinese→English we employ MT06 as development set, MT08 is used as unseen test set. For German→French we employ newstest2009 as development set, newstest2008, newstest2010, and newstest2011 are used as unseen test sets. During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S . Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006). The results on MT08 are checked for statistical significance over the baseline. Confidence intervals have been computed using bootstrapping for B LEU and C"
W13-2258,2012.eamt-1.66,1,0.901944,"ligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently (He et al., 2010a; He et al., 2010b; Hayashi et al., 2010; Huck et al., 2012a), but in none of them are the orientation scores conditioned on the lexical identity of each phrase individually. These models are rather word-based and applied on block boundaries. Experimental results obtained with these other types of lexicalized reordering models have been very encouraging, though. There are certain reasons why assessing the adequacy of phrase reordering should be useful in hierarchical search: Outline The remainder of the paper is structured as follows: We briefly outline important related publications in the following section. We subsequently give a summary of some ess"
W13-2258,P02-1040,0,0.0871338,"nd the correct actual costs added. Delayed scoring can lead to search errors; in order to keep them confined, the delayed scoring needs to be done separately for all derivations, not just for the first-best subderivations along the incoming hyperedges. 7 with non-terminals at the boundaries, three simple count-based binary features, phrase length ratios, and a language model. The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002). Model weights are optimized against B LEU (Papineni et al., 2002) with MERT (Och, 2003) on 100-best lists. For Chinese→English we employ MT06 as development set, MT08 is used as unseen test set. For German→French we employ newstest2009 as development set, newstest2008, newstest2010, and newstest2011 are used as unseen test sets. During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S . Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006). The results on MT08 are checked for statistical significance over the baseline. Confidence intervals have been computed using bootstr"
W13-2258,2006.amta-papers.25,0,0.0243919,"; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002). Model weights are optimized against B LEU (Papineni et al., 2002) with MERT (Och, 2003) on 100-best lists. For Chinese→English we employ MT06 as development set, MT08 is used as unseen test set. For German→French we employ newstest2009 as development set, newstest2008, newstest2010, and newstest2011 are used as unseen test sets. During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S . Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006). The results on MT08 are checked for statistical significance over the baseline. Confidence intervals have been computed using bootstrapping for B LEU and Cochran’s approximate ratio variance for T ER (Leusch and Ney, 2009). Experiments We evaluate the effect of phrase orientation scoring in hierarchical translation on the Chinese→English 2008 NIST task2 and on the French→German language pair using the standard WMT3 newstest sets for development and testing. 7.1 Experimental Setup We work with a Chinese–English parallel training corpus of 3.0 M sentence pairs (77.5 M Chinese / 81.0 M English"
W13-2258,2010.amta-papers.8,1,0.873547,"Missing"
W13-2258,N03-1017,0,0.0994991,"Missing"
W13-2258,P07-2045,0,0.0460072,"training data. Hierarchical decoding is typically carried out with a parsing-based procedure. The parsing algorithm is extended to handle translation candi452 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452–463, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 different scores depending on how a currently translated phrase has been reordered with respect to its context. Popular lexicalized reordering models for phrase-based translation distinguish three orientation classes: monotone, swap, and discontinuous (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008). To obtain such a model, scores for the three classes are calculated from the counts of the respective orientation occurrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in practice. In hierarchical"
W13-2258,N04-4026,0,0.178256,"aligned parallel training data. Hierarchical decoding is typically carried out with a parsing-based procedure. The parsing algorithm is extended to handle translation candi452 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452–463, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 different scores depending on how a currently translated phrase has been reordered with respect to its context. Popular lexicalized reordering models for phrase-based translation distinguish three orientation classes: monotone, swap, and discontinuous (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008). To obtain such a model, scores for the three classes are calculated from the counts of the respective orientation occurrences in the word-aligned training data for each extracted phrase. The left-to-right orientation of phrases during phrase-based search can be easily determined from the start and end positions of continuous phrases. Approximations may need to be adopted for the right-to-left scoring direction. The utility of phrase orientation models in standard phrase-based translation is plausible and has been empirically established in pract"
W13-2258,D09-1105,0,0.121503,"ientations. Empirical results are presented in Section 7. We conclude the paper in Section 8. 3 Related Work Hierarchical phrase-based translation was proposed by Chiang (2005). He et al. (2010a) integrated a maximum entropy based lexicalized reordering model with word- and class-based features. Different classifiers for different rule patterns are trained for their model (He et al., 2010b). A comparable discriminatively trained model which applies a single classifier for all types of rules was developed by Huck et al. (2012a). Hayashi et al. (2010) explored the word-based reordering model by Tromble and Eisner (2009) in hierarchical translation. For standard phrase-based translation, Galley and Manning (2008) introduced a hierarchical phrase orientation model. Similar to previous approaches (Tillmann, 2004; Koehn et al., 2007), it distinguishes the three orientation classes monotone, swap, and discontinuous. However, it differs in that it is not limited to model local reordering phenomena, but allows for phrases to be hierarchically combined into blocks in order to determine the orientation class. This has the advantage that probability mass is shifted from the rather uninformative default category discon"
W13-2258,2005.mtsummit-papers.11,0,0.023636,"o variance for T ER (Leusch and Ney, 2009). Experiments We evaluate the effect of phrase orientation scoring in hierarchical translation on the Chinese→English 2008 NIST task2 and on the French→German language pair using the standard WMT3 newstest sets for development and testing. 7.1 Experimental Setup We work with a Chinese–English parallel training corpus of 3.0 M sentence pairs (77.5 M Chinese / 81.0 M English running words). To train the German→French baseline system, we use 2.0 M sentence pairs (53.1 M French / 45.8 M German running words) that are partly taken from the Europarl corpus (Koehn, 2005) and have partly been collected within the Quaero project.4 Word alignments are created by aligning the data in both directions with GIZA++5 and symmetrizing the two trained alignments (Och and Ney, 2003). When extracting phrases, we apply several restrictions, in particular a maximum length of ten on source and target side for lexical phrases, a length limit of five on source and ten on target side for hierarchical phrases (including non-terminal symbols), and no more than two non-terminals per phrase. A standard set of models is used in the baselines, comprising phrase translation probabilit"
W13-2258,N09-1027,0,0.0732795,"Missing"
W13-2258,W10-1738,1,0.928383,"Missing"
W13-2258,2008.iwslt-papers.8,1,0.880264,"Missing"
W13-2258,C04-1030,1,0.828,"model is to assess the adequacy of phrase reordering during search. In standard phrase-based translation with continuous phrases only and left-to-right hypothesis generation (Koehn et al., 2003; Zens and Ney, 2008), phrase reordering is implemented by jumps within the input sentence. The choice of the best order for the target sequence is made based on the language model score of this sequence and a distortion cost that is computed from the sourceside jump distances. Though the space of admissible reorderings is in most cases contrained by a maximum jump width or coverage-based restrictions (Zens et al., 2004) for efficiency reasons, the basic approach of arbitrarily jumping to uncovered positions on source side is still very permissive. Lexicalized reordering models assist the decoder in taking a good decision. Phrase-based decoding allows for a straightforward integration of lexicalized reordering models which assign Introduction In hierarchical phrase-based translation (Chiang, 2005), a probabilistic synchronous context-free grammar (SCFG) is induced from bilingual training corpora. In addition to continuous lexical phrases as in standard phrase-based translation, hierarchical phrases with usual"
W13-2258,D09-1022,1,\N,Missing
W13-2258,D08-1076,0,\N,Missing
W14-3310,E14-2008,1,0.50059,"nburgh, Scotland † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ {freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ‡ {mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ‡ v1rsennr@staffmail.ed.ac.uk ‡ maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk † {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu ‡ Matthias Abstract joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B"
W14-3310,D08-1089,0,0.0336771,"sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been emp"
W14-3310,N04-1035,0,0.0285873,"gmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an"
W14-3310,P05-1066,1,0.0515024,"employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tune"
W14-3310,P06-1121,0,0.0128251,". The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model"
W14-3310,P13-2071,1,0.0664637,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,2012.iwslt-papers.17,1,0.805256,".1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel"
W14-3310,W13-2212,1,0.898684,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,P12-1031,0,0.00714997,"um Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes"
W14-3310,P13-2121,1,0.0604984,"Missing"
W14-3310,W14-3309,1,0.840972,"btained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translatio"
W14-3310,W11-2123,0,0.00995075,"a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus"
W14-3310,W06-1607,0,0.0222136,"., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering"
W14-3310,W13-0805,1,0.0274022,"GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with s"
W14-3310,2011.iwslt-papers.5,1,0.681344,"rying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase t"
W14-3310,2009.iwslt-papers.4,1,0.346713,"bility, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize B LEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser"
W14-3310,D09-1022,1,0.0473567,"ingle generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on newstest2011 or newstest2012. We kept newstest2013 as an unseen test set wh"
W14-3310,P07-1019,0,0.0254758,"he settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language mod"
W14-3310,2011.iwslt-evaluation.9,1,0.882056,"n, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is d"
W14-3310,W13-2258,1,0.0602517,"r IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHK"
W14-3310,P10-2041,0,0.0226125,"for tuning the system combination or any of the individual systems. In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 6.1 The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1. KIT, UEDIN and RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model. The pairwise difference of the single system performances is up to 1.3 points in B LEU and 2.5 points in T ER. For German→English, our system combination p"
W14-3310,W14-3362,1,0.700694,"ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT trans"
W14-3310,W09-0435,0,0.00463497,"erated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained"
W14-3310,P03-1054,0,0.00425242,"ion obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the sy"
W14-3310,W08-0303,0,0.0192279,"sing an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word ord"
W14-3310,D07-1091,1,0.0290057,"2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→"
W14-3310,E03-1076,1,0.31096,"the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och"
W14-3310,W11-2124,1,0.0228284,"trip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on news"
W14-3310,2005.iwslt-1.8,1,0.035532,"2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single"
W14-3310,J03-1002,1,0.0147027,"of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid,"
W14-3310,E99-1010,0,0.0419329,"l., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 div"
W14-3310,P07-2045,1,0.0154876,"th a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004),"
W14-3310,P03-1021,0,0.0102254,"03) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has"
W14-3310,W14-3317,1,0.820032,"ty, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in B LEU and 1.0 points in T ER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering"
W14-3310,N04-1022,0,0.063305,"it (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual train"
W14-3310,W10-1738,1,0.159493,"Missing"
W14-3310,W08-1005,0,0.0331415,"nce reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2"
W14-3310,P06-1055,0,0.0182004,"dditional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs ar"
W14-3310,W12-3150,1,0.544959,"uster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a"
W14-3310,W08-1006,0,0.0232292,"e system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koeh"
W14-3310,W14-3324,1,0.0949085,"Missing"
W14-3310,2007.tmi-papers.21,0,0.125992,", 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system tra"
W14-3310,C12-3061,1,0.125144,"013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has"
W14-3310,C08-1098,0,0.00933067,"target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newst"
W14-3310,D13-1138,1,0.0721313,", morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules ("
W14-3310,C04-1024,0,0.0194264,"f the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottman"
W14-3310,R13-1079,1,0.28133,"stem and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster e"
W14-3310,N07-1051,0,\N,Missing
W14-3310,W05-0836,1,\N,Missing
W14-3310,W05-0909,0,\N,Missing
W14-3310,N13-1001,1,\N,Missing
W14-3310,2010.iwslt-evaluation.11,1,\N,Missing
W14-3310,2013.iwslt-evaluation.16,1,\N,Missing
W14-3310,W13-2213,1,\N,Missing
W14-3310,W14-3313,1,\N,Missing
W14-3310,W11-2145,1,\N,Missing
W14-3310,2013.iwslt-evaluation.3,1,\N,Missing
W14-3317,popovic-ney-2006-pos,1,\N,Missing
W14-3317,N04-4026,0,\N,Missing
W14-3317,E03-1076,0,\N,Missing
W14-3317,D08-1089,0,\N,Missing
W14-3317,P12-1031,0,\N,Missing
W14-3317,P02-1040,0,\N,Missing
W14-3317,W10-1738,1,\N,Missing
W14-3317,D13-1138,1,\N,Missing
W14-3317,P10-2041,0,\N,Missing
W14-3317,P10-1049,1,\N,Missing
W14-3317,W13-2258,1,\N,Missing
W14-3317,J03-1002,1,\N,Missing
W14-3317,W13-0804,1,\N,Missing
W14-3317,C12-3061,1,\N,Missing
W14-3317,W14-3310,1,\N,Missing
W14-3317,2012.iwslt-papers.7,1,\N,Missing
W14-3317,J07-2003,0,\N,Missing
W14-3317,W11-2123,0,\N,Missing
W14-3317,P13-2121,0,\N,Missing
W14-3317,P03-1021,0,\N,Missing
W14-3317,2011.iwslt-papers.5,1,\N,Missing
W15-3018,popovic-ney-2006-pos,1,\N,Missing
W15-3018,N04-4026,0,\N,Missing
W15-3018,E03-1076,0,\N,Missing
W15-3018,E99-1010,0,\N,Missing
W15-3018,D08-1089,0,\N,Missing
W15-3018,P12-1031,0,\N,Missing
W15-3018,P02-1040,0,\N,Missing
W15-3018,D13-1138,1,\N,Missing
W15-3018,P10-2041,0,\N,Missing
W15-3018,J92-4003,0,\N,Missing
W15-3018,P10-1049,1,\N,Missing
W15-3018,P11-2031,0,\N,Missing
W15-3018,J03-1002,1,\N,Missing
W15-3018,C12-3061,1,\N,Missing
W15-3018,N12-1005,0,\N,Missing
W15-3018,P03-1021,0,\N,Missing
W15-3018,P14-1129,0,\N,Missing
W15-3018,N15-1175,1,\N,Missing
W15-3033,J93-2003,0,0.079735,"Missing"
W15-3033,2014.iwslt-evaluation.1,0,0.345659,"Missing"
W15-3033,P11-2031,0,0.041135,"d cluster language model (Wuebker et al., 2013) and for comparison, we also experiment with a hierarchical reordering model (HRM) (Galley and Manning, 2008). When integrated into a phrase-based decoder, Durrani et al. (2013b) have shown the OSM to outperform bilingual LMs on MTUs. Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature. The OSM is trained on the same data as the ETM for all tasks. Bilingual data statistics for all tasks are shown in Table 1. For each system setting we evaluate three MERT runs using multeval (Clark et al., 2011). Results are reported in B LEU (Papineni et al., 2001) and T ER (Snover et al., 2006). The optimization criterion for all experiments is B LEU. 5.1 model # parameters phrase-based translation 57,155,149 EdTM lexicon alignment deletion 35,511,396 19,899,812 15,276,718 334,866 EiTM lexicon alignment deletion 34,994,534 20,153,114 14,791,722 49,698 Table 2: The number of model parameters for the BOLT Arabic→English bilingual training data after filtering. B LEU T ER Baseline + HRM 30.7 49.3 + EiTM Ge↔En none Ge→En Ge↔En 31.4 31.6 31.6 31.8 48.3 48.1 48.2 48.2 + EdTM none Ge↔En Ge→En Ge↔En Table"
W15-3033,H05-1022,0,0.426715,"d version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an extended lexical and reordering context, whereas the JTR n-gram model predicts the probability of bilingual word pairs. On the other hand, we do not assume linear sequences of dependencies, but propose and explicit treatment of multiply aligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment and can also be applied for more powerful phrase induction. Feng et al. (2013) introduce an reordering model based on sequence labeling techniques by converting the reordering problem into a a tagging task. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 2"
W15-3033,P14-1129,0,0.143344,"Missing"
W15-3033,P11-1105,0,0.074614,"ve been taken to compensate the downside of the phrase translation model. Mari˜no et al. (2006) introduce a translation model based on n-grams of bilingual word pairs, i.e. a bilingual language model (BILM), with an n-gram decoder that requires monotone alignments. In (Niehues et al., 2011), this is further advanced by BILMs operating on non-monotone alignments within a PBT framework. However, this differs from our approach: BILMs treat jointly aligned source words as atomic units, ignore source deletions and do not include reordering context. The Operation Sequence Model (OSM) introduced in (Durrani et al., 2011; Durrani et al., 2013a) includes n-grams of both translation and reordering operations in a consistent framework. It utilizes minimal translation units (MTUs) and is applied in a corresponding OSM decoder. Experiments in (Durrani et al., 2013b) show that a slightly enhanced version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single wo"
W15-3033,D13-1106,0,0.143354,"k. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014). All of these papers focus on lexical context and ignore the reordering aspect covered in our work. 3 (a) forward Figure 3: Overview of the jump classes ∆ j¯, j . same target position i, there are two possible jump classes: ( → (’step forward’), j = j¯ + 1 ∆ j¯, j = y (’jump forward’), j > j¯ + 1. Extended Translation Models In the following, we depict the derivations of the EiTM and the EdTM. Although they operate in opposite translation directions, both models incorporate the inverted alignment bI1 . Given a source sentence f1"
W15-3033,N13-1001,0,0.401112,"nsate the downside of the phrase translation model. Mari˜no et al. (2006) introduce a translation model based on n-grams of bilingual word pairs, i.e. a bilingual language model (BILM), with an n-gram decoder that requires monotone alignments. In (Niehues et al., 2011), this is further advanced by BILMs operating on non-monotone alignments within a PBT framework. However, this differs from our approach: BILMs treat jointly aligned source words as atomic units, ignore source deletions and do not include reordering context. The Operation Sequence Model (OSM) introduced in (Durrani et al., 2011; Durrani et al., 2013a) includes n-grams of both translation and reordering operations in a consistent framework. It utilizes minimal translation units (MTUs) and is applied in a corresponding OSM decoder. Experiments in (Durrani et al., 2013b) show that a slightly enhanced version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an"
W15-3033,J90-2002,0,0.67439,"Missing"
W15-3033,P13-2071,0,0.377167,"nsate the downside of the phrase translation model. Mari˜no et al. (2006) introduce a translation model based on n-grams of bilingual word pairs, i.e. a bilingual language model (BILM), with an n-gram decoder that requires monotone alignments. In (Niehues et al., 2011), this is further advanced by BILMs operating on non-monotone alignments within a PBT framework. However, this differs from our approach: BILMs treat jointly aligned source words as atomic units, ignore source deletions and do not include reordering context. The Operation Sequence Model (OSM) introduced in (Durrani et al., 2011; Durrani et al., 2013a) includes n-grams of both translation and reordering operations in a consistent framework. It utilizes minimal translation units (MTUs) and is applied in a corresponding OSM decoder. Experiments in (Durrani et al., 2013b) show that a slightly enhanced version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an"
W15-3033,C14-1041,0,0.108216,"aligned source words as atomic units, ignore source deletions and do not include reordering context. The Operation Sequence Model (OSM) introduced in (Durrani et al., 2011; Durrani et al., 2013a) includes n-grams of both translation and reordering operations in a consistent framework. It utilizes minimal translation units (MTUs) and is applied in a corresponding OSM decoder. Experiments in (Durrani et al., 2013b) show that a slightly enhanced version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an extended lexical and reordering context, whereas the JTR n-gram model predicts the probability of bilingual word pairs. On the other hand, we do not assume linear sequences of dependencies, but propose and explicit treatment of multiply aligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment and can also be applied for more p"
W15-3033,J06-4004,0,0.508457,"Missing"
W15-3033,P13-1033,0,0.283899,"ligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment and can also be applied for more powerful phrase induction. Feng et al. (2013) introduce an reordering model based on sequence labeling techniques by converting the reordering problem into a a tagging task. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014). All of these papers focus on lexical context and ignore the reordering aspect covered in our work. 3 (a) forward Figure 3: Overview of the jump classes ∆ j¯, j . same target pos"
W15-3033,P10-2041,0,0.060826,"nt models. The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online3 . The baseline systems are trained on all provided bilingual data. All systems are optimized on the dev2010 and evaluated on the test2010 corpus. The ETM is trained on the TED portions of the data: 138K sentences for German→English and 185K sentences for English→French. For German→English, to estimate the 4-gram LM, we additionally make use of parts of the Shuffled News, LDC English Gigaword and 109 French-English corpora, selected by a crossentropy difference criterion (Moore and Lewis, 2010). In total, 1.7 billion running words are taken for LM training. For English→French, we use a large general domain 5-gram LM and an indomain 5-gram LM. Both are estimated with the KenLM toolkit (Heafield et al., 2013) using interpolated Kneser-Ney smoothing. For the general domain LM, we first select 12 of the English Shuffled News, 41 of the French Shuffled News as well as both the English and French Gigaword corpora (10) where sK1 = s1 . . . sK is the hidden phrase alignment. The feature weights λm are tuned with minimum error rate training (MERT) (Och, 2003). The models hm , that are part o"
W15-3033,P13-1032,1,0.751388,"is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an extended lexical and reordering context, whereas the JTR n-gram model predicts the probability of bilingual word pairs. On the other hand, we do not assume linear sequences of dependencies, but propose and explicit treatment of multiply aligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment and can also be applied for more powerful phrase induction. Feng et al. (2013) introduce an reordering model based on sequence labeling techniques by converting the reordering problem into a a tagging task. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM desc"
W15-3033,W14-1616,0,0.433103,"nd can also be applied for more powerful phrase induction. Feng et al. (2013) introduce an reordering model based on sequence labeling techniques by converting the reordering problem into a a tagging task. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014). All of these papers focus on lexical context and ignore the reordering aspect covered in our work. 3 (a) forward Figure 3: Overview of the jump classes ∆ j¯, j . same target position i, there are two possible jump classes: ( → (’step forward’), j = j¯ + 1 ∆ j¯, j = y (’jump forward’), j > j¯ + 1. Extended Translation Models In th"
W15-3033,P02-1038,1,0.673185,"Missing"
W15-3033,D08-1089,0,0.113012,"subset of 67.8K sentences and the test set contains 1844 sentences. For the Arabic→English BOLT task, we use only the in-domain data for training the baseline and the ETM. The training and test sets contain text drawn from discussion forums in Egyptian Arabic. The evaluation set contains 1510 bilingual sentence pairs. The baseline systems for all tasks - except the Arabic→English BOLT task, where preliminary experiments showed no improvement - contain a 7-gram word cluster language model (Wuebker et al., 2013) and for comparison, we also experiment with a hierarchical reordering model (HRM) (Galley and Manning, 2008). When integrated into a phrase-based decoder, Durrani et al. (2013b) have shown the OSM to outperform bilingual LMs on MTUs. Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature. The OSM is trained on the same data as the ETM for all tasks. Bilingual data statistics for all tasks are shown in Table 1. For each system setting we evaluate three MERT runs using multeval (Clark et al., 2011). Results are reported in B LEU (Papineni et al., 2001) and T ER (Snover et al., 2006). The optimization criterion for all experiments is"
W15-3033,J03-1002,1,0.0268062,"e EdTM deletion model approximates the probability of e0 conditioned on all unaligned source words fb0 and is obtained by averaging over all unaligned source words: p(e0 |fbb0I ) = 3.2.2 ∑ j∈b0 Count Models and Smoothing So far, we have introduced the ETM and shown how to include unaligned words and multiple word dependencies. However, there are various possibilities to train the lexicon and alignment probabilities derived in Subsections 3.1 and 3.2. As a starting point, we apply relative frequencies obtained from bilingual training data, where the Viterbi alignment is estimated using GIZA++ (Och and Ney, 2003). In order to address data sparseness, we apply interpolated Kneser-Ney smoothing as described in (Chen and Goodman, 1998). In comparison to monolingual n-grams used in LMs, we lack any clear order of e, f , e0 , f 0 and ∆, since they include bilingual and reordering information. Similar to the approach taken by Mari˜no et al. (2006), we model the probability of the bilingual word pair (e, f ) given its predecessor (e0 , f 0 , ∆) which also includes the jump class. The EdTM lexicon model for dependencies on previously aligned target words is computed as alignment probability The direct probabi"
W15-3033,J04-4002,1,0.620968,"ining data of the IWSLT 2014 German→English, English→French and the DARPA BOLT Chinese→English, Arabic→English translation tasks. 4 Integration into Phrase-based Decoding therefore leads to a larger search space, in practice it does not degrade the search accuracy, as experiments with relaxed pruning parameters have shown. In this work, we apply a standard phrase-based translation system (Koehn et al., 2003). The decoding process is implemented as a beam search for the best translation given a set of models hm (eI1 , sK1 , f1J ). The goal of search is to maximize the log-linear feature score (Och and Ney, 2004): ( ) ˆ eˆI1 = arg max I,eI1 ,sK 1 M ∑ λm hm (eI1 , sK1 , f1J ) m=1 , 5 Evaluation We perform experiments on the largescale IWSLT 20142 (Cettolo et al., 2014) German→English, English→French and the large-scale DARPA BOLT Chinese→English, Arabic→English tasks. As mentioned in Section 4, all baseline systems include phrasal and lexical smoothing scores trained in both directions. Word alignments are trained with GIZA++, by sequentially running 5 iterations each for the IBM-1, HMM and IBM-4 alignment models. The domain of IWSLT consists of lecture-type talks presented at TED conferences which are"
W15-3033,D15-1165,1,0.333454,"e use of reordering gaps, i.e. it utilizes a simpler reordering approach. The OSM uses one joint model for reorderings and translations. In contrast, the ETM incorporates separate models to estimate the probability of words and the probability of reorderings. Furthermore, the OSM has the drawback that it extracts the MTUs sentencewise, thus one word can appear in several MTUs extracted from different sentence pairs. Since an MTUs is treated as an atomic unit, this results in a distribution of probability mass on overlapping events. The ETM overcomes this drawback by operating on single words. Guta et al. (2015) propose the conversion of bilingual sentence pairs and word alignments into joint translation and reordering (JTR) sequences. They investigate n-gram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural networks trained on JTR sequences. In comparison to the OSM, JTR models have smaller vocabulary sizes, as they operate on words, and incorporate simpler reordering structures. Nevertheless, they are shown to perform slightly better than the OSM when included into the log-linear framework of a phrase-based decoder. Previous Work Various approaches have been taken to comp"
W15-3033,W99-0604,1,0.821917,"Missing"
W15-3033,P13-2121,0,0.133495,"Missing"
W15-3033,P03-1021,0,0.0276926,"ifference criterion (Moore and Lewis, 2010). In total, 1.7 billion running words are taken for LM training. For English→French, we use a large general domain 5-gram LM and an indomain 5-gram LM. Both are estimated with the KenLM toolkit (Heafield et al., 2013) using interpolated Kneser-Ney smoothing. For the general domain LM, we first select 12 of the English Shuffled News, 41 of the French Shuffled News as well as both the English and French Gigaword corpora (10) where sK1 = s1 . . . sK is the hidden phrase alignment. The feature weights λm are tuned with minimum error rate training (MERT) (Och, 2003). The models hm , that are part of all baselines presented in this work, are phrasal and lexical translation scores in both directions, an n-gram LM, a simple distance-based distortion model and word and phrase penalties. All phrase pairs that are licensed by the word alignment are extracted from the training corpus and their probabilities estimated as relative frequencies. Moreover, the word alignment each phrase pair has been extracted from is memorized in the phrase table. Our extended translation models are integrated into this framework as additional features hm . They are trained in both"
W15-3033,E14-1003,0,0.361425,"13) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014). All of these papers focus on lexical context and ignore the reordering aspect covered in our work. 3 (a) forward Figure 3: Overview of the jump classes ∆ j¯, j . same target position i, there are two possible jump classes: ( → (’step forward’), j = j¯ + 1 ∆ j¯, j = y (’jump forward’), j > j¯ + 1. Extended Translation Models In the following, we depict the derivations of the EiTM and the EdTM. Although they operate in opposite translation directions, both models incorporate the inverted alignment bI1 . Given a source sentence f1J and its transla"
W15-3033,2001.mtsummit-papers.68,0,0.0602704,"for comparison, we also experiment with a hierarchical reordering model (HRM) (Galley and Manning, 2008). When integrated into a phrase-based decoder, Durrani et al. (2013b) have shown the OSM to outperform bilingual LMs on MTUs. Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature. The OSM is trained on the same data as the ETM for all tasks. Bilingual data statistics for all tasks are shown in Table 1. For each system setting we evaluate three MERT runs using multeval (Clark et al., 2011). Results are reported in B LEU (Papineni et al., 2001) and T ER (Snover et al., 2006). The optimization criterion for all experiments is B LEU. 5.1 model # parameters phrase-based translation 57,155,149 EdTM lexicon alignment deletion 35,511,396 19,899,812 15,276,718 334,866 EiTM lexicon alignment deletion 34,994,534 20,153,114 14,791,722 49,698 Table 2: The number of model parameters for the BOLT Arabic→English bilingual training data after filtering. B LEU T ER Baseline + HRM 30.7 49.3 + EiTM Ge↔En none Ge→En Ge↔En 31.4 31.6 31.6 31.8 48.3 48.1 48.2 48.2 + EdTM none Ge↔En Ge→En Ge↔En Table 3: Results for the German→English IWSLT data. The syste"
W15-3033,2006.amta-papers.25,0,0.0333176,"t with a hierarchical reordering model (HRM) (Galley and Manning, 2008). When integrated into a phrase-based decoder, Durrani et al. (2013b) have shown the OSM to outperform bilingual LMs on MTUs. Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature. The OSM is trained on the same data as the ETM for all tasks. Bilingual data statistics for all tasks are shown in Table 1. For each system setting we evaluate three MERT runs using multeval (Clark et al., 2011). Results are reported in B LEU (Papineni et al., 2001) and T ER (Snover et al., 2006). The optimization criterion for all experiments is B LEU. 5.1 model # parameters phrase-based translation 57,155,149 EdTM lexicon alignment deletion 35,511,396 19,899,812 15,276,718 334,866 EiTM lexicon alignment deletion 34,994,534 20,153,114 14,791,722 49,698 Table 2: The number of model parameters for the BOLT Arabic→English bilingual training data after filtering. B LEU T ER Baseline + HRM 30.7 49.3 + EiTM Ge↔En none Ge→En Ge↔En 31.4 31.6 31.6 31.8 48.3 48.1 48.2 48.2 + EdTM none Ge↔En Ge→En Ge↔En Table 3: Results for the German→English IWSLT data. The systems are optimized with MERT on t"
W15-3033,2014.amta-researchers.3,0,0.482182,"m our approach: BILMs treat jointly aligned source words as atomic units, ignore source deletions and do not include reordering context. The Operation Sequence Model (OSM) introduced in (Durrani et al., 2011; Durrani et al., 2013a) includes n-grams of both translation and reordering operations in a consistent framework. It utilizes minimal translation units (MTUs) and is applied in a corresponding OSM decoder. Experiments in (Durrani et al., 2013b) show that a slightly enhanced version of OSM performs best when integrated into the log-linear framework of a phrase-based decoder. Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can Although our approach is similar, there are the following significant differences: On the one hand, the ETM estimates the probability of single words conditioned on an extended lexical and reordering context, whereas the JTR n-gram model predicts the probability of bilingual word pairs. On the other hand, we do not assume linear sequences of dependencies, but propose and explicit treatment of multiply aligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment"
W15-3033,D14-1003,1,0.866615,"erings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014). All of these papers focus on lexical context and ignore the reordering aspect covered in our work. 3 (a) forward Figure 3: Overview of the jump classes ∆ j¯, j . same target position i, there are two possible jump classes: ( → (’step forward’), j = j¯ + 1 ∆ j¯, j = y (’jump forward’), j > j¯ + 1. Extended Translation Models In the following, we depict the derivations of the EiTM and the EdTM. Although they operate in opposite translation directions, both models incorporate the inverted alignment bI1 . Given a source sentence f1J and its translation eI1 , EiTM models the inverse probability p"
W15-3033,D13-1138,1,0.880176,"sentence pairs and the 5-gram LM on 2.9 billion running words in total. The ETM is trained on an indomain subset of 67.8K sentences and the test set contains 1844 sentences. For the Arabic→English BOLT task, we use only the in-domain data for training the baseline and the ETM. The training and test sets contain text drawn from discussion forums in Egyptian Arabic. The evaluation set contains 1510 bilingual sentence pairs. The baseline systems for all tasks - except the Arabic→English BOLT task, where preliminary experiments showed no improvement - contain a 7-gram word cluster language model (Wuebker et al., 2013) and for comparison, we also experiment with a hierarchical reordering model (HRM) (Galley and Manning, 2008). When integrated into a phrase-based decoder, Durrani et al. (2013b) have shown the OSM to outperform bilingual LMs on MTUs. Therefore, we directly compare ourselves with a 7-gram OSM implemented into our phrasebased decoder as an additional feature. The OSM is trained on the same data as the ETM for all tasks. Bilingual data statistics for all tasks are shown in Table 1. For each system setting we evaluate three MERT runs using multeval (Clark et al., 2011). Results are reported in B"
W15-3033,2002.tmi-tutorials.2,0,0.219139,"Missing"
W15-3033,N13-1002,0,0.173239,"extended lexical and reordering context, whereas the JTR n-gram model predicts the probability of bilingual word pairs. On the other hand, we do not assume linear sequences of dependencies, but propose and explicit treatment of multiply aligned words. Deng and Byrne (2005) present an HMM approach for word-to-phrase alignments, which performs similar to IBM-4 on the task of bitext alignment and can also be applied for more powerful phrase induction. Feng et al. (2013) introduce an reordering model based on sequence labeling techniques by converting the reordering problem into a a tagging task. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs. These are not integrated into decoding, but used in N-best rescoring. Another generative, word-based Markov chain translation model is presented by Feng and Cohn (2013). It exploits 283 a hierarchical Pitman-Yor process for smoothing, but is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on MTUs, similar to the OSM described above. Finally, there has been recent research on applying neural network models for extended context (Le et al., 2012; Auli et al., 2013; Hu"
W15-3033,P02-1040,0,\N,Missing
W15-3033,N03-1017,0,\N,Missing
W15-3033,C12-3061,1,\N,Missing
W15-3033,N12-1005,0,\N,Missing
W15-3033,P14-2023,0,\N,Missing
W16-2212,J92-4003,0,0.450598,"ls in SMT, Introduction Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to be sparse (Koehn, 2010). A fundamental solution to a sparsity problem in natural language processing is to reduce the vocabulary size. By mapping words onto a smaller label space, the models can be trained to have denser distributions (Brown et al., 1992; Miller et al., 2004; Koo et al., 2008). Examples of such labels are part-of-speech (POS) tags or lemmas. In this work, we investigate the vocabulary reduction for phrase translation models with respect to various vocabulary choice. We evaluate two types of smoothing models for phrase translation probability using different kinds of word-level labels. In particular, we use automatically generated word classes (Brown et al., 1992) to obtain 110 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 110–117, c Berlin, Germany, August 11-12, 2016. 2016 Assoc"
W16-2212,N13-1003,0,0.0132699,"aard (2011) verify the effectiveness of word classes as factors. Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 2013), an n-gram-based translation model (Durrani et al., 2014), and sparse word pair features (Haddow et al., 2015). The first and the third are trained with a large-scale discriminative training algorithm. For all usages of word-level labels in SMT, Introduction Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to"
W16-2212,C14-1041,0,0.0150933,"es as factors. Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 2013), an n-gram-based translation model (Durrani et al., 2014), and sparse word pair features (Haddow et al., 2015). The first and the third are trained with a large-scale discriminative training algorithm. For all usages of word-level labels in SMT, Introduction Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to be sparse (Koehn, 2010). A fundamental solution to a spars"
W16-2212,D08-1089,0,0.0619408,"Missing"
W16-2212,W15-3013,0,0.0151807,"words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 2013), an n-gram-based translation model (Durrani et al., 2014), and sparse word pair features (Haddow et al., 2015). The first and the third are trained with a large-scale discriminative training algorithm. For all usages of word-level labels in SMT, Introduction Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to be sparse (Koehn, 2010). A fundamental solution to a sparsity problem in natural language processing is to redu"
W16-2212,J82-2005,0,0.757906,"Missing"
W16-2212,D07-1091,0,0.0662732,"scenarios of different scales, showing that the smoothing works better with more parallel corpora. This work systematically analyzes the smoothing effect of vocabulary reduction for phrase translation models. We extensively compare various word-level vocabularies to show that the performance of smoothing is not significantly affected by the choice of vocabulary. This result provides empirical evidence that the standard phrase translation model is extremely sparse. Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables. 1 2 Related Work Koehn and Hoang (2007) propose integrating a label vocabulary as a factor into the phrase-based SMT pipeline, which consists of the following three steps: mapping from words to labels, labelto-label translation, and generation of words from labels. Rishøj and Søgaard (2011) verify the effectiveness of word classes as factors. Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In"
W16-2212,D13-1138,1,0.932935,"that vocabulary reduction is more effective for smoothing large-scale phrase tables. 1 2 Related Work Koehn and Hoang (2007) propose integrating a label vocabulary as a factor into the phrase-based SMT pipeline, which consists of the following three steps: mapping from words to labels, labelto-label translation, and generation of words from labels. Rishøj and Søgaard (2011) verify the effectiveness of word classes as factors. Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 2013), an n-gram-based translation model (Durrani et al., 2014), and sparse word pair features (Haddow et al., 2015). The first and the third are trained with a large-scale discriminative training algorithm. For all usages of word-level labels in"
W16-2212,N03-1017,0,0.0689207,"In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 2013), an n-gram-based translation model (Durrani et al., 2014), and sparse word pair features (Haddow et al., 2015). The first and the third are trained with a large-scale discriminative training algorithm. For all usages of word-level labels in SMT, Introduction Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to be sparse (Koehn, 2010). A fundamental solution to a sparsity problem in natural language processing is to reduce the vocabulary size. By mapping words onto a smaller label space, the models can be trained to have denser distributions (Brown et al., 1992; Miller et al., 2004; Koo et al., 2008). Examples of such labels are part-of-speech (POS) tags or lemmas. I"
W16-2212,W04-3250,0,0.358142,"Missing"
W16-2212,P08-1068,0,0.0668088,"Missing"
W16-2212,N04-1043,0,0.0187443,"ion Phrase-based systems for statistical machine translation (SMT) (Zens et al., 2002; Koehn et al., 2003) have shown state-of-the-art performance over the last decade. However, due to the huge size of phrase vocabulary, it is difficult to collect robust statistics for lots of phrase pairs. The standard phrase translation model thus tends to be sparse (Koehn, 2010). A fundamental solution to a sparsity problem in natural language processing is to reduce the vocabulary size. By mapping words onto a smaller label space, the models can be trained to have denser distributions (Brown et al., 1992; Miller et al., 2004; Koo et al., 2008). Examples of such labels are part-of-speech (POS) tags or lemmas. In this work, we investigate the vocabulary reduction for phrase translation models with respect to various vocabulary choice. We evaluate two types of smoothing models for phrase translation probability using different kinds of word-level labels. In particular, we use automatically generated word classes (Brown et al., 1992) to obtain 110 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 110–117, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computatio"
W16-2212,E99-1010,0,0.183726,"Missing"
W16-2212,P03-1021,0,0.0228894,"Missing"
W16-2212,W11-2155,0,0.288481,"el vocabularies to show that the performance of smoothing is not significantly affected by the choice of vocabulary. This result provides empirical evidence that the standard phrase translation model is extremely sparse. Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables. 1 2 Related Work Koehn and Hoang (2007) propose integrating a label vocabulary as a factor into the phrase-based SMT pipeline, which consists of the following three steps: mapping from words to labels, labelto-label translation, and generation of words from labels. Rishøj and Søgaard (2011) verify the effectiveness of word classes as factors. Assuming probabilistic mappings between words and labels, the factorization implies a combinatorial expansion of the phrase table with regard to different vocabularies. Wuebker et al. (2013) show a simplified case of the factored translation by adopting hard assignment from words to labels. In the end, they train the existing translation, language, and reordering models on word classes to build the corresponding smoothing models. Other types of features are also trained on wordlevel labels, e.g. hierarchical reordering features (Cherry, 201"
W16-2212,C12-3061,1,\N,Missing
