2006.jeptalnrecital-long.30,P02-1037,0,0.0287426,"Missing"
2006.jeptalnrecital-long.30,J92-1004,0,0.172169,"Missing"
2007.iwslt-1.22,C04-1168,0,0.0955926,"e known for sure, in spoken language translation they must be reconstructed from an audio signal. Instead of translating the raw audio signal, spoken language translation systems usually start from the output of an automatic speech recognition system, which usually takes the form of a word lattice. A straightforward way to translate a word lattice is to feed a textual translation system with the most probable sentence. To overcome some recognition errors, one can instead translate a set of n top ranked sentences extracted from the lattice and select the best translation among the one returned [1, 2]. Finally, a third approach is to translate directly the lattice with a specialized decoder tightly coupled with the speech recognition system [3, 4, 5, 6, 7]. In this paper, we present MISTRAL (Monotone yet Imperfect Statistical TRAnslation of Lattices), a discriminative phrase-based system that translates lattices in two passes. The first pass uses a beam-search decoder to extract a N-Best list of source sentences and their translations from the lattice and the second pass rescores this list with more complex feature functions. Different word-based lattice translation systems are presented i"
2007.iwslt-1.22,2005.iwslt-1.2,0,0.0620097,"language translation systems usually start from the output of an automatic speech recognition system, which usually takes the form of a word lattice. A straightforward way to translate a word lattice is to feed a textual translation system with the most probable sentence. To overcome some recognition errors, one can instead translate a set of n top ranked sentences extracted from the lattice and select the best translation among the one returned [1, 2]. Finally, a third approach is to translate directly the lattice with a specialized decoder tightly coupled with the speech recognition system [3, 4, 5, 6, 7]. In this paper, we present MISTRAL (Monotone yet Imperfect Statistical TRAnslation of Lattices), a discriminative phrase-based system that translates lattices in two passes. The first pass uses a beam-search decoder to extract a N-Best list of source sentences and their translations from the lattice and the second pass rescores this list with more complex feature functions. Different word-based lattice translation systems are presented in [3, 4, 5]. A generative phrase-based system is described in [6]. This system translates the lattice with a sequence of weighted finite state machines applie"
2007.iwslt-1.22,2005.mtsummit-papers.11,0,0.0221687,"on corpus are kept. The first pass is then used to extract the 500 best translations from a new corpus. Rescoring is optimised on this list with the downhill simplex algorithm. 4. Experiments 4.1. Data The corpus used for the shared task is composed of transcriptions of spontaneous conversations in the travel domain. It is divided in train (TRAIN) and development (DEV) sections containing respectively 19,722 and 996 sentence pairs. We also trained our models on the Italian-English section of the proceedings of the European Parliament (EUROPARL), which contains more than 928,000 sentence pairs [9]. The two corpora were converted to lower case and their punctuation marks were removed. The lattices of DEV, which were originally scored with a language model and an acoustic model, were augmented with posterior probabilities using the lattice-tool utility [10]. 1 Recombination is done after pruning because it is slower to execute. did not observe a degradation in translation quality by doing so. We System First Pass Rescoring We then trained a translation table on TRAIN and another on EUROPARL. To do so, we used a script that was provided for the shared task of the NAACL 2006 Workshop on St"
2007.iwslt-1.22,W06-3114,0,0.0343558,"ed to lower case and their punctuation marks were removed. The lattices of DEV, which were originally scored with a language model and an acoustic model, were augmented with posterior probabilities using the lattice-tool utility [10]. 1 Recombination is done after pruning because it is slower to execute. did not observe a degradation in translation quality by doing so. We System First Pass Rescoring We then trained a translation table on TRAIN and another on EUROPARL. To do so, we used a script that was provided for the shared task of the NAACL 2006 Workshop on Statistical Machine Translation [11]. This script uses the heuristics described in [12] to extract phrase pairs from a word alignment that was first produced by GIZA ++ [13]. It then computes five scores for each phrase pair: the posterior probability in each translation direction, the lexical probability in each translation direction and a constant phrase penalty. Knowing that the corpora contain many dates and numbers, we manually created a third translation table containing 122 entries translating days, months and numbers2 . The first 300 sentences of DEV were used to tune the coefficients of the first pass (200 for tuning an"
2007.iwslt-1.22,N03-1017,0,0.0233113,"emoved. The lattices of DEV, which were originally scored with a language model and an acoustic model, were augmented with posterior probabilities using the lattice-tool utility [10]. 1 Recombination is done after pruning because it is slower to execute. did not observe a degradation in translation quality by doing so. We System First Pass Rescoring We then trained a translation table on TRAIN and another on EUROPARL. To do so, we used a script that was provided for the shared task of the NAACL 2006 Workshop on Statistical Machine Translation [11]. This script uses the heuristics described in [12] to extract phrase pairs from a word alignment that was first produced by GIZA ++ [13]. It then computes five scores for each phrase pair: the posterior probability in each translation direction, the lexical probability in each translation direction and a constant phrase penalty. Knowing that the corpora contain many dates and numbers, we manually created a third translation table containing 122 entries translating days, months and numbers2 . The first 300 sentences of DEV were used to tune the coefficients of the first pass (200 for tuning and 100 for validation, see section 3.4) and the 300"
2007.iwslt-1.22,P00-1056,0,0.132471,"acoustic model, were augmented with posterior probabilities using the lattice-tool utility [10]. 1 Recombination is done after pruning because it is slower to execute. did not observe a degradation in translation quality by doing so. We System First Pass Rescoring We then trained a translation table on TRAIN and another on EUROPARL. To do so, we used a script that was provided for the shared task of the NAACL 2006 Workshop on Statistical Machine Translation [11]. This script uses the heuristics described in [12] to extract phrase pairs from a word alignment that was first produced by GIZA ++ [13]. It then computes five scores for each phrase pair: the posterior probability in each translation direction, the lexical probability in each translation direction and a constant phrase penalty. Knowing that the corpora contain many dates and numbers, we manually created a third translation table containing 122 entries translating days, months and numbers2 . The first 300 sentences of DEV were used to tune the coefficients of the first pass (200 for tuning and 100 for validation, see section 3.4) and the 300 following sentences to tune the coefficients of the rescoring pass. The remaining 396"
2007.iwslt-1.22,2001.mtsummit-papers.68,0,0.0698729,"Missing"
2007.iwslt-1.22,P07-2045,0,0.0104834,"Missing"
2007.iwslt-1.22,N03-1031,0,\N,Missing
2007.iwslt-1.22,P02-1040,0,\N,Missing
2007.jeptalnrecital-long.24,denis-etal-2006-deep,1,0.884231,"Missing"
2007.jeptalnrecital-long.24,J86-3001,0,0.0658584,"Missing"
2007.jeptalnrecital-long.24,popescu-belis-etal-2004-online,0,0.0288237,"Missing"
2007.jeptalnrecital-long.24,salmon-alt-romary-2004-towards,0,0.0195307,"Missing"
2007.jeptalnrecital-long.24,2006.jeptalnrecital-long.30,1,0.689931,"Missing"
2007.jeptalnrecital-long.24,J00-4005,0,0.0254864,"Missing"
2007.jeptalnrecital-poster.6,P97-1023,0,0.0380874,"Missing"
2007.jeptalnrecital-poster.6,H05-1043,0,0.100577,"Missing"
2007.jeptalnrecital-poster.6,W03-1014,0,0.0815478,"Missing"
2008.jeptalnrecital-court.21,W97-0204,0,0.139725,"Missing"
2009.jeptalnrecital-long.3,P98-1013,0,0.0221491,"Missing"
2009.jeptalnrecital-long.3,C96-2120,0,0.108306,"Missing"
2009.jeptalnrecital-long.3,sagot-etal-2006-lefff,0,0.069707,"Missing"
2009.jeptalnrecital-long.3,J92-1004,0,0.212364,"Missing"
2011.jeptalnrecital-court.4,W09-3302,0,0.0289141,"Missing"
2011.jeptalnrecital-court.4,W98-1118,0,0.0597957,"Missing"
2011.jeptalnrecital-court.4,W03-0430,0,0.0612071,"Missing"
2011.jeptalnrecital-long.8,esteve-etal-2010-epac,1,0.88325,"Missing"
2011.jeptalnrecital-long.8,J00-3003,0,0.247225,"Missing"
2016.jeptalnrecital-jep.41,giraudel-etal-2012-repere,0,0.0607418,"Missing"
2016.jeptalnrecital-jep.41,N06-2021,0,0.0794578,"Missing"
2016.jeptalnrecital-jep.41,N10-1108,0,0.0380926,"Missing"
2016.jeptalnrecital-long.5,P98-1013,0,0.0775484,"Missing"
2016.jeptalnrecital-long.5,bazillon-etal-2012-syntactic,1,0.846389,"Missing"
2016.jeptalnrecital-long.5,bechet-etal-2012-decoda,1,0.891737,"Missing"
2016.jeptalnrecital-long.5,C10-2046,0,0.0684206,"Missing"
2016.jeptalnrecital-long.5,W04-1013,0,0.0239437,"Missing"
2016.jeptalnrecital-long.5,P14-1115,0,0.0641249,"Missing"
2016.jeptalnrecital-long.5,P11-5003,0,0.0889028,"Missing"
2016.jeptalnrecital-long.5,H01-1054,0,0.136681,"Missing"
2017.jeptalnrecital-court.6,P98-1013,0,0.0815769,"Missing"
2017.jeptalnrecital-court.6,J14-1002,0,0.042123,"Missing"
2017.jeptalnrecital-court.6,L16-1601,0,0.0200449,"Missing"
2017.jeptalnrecital-court.6,johansson-etal-2012-semantic,0,0.0517494,"Missing"
2017.jeptalnrecital-court.6,C16-1040,0,0.0213593,"Missing"
2018.jeptalnrecital-court.4,hara-etal-2010-estimation,0,0.0818392,"Missing"
2018.jeptalnrecital-court.4,D14-1181,0,0.00513954,"Missing"
2018.jeptalnrecital-court.4,N13-1064,0,0.0306075,"Missing"
2018.jeptalnrecital-court.5,W17-4777,0,0.0420788,"Missing"
2018.jeptalnrecital-court.5,D14-1179,0,0.0452025,"Missing"
2018.jeptalnrecital-court.5,W16-2361,0,0.0638141,"Missing"
2018.jeptalnrecital-court.5,D17-1288,0,0.0613478,"Missing"
2018.jeptalnrecital-long.13,P08-1037,0,0.0567262,"Missing"
2018.jeptalnrecital-long.13,P15-1006,0,0.0589067,"Missing"
2018.jeptalnrecital-long.13,D16-1156,0,0.0311152,"Missing"
2018.jeptalnrecital-long.13,P17-1191,0,0.0198975,"Missing"
2018.jeptalnrecital-long.13,E17-2050,0,0.0304591,"Missing"
2018.jeptalnrecital-long.13,W17-6311,1,0.661114,"Missing"
2018.jeptalnrecital-long.13,J93-2004,0,0.0615618,"Missing"
2018.jeptalnrecital-long.13,J16-1002,1,0.87007,"Missing"
2018.jeptalnrecital-long.13,Q14-1006,0,0.100825,"Missing"
2019.jeptalnrecital-court.4,P98-1013,0,0.0571271,"Missing"
2019.jeptalnrecital-court.4,S17-2051,1,0.890372,"Missing"
2019.jeptalnrecital-court.4,W18-0530,0,0.0380174,"Missing"
2019.jeptalnrecital-court.4,P14-2053,0,0.0606828,"Missing"
2019.jeptalnrecital-court.4,D16-1264,0,0.11831,"Missing"
2019.jeptalnrecital-court.4,D17-1090,0,0.029907,"Missing"
2019.jeptalnrecital-court.4,W17-2603,0,0.0497315,"Missing"
2020.jeptalnrecital-taln.28,D19-5803,1,0.892204,"Missing"
2020.jeptalnrecital-taln.28,N19-1423,0,0.0470817,"Missing"
2020.jeptalnrecital-taln.28,fillmore-etal-2004-framenet,0,0.070679,"Missing"
2020.jeptalnrecital-taln.28,P17-1044,0,0.0497604,"Missing"
2020.jeptalnrecital-taln.28,L18-1159,1,0.891341,"Missing"
2020.jeptalnrecital-taln.28,N18-1202,0,0.076873,"Missing"
2020.jeptalnrecital-taln.28,D16-1264,0,0.115656,"Missing"
2020.jeptalnrecital-taln.28,D07-1002,0,0.161947,"Missing"
2020.jeptalnrecital-taln.28,K17-3009,0,0.0558784,"Missing"
2020.lrec-1.674,D19-5803,1,0.856783,"Missing"
2020.lrec-1.674,D19-1169,0,0.0192657,"thods, is the alignment between the source and target language when generating a text span answering a given question, as described in (Cui 5491 et al., 2019). To overcome this problem several studies have proposed to take advantage of multilingual training of word representation in order to capture cross-lingual lexical representations. When training a machine reading model on a large source language corpus, and a much smaller target language corpus, this cross-lingual space allows the target model to benefit from the large source language training examples. Such approaches were proposed in (Cui et al., 2019; Kumar et al., 2019; Lee and Lee, 2019). Another line of research consists in considering only multilingual lexical representations in a zero-shot cross-lingual setting where a machine reading model trained exclusively on a source language is applied to a target language for which no task-related data is available, an approach studied in (Siblini et al., 2019) and (Artetxe et al., 2019). In this study we will investigate this zero-shot cross-lingual paradigm by applying a BERT multilingual model finetuned for an MRC task on a source language to an evaluation dataset on a target language. More"
2020.lrec-1.674,W18-2605,0,0.0186037,"estion-Answering where questions are not generic in scope but are related to a particular document. Recently very large corpora (SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016)) containing triplets (document, question, answer) were made available to the scientific community allowing to develop supervised methods based on deep neural networks with promising results. These methods need very large training corpora to be efficient, however such kind of data only exists for English at the moment. Developing such resources for a new language requires a lot of effort, as presented in (He et al., 2018) for Chinese. Many methods have been proposed to help reducing this cost based on an automatic translation process between MRC resources in English and the target language (Asai et al., 2018; Lee and Lee, 2019). In addition to methods performing a full translation of English corpora into a target language, methods have been proposed to directly perform online translation with a multilingual alignment process (Asai et al., 2018). One of the key issues with translation based methods, is the alignment between the source and target language when generating a text span answering a given question, a"
2020.lrec-1.674,P19-1481,0,0.0119573,"nment between the source and target language when generating a text span answering a given question, as described in (Cui 5491 et al., 2019). To overcome this problem several studies have proposed to take advantage of multilingual training of word representation in order to capture cross-lingual lexical representations. When training a machine reading model on a large source language corpus, and a much smaller target language corpus, this cross-lingual space allows the target model to benefit from the large source language training examples. Such approaches were proposed in (Cui et al., 2019; Kumar et al., 2019; Lee and Lee, 2019). Another line of research consists in considering only multilingual lexical representations in a zero-shot cross-lingual setting where a machine reading model trained exclusively on a source language is applied to a target language for which no task-related data is available, an approach studied in (Siblini et al., 2019) and (Artetxe et al., 2019). In this study we will investigate this zero-shot cross-lingual paradigm by applying a BERT multilingual model finetuned for an MRC task on a source language to an evaluation dataset on a target language. Moreover, we will compar"
2020.lrec-1.674,L18-1159,1,0.826003,"nswer, paragraph-question pairs over 48 articles. Thanks to the paragraph and question identifiers, we generated the same subset from the original English SQUAD corpus, allowing comparisons to be drawn from the same subset in two different languages. 3.2. CALOR-QUEST a machine reading corpus with semantic annotations 3.2.1. Dataset collection One of the contributions of this work is to use a new French MRC corpus called C ALOR -Q UEST , developed from a corpus of encyclopedic documents annotated with semantic information (C ALOR -F RAME ) following the Berkeley Framenet paradigm described in (Marzinotto et al., 2018). The C ALOR -F RAME corpus was initially built in order to alleviate Semantic Frame detection for the French language with two main purposes. The first one was to have a large amount of annotated examples for each Frame with all their possible frame Elements, with the deliberate choice to annotate only the most frequent Frames. As a result, the corpus contains 53 different Frames but around 26k occurrences of them along with around 57k Frame Element occurrences. The second purpose was to study the impact of domain change and style change. To this end the corpus was built by gathering encyclop"
2020.lrec-1.674,D16-1264,0,0.110361,"Missing"
bazillon-etal-2012-syntactic,N07-1051,0,\N,Missing
bazillon-etal-2012-syntactic,C10-1011,0,\N,Missing
bazillon-etal-2012-syntactic,P05-1012,0,\N,Missing
bazillon-etal-2012-syntactic,D07-1101,0,\N,Missing
bazillon-etal-2012-syntactic,W03-3017,0,\N,Missing
bechet-etal-2012-decoda,P05-1012,0,\N,Missing
bechet-etal-2012-decoda,bazillon-etal-2012-syntactic,1,\N,Missing
bechet-etal-2012-decoda,garnier-rizet-etal-2008-callsurf,0,\N,Missing
bechet-etal-2012-decoda,W03-3017,0,\N,Missing
benkoussas-etal-2014-collection,kim-etal-2012-annotated,1,\N,Missing
C04-1082,J99-2004,0,0.0799316,"Missing"
C04-1082,P95-1039,0,0.060758,"Missing"
C04-1082,A00-1031,0,0.0428662,"hen they do not lead to a complete analysis of the sentence. The parser itself acts, in a sense, as a tagger since, while parsing the sentence, it chooses the right tag among a set of possible tags for each word. The reason why we still need a tagger and don’t let the parser do the job is time and space complexity. Parsers are usually more time and space consuming than taggers and highly ambiguous tags assignments can lead to prohibitive processing time and memory requirements. The tagger described in this paper is based on the standard Hidden Markov Model architecture (Charniak et al., 1993; Brants, 2000). Such taggers assign to a sequence of words W = w1 . . . wn , the part of speech tag sequence Tˆ = tˆ1 . . . tˆn which maximizes the joint probability P (T, W ) where T ranges over all possible tag sequences of length n. The probability P (T, W ) is itself decomposed into a product of 2n probabilities, n lexical probabilities P (wi |ti ) (emission probabilities of the HMM) and n syntactic probabilites (transition probabilities of the HMM). Syntactic probabilities model the probability of the occurrence of tag ti given a history which is the knowledge of the h preceding tags (ti−1 . . . ti−h )"
C04-1082,J93-2004,0,0.0234941,"Missing"
C04-1082,2004.jeptalnrecital-long.10,1,0.820729,"Missing"
C04-1082,tufis-etal-2000-principled,0,0.0505436,"Missing"
D19-5803,D17-1219,0,0.0586191,"Missing"
D19-5803,P18-1177,0,0.0329458,"Missing"
D19-5803,D17-1090,0,0.0502955,"Missing"
D19-5803,W18-0530,0,0.0377572,"Missing"
D19-5803,P98-1013,0,0.564217,"Missing"
D19-5803,W17-2603,0,0.0578148,"Missing"
D19-5803,W18-2605,0,0.203005,"n2 (1) Aix-Marseille Univ, Université de Toulon, CNRS, LIS, Marseille, France (2) Orange Labs, Lannion (1) {first.last}@lis-lab.fr (2) {first.last}@orange.com Abstract previous methods based on linguistic analysis or similarity metrics between questions and segments (Hermann et al., 2015). Recently the use of contextual word embeddings such as BERT (Devlin et al., 2018) or XLNet (Yang et al., 2019) lead to obtain another great increase in performance, reaching human-level performance according to some benchmarks 1 . These large corpora are only available in English, and more recently Chinese (He et al., 2018) but for other languages, such as French, there is no comparable resources and the effort required to collect such a large amount of data is very important, limiting the use of these methods to other languages or other application frameworks. To address this problem, several studies have proposed to generate automatically questions and answers directly from a text document such as Wikipedia pages (Du and Cardie, 2018) in order to build a training corpus for MRC models. One of the issues of such methods is the semantic errors that can occur between questions and answers due to the automatic gen"
D19-5803,L18-1159,1,0.509096,"rame, E is one Frame Element of f and C (for Context) is the set of the other Frame Elements. Given a triplet (F, E, C), questions can be produced for which the answer is E. In the case of the Losing Frame of Figure 1, which has three Frame Elements, three triplets (F, E, C) can be produced : corpus of question/answer pairs to train a question generation model, we will rely on simple patterns based on the semantic annotations of our target corpus. The main originality of this work is to use a large encyclopedic corpus in French annotated with a FrameNet semantic model, the CALOR FRAME corpus (Marzinotto et al., 2018), in order to automatically produce a large amount of semantically-valid pairs of questions and answerspans, the CALOR - QUEST corpus. Using FrameNet annotations for generating an MRC training corpus has a major drawback : the human effort needed to build such resources is arguably bigger than building directly a question/answer corpus such as SQuAD. However we believe this method has several advantages : — firstly corpora with frame-based annotations are available for many languages, even if often of limited sizes ; — secondly frame-based annotation is not linked to a single task such as MRC,"
D19-5803,P14-2053,0,0.0685118,"Missing"
D19-5803,D16-1264,0,0.0910622,"ge corpora. The collect of natural questions is reduced to a validation/test set. We applied this method on the French CALOR - FRAME corpus to develop the CALOR - QUEST resource presented in this paper. 1 Introduction Machine Reading Comprehension (MRC) is a Natural Language Understanding task consisting in retrieving text segments from a document thanks to a set of questions, each segment being an answer to a particular question. This task received a lot of attention in the past few years thanks to the availability of very large corpora of triplets (document, question, answer) such as SQuAD (Rajpurkar et al., 2016) or MS MARCO (Nguyen et al., 2016), each containing more than 100k triplets. In these corpora each question has been manually produced, either through crowd-sourcing or by collecting query logs from a search engine. These large corpora opened the door to the development of supervised machine learning approaches for MRC, mostly based on Deep Neural Network (Wang and Jiang, 2016; Seo et al., 2016), improving greatly the state-of-the-art over 1. https ://rajpurkar.github.io/SQuAD-explorer/ 19 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 19–26 c Hong Kong, Ch"
D19-5803,P16-1056,0,0.048778,"Missing"
devillers-etal-2004-french,H92-1003,0,\N,Missing
devillers-etal-2004-french,P01-1066,0,\N,Missing
devillers-etal-2004-french,antoine-etal-2002-predictive,1,\N,Missing
devillers-etal-2004-french,antoine-etal-2000-obtaining,1,\N,Missing
esteve-etal-2010-epac,bazillon-etal-2008-manual,1,\N,Missing
F14-2021,C02-1025,0,0.0242794,"Missing"
F14-2021,H05-1062,1,0.723119,"Missing"
F14-2021,galliano-etal-2006-corpus,0,0.0225259,"trois corpus, ` a savoir, les noms de personnes, les toponymes et les noms d’organisations. Les entit´es structur´ees de Quaero ont ´et´e ”aplaties” en ne gardant que la structuration de plus haut niveau (Ex : <pers.ind> <name.first> Jacques </name.first> <name.last> Chirac </name.last> </pers.ind> : <pers> Jacques Chirac </pers>). Deux corpus ont ´et´e utilis´es : 1. Corpus Quaero d’´emissions t´el´e-radio-diffus´ees annot´e en entit´es nomm´ees (QO) Le corpus QO consiste en l’annotation manuelle du corpus ESTER2 et du corpus d’´evaluation de syst`emes de reconnaissance de la parole Quaero (Galliano et al., 2006). Ce corpus comporte la particularit´e de n’ˆetre 4. http ://www.elra.info/ELRA.html 512 Impact de la nature et de la taille des corpus d’apprentissage sur les performances dans la [P-Et2.2] ´tection automatique des entite ´s nomme ´es de Books Plus tard, <pers> R. Janko </pers> s’est inspir´ e des remarques de <pers> T. Weischadle </pers>. ` A propos de l’introduction, il rel` eve plusieurs exceptions aux remarques formul´ ees par <pers> T. Weischadle </pers>, parmi lesquelles la pr´ esence du nom du dieu au vocatif. Revues <pers> Michael Spens </pers>, (<pers> M. Spens </pers>, Paysages cont"
F14-2021,W01-0521,0,0.179702,"Missing"
F14-2021,C96-1079,0,0.305374,"Missing"
F14-2021,M98-1002,0,0.0927634,"Missing"
F14-2021,2010.jeptalnrecital-court.34,0,0.0643462,"Missing"
F14-2021,E03-1015,0,0.0119931,"rminer quelle est la meilleure strat´egie pour adapter un syst`eme existant ` a un nouveau cadre d’utilisation, tout en minimisant l’effort d’adaptation. 1. http ://www.itl.nist.gov/iad/mig/tests/ace/ 2. http ://duc.nist.gov/ 3. http ://www.openedition.org/ 511 ´bastien, Bellot Patrice, Be ´chet Fre ´de ´ric [P-Et2.2] Ollagnier Ana¨ıs, Fournier Se 2 D´ etecteur d’entit´ es nomm´ ees et corpus d’apprentissage Les travaux en REN sont class´es principalement selon trois types d’approches : les approches symboliques, les approches statistiques et les approches hybrides. Les approches symboliques (Poibeau, 2003) se basent sur des r`egles ´ecrites ` a la main, ces approches ont l’avantage de privil´egier la pr´ecision des d´etections mais pr´esentent l’inconv´enient d’ˆetre tr`es d´ependantes du domaine d’application et d’ˆetre coˆ uteuses `a mettre en place si l’on veut obtenir une couverture satisfaisante (Nadeau & Sekine, 2007). Les approches statistiques (Burger et al., 2002) se basent sur un m´ecanisme d’apprentissage `a partir d’un corpus pr´e-´etiquet´e. De mani`ere g´en´erale, ce type d’approche privil´egie le rappel plutˆ ot que la pr´ecision. Cependant, comme pr´ecis´e pr´ec´edemment, ces ap"
F14-2021,W12-3606,0,0.0137225,"d’apprentissage. 2. Corpus Quaero de presse ancienne ´etendue en entit´es nomm´ees (QP) Le corpus QP consiste en l’oc´erisation de 76 num´eros de journaux, publi´es entre 1890-1891, fournis par la Biblioth`eque Nationale de France. Trois publications sont utilis´ees (Le Temps, La Croix et Le Figaro) pour un total de 295 pages. Ce corpus pr´esente plusieurs caract´eristiques. Premi`erement, il est enti`erement constitu´e de documents OCR-is´es dont le taux de qualit´e a ´et´e estim´e bon par rapport `a l’´etat de l’art dans le domaine (Character Error Rate de 5,09 et Word Error Rate de 36,59) (Rosset et al., 2012). Quelques erreurs r´esiduelles persistent notamment dans la reconnaissance de certain caract`ere comme le « e » souvent ´ecrit « o ». Deuxi`emement, ce corpus r´ef`ere `a une p´eriode assez ancienne dont les informations diff`erent des actuelles. Et troisi`emement, la particularit´e des journaux OCR-is´es bas´es sur des ´editions papiers se retrouve ´egalement dans leurs structures en colonnes. De ce fait, les textes conservent de nombreux sauts de ligne ainsi que de nombreuses c´esures. Dans l’article de Rosset (Rosset et al., 2012), trois syst`emes de REN ` a base de m´ethodes statistiques"
F14-2021,A97-1015,0,0.298822,"Missing"
H05-1062,P03-1006,0,0.00756198,"ke the one observed when processing ASR output. In order to reduce and control the insertion rate of our NER system, we implemented a two level approach: the first level is made of NE grammars coded as Finite State Machine (FSM) transducers and the second level is a statistical HMM-based tagger. 5.2.1 NE transducers To each NE category is attached a set of regular grammars, extracted from the ESTER training corpus and generalized thanks to the annotation guidelines and web-gathered word lists. Theses grammars are represented by Finite State Machines (FSMs) (thanks to the AT&T GRM/FSM toolkit (Allauzen et al., 2003)). These FSMs are transducers that accept word sequences on the input symbols and output NE labels on the output symbols. They are all grouped together in a single transducer, called Tgram , with a filler model that accepts any string of words. Because these FSMs are lexicalized with the words of the ASR lexicon, one can control the generalization capabilities of the grammars thanks to the occurrence contexts of these words in the training corpus. During the NER process, the first step is to compose the FSM representing the NE transducer and the output of the ASR module (either a 1-best word s"
H05-1062,W98-1118,0,0.0234423,"Rtext in the experiment section. The second NER system has been developed for this study and is specifically built for being tightly integrated with the ASR processes. The two main features of this system, called NERasr in the following, are its ability to process word lattices and the fact that the NER models are trained for a specific ASR lexicon. These two systems are going to be presented in the next sections. 5.1 Text-based NER system: NERtext Among all the different methods that have been proposed for NER, one can find rule based models (Cunningham et al., 2002), Maximum Entropy models (Brothwick et al., 1998), Conditionnal Random Fields or probabilistic HMM-based models (Bikel et al., 1999). Lingpipe implements an HMM-based model. It maximizes the probability of a tag sequence Ti over 1 Lingpipe: http://alias-i.com/lingpipe/ 494 a word sequence Wi . A context of two preceding words and one preceding tag is used to approximate this probability. Generalization is done through a simple process: words occurring with low frequency are replaced by feature based categories (capitalized, contains digits, . . . ). In this approach, there must be one tag per word. Words starting and ending entities are labe"
H05-1062,A00-1044,0,0.0400847,"Missing"
H05-1062,H01-1034,0,0.498172,"nderstanding Conferences (MUC), the Conferences on Natural Language Learning (CoNLL), the DARPA HUB-5 program or more recently the French ESTER Rich Transcription program on Broadcast News data. Most of these conferences have studied the impact of using transcripts generated by an Automatic Speech Recognition (ASR) system rather than written texts. It appears from these studies that unlike other IE tasks, NER performance is greatly affected by the Word Error Rate (WER) of the transcripts processed. To tackle this problem, different ideas have been proposed: modeling explicitly the ASR errors (Palmer and Ostendorf, 2001) or using the ASR system alternate hypotheses found in word lattices (Saraclar and Sproat, 2004). However performance in NER decreases dramatically when processing high WER transcripts like the ones that are obtained with unmatched conditions between the ASR training model and the data to process. This paper investigates this phenomenon in the framework of the NER task of the French Rich Transcription program of Broadcast News ESTER (Gravier et al., 2004). Several issues are addressed: Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based"
H05-1062,N04-1017,0,0.143584,"UB-5 program or more recently the French ESTER Rich Transcription program on Broadcast News data. Most of these conferences have studied the impact of using transcripts generated by an Automatic Speech Recognition (ASR) system rather than written texts. It appears from these studies that unlike other IE tasks, NER performance is greatly affected by the Word Error Rate (WER) of the transcripts processed. To tackle this problem, different ideas have been proposed: modeling explicitly the ASR errors (Palmer and Ostendorf, 2001) or using the ASR system alternate hypotheses found in word lattices (Saraclar and Sproat, 2004). However performance in NER decreases dramatically when processing high WER transcripts like the ones that are obtained with unmatched conditions between the ASR training model and the data to process. This paper investigates this phenomenon in the framework of the NER task of the French Rich Transcription program of Broadcast News ESTER (Gravier et al., 2004). Several issues are addressed: Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system. If it gives satisfaction"
H05-1062,N04-4010,0,0.056164,"y language models (built from textual data) or artificially introduced. One should remove these from the transcript to improve the quality of the labeling. In order to deal with ASR errors two approaches have been proposed: • modeling explicitly the ASR errors, thanks to a development corpus and a set of confidence measures, in order to detect the possible errors of the 1-best word string hypothesis (with the type of errors) before extracting the NEs (Palmer and Ostendorf, 2001); • exploiting a search space bigger than the 1-best hypothesis alone, either by taking into account an n-best list (Zhai et al., 2004) or the whole word lattice (Saraclar and Sproat, 2004). The method proposed in this paper is close to this second approach where the whole word lattice output by the ASR system is used in order to increase NER performance from noisy input. We will present also in the next section a new strategy for adapting NER models to ASR transcripts, based on one of the main characteristics of such transcripts: a closed vocabulary is used by the ASR system. To our knowledge this has never been fully exploited by NER systems. Indeed while the key point of NER systems on written text is their generalization"
L16-1166,bechet-etal-2012-decoda,1,0.897744,"Missing"
L16-1166,W15-4633,1,0.898256,"Missing"
L16-1166,nasr-etal-2014-automatically,1,0.806593,"Missing"
L16-1166,W15-0212,1,0.867136,"Missing"
L16-1166,bazillon-etal-2012-syntactic,1,\N,Missing
L16-1701,bechet-etal-2012-decoda,1,0.828923,"Missing"
L16-1701,W09-0505,1,0.873958,"Missing"
L18-1014,W15-4319,0,0.0354559,"Missing"
L18-1014,W16-3916,0,0.0119843,"ng Error Correction 1. Introduction is commonly expected. The typology of errors is slightly different and most recent works focus on one-to-one lexical errors (replacing one word by another). The availability of large corpora has led to the design of normalization lexicons (Han et al., 2012) that directly map correct words to there common ill-formed variants. (Sridhar, 2015) learns a normalization lexicon and converts it into a Finite State Transducer. More recently, the construction of normalization dictionaries using word embeddings on Twitter texts were performed for Brazilian Portuguese (Bertaglia and Nunes, 2016). In this paper, we focus on out-of-vocabulary words. We propose to generate variants of such words using a lexical corrector based on a customized edit distance and to use word embeddings as distributed representations of words to re-rank these hypotheses thanks to contextual distance estimation. In order to adapt POS tagging systems for noisy text, several approaches have proposed to use word clusters provided by hierarchical clustering approaches such as the Brown algorithm. (Owoputi et al., 2013) use word clusters along with dedicated lexical features to enrich their tagger in the context"
L18-1014,W14-4012,0,0.0307382,"Missing"
L18-1014,R13-1026,0,0.0263471,"on out-of-vocabulary words. We propose to generate variants of such words using a lexical corrector based on a customized edit distance and to use word embeddings as distributed representations of words to re-rank these hypotheses thanks to contextual distance estimation. In order to adapt POS tagging systems for noisy text, several approaches have proposed to use word clusters provided by hierarchical clustering approaches such as the Brown algorithm. (Owoputi et al., 2013) use word clusters along with dedicated lexical features to enrich their tagger in the context of online conversations. (Derczynski et al., 2013) use clustering approaches to handle linguistic noise, and train their system from a mixture of hand-annotated tweets and existing POS-labeled data. (Nasr et al., 2016) address the issue of training data mismatch in the context of online conversations and show that equivalent performance can be obtained by training on a small in domain corpus rather than using generic POS-labeled resources. Contact Center chat conversation is a particular type of noisy user generated text in the sense that it is a formal Computer Mediated Communication (CMC) interaction mode. It shares some normalization issue"
L18-1014,D12-1039,0,0.0312327,"es the remaining errors. Word embeddings are trained on a large corpus in order to address both normalization and POS tagging. Experiments are run on Contact Center chat conversations, a particular type of formal Computer Mediated Communication data. Keywords: Part of Speech Tagging, Computer Mediated Communication, Spelling Error Correction 1. Introduction is commonly expected. The typology of errors is slightly different and most recent works focus on one-to-one lexical errors (replacing one word by another). The availability of large corpora has led to the design of normalization lexicons (Han et al., 2012) that directly map correct words to there common ill-formed variants. (Sridhar, 2015) learns a normalization lexicon and converts it into a Finite State Transducer. More recently, the construction of normalization dictionaries using word embeddings on Twitter texts were performed for Brazilian Portuguese (Bertaglia and Nunes, 2016). In this paper, we focus on out-of-vocabulary words. We propose to generate variants of such words using a lexical corrector based on a customized edit distance and to use word embeddings as distributed representations of words to re-rank these hypotheses thanks to"
L18-1014,D12-1000,0,0.199842,"Missing"
L18-1014,P13-1155,0,0.0229557,"plying the corrector with the lower case lexicon described in 3.1. The original case is reintroduced before applying the POS tagger. The lexical corrector provides a list of candidates for correction, until a maximum cost is reach. This upper bound is proportional to the word length n in terms of number of letters and is computed as follows: max cost = n × γ In these experiments γ is set to 0.3. Here again contrastive experiments can be provided showing the impact of the γ parameter. As we are dealing with formal interactions,we did not apply the modification on the edit distance proposed by (Hassan and Menezes, 2013) where edit distance is computed on consonant skeletons, nor do we use Longest Common Subsequence Ratio (LCSR) as it didn’t reveal to be helpful in our case. 3.3. Cemb (w, αi (w)) = C(w, αi (w)) × demb (w, αi (w)) 4. The part of speech tagger used in our experiment is based on Gated Recurrent Units (GRU). GRUs, introduced by (Cho et al., 2014), are recurrent neural networks that work in a similar fashion than LSTMs. GRUs are simpler than LSTMs: they do not have an output gate, and the input and forget gates are merged into an update gate. This property allows GRUs to be computationally more ef"
L18-1014,C08-1056,1,0.761051,"deviations in the design of the tagger. We will show that a good compromise is to handle some of the errors through lexical normalization but also to design a robust POS tagger that handles orthographic errors. We propose to use word embeddings at both levels: for text normalization and for POS tagging. 2. Related work Text normalization has been studied for several years now, with different perspectives over time. When studying SMS style language, researchers tried to handle new phenomena including voluntary slang shortcuts through phonetic models of pronunciation (Toutanova and Moore, 2002; Kobus et al., 2008). Recently, the effort has been more particularly set on Social Media text normalization with specific challenges on Twitter texts (Baldwin et al., 2015), which has been shown to be more formal (Hu et al., 2013) that what 3. Text normalization Our text normalization process operates in two steps, the first one produces in-lexicon variants for an out of lexicon form. The second one reranks the forms produced by the first step, using a distributional distance. The first step is based on a lexicon and an edit distance while the second relies on word embeddings. We focus on one-to-one normalizatio"
L18-1014,W16-3621,1,0.929075,"ted representations of words to re-rank these hypotheses thanks to contextual distance estimation. In order to adapt POS tagging systems for noisy text, several approaches have proposed to use word clusters provided by hierarchical clustering approaches such as the Brown algorithm. (Owoputi et al., 2013) use word clusters along with dedicated lexical features to enrich their tagger in the context of online conversations. (Derczynski et al., 2013) use clustering approaches to handle linguistic noise, and train their system from a mixture of hand-annotated tweets and existing POS-labeled data. (Nasr et al., 2016) address the issue of training data mismatch in the context of online conversations and show that equivalent performance can be obtained by training on a small in domain corpus rather than using generic POS-labeled resources. Contact Center chat conversation is a particular type of noisy user generated text in the sense that it is a formal Computer Mediated Communication (CMC) interaction mode. It shares some normalization issues with other CMC texts such as chatroom conversations or social media interactions but unlike the aforementioned cases, the professional context implies some specificit"
L18-1014,N13-1039,0,0.0795164,"Missing"
L18-1014,sagot-2010-lefff,0,0.0253431,"ir associated morphological and lexical features. The words are encoded using a lookup table which associates each word with its word embedding representation. These word embeddings can be initialized with pretrained embeddings and/or learned when training the model. For the morphological and typographic features, we use a boolean value for the presence of an uppercase character as the first letter of the word as well as the word suffixes of length 3 and 4 represented as onehot vectors. Finally, we also input as onehot vectors external lexicon information, constructed using the Lefff lexicon (Sagot, 2010). Such vectors represent the possible part-of-speech labels of a word. On the output layer, we use a softmax activation. During training, categorical Rescoring with word embeddings The edit distance based variant generation process described above does not take into account the context of a word when generating variants. In order to take it into 1 Part of speech tagging https://github.com/Orange-OpenSource/lexical-corrector 89 cross-entropy is used as the loss function and the Adam optimiser (Kingma and Ba, 2014) is used for the gradient descent optimisation. 5. # of correctable err. # of non"
L18-1014,W15-1502,0,0.014803,"ss both normalization and POS tagging. Experiments are run on Contact Center chat conversations, a particular type of formal Computer Mediated Communication data. Keywords: Part of Speech Tagging, Computer Mediated Communication, Spelling Error Correction 1. Introduction is commonly expected. The typology of errors is slightly different and most recent works focus on one-to-one lexical errors (replacing one word by another). The availability of large corpora has led to the design of normalization lexicons (Han et al., 2012) that directly map correct words to there common ill-formed variants. (Sridhar, 2015) learns a normalization lexicon and converts it into a Finite State Transducer. More recently, the construction of normalization dictionaries using word embeddings on Twitter texts were performed for Brazilian Portuguese (Bertaglia and Nunes, 2016). In this paper, we focus on out-of-vocabulary words. We propose to generate variants of such words using a lexical corrector based on a customized edit distance and to use word embeddings as distributed representations of words to re-rank these hypotheses thanks to contextual distance estimation. In order to adapt POS tagging systems for noisy text,"
L18-1014,P02-1019,0,0.072417,"directly handling language deviations in the design of the tagger. We will show that a good compromise is to handle some of the errors through lexical normalization but also to design a robust POS tagger that handles orthographic errors. We propose to use word embeddings at both levels: for text normalization and for POS tagging. 2. Related work Text normalization has been studied for several years now, with different perspectives over time. When studying SMS style language, researchers tried to handle new phenomena including voluntary slang shortcuts through phonetic models of pronunciation (Toutanova and Moore, 2002; Kobus et al., 2008). Recently, the effort has been more particularly set on Social Media text normalization with specific challenges on Twitter texts (Baldwin et al., 2015), which has been shown to be more formal (Hu et al., 2013) that what 3. Text normalization Our text normalization process operates in two steps, the first one produces in-lexicon variants for an out of lexicon form. The second one reranks the forms produced by the first step, using a distributional distance. The first step is based on a lexicon and an edit distance while the second relies on word embeddings. We focus on on"
L18-1159,P98-1013,0,0.900593,"rated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. The models compared are CRFs and multitasks bi-LSTMs. Keywords: Frame Semantic Parsing, LSTM, CRF 1. Introduction Semantic Frame parsing is a Natural Language Understanding task that involves detecting in a sentence an event or a scenario, called Frame, as well as all the elements or roles that can be associated to this event in the sentence, called Frame Elements. One of the most popular semantic frame model is the Berkeley FrameNet project developed by ICSI Berkeley (Baker et al., 1998). This model is composed of an inventory of Frames with, for each of them, a list of words, called Lexical Units (or LU), that can trigger a frame in a sentence. Besides, for each frame, a list of Frame Elements (FE), core or optional, is defined. LUs are pairings of a word with a sense; Frame Elements are the components of a frame, represented by sequences of words in a sentence. Two kinds of parsing can be done with a Semantic Frame model: full text parsing where each word in a sentence is analyzed to check if it can trigger a frame; and partial parsing where only a subset of frames and LUs"
L18-1159,candito-etal-2014-developing,0,0.0449117,"Missing"
L18-1159,P14-1136,0,0.159427,"CRF relatively small, limited to the frames that can be triggered by the word considered. Therefore the ambiguity is limited and CRFs can be trained efficiently even with a large number of features. However the drawback is that the training data is split across words in the LU lexicon, therefore similarities among LU are not exploited. This situation is acceptable if enough training examples are provided for each LUs, which is the case for the CALOR corpus. 4.2. Multi-task LSTM approach Deep Neural Networks (DNN) with word embedding is the state of the art approach for semantic frame parsing (Hermann et al., 2014). More recently recurrent neural networks (RNN) with Long Short Memory (LSTM) cells have been applied to several semantic tagging tasks such as slot filling (Mesnil et al., 2015) or even frame parsing (Hakkani-T¨ur et al., 2016; Tafforeau et al., 2016) for Spoken Language Understanding. Following these previous works, we propose in this study a single-layered bidirectional LSTM sequence to sequence architecture to perform frame tagging. To deal with the multi-label issue we could train a biLSTM model per LU, using the same approach as for the CRF, however, the number of examples per LU is redu"
L18-1159,W03-0430,0,0.0251963,"he absolute value of these links is not meaningful and cannot be predicted the same way as the semantic labels are. In this study we compare two different strategies in order to deal with the multi-label issue, one based on CRF with a multi-model approach (each LU has its own prediction model) and one based on a bi-LSTM model following a multi-task approach. They are described in the next section. 4. 4.1. Sequence labeling models Multi-model CRF approach CRF-based approaches have been used in many NLP tasks involving sequence labeling such as POS tagging, chunking or named entity recognition (McCallum and Li, 2003). In order to apply CRF to frame parsing, as described in section 3., we need to address the multi-label issue. Since we want to perform frame disambiguation and semantic role detection in one step, and because each word in a sentence cannot trigger more than one frame, we chose a multimodel approach where a CRF-model is trained for each word belonging to the LU lexicon. This approach is described in Figure 3. At training time, the corpus is split according to the LU lexicon: to each word Wi belonging to this lexicon is attached a sub-corpus containing all the sentences CWi where Wi occurs. Fo"
L18-1159,P11-4015,1,0.681968,"Missing"
L18-1716,D11-1113,0,0.0240604,"Missing"
L18-1716,N07-1049,0,0.0621939,"Missing"
L18-1716,W16-3210,0,0.0493476,"Missing"
L18-1716,W05-1505,0,0.0831543,"Missing"
L18-1716,P11-4015,1,0.873913,"Missing"
L18-1716,Q14-1006,0,0.0432872,"age is described with five captions, each annotated with entities. Entities that corefer with a visual element in the image are linked to the corresponding bounding box. Finally, for every preposition manually attached, a set of possible attachment alternatives for use in a reranking system is produced. 2. Enriching the Flickr30k Entities Corpus with PP-Attachment Annotations Corpora with joint annotation of image and text has recently become widely available. The corpus used in this work is the Flickr30k Entities (F30kE) (Plummer et al., 2017), an extension of the original Flickr30k dataset (Young et al., 2014). This corpus is composed of almost 32K images and, for each image, five captions describing the image have been produced. Besides, every object in the image that corresponds to a mention in the captions has been manually identified with a bounding box. 4520 Preposition in with for near through on next to from into over by at of around in front of under behind along during across down against outside towards out of beside above in the middle of onto outside of inside between past toward on top of like among after away from off up up to before atop about along with underneath without out at the"
N19-2021,P98-1013,0,0.0871379,"Missing"
N19-2021,D18-1002,0,0.0267427,"nguage classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al., 2018) and removal of protected attributes from social media textual data (Elazar and Goldberg, 2018). 2.2 Semantic parsing model with an adversarial training scheme 3.2 Sequence encoding/decoding For all experiments we use a BIO label encoding. To ensure that output sequences respect the BIO constrains we implement an A∗ decoding strategy similar to the one proposed by (He et al., 2017). We further apply a coherence filter to the output of the tagging process. This filter ensures that the predicted semantic structure is acceptable. Given a sentence and a word w that is a Lexical Unit (LU) trigger, we select the frame F as being the most probable frame among the ones that can have w as a trig"
N19-2021,D15-1112,0,0.0293163,"Missing"
N19-2021,passonneau-etal-2012-masc,0,0.0148032,"null hypothesis is selected if its probability is higher than P (yt = O). Varying δ > 0 (resp. δ < 0) is equivalent to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 201"
N19-2021,D18-1131,0,0.0229096,"et al., 2017) and sentiment analysis (Chen et al., 2016). These approaches introduce language classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al., 2018) and removal of protected attributes from social media textual data (Elazar and Goldberg, 2018). 2.2 Semantic parsing model with an adversarial training scheme 3.2 Sequence encoding/decoding For all experiments we use a BIO label encoding. To ensure that output sequences respect the BIO constrains we implement an A∗ decoding strategy similar to the one proposed by (He et al., 2017). We further apply a coherence filter to the output of the tagging process. This filter ensures that the predicted semantic structure is acceptable. Given a sentence and a word w that is a Lexical Unit (LU) trigger,"
N19-2021,E17-1045,0,0.0149287,"probability is higher than P (yt = O). Varying δ > 0 (resp. δ < 0) is equivalent to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Y"
N19-2021,D18-1548,0,0.0502383,"Missing"
N19-2021,P17-1044,0,0.254675,"e to obtain labels for the classification task. Firstly we perform experiments on a large multi-domain frame corpus (Marzinotto et al., 2018a) where only a relatively small number of frames where annotated, corresponding to possible targets in an Information Extraction applicative framework. We evaluate our adversarial framework with a semantic frame parser we developed on this corpus and presented in (Marzinotto et al., 2018b). Secondly we checked the genericity of our approach on the standard PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark, with a tagging model proposed by (He et al., 2017). We show that in both cases adversarial learning increases all models generalization capabilities both on in and out-of-domain data. This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations."
N19-2021,D17-1128,0,0.638184,"s searched where the taskspecific classifier is good and the domain classifier is bad. It has been shown in (Ganin and Lempitsky, 2015) that this implicitly optimizes the hidden representation towards domain independence. 3.1 Semantic parsing model: biGRU We use in this study a sequence tagging semantic frame parser that performs frame selection and argument classification in one step based on a deep bi-directional GRU tagger (biGRU ). The advantage of this architecture is its flexibility as it can be applied to several semantic parsing schemes such as PropBank (He et al., 2017) and FrameNet (Yang and Mitchell, 2017). More precisely, the model consists of a 4 layer bi-directional Gated Recurrent Unit (GRU) with highway connections (Srivastava et al., 2015). This model does not rely solely on word embeddings as input. Instead, it has a richer set of features including syntactic, morphological and surface features. (see (Marzinotto et al., 2018b) for more details). Except for words where we use pre-trained embeddings, we use randomly initialized embedding layers for categorical features. In NLP problems this approach has successfully been used to train cross-lingual word representations (Conneau et al., 201"
N19-2021,P14-1136,0,0.0166367,"YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Yang and Mitchell, 2017). In this paper we propose to study the generalization issue within the framework of a sequence tagging semantic frame parser that performs frame selection and argument classification in one step. 167 3.3 Adversarial Domain Classifier In order to design an efficient adversarial task, several criteria have to be met. The task has to be related to the biases it is supposed to alleviate. And furthermore, the adversarial task should not be correlated to the main task (i.e semantic parsing here), otherwise it may harm the model’s performances. Determining where these b"
N19-2021,Q15-1020,0,0.0307751,"words surrounding the LU. 6 Generalization to PropBank Parsing We further show that this adversarial learning technique can be used on other semantic frameworks such as Propbank. In PropBank Semantic Role Labeling, CoNLL-2005 uses Wall Street Journal (WSJ) for training and two test corpora. The in-domain (ID) test set is derived from WSJ and the out-of-domain (OOD) test set contains ’general fiction’ from the Brown corpus. In published works, there is always an important gap in performances between ID and OOD. Several studies have tried to develop models with better generalization capacities (Yang et al., 2015), (FitzGerald et al., 2015). In recent works, PropBank SRL systems have evolved and span classifier approaches have been replaced by current state of the art sequence tagging models that use recurrent neural networks (He et al., 2017) and neural atten171 biGRU biGRU +AC Target Identification in-domain out-of-domain D1 D2 D3 97.6 95.5 93.3 97.6 95.6 94.3 Frame Identification in-domain out-of-domain D1 D2 D3 93.8 93.4 90.9 95.3 94.5 91.2 Argument Identification in-domain out-of-domain D1 D2 D3 58.2 46.1 43.6 60.0 47.1 45.2 Table 3: Frame semantic parsing performances (Fmax). Models trained on D1"
N19-2021,C08-1050,0,0.0327132,"t to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Yang and Mitchell, 2017). In this paper we propose to study the generalization issue"
N19-2021,D17-1302,0,0.0504865,"t (GRU) with highway connections (Srivastava et al., 2015). This model does not rely solely on word embeddings as input. Instead, it has a richer set of features including syntactic, morphological and surface features. (see (Marzinotto et al., 2018b) for more details). Except for words where we use pre-trained embeddings, we use randomly initialized embedding layers for categorical features. In NLP problems this approach has successfully been used to train cross-lingual word representations (Conneau et al., 2017) and to transfer learning from English to low resource languages for POS tagging (Kim et al., 2017) and sentiment analysis (Chen et al., 2016). These approaches introduce language classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al.,"
N19-2021,P17-1001,0,0.06826,"Missing"
N19-2021,L18-1159,1,0.864948,"Missing"
N19-2021,C98-1013,0,\N,Missing
N19-2021,Q18-1039,0,\N,Missing
nasr-etal-2014-automatically,H05-1066,0,\N,Missing
nasr-etal-2014-automatically,bazillon-etal-2012-syntactic,1,\N,Missing
nasr-etal-2014-automatically,bechet-etal-2012-decoda,1,\N,Missing
oger-etal-2008-local,gravier-etal-2004-ester,0,\N,Missing
P00-1011,J93-2006,0,\N,Missing
P00-1011,A97-1030,0,\N,Missing
P00-1011,W99-0613,0,\N,Missing
P11-4015,W10-1408,1,0.44138,"Missing"
P11-4015,2006.jeptalnrecital-long.5,0,0.0461776,"rk has been funded by the French Agence Nationale pour la Recherche, through the projects SEQUOIA (ANR-08EMER-013) and DECODA (2009-CORD-005-01) 1 Annotation must be taken here in a general sense which includes tagging, segmentation or the construction of more complex objets as syntagmatic or dependencies trees. 86 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86–91, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics of processing. 2 Several processing tools suites alread exist for French among which SXPIPE (Sagot and Boullier, 2008), OUTILEX (Blanc et al., 2006), NOOJ2 or UNI TEX 3 . A general comparison of MACAON with these tools is beyond the scope of this paper. Let us just mention that MACAON shares with most of them the use of finite state machines as core data representation. Some modules are implemented as standard operations on finite state machines. The MACAON exchange format is based on four concepts: segment, attribute, annotation level and segmentation. A segment refers to a segment of the text or speech signal that is to be processed, as a sentence, a clause, a syntactic constituent, a lexical unit, a named entity . . . A segment can be"
P11-4015,P06-1055,0,0.0181302,"ace that allows to inspect MACAON XML files and run the components. 3.3 sentation of many NLP tools input and output in the MACAON format. MACAON has been interfaced with the SPEERAL Automatic Speech Recognition System (Nocera et al., 2006). The word lattices produced by SPEERAL can be converted to pre-lexical MACAON automata. MACAON does not provide any native module for parsing yet but it can be interfaced with any already existing parser. For the purpose of this demonstration we have chosen the LORG parser developed at NCLT, Dublin14 . This parser is based on PCFGs with latent annotations (Petrov et al., 2006), a formalism that showed state-of-the-art parsing accuracy for a wide range of languages. In addition it offers a sophisticated handling of unknown words relying on automatically learned morphological clues, especially for French (Attia et al., 2010). Moreover, this parser accepts input that can be tokenized, postagged or pre-bracketed. This possibility allows for different settings when interfacing it with MACAON. 4 Applications MACAON has been used in several projects, two of which are briefly described here, the D EFINIENS project and the L UNA project. D EFINIENS (Barque et al., 2010) is"
P11-4015,W05-0618,0,\N,Missing
P11-4015,J08-2002,0,\N,Missing
P11-4015,sagot-etal-2006-lefff,0,\N,Missing
S13-2075,baccianella-etal-2010-sentiwordnet,0,0.0957417,"Missing"
S13-2075,C10-2005,0,0.042988,"ated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) (Go, Bhayani et al. 2009). Go et al. (2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decline the results. N-gram with lexicon features and microbloging features were useful but POS features were not (Kouloumpis, Wilson et al. 2011). In contrast, Pak & Paroubek (2010) reported that POS and bigrams both help. Barbosa & Feng (2010) proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words, Agarwal et al. (2011) extended their approach by using real valued prior polarity and by combining prior polarity with POS. They build models for classifying tweets into positive, negative and neutral sentiment classes and three models were proposed: a unigram model, a feature based model and a tree kernel based model which presented a new tree representation for tweets. Both combining unigrams with their feat"
S13-2075,P11-4022,0,0.0362534,"Missing"
S13-2075,pak-paroubek-2010-twitter,0,0.0345866,"l. (2012). Machine learning approaches were employed from annotated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) (Go, Bhayani et al. 2009). Go et al. (2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decline the results. N-gram with lexicon features and microbloging features were useful but POS features were not (Kouloumpis, Wilson et al. 2011). In contrast, Pak & Paroubek (2010) reported that POS and bigrams both help. Barbosa & Feng (2010) proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words, Agarwal et al. (2011) extended their approach by using real valued prior polarity and by combining prior polarity with POS. They build models for classifying tweets into positive, negative and neutral sentiment classes and three models were proposed: a unigram model, a feature based model and a tree kernel based model which presented a new tree rep"
S13-2075,W02-1011,0,0.0167289,"Missing"
S13-2075,W11-2207,0,0.138483,"Missing"
S13-2075,S13-2052,0,0.0683576,"Missing"
S13-2075,W11-0705,0,\N,Missing
S13-2075,W11-2200,0,\N,Missing
S14-2104,P97-1023,0,0.915874,"Missing"
S14-2104,D10-1101,0,0.197463,"ect Sentiment model (HASM) was proposed by Kim et al (Kim, Zhang et al. 2013) to discover a hierarchical structure of aspect-based sentiments from unlabelled online reviews. Supervised methods uses normally a CRF or HMM models. Jin and Ho (Jin and Ho 2009) applied a lexicalized HMM model to extract aspects using the words and their part-of-speech tags in order to learn a model, then unsupervised algorithm for determining the aspect sentiment using the nearest opinion word to the aspect and taking into account the polarity reversal words (such as not). CRF model was used by Jakob and Gurevych (Jakob and Gurevych 2010) with these features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the sentence containing an opinionated expression is labelled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspect-expression from the training set. Our method for aspect extraction is closed to"
S14-2104,S14-2004,0,0.0616281,"Missing"
S14-2104,P10-1042,0,0.743821,"t works did not take this case into account. Therefore, we could define the opinion by the quintuple (Liu 2012) (ei, aij, sijkl, hk, tl) where ei is the entity i, aij are the aspects of the entity i, sijkl is the expressed sentiment on the aspect at the time tl, hk the holder which created the document or the text. This definition does not take into account that the entity has aspects that could have also other aspects which leads to an aspect hierarchy, in order to avoid this information loss, few works have handled this issue, they proposed to represent the aspect as a tree of aspect terms (Wei and Gulla 2010; Kim, Zhang et al. 2013). Supervised and unsupervised methods have been used for handling this task, in this paper, we propose supervised methods and test them over two datasets related to laptop reviews and restaurant reviews provided by the ABSA task of SemEval2014 (Pontiki, Galanis et al. 2014). We tackle four subtasks: 1. Aspect term extraction: CRF model is proposed. 2. Aspect Term Polarity Detection: Multinomial Naive-Bayes classifier with some features such as Z-score, POS and prior polarity extracted from Subjectivity 596 Proceedings of the 8th International Workshop on Semantic Evalu"
S14-2104,H05-1044,0,0.136164,"Missing"
S14-2104,N10-1122,0,\N,Missing
S14-2104,S13-2075,1,\N,Missing
S14-2113,pak-paroubek-2010-twitter,0,0.0390112,"s were employed from annotated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM). The authors (Go, Bhayani et al. 2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the quality of results. The authors in (Kouloumpis, Wilson et al. 2011) found that N-gram with lexicon features and micro-blogging features are useful but POS features are not. In contrast, in (Pak and Paroubek 2010) they reported that POS and bigrams both help. In (Barbosa and Feng 2010) the authors proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS tags, in (Agarwal, Xie et al. 2011) this approach was extended by using real valued prior polarity and by combining prior polarity with POS. Authors in (Saif, He et al. 2012) proposed to use the semantic features, therefore they extracted the named entities in the tweets. Authors in (Hamdan, Béchet et al. 2013) used the concepts extract"
S14-2113,W02-1011,0,0.016803,"Missing"
S14-2113,W11-2207,0,0.209768,"Missing"
S14-2113,H05-1044,0,0.0489275,"Missing"
S14-2113,W11-0705,0,\N,Missing
S14-2113,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S14-2113,C10-2005,0,\N,Missing
S14-2113,S14-2009,0,\N,Missing
S14-2113,S13-2075,1,\N,Missing
S14-2113,P11-4022,0,\N,Missing
S14-2113,S13-2052,0,\N,Missing
S15-2095,baccianella-etal-2010-sentiwordnet,0,0.329518,"on 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work 3 Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia co"
S15-2095,S13-2075,1,0.853834,"Missing"
S15-2095,S14-2113,1,0.775034,"iment Lexicons The system extracts four features from the manual constructed lexicons and six features from the automatic ones. For each sentence the number of positive words, the number of negative ones, the number of positive words divided by the number of negative ones and the polarity of the last word are extracted from manual constructed lexicons. In addition to the sum of the positive scores and the sum of the negative scores from the automatic constructed lexicons. 4.5 Z score Z score can distinguish the importance of each term in each class, their performances have been 570 proved in (Hamdan et al., 2014). We assume as in the mentioned work that the term frequencies are following a multi-nomial distribution. Thus, Z score can be seen as a standardization of the term frequency using multi-nomial distribution. We compute the Z score for each term ti in a class Cj (tij ) by calculating its term relative frequency tf rij in a particular class Cj , as well as the mean (meani ) which is the term probability over the whole corpus multiplied by the number of terms in the class Cj , and standard deviation (sdi ) of term ti according to the underlying corpus. Like in (Hamdan et al., 4 29) we tested diff"
S15-2095,S15-2095,1,0.106861,"noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervise"
S15-2095,S14-2111,0,0.109778,"Missing"
S15-2095,S12-1033,0,0.1551,"Missing"
S15-2095,S13-2053,0,0.544432,"adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classificatio"
S15-2095,S13-2052,0,0.125799,"y wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3.1 569 Data and Resources Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training, the provided development set containing 1654 tweets is used for tuning the machine learner. The test data set 2015 contains about 2390 tweets (Rosenthal et al., 5 06). Table 1 shows the distribution of each label in each data set. Twitter all neg. pos. neut. train 9684 1458 3640 4586 dev 1654 340 739 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in"
S15-2095,S15-2078,0,0.143618,"Missing"
S15-2095,W12-3716,0,0.0495006,"ded to feature space where each feature represents the number of words in the text mapped to each cluster. The 1000 clusters is provided in Twitter Word Clusters of CMU ARK group. 1000 clusters were constructed from approximately 56 million tweets. 4.6.2 Topic features Latent dirichlet association or topic modeling is used to extract 10 features. Lda-c is configured with 10 topics and the training data is used for training the model, then for each sentence in the test set, the trained model estimates the number of words assigned to each topic. 4.6.3 Semantic Role Labeling Features Authors in (Ruppenhofer and Rehbein, 2012) encode semantic role labeling features in SVM classifier. Our system also extract two types of features, the names: the whole term which represents an argument of the predicate and the tags: the type of each argument in the text (A0 represents the subject of predicate, A1 the object, AM-TMP the time, AMADV the situation, AM-loc the location). These encodings are defined by the tool which we used (Senna). We think that the predicate arguments can constitute a multi-word expression which may be helpful in Sentiment Classification. 5 5.1 Experiments Experiment Setup We trained the L1-regularized"
S15-2095,W11-2207,0,0.0366967,"es classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3.1 569 Data and Resources Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downl"
S15-2095,H05-2018,0,0.862824,"described in section 6 and future work is presented in section 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work 3 Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia a"
S15-2128,baccianella-etal-2010-sentiwordnet,0,0.00680142,"egative labels with the following features: - Word n-grams Features Unigrams and bigrams are extracted for each word in the context without any stemming or stop-word removing, all terms with occurrence less than 3 are removed from the feature space. - Sentiment Lexicon-based Features The system extracts four features from the manual constructed lexicons (Bing Liu Lexicon (Hu and Liu, 2004) and MPQA subjectivity Lexicon (Wilson et al., 2005)) and six features from the automatic ones (NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010)). For each context the number of positive words, the number of negative ones, the number of positive words divided by the number of negative ones and the polarity of the last word are extracted from manual constructed lexicons. In addition to the sum of the positive scores and the sum of the negative scores from the automatic constructed 756 lexicons. - Negation Features The rule-based algorithm presented in Christopher Potts Sentiment Symposium Tutorial is implemented. This algorithm appends a negation suffix to all words that appear within a negation scope which is determined by the negatio"
S15-2128,S13-2075,1,0.918282,"Missing"
S15-2128,S14-2113,1,0.925633,", 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment gra"
S15-2128,S14-2104,1,0.927942,", 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment gra"
S15-2128,S15-2095,1,0.741347,"oduct attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervised methods have been widely exploited for this purpose, a classification algorithms with a wise feature extraction could achieve good results (Mohammad et al., 2013) (Hamdan et al., 2015a) (Hamdan et al., 4 29). 3 Opinion Target Expression (OTE) An opinion target expression (OTE) is an expression used in the given text to refer to an aspect or an aspect term related to the reviewed entity. The objective of OTE slot is to extract all opinion target expressions in a restaurant review, OTE could be a word or multiple words. For this purpose, we have used CRF (Conditional Random Field) which have proved its performance in information extraction. We choose the IOB notation for representing each sentence in the review. Therefore, we distinguish the terms at the Beginning, the Insid"
S15-2128,P97-1023,0,0.0380864,"e input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment graph building from the pairs of aspect and related adjectives. Lin and He (Lin et al., 2012) proposed Joint model of Sentiment and Topic (JST) which extends the state-of-the-art topic model (LDA) by adding a sentiment layer, this model is fully unsupervised and it can detect sentiment and topic simultaneously. Wei and Gulla (Wei and Gulla, 2010) modeled the hierarchical"
S15-2128,D10-1101,0,0.0371309,"rithm to extract the frequent aspects. KNN algorithm is applied to estimate the aspect rating scaling from 1 to 5 stands for (Excellent, Good, Average, Poor, Terrible). Supervised methods uses normally the CRF or HMM models. Jin and Ho (Jin and Ho, 2009) applied a lexicalized HMM model to extract aspects using the words and their part-of-speech tags in order to learn a model, then unsupervised algorithm for determining the aspect sentiment using the nearest opinion word to the aspect and taking into account the polarity reversal words (such as not). A CRF model was used by Jakob and Gurevych (Jakob and Gurevych, 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan"
S15-2128,S12-1033,0,0.0642798,"Missing"
S15-2128,S13-2053,0,0.185212,"l relationships among product attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervised methods have been widely exploited for this purpose, a classification algorithms with a wise feature extraction could achieve good results (Mohammad et al., 2013) (Hamdan et al., 2015a) (Hamdan et al., 4 29). 3 Opinion Target Expression (OTE) An opinion target expression (OTE) is an expression used in the given text to refer to an aspect or an aspect term related to the reviewed entity. The objective of OTE slot is to extract all opinion target expressions in a restaurant review, OTE could be a word or multiple words. For this purpose, we have used CRF (Conditional Random Field) which have proved its performance in information extraction. We choose the IOB notation for representing each sentence in the review. Therefore, we distinguish the terms at the"
S15-2128,S15-2082,0,0.0675407,"pects of the entity i, sijkl is the expressed sentiment on the aspect at the time tl , hk the holder which created the document or the text. This definition does not take into account that the entity has aspects that could have also other aspects which leads to an aspect hierarchy, in order to avoid this information loss, few work has handled this issue, they proposed to represent the aspect as a tree of aspect terms. In this paper, we focus on Opinion Target Extraction (OTE) and Sentiment Polarity towards a target or a category. The description of each subtask is provided by ABSA organizers (Pontiki et al., 2015). For OTE or aspect term extraction, a CRF model is proposed with IOB annotation and several groups of features including syntactic, lexical, semantic, sentiment lexicon features. For aspect term polarity detection, a logistic regression classifier is trained with weighting schema for positive and negative labels and several groups of features are extracted includ753 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 753–758, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics ing lexical, syntactic, semantic, lexicon and Z"
S15-2128,P10-1042,0,0.0891609,"d McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment graph building from the pairs of aspect and related adjectives. Lin and He (Lin et al., 2012) proposed Joint model of Sentiment and Topic (JST) which extends the state-of-the-art topic model (LDA) by adding a sentiment layer, this model is fully unsupervised and it can detect sentiment and topic simultaneously. Wei and Gulla (Wei and Gulla, 2010) modeled the hierarchical relation between product aspects. They defined Sentiment Ontology Tree (SOT) to formulate the knowledge of hierarchical relationships among product attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervi"
S15-2128,H05-2018,0,0.707121,"3 previous and subsequent words, respectively. -word lemma using WordNet. -word POS using NLTK parser. -word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) -word type: the type of the word (uppercase, digit, symbol, combination ) -Named entity: the IOB annotation for the named entity extracted from the review using Senna (Collobert, 2011). -chunk: the chunk of the word (NP, VP, PP) extracted using Senna. -polarity: the sum of word polarity score calculated using Bing Liu Lexicon (Hu and Liu, 2004) and MPQA subjectivity Lexicon (Wilson et al., 2005). -Prefixes (all prefixes having length between one to four ). -Suffixes (all suffixes having length between one to four). -Stop word: if the word is a stop word or not. -if the initial letter is uppercase, if all letters are uppercase, All letters lowercase, All letters digit, Contains a uppercase letter, Contains a lowercase letter, Contains a digit, Contains a alphabet letter, Contains a symbol. We also extract the value of each two successive features in the the range -2,2 (the previous and subsequent two words of actual word) for the following features: word surface, word POS, word chunk,"
S15-2128,N10-1122,0,\N,Missing
W03-0702,W97-0800,0,0.289643,"s generated by an Automatic Speech Recognition System (ASR). 2 Hypothesis evaluation and search Let a dialogue system have a belief which generates expectations B about conceptual structures. Expectation uncertainty is represented by a probability distribution P(B) which is non-zero for a set of conceptual structures expected at a given time. Thus for a general concept structure Γ and a description Y of the speech signal, one gets: Γ * = arg max P ( Γ |Y ) = arg max P ( Γ , Y ) Γ Γ P ( Γ , Y ) = ∑ P (Γ , Y , B ) ≈ max P ( Γ , Y , B ) B B Word associations found with networks of word relations [3] can also be useful for suggesting compositions of semantic constituents into conceptual structures. Thus, given an observed example, other examples can be manually derived and generalized automatically. P ( Γ , Y , B) = ∑ P ( W ,Γ , Y , B) ≈ W max P ( Y |W ) P ( W , Γ , B) (1) W Γ* ≈ arg max {P ( Y |W ) P ( W , Γ , B)} Γ ,W , B Computer understanding of a spoken sentences is problem solving activity whose central engine is a search process involving various types of models. P ( Γ , W , B) = P ( Γ |BW ) P ( W |B) P ( B) Searching for concepts can be combined with searching for words. This sugg"
W03-0702,H91-1020,0,\N,Missing
W03-0702,W97-0801,0,\N,Missing
W04-2321,P00-1011,1,\N,Missing
W04-2321,P03-1006,0,\N,Missing
W04-3218,P02-1049,0,0.0217751,"ing methods. Preliminary results are given for cluster interpretation and dynamic model adaptation using the clusters obtained. 1 Introduction The deployment of large scale automatic spoken dialog systems, like How May I Help You?SM (HMIHY) (Gorin et al., 1997), makes available large corpora of real human-machine dialog interactions. Traditionally, this data is used for supervised system evaluation. For instance, in (Kamm et al., 1999) they propose a static analysis aimed at measuring the performance of a dialog system, especially in an attempt to automatically estimate user satisfaction. In (Hastie et al., 2002), a dynamic stratDilek Hakkani-Tur AT&T Labs Florham Park, NJ, USA dtur@ research.att.com egy in the error handling process is proposed. In all these studies, supervised learning techniques are used in order to classify dialogs to predict user satisfaction or dialog failures. A novel approach to the exploitation of dialog corpora is for speech recognition and language understanding modeling. In fact, such corpora allow for a multidimensional analysis of speech and language models of dialog systems. Our work differs from previous studies in the algorithmic approach and learning scenario. First"
W07-0307,P03-1006,0,0.014449,"Rate (WER), Concept Error Rate (CER) and Interpretation Error Rate (IER) according to the SLU strategy γˆ = argmaxP (γ|C)P (C|W )P (W |Y ) W,C,γ The stochastic models proposed are implemented with a Finite State Machine (FSM) paradigm thanks to the AT&T FSM toolkit (Mohri et al., 2002). Following the approach described in (Raymond et al., 2006), the SLU first stage is implemented by means of a word-to-concept transducer that translates a word lattice into a concept lattice. This concept lattice is rescored with a Language Model on the concepts (also encoded as FSMs with the AT&T GRM toolkit (Allauzen et al., 2003)). The rule database of the SLU second stage is encoded as a transducer that takes as input concepts and output semantic interpretations γ. By applying this transducer to an FSM representing a concept lattice, we directly obtain a lattice of interpretations. 53 The comparison among the two strategies is given in table 4. As we can see a small improvement is obtained for the interpretation error rate (IER) with the integrated strategy (strat2). This gain is small; however it is interesting to look at the Oracle IER that can be obtained on an n-best list of interpretations produced by each strat"
W07-0307,H90-1021,0,0.384908,"Missing"
W12-0508,E06-1002,0,0.0382143,"ection to improve the result precision but do not provide referential information, which can be useful in IE applications. EL achieves the association of NER results with uniquely identified entities, by relying on an entity repository, available to the extraction system and defined beforehand in order to serve as a target for mention linking. Knowledge about entities is gathered in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee e"
W12-0508,D07-1074,0,0.073248,"n can follow the detection to improve the result precision but do not provide referential information, which can be useful in IE applications. EL achieves the association of NER results with uniquely identified entities, by relying on an entity repository, available to the extraction system and defined beforehand in order to serve as a target for mention linking. Knowledge about entities is gathered in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0."
W12-0508,doddington-etal-2004-automatic,0,0.145699,"a set of possible mentions, preserving a number of ambiguous readings. The linking process must thereafter evaluate which readings are the most probable, based on the most likely entity matches inferred from a similarity measure with the context. NER has been widely addressed by symbolic, statistical as well as hybrid approaches. Its major part in information extraction (IE) and other NLP applications has been stated and encouraged by several editions of evaluation campaigns such as MUC (Marsh and Perzanowski, 1998), the CoNLL-2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) or ACE (Doddington et al., 2004), where NER systems show near-human performances for the English language. Our system aims at benefitting from both symbolic and statistical NER techniques, which have proven efficient 52 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 52–60, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics but not necessarily over the same type of data and with different precision/recall tradeoff. NER considers the surface form of entities; some type disambiguation and name normalization can follow the"
W12-0508,C10-1032,0,0.0808692,"Missing"
W12-0508,M98-1002,0,0.304329,"Missing"
W12-0508,mcnamee-etal-2010-evaluation,0,0.0120695,"ca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al., 2010). Our approach of this issue is closely related to the method of Dredze et al. in (2010), where the out-of-base entity is considered"
W12-0508,sagot-stern-2012-aleda,1,0.842661,"methodology applied are presented in section 4. Section 5 illustrates this methodology with a number of experiments and evaluation results. 2 Entity Resources Our system relies on two large-scale resources which are very different in nature: • the entity database Aleda, automatically extracted from the French Wikipedia and Geonames; • a knowledge base extracted from a large corpus of AFP news wires, with distributional and contextual information about automatically detected entites. 2.1 Aleda The Aleda entity repository2 is the result of an extraction process from freely available resources (Sagot and Stern, 2012). We used the French Aleda databased, extracted the French Wikipedia3 and Geonames4 . In its current development, it provides a generic and wide coverage entity resource accessible via a database. Each entity in Aleda is associated with a range of attributes, either referential (e.g., the type of the entity among Person, Location, Organization and Company, the population for a location or the gender of a person, etc.) 2 Aleda is part of the Alexina project and freely available at https://gforge.inria.fr/projects/alexina/. 3 4 53 www.fr.wikipedia.org www.geonames.org or formal, like the entity’"
W12-0508,sagot-2010-lefff,1,0.912742,"set of entities forms the majority of occurrences. Our particular context can thus justify the need for a domain specific KB. As opposed to Wikipedia where entities are identifiable by hyperlinks, AFP corpora provide no such indications. Wikipedia is in fact a corpus where entity mentions are clearly and uniquely linked, whereas this is what we aim at achieving over AFP’s raw textual data. The acquisition of domain specific knowledge about entities from AFP corpora must circumvent this lack of indications. In this perspective we use an implementation of a naive linker described in (Stern and Sagot, 2010). For the main part, this system is based on heuristics favoring popular entities in cases of ambiguities. An evaluation of this system showed good accuracy of entity linking (0.90) over the subset of correctly detected entity mentions:5 on the evaluation data, the resulting NER reached a precision of 0.86 and a recall of 0.80. Therefore we rely on the good accuracy of this system to identify entities in our corpus, bearing in mind that it will however include cases of false detections, while knowledge will not be available on missed entities. It can be observed that by doing so, we aim at per"
W12-0508,2010.jeptalnrecital-court.23,1,0.778606,"ce a small set of entities forms the majority of occurrences. Our particular context can thus justify the need for a domain specific KB. As opposed to Wikipedia where entities are identifiable by hyperlinks, AFP corpora provide no such indications. Wikipedia is in fact a corpus where entity mentions are clearly and uniquely linked, whereas this is what we aim at achieving over AFP’s raw textual data. The acquisition of domain specific knowledge about entities from AFP corpora must circumvent this lack of indications. In this perspective we use an implementation of a naive linker described in (Stern and Sagot, 2010). For the main part, this system is based on heuristics favoring popular entities in cases of ambiguities. An evaluation of this system showed good accuracy of entity linking (0.90) over the subset of correctly detected entity mentions:5 on the evaluation data, the resulting NER reached a precision of 0.86 and a recall of 0.80. Therefore we rely on the good accuracy of this system to identify entities in our corpus, bearing in mind that it will however include cases of false detections, while knowledge will not be available on missed entities. It can be observed that by doing so, we aim at per"
W12-0508,W03-0419,0,0.422026,"Missing"
W12-0508,C10-1145,0,0.0227266,"in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary"
W12-0508,N10-1072,0,0.0182522,"ity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al., 2010). Our approach of this issue is closely related to the method of Dredze et al. in (2010), where the out-of-base entity is considered as another entry to rank. Our task differs from EL configurations outlined previously, in that its target is entity extraction from raw news wires from the news agency Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input of the linking system is the result of an automatic NER step, which will produce errors of various kinds. In particular, spans erroneously detected as NEs will have to be discarded by our EL system. This cas"
W15-0212,N09-2022,0,\N,Missing
W15-0212,J14-1002,0,\N,Missing
W15-0212,P11-4015,1,\N,Missing
W15-0212,J02-3001,0,\N,Missing
W15-0212,bazillon-etal-2012-syntactic,1,\N,Missing
W15-0212,bechet-etal-2012-decoda,1,\N,Missing
W15-4633,W12-1642,0,0.0704904,"Missing"
W15-4633,W09-0505,1,0.806537,"sk, manual transcripts were provided to the participants. While the original language of the conversations is French, the SENSEI project provided man• A man is calling cause he got a fine. He is waiting for a new card so he used his wife’s card. He must now write a letter asking for clemency. • A user wants to go to the Ambroise Par´e clinic but the employee misunderstands and gives her the wrong itinerary. Luckily the employee realises her mistake and gives the passenger the right information in the end. • School bag lost on line 4, not found. Luna corpus The Italian human-human Luna corpus (Dinarelli et al., 2009) consists of 572 dialogs (≈ 26.5K turns & 30 hours of speech) in the hardware/software help desk domain, where a 233 4 client and an agent are engaged in a problem solving task over the phone. The dialogs are organised in transcriptions and annotations created within the FP6 LUNA project. For the CCCS shared task, manual transcriptions were used. Metric Evaluation is performed with the ROUGE-2 metric (Lin, 2004). ROUGE-2 is the recall in term of word bigrams between a set of reference synopses and a system submission. The ROUGE 1.5.5 toolkit was adapted to deal with a conversation-dependent le"
W15-4633,W04-1013,0,0.0698795,"Missing"
W15-4633,W13-2117,0,0.0992767,"Missing"
W15-4633,stepanov-etal-2014-development,1,0.892092,"Missing"
W15-4633,bechet-etal-2012-decoda,1,\N,Missing
W16-3621,bazillon-etal-2012-syntactic,1,0.849545,"d to tweets and that adaptation with in-domain data helps increasing these performances. More recently (Kong et al., 2014) described a dependency parser for tweets. However, to the best of our knowledge, no such study has been published on social media data from formal on line web conversations. We believe that the current models used in the fields of syntactic and semantic parsing are mature enough to go beyond normative data that we find in benchmark corpora and process text that comes from CRM chat. The experience we gathered on parsing speech transcriptions in the framework of the DECODA (Bazillon et al., 2012) and ORFEO (Nasr et al., 2014) projects showed that current parsing techniques can be successfully used to parse disfluent speech transcriptions. Syntactic parsing of non canonical textual input in the context of human-human conversations has been mainly studied in the context of textual transcription of spontaneous speech. In such data, the variation with respect to canonical written text comes mainly from syntactic structures that are specific to spontaneous speech, as well as disfluencies, such as filled pauses, repetitions and false starts. Our input has some of the specificities of sponta"
W16-3621,D13-1035,0,0.0134229,"ra Guerraz2 , Frederic Bechet1 (1) Aix Marseille Universite - CNRS-LIF, Marseille, France (2) Orange Labs - Lannion, France Abstract Recent projects in Europe, such as the CoMeRe (Chanier et al., 2014) or the STAC (Asher, 2011) project gathered collections of CMC data in several languages in order to study this new kind of language. Most of the effort has been dedicated to ”chat room” data as it is the kind of data which is the most accessible on the WEB. (Achille, 2005) constituted a corpus in French. (Forsyth and Martell, 2007) and (Shaikh et al., 2010) describe similar corpora in English. (Cadilhac et al., 2013) have studied the relational structure of such conversations through a deep discursive analysis of chat sessions in an online video game. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. This study is realized in the context of the DATCHA project, a collaborative project funded by the French National Research Agency, which aims at performing unsupervised knowledge extraction from very large databases of WEB chat conversations between operators and clients in"
W16-3621,L16-1319,1,0.841476,"here are no repetitions nor false starts. Orthographic errors are numerous and some of them are challenging for a syntactic parser. We present in this paper a detailed analysis of the impact of all these phenomena on syntactic parsing. Other types of social media data have been studied in the literature. In particular tweets have received lately more attention. (Ritter et al., 2011) for example provide a detailed evaluation of a pos tagger on tweets, with the final objec4 A study on orthographic errors in agent/customer chat dialogs Chat conversations are unique from several perspectives. In (Damnati et al., 2016), we conducted a study comparing contact center chat conversations and phone conversations, both in the domain of technical assistance for Orange customers. The comparative analysis showed significant differences in terms of interaction flow. If chat conversations were on average twice as long in terms of effective duration, phone conversations contain on average four times more turns than chat conversations. This can be explained by several factors: chat is not an exclusive activity and latencies are more easily accepted than in an oral conversation. Chat utterances are formulated in a more d"
W16-3621,D14-1108,0,0.0569385,"rd. ok fine within 48h maximum 72h for the card You will receive it according to delivery time at the address in your record. ok fine thank you You’re welcome Before you go, do you any other question? no thank you Figure 1: Example of conversation in the TV assistance domain, in its original forme (above) and a translation without errors (below) 177 tive of performing Named Entity detection. They showed that the performances of a classical tagger trained on generic news data drop when applied to tweets and that adaptation with in-domain data helps increasing these performances. More recently (Kong et al., 2014) described a dependency parser for tweets. However, to the best of our knowledge, no such study has been published on social media data from formal on line web conversations. We believe that the current models used in the fields of syntactic and semantic parsing are mature enough to go beyond normative data that we find in benchmark corpora and process text that comes from CRM chat. The experience we gathered on parsing speech transcriptions in the framework of the DECODA (Bazillon et al., 2012) and ORFEO (Nasr et al., 2014) projects showed that current parsing techniques can be successfully u"
W16-3621,2005.jeptalnrecital-recital.10,0,0.11855,"Missing"
W16-3621,nasr-etal-2014-automatically,1,0.860161,"th in-domain data helps increasing these performances. More recently (Kong et al., 2014) described a dependency parser for tweets. However, to the best of our knowledge, no such study has been published on social media data from formal on line web conversations. We believe that the current models used in the fields of syntactic and semantic parsing are mature enough to go beyond normative data that we find in benchmark corpora and process text that comes from CRM chat. The experience we gathered on parsing speech transcriptions in the framework of the DECODA (Bazillon et al., 2012) and ORFEO (Nasr et al., 2014) projects showed that current parsing techniques can be successfully used to parse disfluent speech transcriptions. Syntactic parsing of non canonical textual input in the context of human-human conversations has been mainly studied in the context of textual transcription of spontaneous speech. In such data, the variation with respect to canonical written text comes mainly from syntactic structures that are specific to spontaneous speech, as well as disfluencies, such as filled pauses, repetitions and false starts. Our input has some of the specificities of spontaneous speech but adds new ones"
W16-3621,W03-3017,0,0.0902461,"ine of the table corresponds to the status of a token. If the token is correct, the status is CORR, otherwise it corresponds to one label of the split REF HYP tok tag tok tag AB TAB A TA B TB Table 5: Conventions defined when computing the accuracy of the tagger for a token. Tags in bold face are compared 181 status CORR DIACR AGGLU SUB1C INFL DEL1C OTHER INS1C APOST SPLIT SWITCH occ. 5916 201 76 46 67 43 40 20 47 6 2 corr. 5547 120 23 13 45 22 23 12 40 3 2 acc. 93.76 59.70 30.26 28.26 67.16 51.16 57.50 60.00 85.11 50.00 100.00 contrib. 59.23 13.00 8.51 5.30 3.53 3.37 2.73 1.28 1.12 0.48 0.00 Nivre, 2003). It is a dependency parser that takes as input tokens with their pos tag and selects for every token a syntactic governor (which is another token of the sentence) and a syntactic label. The prediction is based on several features that combine lexical information and pos tags. Orthographic errors have therefore a double impact on the parsing process: through the errors they provoke on the pos tagging process and the errors they provoke directly on the parsing process. The parser was trained on the French Treebank. Contrary to taggers, a single parser was used for our experiments since we do no"
W16-3621,D11-1141,0,0.0851138,"Missing"
W16-3621,shaikh-etal-2010-mpc,0,0.0320112,"rsation corpus Alexis Nasr1 , Geraldine Damnati2 , Aleksandra Guerraz2 , Frederic Bechet1 (1) Aix Marseille Universite - CNRS-LIF, Marseille, France (2) Orange Labs - Lannion, France Abstract Recent projects in Europe, such as the CoMeRe (Chanier et al., 2014) or the STAC (Asher, 2011) project gathered collections of CMC data in several languages in order to study this new kind of language. Most of the effort has been dedicated to ”chat room” data as it is the kind of data which is the most accessible on the WEB. (Achille, 2005) constituted a corpus in French. (Forsyth and Martell, 2007) and (Shaikh et al., 2010) describe similar corpora in English. (Cadilhac et al., 2013) have studied the relational structure of such conversations through a deep discursive analysis of chat sessions in an online video game. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. This study is realized in the context of the DATCHA project, a collaborative project funded by the French National Research Agency, which aims at performing unsupervised knowledge extraction from very large database"
W16-3621,W03-3023,0,0.127109,"Missing"
W17-6311,J16-1002,1,0.539863,"-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 pr"
W17-6311,P11-4015,1,0.883412,"Missing"
W17-6311,P08-1037,0,0.100252,"representation for both image and language features and generate in an end-to-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271"
W17-6311,W03-3017,0,0.11765,"or a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Prediction The train part of the PP-corpus has been used to train a classifier that predicts whether a PPattachment proposed by a parser is correct or not. The parser used is a standard arc-eager transition based parser (Nivre, 2003), trained on sections 0 − 18 of the Penn Treebank (Marcus et al., 1993). The parser was run on the train set of the corpus and, for each occurrence of a manually attached preposition, a negative or a positive example has been produced depending on whether the parser has predicted the correct attachment or not. This data set is composed of 17643 positive and 5611 negative examples. It has been used to train a classifier that predicts whether the attachment made by the parser is correct or not. The classifier used for this task is the Icsiboost classifier (Favre et al., 2007). This Adaboost clas"
W17-6311,D11-1113,0,0.0220038,"f these prepositions, column two displays its number of occurrences, column three (BL) shows the attachment accuracy for this preposition in the output the parser. Columns four (T), five (C), six (V) and seven (TCV) show the attachment accuracy for the corrected output for four different configurations The classifier developed in the previous section only checked if a PP-attachment proposed by the parser is correct or not. In this section we integrate this classifier in a correction strategy in order to improve the accuracy of our parser. This correction strategy is inspired from the ideas of Anguiano and Candito (2011); Attardi and Ciaramita (2007); Hall and Nov´ak (2005): given a sentence S, a parse T for S and a target preposition p, a set 75 Prep into with through behind under down in in front of outside on around for at along across against near towards next to by of over during from TOTAL Occ 116 310 145 35 58 41 369 51 35 143 59 168 63 50 49 31 159 30 137 76 72 111 41 140 2907 BL 0.89 0.65 0.95 0.74 0.84 0.63 0.76 0.90 0.63 0.85 0.73 0.73 0.84 0.52 0.88 0.77 0.33 0.90 0.89 0.84 0.93 0.66 0.71 0.76 0.75 T 0.93 0.78 0.96 0.86 0.84 0.73 0.84 0.88 0.74 0.90 0.81 0.82 0.86 0.86 0.96 0.94 0.84 0.93 0.89 0.8"
W17-6311,N07-1049,0,0.0243527,"two displays its number of occurrences, column three (BL) shows the attachment accuracy for this preposition in the output the parser. Columns four (T), five (C), six (V) and seven (TCV) show the attachment accuracy for the corrected output for four different configurations The classifier developed in the previous section only checked if a PP-attachment proposed by the parser is correct or not. In this section we integrate this classifier in a correction strategy in order to improve the accuracy of our parser. This correction strategy is inspired from the ideas of Anguiano and Candito (2011); Attardi and Ciaramita (2007); Hall and Nov´ak (2005): given a sentence S, a parse T for S and a target preposition p, a set 75 Prep into with through behind under down in in front of outside on around for at along across against near towards next to by of over during from TOTAL Occ 116 310 145 35 58 41 369 51 35 143 59 168 63 50 49 31 159 30 137 76 72 111 41 140 2907 BL 0.89 0.65 0.95 0.74 0.84 0.63 0.76 0.90 0.63 0.85 0.73 0.73 0.84 0.52 0.88 0.77 0.33 0.90 0.89 0.84 0.93 0.66 0.71 0.76 0.75 T 0.93 0.78 0.96 0.86 0.84 0.73 0.84 0.88 0.74 0.90 0.81 0.82 0.86 0.86 0.96 0.94 0.84 0.93 0.89 0.86 0.93 0.85 0.76 0.86 0.85 C 0"
W17-6311,Q14-1043,0,0.021789,"from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Pr"
W17-6311,D16-1156,0,0.234932,"Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Prediction The train part of the PP-corpus has been used to train a classifier that predicts whether a PPattachment proposed by a parser is correct or not. The parser used"
W17-6311,Q14-1006,0,0.0483338,"ki, 1994) computes first spatial relations among objects detected in images with knowledge-based language generation model in order to generate short descriptions of videos in limited domains (traffic scenes, soccer matches). Recently open-domain language generation from 72 Proceedings of the 15th International Conference on Parsing Technologies, pages 72–77, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics 3 Data The multimodal corpus used in this work is the Flickr30k Entities (F30kE) (Plummer et al., 2017), an extension of the original Flickr30k dataset (Young et al., 2014). This corpus is composed of almost 32K images and, for each image, five captions describing the image have been produced. Besides, every object in the image that corresponds to a mention in the captions has been manually identified with a bounding box. Bounding boxes and the mentions in the captions have been paired together via co-reference links. A total of 244K such links have been annotated. Furthermore, each mention in the captions has been categorized into eight coarse-grained conceptual types using manually constructed dictionaries. The types are: people, body parts, animals, clothing,"
W17-6311,P17-1191,0,0.150617,"oth image and language features and generate in an end-to-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annota"
W17-6311,E17-2050,0,0.162099,"Missing"
W17-6311,W05-1505,0,0.122245,"Missing"
W17-6311,J93-2004,0,\N,Missing
