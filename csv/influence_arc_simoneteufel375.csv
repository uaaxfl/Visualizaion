2020.conll-1.12,D18-1443,0,0.0320952,"Missing"
2020.conll-1.12,N18-1065,0,0.0427139,"Missing"
2020.conll-1.12,D19-1371,0,0.021808,"racting sentences from the input source, so the total summary length has no strict limitation compared with the other two abstractive models. At test time, input content was not limited, but the output summary was constrained to a maximum of 30 tokens per sentence for the abstractor. We also tested the re-ranking mechanism of this algorithm, but decided not to include results as they produced similar ROUGE scores and lower percentages of novel n-grams. 4.2.3 Transformer We adapt the open-source code provided by Liu and Lapata and replace the “BERT-base-uncased” pre-trained model with SciBERT (Beltagy et al., 2019). SciBERT follows the BERT model architecture, which is a multi-bidirectional transformer, by training an objective which predicts masked tokens and the next sentence, but is trained on scientific texts including PubMed. The input source content and target summaries were tokenised with BERT’s subword tokeniser. We refer to this model as SciBERT Abstractive, SciBERTA for short. 156 The specific hyperparameter values of the abstractive component are shown in Appendix A.1. The maximum encoding text size is set to 512, because only 210 input texts in our corpus are longer than 512, and 512 is the"
2020.conll-1.12,W04-3252,0,0.0129159,"the publisher. There are also domain-specific datasets like arXiv/PubMed (Cohan et al., 2018) for mixed science summarisation and BIGPATENT (Sharma et al., 2019), both of which use abstracts as the ground truth. The number of documents in these datasets varies between 200,000 and 1.3 million. 153 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 153–164 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 While initial efforts in the field concentrated on unsupervised extractive summarisation methods (Mihalcea and Tarau, 2004; Vanderwende et al., 2007; Moratanch and Chitrakala, 2017), recent work has seen an explosion of deep learning-based models that leverage these large datasets to produce more abstractive and natural-sounding summaries. Out of these we choose three methods that each take a different approach. First is the pointergenerator method by See et al. (2017), which balances extractive and abstractive summarisation by substituting phrases according to a learned parameter. The second method from Chen and Bansal (2018) uses a reinforcement learning (RL) algorithm to extract informative sentences and then"
2020.conll-1.12,P18-1063,0,0.135908,"l efforts in the field concentrated on unsupervised extractive summarisation methods (Mihalcea and Tarau, 2004; Vanderwende et al., 2007; Moratanch and Chitrakala, 2017), recent work has seen an explosion of deep learning-based models that leverage these large datasets to produce more abstractive and natural-sounding summaries. Out of these we choose three methods that each take a different approach. First is the pointergenerator method by See et al. (2017), which balances extractive and abstractive summarisation by substituting phrases according to a learned parameter. The second method from Chen and Bansal (2018) uses a reinforcement learning (RL) algorithm to extract informative sentences and then rewrites them using a sequence-2-sequence model with an additional re-ranking algorithm to avoid repetitive phrasings. The final method we test is a BERT-based model introduced by Liu and Lapata (2019) which uses BERT embeddings as the pretrained encoder, stacked Transformer layers as a decoder and a fine-tuning process to produce more natural abstractive summaries. Although Cohan et al. (2018) designed a method for use with scientific text, it was specifically created for full text documents and our input"
2020.conll-1.12,N18-2097,0,0.276788,"that can accurately substitute multiword terms for chemical formulae or an accurate hyponym like ketone, without ontological knowledge. This corpus is one in a set of tools that will help us compare models’ abilities to do this. 2 Related Work Supervised summarisation tasks are primarily evaluated on large news text corpora: CNN/Daily Mail (See et al., 2017), XSum (Narayan et al., 2018), and Newsroom (Grusky et al., 2018). Most of these use professionally written summaries consisting of one or more sentences provided by the publisher. There are also domain-specific datasets like arXiv/PubMed (Cohan et al., 2018) for mixed science summarisation and BIGPATENT (Sharma et al., 2019), both of which use abstracts as the ground truth. The number of documents in these datasets varies between 200,000 and 1.3 million. 153 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 153–164 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 While initial efforts in the field concentrated on unsupervised extractive summarisation methods (Mihalcea and Tarau, 2004; Vanderwende et al., 2007; Moratanch and Chitrakala, 2017), recent w"
2020.conll-1.12,D18-1206,0,0.018714,"addressing this problem by introducing a summarisation corpus, but we hope this is the first of many resources that will aid researchers in this field. Ultimately, underlying all these tasks is an ability to produce representations that can accurately substitute multiword terms for chemical formulae or an accurate hyponym like ketone, without ontological knowledge. This corpus is one in a set of tools that will help us compare models’ abilities to do this. 2 Related Work Supervised summarisation tasks are primarily evaluated on large news text corpora: CNN/Daily Mail (See et al., 2017), XSum (Narayan et al., 2018), and Newsroom (Grusky et al., 2018). Most of these use professionally written summaries consisting of one or more sentences provided by the publisher. There are also domain-specific datasets like arXiv/PubMed (Cohan et al., 2018) for mixed science summarisation and BIGPATENT (Sharma et al., 2019), both of which use abstracts as the ground truth. The number of documents in these datasets varies between 200,000 and 1.3 million. 153 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 153–164 c Online, November 19-20, 2020. 2020 Association for Computational Lingu"
2020.conll-1.12,N18-1202,0,0.00783753,"tokens in the summary Sit , disregarding the tokens that belong to chemical named entities tc . We take the maximum value of all sentence 4 Methods We implement three extractive baselines and three abstractive deep models. 4.1 Baselines The most basic extractive baseline is Lead-2, where we take the first two sentences of an abstract as the summary. We also use SumBasic3 and a GloVe vector4 enhanced TextRank algorithm. 4.2 Deep models 4.2.1 Pointer–Generator We adapt the PyTorch re-implementation5 of the original pointer–generator network (PGN) (See et al., 2017) by inserting ELMo embeddings (Peters et al., 2018) trained on PubMed texts,6 hereafter refered to as PGN-E. The added ELMo embeddings were computed by a pre-trained two-layered bidirectional language model (biLM) resulting in 512-dimensional word vectors, which is more than twice of the original pointer generator embedding size. We also experimented with the original PGN, and while we found the difference in performance as evaluated by ROUGE metrics negligible, PGN-E produced a higher rate of novel n-grams indicating better abstractiveness. 2 The data for CNN/DM and XSum is gathered from Hugging Face (https://huggingface.co/datasets), and Pub"
2020.conll-1.12,P17-1099,0,0.645235,". In this paper we start addressing this problem by introducing a summarisation corpus, but we hope this is the first of many resources that will aid researchers in this field. Ultimately, underlying all these tasks is an ability to produce representations that can accurately substitute multiword terms for chemical formulae or an accurate hyponym like ketone, without ontological knowledge. This corpus is one in a set of tools that will help us compare models’ abilities to do this. 2 Related Work Supervised summarisation tasks are primarily evaluated on large news text corpora: CNN/Daily Mail (See et al., 2017), XSum (Narayan et al., 2018), and Newsroom (Grusky et al., 2018). Most of these use professionally written summaries consisting of one or more sentences provided by the publisher. There are also domain-specific datasets like arXiv/PubMed (Cohan et al., 2018) for mixed science summarisation and BIGPATENT (Sharma et al., 2019), both of which use abstracts as the ground truth. The number of documents in these datasets varies between 200,000 and 1.3 million. 153 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 153–164 c Online, November 19-20, 2020. 2020 Associ"
2020.conll-1.12,P19-1212,0,0.033687,"e or an accurate hyponym like ketone, without ontological knowledge. This corpus is one in a set of tools that will help us compare models’ abilities to do this. 2 Related Work Supervised summarisation tasks are primarily evaluated on large news text corpora: CNN/Daily Mail (See et al., 2017), XSum (Narayan et al., 2018), and Newsroom (Grusky et al., 2018). Most of these use professionally written summaries consisting of one or more sentences provided by the publisher. There are also domain-specific datasets like arXiv/PubMed (Cohan et al., 2018) for mixed science summarisation and BIGPATENT (Sharma et al., 2019), both of which use abstracts as the ground truth. The number of documents in these datasets varies between 200,000 and 1.3 million. 153 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 153–164 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 While initial efforts in the field concentrated on unsupervised extractive summarisation methods (Mihalcea and Tarau, 2004; Vanderwende et al., 2007; Moratanch and Chitrakala, 2017), recent work has seen an explosion of deep learning-based models that leverag"
2020.figlang-1.30,D17-1169,0,0.0327174,"aining Details We train in batches of 32 sentences, and employ early stopping after 20 stable steps (based on F1 on dev). As an optimizer, we use AdamW (Loshchilov and Hutter, 2017). We experimented with three fine-tuning options: (1) unfreezing the whole network and training it all at once, (2) freezing BERT and training until early stopping activates, then unfreezing BERT and training until early stopping again, and (3) freezing BERT and training until early stopping, then sequentially unfreezing and training a single layer of BERT at a time, and finally the whole model at once (inspired by Felbo et al., 2017). We used option (2) in the end, since it offered a large improvement over (1) when we used a lower learning rate for the second phase. We found that (3) offered no additional advantage. To find hyperparameters, we performed a random search over the parameter space; final hyperparameters are reported in Table 2. Threshold Shifting The ratio of metaphors to non-metaphors in the entire VUA dataset was not the same as that of the verb and all-pos subsets used by the Shared Task. Having trained the model on all the data, we then adjust it to each different distribution. To do this, we find the thr"
2020.figlang-1.30,W15-1402,0,0.399931,"the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse of concreteness. Using concreteness ratings for metaphor identification is clearly well motivated, as evidenced by previous work (e.g. Tsvetkov et al., 2014, 2013; Beigman Klebanov et al., 2015). For a word to be metaphorical in a particular context, then, it needs to have a concrete sense and an abstract sense, with the abstract sense activated in that context. The concrete sense would belong to the source domain, and the abstract sense to the target domain. For instance, the meaning of the word attacked in “she attacked the soldier” is concrete, but in “she attacked the problem” it is abstract—and thus that usage is metaphorical. Polysemy of the word is a necessary condition; the existence of an abstract sense is not enough, otherwise a monosemously abstract word such as considered"
2020.figlang-1.30,N19-1423,0,0.0881981,"embeddings, to provide information about the source domain. Since these static type-level embeddings will clearly contain information about both source and target, we compliment them with type-level concreteness ratings. Such ratings should reflect the concreteness of the most concrete sense of the word, thus allowing the network to differentiate between the left and right columns of Figure 1. Figure 2 shows an overview of our architecture. In the following paragraphs, we detail each individual component of the model. Contextual Word Embedding For contextualised embeddings, we fine-tune BERT (Devlin et al., 2019). BERT is a sentence encoder which utilises a transformer architecture (Vaswani et al., 2017), and is trained with two separate tasks— masked language modelling (a cloze task), and next-sentence prediction. The latent space (the final hidden state of the encoder) contains vector representations of each input token, which change in different contexts. Several pre-trained BERT models are available—we use BERT large.2 Model Architecture We now describe a model which uses semantic representations of a word in and out of context to predict metaphoricity. Ideally, we would only provide the model wit"
2020.figlang-1.30,D18-1060,0,0.135355,"s a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells us about the source. Several existing models have exploited this difference (e.g. Mao et al., 2019; Gao et al., 2018). Usually, the target domain is something intangible, whilst the source domain relates more closely to our real-world experience. Concreteness refers to the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse of concreteness. Us"
2020.figlang-1.30,2020.figlang-1.3,0,0.446354,"ate whether or not it is metaphorical. Some metaphors occur so frequently as to be considered word senses in their own right (so-called conventional metaphors), whilst others are creative, and involve the use of words in unexpected ways (novel metaphors). Sometimes whole phrases or even sentences can lend themselves to metaphorical or literal interpretations.1 For these reasons and others, human annotators might disagree about what constitutes a metaphor—computational metaphor detection is no doubt a challenging problem. In this work, we participate in the 2020 Metaphor Detection Shared Task (Leong et al., 2020). First, we offer a description of metaphoricity, framing it in terms of the concreteness of a word in different contexts. Concreteness of a word in context is not a quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on i"
2020.figlang-1.30,W18-0907,0,0.566273,"quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on its context. contain the information needed to derive this contextual concreteness. This model outperforms the highest performing system of the previous shared task (Leong et al., 2018), and finishes 4th in the two subtasks in which we participate. 2 Concreteness and Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells"
2020.figlang-1.30,P19-1378,0,0.504792,"Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells us about the source. Several existing models have exploited this difference (e.g. Mao et al., 2019; Gao et al., 2018). Usually, the target domain is something intangible, whilst the source domain relates more closely to our real-world experience. Concreteness refers to the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse"
2020.figlang-1.30,P14-1024,0,0.421066,"r—computational metaphor detection is no doubt a challenging problem. In this work, we participate in the 2020 Metaphor Detection Shared Task (Leong et al., 2020). First, we offer a description of metaphoricity, framing it in terms of the concreteness of a word in different contexts. Concreteness of a word in context is not a quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on its context. contain the information needed to derive this contextual concreteness. This model outperforms the highest performing system of the previous shared task (Leong et al., 2018), and finishes 4th in the two subtasks in which we participate. 2 Concreteness and Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”"
2020.figlang-1.30,W13-0906,0,0.200661,"Missing"
2020.figlang-1.30,D11-1063,0,0.133213,"sentations of a word in and out of context to predict metaphoricity. Ideally, we would only provide the model with a representation of the concreteness of a word in context (since we believe that would do most of the lifting), but to our knowledge, no large-scale annotated datasets exist for contextdependent concreteness. In most popular datasets of concreteness annotation (e.g. Coltheart, 1981; Brysbaert et al., 2014), concreteness is a property assigned to each word type—but we would need the concreteness of a word instance. In this respect, our work resembles the abstractness classifier in Turney et al. (2011)—although this work uses word senses Concreteness Model We define a simple model which represents the concreteness of a word as a linear interpolation between two vectors, representing maximal concreteness and abstractness, vcon and vabs respectively. For each word w we obtain a real number estimate of its concreteness, c, from Brysbaert et al. (2014), where c = 5 indicates maximum abstractness, and c = 0 indicates maximum 222 2 BERT accepts WordPiece units (Wu et al., 2016) as tokens, rather than words. There is not a single accepted way of converting multiple WordPiece unit vector representa"
2020.figlang-1.30,W18-0913,0,0.125319,"Missing"
2020.lrec-1.854,D17-1141,0,0.0226897,"other aspects of how it is connected to the argument. Discourse structure provides not only a means to reorder sentences automatically, but it also ex1 The concept behind a major claim has also variously been referred to as main stance or conclusion in the literature. plains the ways which the improved texts are better than the original ones. Therefore, the annotation of discourse structure is a prerequisite for a sentence order analysis. In its final application, an analysis of both the discourse structure and sentence order is also helpful for downstream tasks ˇ such as essay assessment (Al Khatib et al., 2017; Snajder et al., 2019) and education (Iida and Tokunaga, 2014; Cullen et al., 2018; Matsumura and Sakamoto, 2019) Since the texts we wish to annotate are argumentative, we employ the approach from the argument mining field. In the NLP community, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discour"
2020.lrec-1.854,L18-1304,0,0.0385498,"Missing"
2020.lrec-1.854,J86-3001,0,0.811577,"s based on standard web technologies and can be easily customised to other annotation schemes, it can be easily used by anybody. Apart from the project it was originally designed for, in which hundreds of texts were annotated by three annotators, TIARA has already been adopted by a second discourse annotation study, which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of senten"
2020.lrec-1.854,J95-2003,0,0.64692,"which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (which we call first drafts) and in their improved versions is necessary. A tool is needed to support these annotations, and we believe that it can be designed in such a way that it is useful for language and argument education in general. Existing theory of text coherence stipulate that the"
2020.lrec-1.854,iida-tokunaga-2014-building,1,0.490454,"ourse structure provides not only a means to reorder sentences automatically, but it also ex1 The concept behind a major claim has also variously been referred to as main stance or conclusion in the literature. plains the ways which the improved texts are better than the original ones. Therefore, the annotation of discourse structure is a prerequisite for a sentence order analysis. In its final application, an analysis of both the discourse structure and sentence order is also helpful for downstream tasks ˇ such as essay assessment (Al Khatib et al., 2017; Snajder et al., 2019) and education (Iida and Tokunaga, 2014; Cullen et al., 2018; Matsumura and Sakamoto, 2019) Since the texts we wish to annotate are argumentative, we employ the approach from the argument mining field. In the NLP community, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discourse structure consists of two main steps. The first of these is"
2020.lrec-1.854,kaplan-etal-2010-annotation,1,0.849305,"eordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simply better for the annotation process and can lead to a better inter-annotator agreement (Sonntag and Stede, 2014). This paper presents TIARA,3 a new client-side tool for annotating discourse structure and sentence reordering to support our goal."
2020.lrec-1.854,W15-0501,0,0.420687,"iates them into argumentative or non-argumentative components (Stab and Gurevych, 2014; Lippi and Torroni, 2016). Argumentative components (ACs) can be further classified according to their rhetorical function in the discourse, e.g., into major claim, claim and premise (Stab and Gurevych, 2014). The second step is argumentative discourse structure prediction, which links ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far unserved needs, and we are no exception. Taking an empirical approach to the task of sentence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. Whil"
2020.lrec-1.854,P12-2049,0,0.0246766,"technologies and can be easily customised to other annotation schemes, it can be easily used by anybody. Apart from the project it was originally designed for, in which hundreds of texts were annotated by three annotators, TIARA has already been adopted by a second discourse annotation study, which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (wh"
2020.lrec-1.854,D17-1019,0,0.0205955,"While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific too"
2020.lrec-1.854,L16-1371,0,0.0187,"nd Y are potentially confusing when annotators often change the links labelled with X to Y (and vice versa). TIARA does this by logging the actions performed by annotators in each annotation-file. (d) Ease of use, installation and deployment Ease of use and installation for annotators is often prioritised for annotation design, but we believe that deployment is equally important. Not every project owner is tech-savvy; for them, an annotation tool that is hard to deploy is practically unusable. In contrast, tools that are usable without deployment and may run at client-side, such as EasyTree5 (Little and Tratz, 2016), are able to reach and help as many potential users as possible, including those who have no knowledge in programming. Therefore, TIARA shares the same principle. Users only need a web browser and the TIARA package. This tool is written in javascript, html and css. We use JsPlumb6 and Treant-js7 as the visualisation libraries. We understand that the necessity of deployment (server-side) is often coupled with file and/or annotation management features (Yimam et al., 2013), and this is important in a large annotation project. While the current version of TIARA does not actively support such ann"
2020.lrec-1.854,C04-1108,0,0.0459512,"eir improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too m"
2020.lrec-1.854,prasad-etal-2008-penn,0,0.111939,"re easy to customise, offering the flexibility to accomodate a wide range of annotation tasks. However, BRAT and WebAnno were originally designed for morphological, syntactic and semantic annotations (i.e., rather local word or phrase-level annotation). While they support link display and could thus theoretically be used for discourse annotation, the visual display of links appears as drawn directly on top of text. This style of display has already been identified by others as a source of confusion for argumentation and discourse annotation projects (Kirschner et al., 2015). PDTB annotator10 (Prasad et al., 2008) also falls into the class of annotation tools designed for local relations. When it comes to the display of larger-scale hierarchical or graphical structure of discourse, this falls entirely outside the purview of these tools. Annotation tools which are specifically aimed at visualising larger-scale and more global discourse structure have also been developed, e.g., RSTTool11 (Mann and Thompson, 1988), TreeAnno12 (De Kuthy et al., 2018), OVA13 (Janier et al., 2014), DiGAT14 (Kirschner et al., 8 Figure 3: Example of TIARA’s configuration script (written in javascript). (h) Saving annotation Us"
2020.lrec-1.854,W19-4405,0,0.184556,"ks ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far unserved needs, and we are no exception. Taking an empirical approach to the task of sentence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no exist"
2020.lrec-1.854,sonntag-stede-2014-grapat,0,0.281546,"they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simply better for the annotation process and can lead to a better inter-annotator agreement (Sonntag and Stede, 2014). This paper presents TIARA,3 a new client-side tool for annotating discourse structure and sentence reordering to support our goal. We outline our annotation needs (i.e., target domain, annotation scheme) in Section 2, and describe how these requirements translate to design considerations and features of the tool in Section 3. Section 4 shows how it sits among other annotation tools. Section 5 outlines how TIARA benefits other domains such as education, i.e., in the teaching of argumentative writing. Finally, Section 6 concludes this paper and describes what can be improved in the future. 2."
2020.lrec-1.854,C14-1142,0,0.723504,"munity, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discourse structure consists of two main steps. The first of these is argument component detection. This step determines the boundaries of discourse units (segmentation) and differentiates them into argumentative or non-argumentative components (Stab and Gurevych, 2014; Lippi and Torroni, 2016). Argumentative components (ACs) can be further classified according to their rhetorical function in the discourse, e.g., into major claim, claim and premise (Stab and Gurevych, 2014). The second step is argumentative discourse structure prediction, which links ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far u"
2020.lrec-1.854,J17-3005,0,0.350654,"ntence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory pow"
2020.lrec-1.854,E12-2021,0,0.127336,"Missing"
2020.lrec-1.854,J05-2005,0,0.376165,"n. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (which we call first drafts) and in their improved versions is necessary. A tool is needed to support these annotations, and we believe that it can be designed in such a way that it is useful for language and argument education in general. Existing theory of text coherence stipulate that the order of sentences mirrors the intentional struc"
2020.lrec-1.854,P19-1067,0,0.0174254,"ly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simpl"
2020.lrec-1.854,P13-4001,0,0.0544213,"Missing"
2021.argmining-1.2,W19-4505,0,0.0182599,"l structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a seq"
2021.argmining-1.2,2021.findings-acl.84,0,0.0193075,"t five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. els on the cross-topic relation labelling task. Data augmentation can also be applied to mitigate the data sparsity problem. This aims to increase the amount of training data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-"
2021.argmining-1.2,N16-1165,0,0.0291934,"evych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specif"
2021.argmining-1.2,W18-2501,0,0.0285866,"Missing"
2021.argmining-1.2,P17-2061,0,0.0263478,"paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. els on the cross-topic relation labelling task. Data augmentation can also be applied to mitigate the data sparsity problem. This aims to increase the amount of training data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-genre corpora only perform as good as training on each specific corpus. The use of data augmentation may cause the dist"
2021.argmining-1.2,D18-1370,0,0.0158178,"proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. e"
2021.argmining-1.2,D17-1218,0,0.0203554,"ining data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-genre corpora only perform as good as training on each specific corpus. The use of data augmentation may cause the distributional shift problem as well, where the augmented data alter the target distribution that should be learned by the model (Feng et al., 2021). Our target texts are sourced from the ICNALE-AS2R, a corpus of 434 essays written by Asian college students with intermediate proficiency.4 There are 6,021 sentences in total with 13.9 sentences on average per essay. To the best of our knowledge, this is the only currently"
2021.argmining-1.2,2020.acl-main.298,0,0.019308,"ned a BIO tagging scheme and performed end-to-end parsing at token-level, executing all subtasks (i.e., segmentation, unit type classification, linking and relation labelling) at once. Ye and Teufel (2021) also performed endto-end parsing at the token-level. They proposed a more efficient representation for the dependency structure of arguments, and achieved the state-ofthe-art performance for component and relation identifications on the PEC using a biaffine attention model (Dozat and Manning, 2017). The biaffine attention model was originally designed to parse token-to-token dependency, but Morio et al. (2020) extended it to parse proposition (segment) level dependency. Their model dealt with graph-structured arguments in the Cornell eRulemaking corpus (Park and Cardie, 2018). Using the same architecture, Putra et al. (2021b) parsed tree-structured EFL essays in the ICNALE-AS2R corpus (Ishikawa, 2013, 2018; Putra et al., 2021a,b). In tree-structured argumentation, it is common for groups of sentences about the same sub-topic to operate as a unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group"
2021.argmining-1.2,P16-1176,0,0.0124613,"t were annotated using different schemes and of different quality, while ensuring that the model still learns the properties of the target in-domain data well. The choice of EFL texts in this study aims to contribute to a less attended area. In AM, it is common to use well-written texts by proficient authors (e.g., Ashley, 1990; Peldszus and Stede, 2016). However, student texts often suffer from many problems because they are still in the learning phase. Even more, EFL texts are also less coherent and less lexically rich, and exhibit less natural lexical choices and collocations (Silva, 1993; Rabinovich et al., 2016). There are more non-native English speakers (Fujiwara, 2018), and yet, to the best of our knowledge, only one preceding study in AM concerned the EFL genre (Putra et al., 2021b). The codes accompanying this paper are publicly available.3 2 2.1 assuming that the segmentation and AC vs non-AC categorisation have been pre-completed. They experimented on the microtext corpus (Peldszus and Stede, 2016) and the persuasive essay corpus (PEC, Stab and Gurevych (2017)). Eger et al. (2017) formulated argumentative structure parsing in three ways: as relation extraction, as sequence tagging and as depen"
2021.argmining-1.2,D19-1410,0,0.024315,"Missing"
2021.argmining-1.2,L18-1257,0,0.0228185,"abelling) at once. Ye and Teufel (2021) also performed endto-end parsing at the token-level. They proposed a more efficient representation for the dependency structure of arguments, and achieved the state-ofthe-art performance for component and relation identifications on the PEC using a biaffine attention model (Dozat and Manning, 2017). The biaffine attention model was originally designed to parse token-to-token dependency, but Morio et al. (2020) extended it to parse proposition (segment) level dependency. Their model dealt with graph-structured arguments in the Cornell eRulemaking corpus (Park and Cardie, 2018). Using the same architecture, Putra et al. (2021b) parsed tree-structured EFL essays in the ICNALE-AS2R corpus (Ishikawa, 2013, 2018; Putra et al., 2021a,b). In tree-structured argumentation, it is common for groups of sentences about the same sub-topic to operate as a unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group of sentences that should belong together into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-"
2021.argmining-1.2,N18-2006,0,0.101605,"uld know this rule. ... (S18) It is to avoid harming other people. Figure 1: Illustration of linking task, using part of an essay discussing the topic “Smoking should be banned at all restaurants in the country."" The linking task has been identified by earlier works as particularly challenging (Lippi and Torroni, 2016; Cabrio and Villata, 2018; Lawrence and Reed, 2020). There are many possible combinations of links between textual units, and a linking model has to find the most proper structure out of a very large search space. Another typical challenge in AM is the size of annotated corpora (Schulz et al., 2018). Corpus construction is a complex and time-consuming process; it also often requires a team of expert annotators. Existing corpora in AM are relatively “small"" compared with more established fields, such as machine translation or document classification. This hinders training AM models when using a supervised machine learning framework. In this paper, we perform the linking task for essays written by English-as-a-foreign-language (EFL) learners. Given an essay, we identify links between sentences, forming a tree-structured representation of argumentation in the text. Figure 1 illustrates the"
2021.argmining-1.2,D15-1110,0,0.0268365,"ther into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-trees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned"
2021.argmining-1.2,2020.emnlp-main.225,0,0.166197,"pth (ND) prediction. There are six depth categories employed in this paper: depth 0 to depth 4, and depth 5+. The argumentative structure in ICNALE-AS2R corpus is hierarchical, and there is no relations between nodes (sentences) at the same depth. The ND prediction task should help the model to learn the placement of sentences in the hierarchy and guide where each sentence should point at, that is, sentences at depth X point at sentences at depth X − 1. We also propose to use sentence position (spos) embedding as an input feature because it has been proved to be useful in other studies (e.g., Song et al., 2020). The sentence position encoding is G hN(target) BiLSTM Dense Sentence-BERT Encoder s1 s2 ... Multi-Task Learning with Structural Signal We propose to extend the B IAF model in an MTL setup using two novel structural-modelling-related auxiliary tasks. The first auxiliary task is a quasi argumentative component type (QACT) prediction. ICNALE-AS2R corpus does not assign AC types per se, but we can compile the following four sentence types from the tree typology: hN(source) h2(target) (2) The Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) is applied to create a minimum spanning tree"
2021.argmining-1.2,D14-1006,0,0.0318209,"unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group of sentences that should belong together into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-trees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et"
2021.argmining-1.2,D17-1143,0,0.0195564,"ees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classif"
2021.argmining-1.2,J17-3005,0,0.156408,"gure 1 illustrates the task. Our contributions are twofold. First, we propose two structural-modelling related Introduction Argument mining (AM) is an emerging area that addresses the automatic analysis of argumentation. Many recent studies commonly try to tackle two major tasks (Lawrence and Reed, 2020). The first of these is argumentative component identification, in which argumentative units (ACs) and nonargumentative components (non-ACs) including their boundaries are determined. ACs can be further classified according to their role in argumentation, e.g., major claim,1 claim and premise (Stab and Gurevych, 2017). The second task is called argumentative structure prediction, which first establishes links from source to target ACs (this is called the linking task) and then labels the relationship between them, for instance using the support 1 The major claim is the statement expressing the writer’s view on the discussion topic; also called main stance or main claim. 12 Proceedings of The 8th Workshop on Argument Mining, pages 12–23 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics auxiliary tasks to train our model in a multi-task learning (MTL) fashi"
2021.argmining-1.2,D18-1402,0,0.0382285,"Missing"
2021.argmining-1.2,2021.bea-1.10,1,0.914761,"s in this study aims to contribute to a less attended area. In AM, it is common to use well-written texts by proficient authors (e.g., Ashley, 1990; Peldszus and Stede, 2016). However, student texts often suffer from many problems because they are still in the learning phase. Even more, EFL texts are also less coherent and less lexically rich, and exhibit less natural lexical choices and collocations (Silva, 1993; Rabinovich et al., 2016). There are more non-native English speakers (Fujiwara, 2018), and yet, to the best of our knowledge, only one preceding study in AM concerned the EFL genre (Putra et al., 2021b). The codes accompanying this paper are publicly available.3 2 2.1 assuming that the segmentation and AC vs non-AC categorisation have been pre-completed. They experimented on the microtext corpus (Peldszus and Stede, 2016) and the persuasive essay corpus (PEC, Stab and Gurevych (2017)). Eger et al. (2017) formulated argumentative structure parsing in three ways: as relation extraction, as sequence tagging and as dependency parsing tasks. They defined a BIO tagging scheme and performed end-to-end parsing at token-level, executing all subtasks (i.e., segmentation, unit type classification, li"
2021.bea-1.10,C16-1324,0,0.0555068,"Missing"
2021.bea-1.10,P12-2041,0,0.0311027,"in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the two encoders perform in our task. All resources of this paper are available on github.1 2 episode in response to the given writing prompt. ACs can be further classified according to their communicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we id"
2021.bea-1.10,P18-1058,0,0.0299086,"Missing"
2021.bea-1.10,D17-1070,0,0.0176396,"r and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the two encoders perform in our task. All resources of this paper are available on github.1 2 episode in response to the given writing prompt. ACs can be further classified according to their communicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, dire"
2021.bea-1.10,W19-4510,0,0.0148847,"e biaffine model (“B IAF”) by Dozat and Manning (2017), treating the sentence linking task as sentence-level dependency parsing (Figure 4). The first three layers produce contextual sentence representations in the same manner as in the By averaging subword embeddings. 101 5 We conducted a preliminary fine-tuning experiment on sentence linking task, but the performance did not improve. and finally fed into a prediction layer. As the second option (FF LSTM, Figure 5b), we feed rsource and rtarget to an LSTM layer, and the hidden units of LSTM are concatenated before being sent to a dense layer (Deguchi and Yamaguchi, 2019). T H(source) . h1(source) h1(target) . U H(target) = h2(source) G hN(source) hN(target) h2(target) BiLSTM label label Prediction Prediction Dense Encoder s1 s2 ... sN Concat S EQ T G model. These representations are then fed into two different dense layers, in order to encode the corresponding sentence when   it acts as a source h(source) or target h(target) in a relation. Finally, a biaffine transformation is applied to all source and target representations to produce the final output matrix G ∈ RN ×N , in which each row gi represents where the source sentence si should point to (its highe"
2021.bea-1.10,N19-1423,0,0.00864795,"want to investigate how far the existing labelled corpora for well-written texts can also be useful for training parsers for noisy texts. To this end, we train parsers on both in-domain and out-of-domain texts and evaluate them on the in-domain task. For our out-of-domain texts, we use the improved versions of noisy EFL texts. These improvements were produced by an expert annotator and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the"
2021.bea-1.10,P17-1002,0,0.0153622,"hed from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependency parsing formulati"
2021.bea-1.10,W18-2501,0,0.0529767,"Missing"
2021.bea-1.10,P16-2089,0,0.0152679,"se. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional"
2021.bea-1.10,J86-3001,0,0.518065,"Missing"
2021.bea-1.10,J17-1004,0,0.0165336,"he biaffine model is the current state-of-the-art of syntactic dependency parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e"
2021.bea-1.10,P82-1020,0,0.751772,"Missing"
2021.bea-1.10,iida-tokunaga-2014-building,1,0.802739,"ot an explicitly annotated category. As the last step, annotators rearrange sentences and performed text repair to improve the texts from a discourse perspective. There are four relations between ACs: SUPPORT (sup), ATTACK (att), DETAIL (det) and RESTATE - To improve the texts, annotators were asked to rearrange sentences so that it results in the most logically well-structured texts they can think of. This is the second annotation layer in our corpus. No particular reordering strategy was instructed. Reordering, however, may cause irrelevant or incorrect referring and connective expressions (Iida and Tokunaga, 2014). To correct these expressions, annotators were instructed to minimally repair the text where this is necessary to retain the original meaning of the sentence. For instance, they replaced pronouns with their referents, and removed or replaced inappropriate connectives. Text repair is also necessary to achieve standalone major claims. For example, “I think so” with so referring to the writing prompt (underlined in what follows) can be rephrased as “I think smoking should be banned at all restaurants.” Figure 1 shows an example of our annotation scheme using a real EFL essay. Figure 2 then illus"
2021.bea-1.10,D10-1023,0,0.0225072,"ommunicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which ada"
2021.bea-1.10,W15-0501,0,0.0255707,"ntroduces a topic of the discussion in a neutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argument are repeated or summarised for the second time, e.g., when the major claim is restated in the conclusion of the essay. D ETAIL and RESTATEMENT links are not common in AM; the first was introduced by Kirschner et al. (2015) and the second by Skeppstedt et al. (2018), but both work on well-written texts. The combination of these four relations is unique in AM. Dataset We use part of the “International Corpus Network of Asian Learners of English” (Ishikawa, 2013, 2018), which we annotated with Argumentative Structure and Sentence Reordering (“ICNALE-AS2R” corpus).2 This corpus contains 434 essays written by college students in various Asian countries. They are written in response to two prompts: (1) about banning smoking and (2) about students’ part-time jobs. Essays are scored in the range of [0, 100]. There are"
2021.bea-1.10,P16-1105,0,0.0274268,"he features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependency parsing formulation is also a straightforward adaptation as it models tree structures. The biaffine model is the current state-of-the-art of syntactic dependency parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffin"
2021.bea-1.10,W18-5202,0,0.015782,"cy parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021"
2021.bea-1.10,2020.lrec-1.854,1,0.82827,"Missing"
2021.bea-1.10,P16-1176,0,0.289279,"ul to train argument mining system for noisy texts. 1 Introduction Real-world texts are not always well-written, especially in the education area where students are still learning how to write effectively. It has been observed that student texts often require improvements at the discourse-level, e.g., in persuasiveness and content organisation aspects (Bamberg, 1983; Zhang and Litman, 2015; Carlile et al., 2018). Worse still, texts written by non-native speakers are also less coherent, exhibit less lexical richness and more unnatural lexical choices and collocations (Johns, 1986; Silva, 1993; Rabinovich et al., 2016). Our long-term goal is to improve EFL essays from the discourse perspective. One way to do this is by recommending a better arrangement of sentences, 97 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 97–109 April 20, 2021 ©2021 Association for Computational Linguistics but also perform a structural analysis, giving more insights into the models’ ability to learn different aspects of the argumentative structure. The second contribution is showing the effectiveness of well-written texts as training data for argumentative parsing of noisy t"
2021.bea-1.10,D19-1410,0,0.028206,"o separate domains, and AM systems were trained separately on each domain. We want to investigate how far the existing labelled corpora for well-written texts can also be useful for training parsers for noisy texts. To this end, we train parsers on both in-domain and out-of-domain texts and evaluate them on the in-domain task. For our out-of-domain texts, we use the improved versions of noisy EFL texts. These improvements were produced by an expert annotator and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used b"
2021.bea-1.10,W18-5218,0,0.0183935,"eutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argument are repeated or summarised for the second time, e.g., when the major claim is restated in the conclusion of the essay. D ETAIL and RESTATEMENT links are not common in AM; the first was introduced by Kirschner et al. (2015) and the second by Skeppstedt et al. (2018), but both work on well-written texts. The combination of these four relations is unique in AM. Dataset We use part of the “International Corpus Network of Asian Learners of English” (Ishikawa, 2013, 2018), which we annotated with Argumentative Structure and Sentence Reordering (“ICNALE-AS2R” corpus).2 This corpus contains 434 essays written by college students in various Asian countries. They are written in response to two prompts: (1) about banning smoking and (2) about students’ part-time jobs. Essays are scored in the range of [0, 100]. There are two novelties in this corpus: (1) it uses a"
2021.bea-1.10,W14-2110,0,0.0317128,"g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing va"
2021.bea-1.10,C14-1142,0,0.0203034,"non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021 98 lems with the existing corpora is the unclear distinction between native and non-native speakers. Additionally, to investigate and bridge the gap of performance between AM systems on noisy and well-written texts, it is necessary to use a parallel corpus containing both versions of texts. However, none of the above studies did. 3 MENT (res). S UPPORT and ATTACK relations are common in AM. They are used when the source sentence supports or attacks the argument in the target sentence (Peldszus and Stede, 2013; Stab and Gurevych, 2014). We use the DETAIL relation in two cases. First, when the source presents additional details (further explanations, descriptions or elaborations) about the target sentence, and second, when the source introduces a topic of the discussion in a neutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argum"
2021.bea-1.10,J17-3005,0,0.0481687,"AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021 98 lems with the existing corpora is the unclear distinction between native and non-native speakers. Additi"
2021.bea-1.10,2020.coling-main.535,0,0.0126975,"ations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependen"
2021.bea-1.10,C16-1158,0,0.0983403,"sz and Sidner, 1986; Hovy, 1991; Webber and Joshi, 2012). This paper describes the application of Argument Mining (AM) to EFL essays. AM is an emerging area in computational linguistics which aims to explain how argumentative discourse units (e.g., sentences, clauses) function and relate to each other in the discourse, forming an argument as a whole (Lippi and Torroni, 2016). AM has broad applications in various areas, such as in the legal (Ashley, 1990) and news (Al-Khatib et al., 2016) domains. Also in the education domain, AM is beneficial for many downstream tasks such as text assessment (Wachsmuth et al., 2016), text improvement (as described above) and teaching (Putra et al., 2020). It is common in AM to use well-written texts written by proficient authors, as do Peldszus and Stede (2016), Al-Khatib et al. (2016), among others. However, there are more non-native speakers of English than native speakers in the world, and their writings are often noisy as previously described. Yet, EFL is a niche domain in AM. This paper presents three contributions. First, this paper presents an application of AM to EFL essays. We parse the argumentative structure in two steps: (i) a sentence linking step where we i"
2021.bea-1.10,W12-3205,0,0.0606183,"Missing"
2021.bea-1.10,W15-0616,0,0.0584445,"Missing"
2021.eacl-main.55,N19-1423,0,0.190924,"n encode the same information topologically2 , making our representation potentially computationally more efficient. 3.2 Modified Biaffine Parser To our knowledge, the state-of-the-art dependency parser is the one in Dozat and Manning (2018). It consists of a BiLSTM to encoder input text, and a deep biaffine attention module to score each possible head-dependent pair, as shown in Figure 3(a). The structure of our biaffine parser, as shown in Figure 3(b), closely follows that of Dozat and Manning (2018). We have replaced the Embedding layer and the BiLSTM layer with a pre-trained BERT encoder (Devlin et al., 2019), so that the parser can benefit from rich unsupervised data. Given a text sequence S = s1 s2 ...sn , its encoded representation r S ∈ Rn×denc is calculated as in Eq. (1), where BERT means the BERT encoder. 672 r S = BERT(s1 s2 ...sn ) r ROOT (1) = FFN(mean(r S ), axis = 0) R = [r S ; r (2) ROOT ], axis = 0 H edge parent H label parent H edge child H label child (3) edge parent = FFN = FFN (R) label parent edge child = FFN (R) (5) (R) label child = FFN (4) (6) (R) (7) Biaff(x, y) = x> Uy + W(x ⊕ y) + b edge sc label sc 0 edge yi,j = Biaff edge = Biaff = (H label {scedge i,j edge parent (H ,H l"
2021.eacl-main.55,P18-2077,0,0.10061,"al end-to-end AM that addresses all subtasks in one model. They make a comprehensive comparison of methods, among which a model originally proposed for extracting entities and relations (LSTM-ER) achieves the best performance on a popular AM benchmark (Stab and Gurevych, 2017), far outperforming one that uses a dependency parsing (DP) approach. In this paper, we propose another neural endto-end approach for the full AM task, called Biaffine Dependency Parsing for Argument Mining (BiPAM). In our approach, AM is formalised as a DP problem, which we model with a modified biaffine neural network (Dozat and Manning, 2018), using our own dependency representation for arguments. Our representation, like the one in Eger et al. (2017), also unifies all AM subtasks under a token-level framework, so that they can be modelled with one single neural network. The biaffine parser in our approach mainly consists of a neural encoder to extract contextualized features of each token in a sequence, and a biaffine classifier that decides which relation holds between any two tokens, and returns a directed acyclic graph (DAG) expressing this information. Compared with the DP approach in Eger et al. (2017), our model performs at"
2021.eacl-main.55,P15-1033,0,0.0144282,"ns. They report satisfactory results for component classification and relation identification on the Cornell eRulemaking Corpus (CDCP) (Niculae et al., 2017; Park and Cardie, 2018). In contrast, Eger et al. (2017) take the second approach, taking raw text as input so that the component segmentation task is addressed. They design a dependency representation for argument structures, in which dependencies are represented at the token-level, with all edges pointing from parent to child1 . They have evaluated feature-based parsers (McDonald et al., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard and Goldberg, 2016), and relation extraction (Miwa and Bansal,"
2021.eacl-main.55,P17-1002,0,0.0909218,"oses four subtasks: 1) component segmentation, i.e., cutting a raw sequence into text spans that are either argumentative or nonargumentative segments (only argumentative segments are called argument components); 2) component classification, i.e., labelling each argument components with a tag in a pre-defined scheme, (e.g., “P REMISE” or “C LAIM”); 3) relation detection, i.e., deciding if two argument components are directly related; and 4) relation classification, i.e., categorizing a detected relation into a class in a predefined scheme, (e.g., “ATTACK” or “S UPPORT”) (Persing and Ng, 2016; Eger et al., 2017; Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Lawrence and Reed, 2020). The first two subtasks are often referred together as component identification, and the last two as relation identification. An example of the argument structure is illustrated in Figure 1. We see that the “rain” component acts as the premise, supporting the claim of “beautiful”. Some of the AM approaches that try to solve all four subtasks use a pipelined architecture. Independent models are first trained for each subtask; the final results are then achieved by using model ensemble methods such as Integer Linear"
2021.eacl-main.55,J17-1004,0,0.11582,"1) component segmentation, i.e., cutting a raw sequence into text spans that are either argumentative or nonargumentative segments (only argumentative segments are called argument components); 2) component classification, i.e., labelling each argument components with a tag in a pre-defined scheme, (e.g., “P REMISE” or “C LAIM”); 3) relation detection, i.e., deciding if two argument components are directly related; and 4) relation classification, i.e., categorizing a detected relation into a class in a predefined scheme, (e.g., “ATTACK” or “S UPPORT”) (Persing and Ng, 2016; Eger et al., 2017; Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Lawrence and Reed, 2020). The first two subtasks are often referred together as component identification, and the last two as relation identification. An example of the argument structure is illustrated in Figure 1. We see that the “rain” component acts as the premise, supporting the claim of “beautiful”. Some of the AM approaches that try to solve all four subtasks use a pipelined architecture. Independent models are first trained for each subtask; the final results are then achieved by using model ensemble methods such as Integer Linear Programming (Persing and Ng,"
2021.eacl-main.55,P15-1162,0,0.0410799,"Missing"
2021.eacl-main.55,P15-1000,0,0.225479,"Missing"
2021.eacl-main.55,Q16-1023,0,0.0289155,"isfactory results for component classification and relation identification on the Cornell eRulemaking Corpus (CDCP) (Niculae et al., 2017; Park and Cardie, 2018). In contrast, Eger et al. (2017) take the second approach, taking raw text as input so that the component segmentation task is addressed. They design a dependency representation for argument structures, in which dependencies are represented at the token-level, with all edges pointing from parent to child1 . They have evaluated feature-based parsers (McDonald et al., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard and Goldberg, 2016), and relation extraction (Miwa and Bansal, 2016). Among them, the LSTM-ER m"
2021.eacl-main.55,P16-1101,0,0.036776,"ed feature-based parsers (McDonald et al., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard and Goldberg, 2016), and relation extraction (Miwa and Bansal, 2016). Among them, the LSTM-ER model for relation extraction performs the best. However, it requires an additional syntactic parser to produce syntactic dependency trees for the model input. Moreover, the tree-structured LSTM module used to encode those syntactic dependency trees is both conceptually and structurally complicated. In contrast, our proposed approach does not require such syntactic information to break the state-of-the-art (although we will show that it can be enhanced if this information"
2021.eacl-main.55,H05-1066,0,0.0603159,"it killed much marine life, tourism has threatened nature.” relations. They report satisfactory results for component classification and relation identification on the Cornell eRulemaking Corpus (CDCP) (Niculae et al., 2017; Park and Cardie, 2018). In contrast, Eger et al. (2017) take the second approach, taking raw text as input so that the component segmentation task is addressed. They design a dependency representation for argument structures, in which dependencies are represented at the token-level, with all edges pointing from parent to child1 . They have evaluated feature-based parsers (McDonald et al., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard"
2021.eacl-main.55,P16-1105,0,0.0317365,"yer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard and Goldberg, 2016), and relation extraction (Miwa and Bansal, 2016). Among them, the LSTM-ER model for relation extraction performs the best. However, it requires an additional syntactic parser to produce syntactic dependency trees for the model input. Moreover, the tree-structured LSTM module used to encode those syntactic dependency trees is both conceptually and structurally complicated. In contrast, our proposed approach does not require such syntactic information to break the state-of-the-art (although we will show that it can be enhanced if this information is provided), and the architecture of our biaffine dependency parser is both simpler and more gen"
2021.eacl-main.55,2020.acl-main.298,0,0.32988,"xample argument structure for “Cambridge is beautiful, because it rains a lot.” 669 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 669–678 April 19 - 23, 2021. ©2021 Association for Computational Linguistics classification are usually approached at the tokenlevel, but relation detection and classification at the segment-level. Therefore, many researchers try to solve only some of them with neural approaches. For example, Morio and Fujita (2019) focus only on component segmentation and classification; Niculae et al. (2017) and Morio et al. (2020) both ignore the component segmentation task, feeding manually segmented text as input into their models. of-the-art. Compared with LSTM-ER, our biaffine parser is conceptually and structurally simpler, and our approach is more general because the output of our biaffine parser is a DAG instead of a tree; this is required by several argument schemes which go beyond trees (Park and Cardie, 2018; Lawrence and Reed, 2020). Our main contributions are as follows: • We are the first to apply the biaffine parser to AM in an end-to-end approach addressing all four subtasks in one model. Our proposed bi"
2021.eacl-main.55,P17-1091,0,0.0942322,"ntation and Figure 1: An example argument structure for “Cambridge is beautiful, because it rains a lot.” 669 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 669–678 April 19 - 23, 2021. ©2021 Association for Computational Linguistics classification are usually approached at the tokenlevel, but relation detection and classification at the segment-level. Therefore, many researchers try to solve only some of them with neural approaches. For example, Morio and Fujita (2019) focus only on component segmentation and classification; Niculae et al. (2017) and Morio et al. (2020) both ignore the component segmentation task, feeding manually segmented text as input into their models. of-the-art. Compared with LSTM-ER, our biaffine parser is conceptually and structurally simpler, and our approach is more general because the output of our biaffine parser is a DAG instead of a tree; this is required by several argument schemes which go beyond trees (Park and Cardie, 2018; Lawrence and Reed, 2020). Our main contributions are as follows: • We are the first to apply the biaffine parser to AM in an end-to-end approach addressing all four subtasks in on"
2021.eacl-main.55,L18-1257,0,0.019696,"nput is manually segmented, with argumentative and non-argumentative segments already given. They use task-specific BiLSTMs to encode those segments, and a biaffine dependency parser (Dozat and Manning, 2018) with minor alternation to classify argument components and their 670 Figure 2: (a) Eger et al. (2017)’s representation, and (b) our representation, for “Just because it killed much marine life, tourism has threatened nature.” relations. They report satisfactory results for component classification and relation identification on the Cornell eRulemaking Corpus (CDCP) (Niculae et al., 2017; Park and Cardie, 2018). In contrast, Eger et al. (2017) take the second approach, taking raw text as input so that the component segmentation task is addressed. They design a dependency representation for argument structures, in which dependencies are represented at the token-level, with all edges pointing from parent to child1 . They have evaluated feature-based parsers (McDonald et al., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. ("
2021.eacl-main.55,D14-1162,0,0.085442,"there is one or more major claims in a paragraph, ROOT takes them all as children. Otherwise, ROOT takes all claims in that paragraph as children. 4.2 Competing Models We choose two models in Eger et al. (2017) for comparison: 1) LSTM-Parser, the best-performing DP model, and 2) LSTM-ER, the overall bestperforming model. We use our reimplementation of these models and observe results close to those 673 reported in Eger et al. (2017). Both models are trained at the paragraph-level, with the default hyperparameter configuration provided in the source code. The same pre-trained GloVe embeddings (Pennington et al., 2014) are used as in Eger et al. (2017)3 . We use the Stanford syntactic dependency parser (Chen and Manning, 2014) to produce syntactic trees required by the LSTM-ER model. 4.3 Training Final Graph Generation First, several post-processing techniques are applied to make the parser output compatible with the AM dataset (similar techniques are also applied in Eger et al. (2017)), in the following order. 1. The parser sometimes recognises a segment as (mainly) belonging to one label, but interrupts it with a small number of non-fitting labels. We assume the label intended is the majority label and th"
2021.eacl-main.55,N16-1164,0,0.178011,"nts, AM typically proposes four subtasks: 1) component segmentation, i.e., cutting a raw sequence into text spans that are either argumentative or nonargumentative segments (only argumentative segments are called argument components); 2) component classification, i.e., labelling each argument components with a tag in a pre-defined scheme, (e.g., “P REMISE” or “C LAIM”); 3) relation detection, i.e., deciding if two argument components are directly related; and 4) relation classification, i.e., categorizing a detected relation into a class in a predefined scheme, (e.g., “ATTACK” or “S UPPORT”) (Persing and Ng, 2016; Eger et al., 2017; Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Lawrence and Reed, 2020). The first two subtasks are often referred together as component identification, and the last two as relation identification. An example of the argument structure is illustrated in Figure 1. We see that the “rain” component acts as the premise, supporting the claim of “beautiful”. Some of the AM approaches that try to solve all four subtasks use a pipelined architecture. Independent models are first trained for each subtask; the final results are then achieved by using model ensemble methods suc"
2021.eacl-main.55,K18-2016,0,0.0287027,"best-performing approach (LSTM-ER) in Eger et al. (2017) and thus achieves a new state2 Related Work Our work is closely related to existing approaches framing AM as DP and to research on neural endto-end AM. The essential aim of AM is to analyse the structure of arguments. Most argument schemes (Toulmin, 2003; Peldszus and Stede, 2013; Habernal and Gurevych, 2017; Visser et al., 2019) represent argument structures as trees or DAGs. Similar structures also exist in syntactic and semantic parsing, and can be efficiently analysed with existing dependency parsers (Dozat and Manning, 2017, 2018; Qi et al., 2018). However, unlike syntactic or semantic parsing, dependency structures in AM operate at the segment-level, not the tokenlevel. To apply existing DP techniques to AM, one can either ignore the component segmentation task, only working on already segmented text, or one can convert segment-level dependencies to token-level dependencies. Morio et al. (2020) take the first approach. In their work, the input is manually segmented, with argumentative and non-argumentative segments already given. They use task-specific BiLSTMs to encode those segments, and a biaffine dependency parser (Dozat and Manni"
2021.eacl-main.55,P16-2038,0,0.032207,"., 2005; Bohnet and Nivre, 2012) and neural parsers (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), both of which show unsatisfactory performance. As a result, they discarded DP as inferior. Our work is similar to Morio et al. (2020) in that we also use a modified biaffine dependency parser to model dependencies in argument structures, but similar to Eger et al. (2017) in that we model AM as a full task starting from tokens, not segments. Besides DP, Eger et al. (2017) also study other approaches to neural end-to-end AM, including sequence tagging (Ma and Hovy, 2016), multi-task tagging (Søgaard and Goldberg, 2016), and relation extraction (Miwa and Bansal, 2016). Among them, the LSTM-ER model for relation extraction performs the best. However, it requires an additional syntactic parser to produce syntactic dependency trees for the model input. Moreover, the tree-structured LSTM module used to encode those syntactic dependency trees is both conceptually and structurally complicated. In contrast, our proposed approach does not require such syntactic information to break the state-of-the-art (although we will show that it can be enhanced if this information is provided), and the architecture of our biaffi"
2021.eacl-main.55,J17-3005,0,0.39979,".e., cutting a raw sequence into text spans that are either argumentative or nonargumentative segments (only argumentative segments are called argument components); 2) component classification, i.e., labelling each argument components with a tag in a pre-defined scheme, (e.g., “P REMISE” or “C LAIM”); 3) relation detection, i.e., deciding if two argument components are directly related; and 4) relation classification, i.e., categorizing a detected relation into a class in a predefined scheme, (e.g., “ATTACK” or “S UPPORT”) (Persing and Ng, 2016; Eger et al., 2017; Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Lawrence and Reed, 2020). The first two subtasks are often referred together as component identification, and the last two as relation identification. An example of the argument structure is illustrated in Figure 1. We see that the “rain” component acts as the premise, supporting the claim of “beautiful”. Some of the AM approaches that try to solve all four subtasks use a pipelined architecture. Independent models are first trained for each subtask; the final results are then achieved by using model ensemble methods such as Integer Linear Programming (Persing and Ng, 2016; Stab and Gurevych,"
2021.emnlp-main.653,P96-1041,0,0.581535,"hrough n-gram, or in this case n-phone, modelling. Specifically, we can estimate the probability of observing some phone wt given the previous n − 1 phones by computing the proportion of times this phone follows those previous n − 1 phones in a corpus. By this definition, sequences not present in the corpus will be assigned 0 probability under the model. This, among other factors, contributes to the often poor generalisation abilities of basic n-gram models. Indeed, there exists an entire literature on smoothing and regularisation techniques for n-gram modelling (Katz, 1987; Ney et al., 1994; Chen and Goodman, 1996). Laplace smoothing is a popular choice, being used in a number of recent works in computational linguistics (e.g. Dautriche et al., 2017a; Trott and Bergen, 2020). However, it is perhaps the simplest of such regularisation techniques, and usually leads to much weaker empirical performances than, e.g., Kneyser–Ney (Ney et al., 1994). It is therefore natural to question whether an n-gram model with simple Laplace smoothing can provide a good representation of the true phonotactic distribution of a language. In our experiments, we follow Trott and Bergen in using a 5-gram model with Laplace smoo"
2021.emnlp-main.653,C65-1001,0,0.244338,"ted lexicon would contain more homophony than natural ones; as we will show, overfit distributions will likely produce more collisions. Thus, it is not entirely clear what we can conclude from their experiments. 3 Quantifying Homophony tion over wordforms. In this section, we first provide a definition of a language’s phonotactic distribution. We then present both the R´enyi collision entropy and the sample R´enyi entropy as new measures of homophony. 3.1 Phonotactics and Wordforms Formally, phonotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely word"
2021.emnlp-main.653,2020.emnlp-main.328,1,0.779516,"onotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely words are more frequent (Mahowald et al., 2018). We model the phonotactic distribution over possible wordforms as a language model: p(w) = |w| Y p(wt |w<t ) (1) t=1 whose support is the infinite set W—defined here as the Kleene closure of a phonetic alphabet Σ∗ , albeit where all w ∈ W are padded with beginningof- and end-of-word symbols. Under this definition, highly plausible wordforms would be assigned high probability, and vice-versa. 3.2 Entropy as a Measure of Homophony The R´enyi entropy"
2021.emnlp-main.653,P19-1171,1,0.892198,"Missing"
2021.emnlp-main.653,2020.tacl-1.1,1,0.837036,"onotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely words are more frequent (Mahowald et al., 2018). We model the phonotactic distribution over possible wordforms as a language model: p(w) = |w| Y p(wt |w<t ) (1) t=1 whose support is the infinite set W—defined here as the Kleene closure of a phonetic alphabet Σ∗ , albeit where all w ∈ W are padded with beginningof- and end-of-word symbols. Under this definition, highly plausible wordforms would be assigned high probability, and vice-versa. 3.2 Entropy as a Measure of Homophony The R´enyi entropy"
2021.emnlp-main.654,J10-4006,0,0.0567654,", gable relax, family, place, live, friend bathroom, bedroom, brick, door, family, home, kitchen, place, roof, room Figure 2: Synthetic features generated by three models, and human features. form) concepts. We exploit this in using particular areas of the web pages which describe the concepts a Wikipedia page is dedicated to. We extract from these contexts all nouns except the concept itself2 . The second type of synthetic feature we use comes from the distributional space Distributional Memory (DM), which is a more recent, larger and better-performing version of the afore-mentioned Strudel (Baroni and Lenci, 2010). DM uses relations and co-occurrence between words to describe words. It extracts distributional information from corpora (which are POS tagged and parsed) based on a set of weighted word-link-word tuples. Given a concept, DM outputs a series of triples, consisting of the concept itself, the generated feature, and the relation between the concept and the feature. BART provides our third set of synthetic features. Having access to the parallel data of concepts and their human-provided features (McRae et al., 2005; Buchanan et al., 2019), we can fine-tune the pretrained BART base model (Lewis e"
2021.emnlp-main.654,E17-2084,0,0.0458125,"Missing"
2021.emnlp-main.654,W17-1903,0,0.0624829,"Missing"
2021.emnlp-main.654,2020.acl-main.703,0,0.0206756,"Missing"
2021.emnlp-main.654,2020.tacl-1.47,0,0.0221227,"Missing"
2021.emnlp-main.654,2018.gwc-1.35,0,0.203943,"ough to enable priming. More recently, powerful neural language models such as BERT and GPT have been shown to capture large-scale semantic contexts even better. The availability of different distributional semantic models allows us to empirically test whether Rosch et al.’s idea of cue validity holds true for a large set of concepts, as opposed to some clear-cut toy examples. We present an algorithm which has the ability to determine for each WN synset whether it is BLC or not, and compare three methods for generating synthetic features that are used in our algorithm. This is not a new task: Mills et al. (2018) were the first researchers to perform at-scale BLC classification of all synsets in WordNet. They used several features to do so, including the depth of a concept in WN. But solution differs from theirs because we create synthetic features and can thus directly use Rosch’s original idea of cue validity to detect BLC. The features we create are rather different from those chosen by humans in semanAnother obstacle to the automatic detection of tic norm studies, and it is an empirical question BLC is the vague definition of the concept itself. whether our somewhat compromised form of cue 8295 va"
2021.emnlp-main.654,W13-4302,0,0.0129158,"l., 2018). Concreteness ratings were drawn from a list of 3 million rated word forms (Köper and im Walde, 2017), where each word has been given a 10-point rating from abstract (1) to concrete (10). We expect this to help in classification as BLC are by definition never abstract. We also use a 300-dimension standard Word2Vec representation (Mikolov et al., 2013) for English and 200-dimension one for Mandarin (Song et al., 2018). Despite not having interpretable dimensions, we hypothesise that such representations may capture context in an indirect manner. We applied Chinese Open WordNet (COW) (Wang and Bond, 2013), which was built on top of the synsets of English WN, to match Mandarin concepts to their English counterparts. COW contains 42,312 synsets, 27,888 of which are nouns, compared to the 82,115 nouns in WN. The acquisition age indicator we use is based on test-based age of acquisition norms of 30,000 words/senses with information of the age when children learned this word/sense (Kuperman et al., 2012)1 We successfully matched 22,138 of our (bilingual) WN synsets to the AoA test words/senses, as well to their concreteness ratings. The remaining indicators depend on our method of generating semant"
2021.emnlp-main.654,N18-2028,0,0.0237822,"76). The features of BLC should often be BLC themselves (our novel claim). (Mandarin only): Characters expressing BLC concepts should have fewer brush strokes. Table 1: Our Bilingual Indicators for BLC detection. precision. 3 Approach and characters, for English and Mandarin respectively. Number of brushstrokes is only defined for Mandarin and expresses the intuition that culturally older, more important concepts would exhibit fewer brush strokes. We extracted the English frequency data from the American part of the Google Books corpus and Chinese one from the Tencent AI Lab embedding corpus (Song et al., 2018). Concreteness ratings were drawn from a list of 3 million rated word forms (Köper and im Walde, 2017), where each word has been given a 10-point rating from abstract (1) to concrete (10). We expect this to help in classification as BLC are by definition never abstract. We also use a 300-dimension standard Word2Vec representation (Mikolov et al., 2013) for English and 200-dimension one for Mandarin (Song et al., 2018). Despite not having interpretable dimensions, we hypothesise that such representations may capture context in an indirect manner. We applied Chinese Open WordNet (COW) (Wang and"
2021.emnlp-main.73,2020.lrec-1.521,0,0.0781886,"amantis. While Unitran is particularly error-prone for languages with opaque orthographies (Salesky et al., 2020), we filter out the languages with lower-quality alignments (as we detail below). This original dataset has 690 doculects from 635 languages in 70 language families. In order to study the trade-off hypothesis we require two measurements: phone durations and phone-level surprisals. As mentioned above, phone 4 These texts were crawled from bible.is and utterancealigned by Black (2019) for the CMU Wilderness dataset. 5 A list of all languages can be found in App. C. 6 We set Wikipron (Lee et al., 2020) alignments aside because we could not obtain word position information for them. 7 The term doculect refers to a dialect as recorded in a specific document, in this case a Bible reading. Figure 2: The languages of the VoxClamantis corpus geo-located and coloured by language family. durations are readily available in VoxClamantis. Phone-level surprisals, on the other hand, are not, so we employed phone-level language models in order to estimate them (as detailed in §3). Given both these values, we can now perform our crosslinguistic analysis. First, though, we will describe some data quality c"
2021.emnlp-main.73,W18-0102,0,0.0142434,"ne st appears. Unfortunately, this surprisal is not readily available, since we would need access to the true distribution p(st |s<t ) to compute it. We will use an approximation pθ (st |s<t ) instead, i.e. a phone-level model with estimated parameters θ. 3.1 Approximating p(st |s<t ). While much of the original psycholinguistic work on surprisal estimated pθ using n-gram models (Levy and Jaeger, 2007; Coupé et al., 2019, inter alia), recent work has shown that a language model’s psychometric predictive power correlates directly with its quality, measured by its crossentropy in held-out data (Goodkind and Bicknell, 2018; Wilcox et al., 2020). We will thus make use of LSTMs in this work, since they have been shown to outperform n-grams on phone-level language modelling tasks (Pimentel et al., 2020). We first encode each phone st into a high-dimensional lookup embedding et ∈ d1 , where d1 is its embedding size. We then process these embeddings using an LSTM (Hochreiter and Schmidhuber, 1997), which outputs contextualised hidden state vectors: R ht = LSTM(ht−1 , et−1 ) ∈ Rd 2 (2) bol. The hidden states are then linearly transformed and projected onto ∆|S|+1 , the probability simplex, via a softmax to compute th"
2021.emnlp-main.73,N01-1021,0,0.184808,"ve channel’s capacity 950 (Frank and Jaeger, 2008; Piantadosi et al., 2011). If we assume this channel’s capacity to be constant across languages, we may derive a cross-linguistic version of UID. Such a hypothesis would predict, for instance, that speakers of languages with less informative phones will make them faster. Under this specific interpretation, our study can be seen as evidence of UID as a cross-linguistic phenomenon. 3 Measuring Surprisal To formalise our approach, we first present a standard measure of information content: surprisal. In the context of natural language, surprisal (Hale, 2001) measures the Shannon information content a linguistic unit conveys in context, which can be measured as its negative log-probability: H(St = st |S <t = s<t ) = − log p(st |s<t ) (1) In this equation, S is a sentence-level random variable, with instances s ∈ S ∗ , and t indexes a position in the sentence. Accordingly, we define S as the set of phones in a given phonetic alphabet, and we use s<t to indicate the context in which phone st appears. Unfortunately, this surprisal is not readily available, since we would need access to the true distribution p(st |s<t ) to compute it. We will use an a"
2021.emnlp-main.73,P82-1020,0,0.737253,"Missing"
2021.emnlp-main.73,J86-2003,0,0.474383,"ive effect, duration is multiplied by y per bit of information. Sorted dots represent languages in Unitran; ‘+’ are languages in Epitran. the uniform information density hypothesis (UID; Fenk and Fenk, 1980; Aylett and Turk, 2004; Levy and Jaeger, 2007), which predicts that speakers smooth the information rate in a linguistic signal so as to keep it roughly constant; by smoothing their information rate, natural languages can stay close to a (hypothetical) channel capacity. Across languages, a unified channel capacity allows us to derive a specific instantiation of the compensation hypothesis (Hockett, 1958), with information density (measured in, e.g., bits per phone) being compensated by utterance speed (in, e.g., milliseconds per phone). We may thus predict a trade-off between surprisal1 and duration both within and across the world’s languages. This trade-off has been studied amply within high resource languages (Genzel and Charniak, 2002; Bell et al., 2003; Mahowald et al., 2018, inter alia). Cross-linguistically, however, this trade-off has received comparatively little attention, with a few notable exceptions such as Pellegrino During the course of human evolution, countless languages have"
2021.emnlp-main.73,2021.emnlp-main.74,1,0.682102,"n trade-off. Indeed, Pellegrino et al. (2011) and Coupé et al. (2019) present initial evidence of this trade-off across languages. Analogously, the UID hypothesis posits that, within a language, users balance the amount of information per linguistic unit with the duration of its utterance. This hypothesis has been used to explain a range of experimental data in psycholinguistics, including syntactic reduction (Levy and Jaeger, 2007) and contractions, such as are vs ’re (Frank and Jaeger, 2008). While this theory is somewhat under-specified with respect to its causal mechanisms, as we argue in Meister et al. (2021), one of its typical interpretations is that users are maximising a communicative channel’s capacity 950 (Frank and Jaeger, 2008; Piantadosi et al., 2011). If we assume this channel’s capacity to be constant across languages, we may derive a cross-linguistic version of UID. Such a hypothesis would predict, for instance, that speakers of languages with less informative phones will make them faster. Under this specific interpretation, our study can be seen as evidence of UID as a cross-linguistic phenomenon. 3 Measuring Surprisal To formalise our approach, we first present a standard measure of"
2021.emnlp-main.73,qian-etal-2010-python,0,0.0180848,"gned using either multilingual acoustic models (Wiesner et al., 2019; Povey et al., 2011) or language-specific acoustic models (Black, 2019; Anumanchipalli et al., 2011). VoxClamantis offers its phonetic measurements under three G2P models, which trade-off language coverage and quality. We will focus on two:6 • Epitran (Mortensen et al., 2018). This is a collection of high quality G2P models based on language-specific rules. Phonetic measurements produced with Epitran are available for a collection of 39 doculects7 from 29 languages (as defined by ISO codes) in 8 language families. • Unitran (Qian et al., 2010). This is a naïve and deterministic G2P model, but its derived measurements are available for all languages in VoxClamantis. While Unitran is particularly error-prone for languages with opaque orthographies (Salesky et al., 2020), we filter out the languages with lower-quality alignments (as we detail below). This original dataset has 690 doculects from 635 languages in 70 language families. In order to study the trade-off hypothesis we require two measurements: phone durations and phone-level surprisals. As mentioned above, phone 4 These texts were crawled from bible.is and utterancealigned b"
2021.emnlp-main.73,2020.acl-main.415,1,0.888884,"ugh for this analysis. Further, as we have a specific hypothesis for why this trade-off should arise (humans’ information processing capacity), we are not interested in simply finding any correlation between surprisal and duration. Several confounds could drive such a correlation, but most of these are either trivially true or uninteresting from our perspective. Therefore, a thorough analysis of this trade-off needs to control for these potential confounds. In this work, we investigate the surprisal– duration trade-off by analysing a massively multi-lingual dataset of more than 600 languages (Salesky et al., 2020). We present an experimental framework, controlling for several possible confounds, and evaluate the surprisal–duration trade-off at the phone level. We find evidence of a trade-off across languages: languages with more surprising phones compensate by making utterances longer. We also confirm mono-lingual trade-offs in 319 languages, out of 600;2 within these languages, more surprising phones are pronounced with a significantly longer duration. This is the most representative evidence of the uniform information density hypothesis to date. Moreover, we did not find evidence of a single language"
2021.emnlp-main.73,2021.eacl-main.3,1,0.730146,"Nooteboom, 1981). Under an information-theoretic analysis, it has 8 Probabilities must sum to 1. This finite probability mass means average probability must go down with more classes. 9 Concatenative languages, for instance, would have both longer and less predictable words. Take the German word Hauptbahnhof which can be translated into English as central train station. Predicting this single (and long) German word is equivalent to predicting three words in English. been observed that earlier segments in a word are more surprising than later ones (van Son and Pols, 2003; King and Wedel, 2020; Pimentel et al., 2021). Word-initial segments are both lengthened and more surprising, potentially for unrelated reasons. An analysis which does not control for such word-positioning is thus doomed to find trivial correlations. To account for this word-initial and word-final lengthening, we include three word position fixed effects (initial, middle, or final) in our mixed effects models. Sentential Context. The amount of context that a model conditions on when estimating probabilities will undoubtedly have an impact on a study of this nature. For example, a model that cannot look back beyond the current word, such"
2021.emnlp-main.73,2020.tacl-1.1,1,0.907824,"egrino et al. (2011) recently analysed the speech rate of 8 languages using a semantically controlled corpus. They found strong evidence towards non-uniform speech rates across these languages. This result is not surprising, however, given that natural languages vary widely in their phonology, morphology, and syntax. Despite these differences, researchers have hypothesised that there exist compensatory relationships between the complexity of these components (Hockett, 1958; Martinet, 1955). For instance, a larger phonemic diversity could be compensated by shorter words (Moran and Blasi, 2014; Pimentel et al., 2020) or a larger number of irregular inflected forms could lead to less complex morphological paradigms (Cotterell et al., 2019). Such a compensation can be thus seen as a type of balance, where languages compromise reliability versus effort in communication (Zipf, 1949; Martinet, 1962). One natural avenue for creating this balance would be a language’s information rate. If this were kept roughly constant, the needs of both speakers (who prefer shorter utterances) and listeners (who value easier comprehension) could be accommodated. Speech rate would then be compensated by information density, res"
2021.naacl-main.181,2020.acl-main.463,0,0.0199868,"of that primary right. While there might be a precedent present for the secondary Articles, the probability is high that our models will not have the chance to train on them because they appear late and because our method truncates text due to computational complexity reasons. This could explain why for these Articles, all our models trained on the precedent cases underperform when compared to the models trained on the facts of the case alone. 6.3 Model Limitations BERT being able to reason, it is merely very good at utilising the artefacts in the data when compared to previous approaches. As Bender and Koller (2020) contend a system can’t ever learn meaning from form alone. According to their view, description of the case facts alone will never fully capture the reality of the world the claimant inhabits. On the other hand, there is some evidence towards transformers being able to reason over simple sentences Clark et al. (2020). While this is encouraging, legal documents are far more complicated than the simple sentences considered in the study above. Either way, the models’ ability to reason in the way a human lawyer would is certainly limited and could explain the diminished performance for the more c"
2021.naacl-main.181,P19-1424,0,0.267634,"eated as operating under precedential law, in the vein of common law countries. This is not a given, as the ECtHR is an international court of highest appeal without a formal doctrine of stare decisis (Jacob, 2014), but there is nevertheless strong evidence that it is precedential. This evidence comes from the court’s own guidelines (ECtHR, 2014), but can also be found in the writings of a former judge of the ECtHR (Zupancic, 2016) and of legal scholars (Lupu and Voeten, 2010). Second, there is existing research on the neural modeling of ECtHR case law we can build upon (Aletras et al., 2016; Chalkidis et al., 2019, 2020). Third, the documents of the ECtHR 3 Nats are computed with ln, while bits use log2 . case law, unlike those of most other courts, textually separate the facts from the arguments, which is crucial for our experiments. Case facts are descriptions of what had happened to the claimant before they went to the court; they include domestic proceedings of their case before it was appealed to the ECtHR as a form of a last resort. They do not contain any reference to European Convention of Human Rights (ECHR) Articles or ECtHR case law. Arguments on the other hand contain judges’ discussion of"
2021.naacl-main.181,N19-1423,0,0.0139735,"y for this, approximating them as the difference between two cross-entropies: MI(O; H |F ) ≈ Hθ (O |F ) − Hθ (O |H, F ) MI(O; G |F ) ≈ Hθ (O |F ) − Hθ (O |G, F ) Indeed, although several estimates for the mutual information exist, McAllester and Stratos (2020) argues that estimating it as this difference is the most statistically justified way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ"
2021.naacl-main.181,N18-1175,0,0.0273371,"cle 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we have shifted the focus of legal AI research from practical tasks such as precedent retrieval or outcome pred"
2021.naacl-main.181,S19-2145,0,0.0228148,"way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ (o |•) for its estimate. The better this model, the tighter our estimates will be. The only thing left to do now, is to obtain these probability estimates. We thus model Halsbury’s view as a classification task (see Figure 2) estimating the probability: pθ (o |h, f ) = K Y pθ (ok |h, f ) (11) k=1 We analogously model Goodhart’s view as: pθ"
2021.naacl-main.181,P11-1015,0,0.00562464,"this difference is the most statistically justified way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ (o |•) for its estimate. The better this model, the tighter our estimates will be. The only thing left to do now, is to obtain these probability estimates. We thus model Halsbury’s view as a classification task (see Figure 2) estimating the probability: pθ (o |h, f ) = K Y pθ (ok |h, f"
2021.naacl-main.181,P19-1459,0,0.0157045,"loped a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we hav"
2021.naacl-main.181,C18-1041,0,0.0278454,"mance by the early 2000’s (Ashley, 2017). These systems however proved too brittle to keep up with the everchanging legal landscape and never transitioned from research into industry. More recently, a new wave of deep learning methods has reinvigorated the research interest in legal AI. The majority of this new work has been conducted on statutory legal systems which do not rely on the doctrine of precedent to nearly the same extent as their common law counterparts. For instance, in Chinese law the use of neural models for case outcome classification has already been investigated extensively (Hu et al., 2018; Zhong et al., 2018; Xu et al., 2020). In the precedential legal domain, smaller corpora of annotated cases have been investigated over the years (Grover et al., 2003; Valvoda et al., 2018). However, large-scale corpora necessary for deep learning architectures have become available only recently. The Caselaw Access Project10 introduced a large dataset of American case law in 2018. Aletras et al. (2016) have introduced the ECtHR corpus, and Chalkidis et al. (2019) have run deep neural networks on it in order to predict outcome. Similarly, the Canadian Supreme Court Case corpus has been used i"
2021.naacl-main.181,P19-1171,1,0.920019,"uncated to 512 tokens. Outcome of the precedent is concatenated with either the precedent facts or arguments, and both are jointly truncated at 512 tokens. Finally, these are concatenated together and embedded in 768 dimensions before being fed into the L ONG FORMER . analogy; hence it is the good alignment between the facts of the two cases that leads to consistent outcomes. 3 Operationalising Halsbury and Goodhart. In this work, we intend to measure the use of Halsbury’s and Goodhart’s views in practice, which we operationalise information-theoretically following the methodology proposed by Pimentel et al. (2019). To test the hypothesis, we construct two collections of random variables, which we denote H and G. We define an instance hc of random variable H as the union of arguments S and outcomes for all precedent cases of c, i.e. c0 ∈Pc {ac0 , oc0 }. We will denote the instance h when referring to it in the abstract (without referring to a particular case). We analogously Sdefine instances of random variable G as gc = c0 ∈Pc {fc0 , oc0 }. While the set-theoretic notation may seem tedious, it encompasses the essence of the distinction between Halsbury’s and Goodhart’s view: Each view hypothesises a di"
2021.naacl-main.181,2021.naacl-main.349,1,0.752052,"Missing"
2021.naacl-main.181,2020.acl-main.420,1,0.833816,"3 counterpart. So while the judges might have developed a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision"
2021.naacl-main.181,2020.tacl-1.54,0,0.0161054,"g than their Article 3 counterpart. So while the judges might have developed a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US"
2021.naacl-main.181,D16-1178,0,0.0263346,"RMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we have shifted the focus of legal AI research from practical tasks such as precedent retrieval or outcome prediction, to a theoretical question: which aspect of the precedent is most important in forming the law? To this end, we trained a similar neural modeling approach as Chalkidis et al. (2019) to predict the outcome of a case on the ECtHR dataset, and inspected the difference in the mutual information between our operationalisations o"
2021.naacl-main.181,2020.acl-main.597,1,0.842025,"no uncertainty left. Conversely, if the variables are independent, then H(O) = H(O |G), where H(O) denotes the unconditional entropy of the outcomes O. We now note a common decomposition of mutual information that will help with the approximation: MI(O; H |F ) = H(O |F ) − H(O |H, F ) (5) MI(O; G |F ) = H(O |F ) − H(O |G, F ) (6) In this work, we consider the conditional probabilities p(o |•) as the independent QK product of each Article’s probability, i.e. k=1 p(ok |•). Information-theoretically, then, they are related through the following equation: H(O |•) = K X H(Ok |•) (7) k=1 Following Williams et al. (2020), we further calculate the uncertainty coefficient (Theil, 1970) of each of these mutual informations. These coefficients are easier to interpret, representing the percentage of uncertainty reduced by the knowledge of a random variable: MI(O; H |F ) H(O |F ) MI(O; G |F ) U(O |G; F ) = H(O |F ) U(O |H; F ) = 4 (8) (9) Experimental Setup We choose to work with the ECtHR corpus for three reasons. First, it can be treated as operating under precedential law, in the vein of common law countries. This is not a given, as the ECtHR is an international court of highest appeal without a formal doctrine"
2021.naacl-main.181,2020.acl-main.280,0,0.0210805,"17). These systems however proved too brittle to keep up with the everchanging legal landscape and never transitioned from research into industry. More recently, a new wave of deep learning methods has reinvigorated the research interest in legal AI. The majority of this new work has been conducted on statutory legal systems which do not rely on the doctrine of precedent to nearly the same extent as their common law counterparts. For instance, in Chinese law the use of neural models for case outcome classification has already been investigated extensively (Hu et al., 2018; Zhong et al., 2018; Xu et al., 2020). In the precedential legal domain, smaller corpora of annotated cases have been investigated over the years (Grover et al., 2003; Valvoda et al., 2018). However, large-scale corpora necessary for deep learning architectures have become available only recently. The Caselaw Access Project10 introduced a large dataset of American case law in 2018. Aletras et al. (2016) have introduced the ECtHR corpus, and Chalkidis et al. (2019) have run deep neural networks on it in order to predict outcome. Similarly, the Canadian Supreme Court Case corpus has been used in inforAnother possible explanation fo"
C02-1073,J93-1004,0,\N,Missing
C02-1073,A00-2024,0,\N,Missing
C02-1073,saggion-etal-2002-developing,1,\N,Missing
C02-1073,A00-2035,0,\N,Missing
C02-1073,W00-0401,1,\N,Missing
C02-1073,W00-0408,0,\N,Missing
C02-1073,E99-1011,0,\N,Missing
C02-1073,W97-0704,0,\N,Missing
C02-1073,W00-0403,1,\N,Missing
C02-1073,grover-etal-2000-lt,0,\N,Missing
C14-1002,D10-1111,0,0.0286972,"er from an LDA-style topic model or from a distribution associated with the rhetorical category assigned to the sentence. The former captures the subject matter of the document; the latter captures conventional language that is independent of the document’s subject matter. The sentence categories are generated from a first-order Markov model. The assignment of responsibility for a word is implemented through a so-called “switching variable”, a binary-valued latent variable. This is a commonly used mechanism for interpolating language models (Griffiths et al., 2004; Reisinger and Mooney, 2010; Ahmed and Xing, 2010); in many cases, the goal is to assign common words to a “background” distribution that is not considered an object of interest from a topic modelling perspective. In our case it is this non-topical part of the text that is the object of interest. The dependencies between variables in our full B OILERPLATE -LDA model are shown by the plate diagram in Figure 1. The corresponding “generative story” is as follows: 1 It would be interesting to swap in Chen et al.’s generalised Mallows model for the HMM-style ordering model in B OILERPLATE -LDA. The former has the advantage of capturing non-local o"
C14-1002,N09-1042,0,0.0113984,"are many other textual genres where argumentation is conventionalised. For example, Burstein et al. (2003) identify building blocks analogous to AZ zones in the writing of English language learners and demonstrate that a supervised classification approach can be used to mark up their essays. Also in the educational domain, Madnani et al. (2012) train a supervised classifier to detect the “shell” language that learners use to organise the high-level structure of their compositions; this is quite close to our idea of “templates” or “recipes” for scientific papers. Sauper and Barzilay (2009) and Chen et al. (2009) both present models that learn structural conventions in Wikipedia articles without relying on human annotation. Sauper and Barzilay’s model induces the typical section structure of Wikipedia articles 3 about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section for a target entity, before performing multidocument summarisation to produce a new entry for posting to Wikipedia. Chen et al. take a Bayesian segmentation approach to implicitly learn the topical section structure of articles and use a generalised Mallows model, a distribution over per"
C14-1002,C12-1041,0,0.124835,"Missing"
C14-1002,N13-1019,0,0.00651441,"e used to induce meaningful generalisations from observations of co-occurrences. Blei et al. (2003) introduced Latent Dirichlet Allocation (LDA) as a model of thematic structure in documents, but subsequent work has adapted the general framework to many different purposes in modelling text as well as other kinds of data. This includes research on modelling aspects of document structure such as topic segmentation, implementing the intuitions that neighbouring blocks of text are coherent in the sense of lexical similarity (Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (20"
C14-1002,D08-1035,0,0.0614839,"esian latent-variable models are used to induce meaningful generalisations from observations of co-occurrences. Blei et al. (2003) introduced Latent Dirichlet Allocation (LDA) as a model of thematic structure in documents, but subsequent work has adapted the general framework to many different purposes in modelling text as well as other kinds of data. This includes research on modelling aspects of document structure such as topic segmentation, implementing the intuitions that neighbouring blocks of text are coherent in the sense of lexical similarity (Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 201"
C14-1002,D08-1036,0,0.011499,"mial(Φti ) end if end for end for end for We train the model using Gibbs sampling. Due to Dirichlet-multinomial and beta-Bernoulli conjugacy it is relatively straightforward to integrate out the multinomial and Bernoulli distribution parameters θ, Φ, Ψ and Σ and derive update rules for a collapsed Gibbs sampler. Each iteration of the sampler visits each sentence in the corpus in turn, first sampling the sentence label assignment zs and then sampling for each word in the sentence the switch indicator bi and (if bi = 1) the topic assignment ti . The sentence label update is performed using what Gao and Johnson (2008) call a pointwise collapsed Gibbs sampler. Omitting hyperparameters for clarity, the sampling probabilities can be written as −i fzi−1 →z + κz fz→zi+1 + I(z = zi+1 ) + κzi+1 Y Γ(fzv,b=0 + fsi v,b=0 + γ) P P (zi = z|z , w, b) ∝ P fzi−1 + z 0 κz 0 fz−i + I(z = zi+1 ) + z 0 κz 0 Γ(fz−i + fsi + γ|V |) v∈V −i (1) where fz−&gt;z 0 is the transition frequency from zone z to zone z 0 , fz is the number of sentences assigned zone z; I(z = zi+1 ) has value 1 if the two zone assignments are equal and 0 otherwise; V is the vocabulary of word types; fzv,b=0 is the number of words of type z that appear in sent"
C14-1002,W10-1913,0,0.183042,"Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012). Varga et al. use a topic model variant called Z ONE -LDA that assigns each sentence a latent variable index or “topic” and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone. Such a model will have the effect of clustering sentences that share lexical items. Varga"
C14-1002,D11-1025,0,0.0229085,"et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012). Varga et al. use a topic model variant called Z ONE -LDA that assigns each sentence a latent variable index or “topic” and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone. Such a model will have the effect of clustering sentences that share lexical items. Varga et al. also propos"
C14-1002,I08-1050,0,0.0123483,"., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012). Varga et al. use a topic model variant called Z ONE -LDA that assigns each sentence a latent variable index or “topic” and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone. Such a model will have the effect of clustering sentences that share le"
C14-1002,N12-1003,0,0.0278689,"re connected by potentials weighted according to adjacency and sentence similarity, as well as hand-defined rules about passivisation and sentence location. The papers cited in the two preceding paragraphs have focused on rhetorical analysis in scientific writing, yet there are many other textual genres where argumentation is conventionalised. For example, Burstein et al. (2003) identify building blocks analogous to AZ zones in the writing of English language learners and demonstrate that a supervised classification approach can be used to mark up their essays. Also in the educational domain, Madnani et al. (2012) train a supervised classifier to detect the “shell” language that learners use to organise the high-level structure of their compositions; this is quite close to our idea of “templates” or “recipes” for scientific papers. Sauper and Barzilay (2009) and Chen et al. (2009) both present models that learn structural conventions in Wikipedia articles without relying on human annotation. Sauper and Barzilay’s model induces the typical section structure of Wikipedia articles 3 about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section for a target ent"
C14-1002,J00-3005,0,0.0547693,"ction structure of Wikipedia articles 3 about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section for a target entity, before performing multidocument summarisation to produce a new entry for posting to Wikipedia. Chen et al. take a Bayesian segmentation approach to implicitly learn the topical section structure of articles and use a generalised Mallows model, a distribution over permutations, to identify a canonical ordering for sections.1 Other forms of general rhetorical analysis include Rhetorical Structure Theory (Mann and Thompson, 1988; Marcu, 2000), which captures local discourse relations between segments of text; RST provides a layer of analysis that is separate and complementary to more global schemes such as argumentative zoning. 3 Intuitions The performance of unsupervised learning depends on how intuitions about the task are incorporated in the statistical model. Our approach relies on three main intuitions: Sentence similarity: All else being equal, we expect that lexically similar sentences will have similar purposes. At the same time, lexical similarity alone is not sufficient to capture shared argumentative function: all sente"
C14-1002,P06-1003,0,0.0328689,"Missing"
C14-1002,C12-2097,0,0.12266,"Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012). Varga et al. use a topic model variant called Z ONE -LDA that assigns each sentence a latent variable index or “topic” and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone. Such a model will have the effect of clustering sentences that share lexical items. Varga et al. also propose a model they call Z ONE -LDA- B, in which some common words are assigned to a “background” distribution that is independent of the sentence category; this model performs worse tha"
C14-1002,D10-1114,0,0.0231708,"a sentence is generated either from an LDA-style topic model or from a distribution associated with the rhetorical category assigned to the sentence. The former captures the subject matter of the document; the latter captures conventional language that is independent of the document’s subject matter. The sentence categories are generated from a first-order Markov model. The assignment of responsibility for a word is implemented through a so-called “switching variable”, a binary-valued latent variable. This is a commonly used mechanism for interpolating language models (Griffiths et al., 2004; Reisinger and Mooney, 2010; Ahmed and Xing, 2010); in many cases, the goal is to assign common words to a “background” distribution that is not considered an object of interest from a topic modelling perspective. In our case it is this non-topical part of the text that is the object of interest. The dependencies between variables in our full B OILERPLATE -LDA model are shown by the plate diagram in Figure 1. The corresponding “generative story” is as follows: 1 It would be interesting to swap in Chen et al.’s generalised Mallows model for the HMM-style ordering model in B OILERPLATE -LDA. The former has the advantage o"
C14-1002,N10-1020,0,0.020609,"al. (2003) introduced Latent Dirichlet Allocation (LDA) as a model of thematic structure in documents, but subsequent work has adapted the general framework to many different purposes in modelling text as well as other kinds of data. This includes research on modelling aspects of document structure such as topic segmentation, implementing the intuitions that neighbouring blocks of text are coherent in the sense of lexical similarity (Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsuper"
C14-1002,P09-1024,0,0.0122628,"scientific writing, yet there are many other textual genres where argumentation is conventionalised. For example, Burstein et al. (2003) identify building blocks analogous to AZ zones in the writing of English language learners and demonstrate that a supervised classification approach can be used to mark up their essays. Also in the educational domain, Madnani et al. (2012) train a supervised classifier to detect the “shell” language that learners use to organise the high-level structure of their compositions; this is quite close to our idea of “templates” or “recipes” for scientific papers. Sauper and Barzilay (2009) and Chen et al. (2009) both present models that learn structural conventions in Wikipedia articles without relying on human annotation. Sauper and Barzilay’s model induces the typical section structure of Wikipedia articles 3 about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section for a target entity, before performing multidocument summarisation to produce a new entry for posting to Wikipedia. Chen et al. take a Bayesian segmentation approach to implicitly learn the topical section structure of articles and use a generalised Mallows model,"
C14-1002,N07-1040,1,0.821611,"ver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations. Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning. Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training. Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012). Varga et al. use a topic model variant called Z ONE -LDA that assigns each sentence a latent variable index or “topic” and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone. Such a model will have the effect of clustering"
C14-1002,J02-4002,1,0.829868,") describes how a scientific article can be analysed in terms of text blocks (or zones) that share a rhetorical function (Teufel, 2010). For example: part of the article may consist of background information, another part may describe the aim of the research, other parts may report the authors’ own work or compare that work to alternative approaches in the literature. Supervised computational systems can be trained to mark up the AZ structure of a text automatically (see Section 2); the output of such systems has been shown to aid summarisation and human browsing of the scientific literature (Teufel and Moens, 2002; Guo et al., 2011a; Contractor et al., 2012). However, supervised systems require manually annotated training data that must be created anew for each discipline (and language) before they can be deployed, while large quantities of unannotated text are often available. For this reason, there is considerable value in developing unsupervised systems that induce aspects of rhetorical structure from unannotated text. In this paper we advance a hypothesis about the generality of rhetorical language. We propose that the words and linguistic constructs used to express rhetorical function in a scienti"
C14-1002,varga-etal-2012-unsupervised,0,0.148931,"Missing"
C16-1055,J05-3002,0,0.0606215,"therefore mainly concerns compressing individual sentences or merging sentences of similar content. Sentence compression using machine learning and/or constraint-solving methods is an active area of research. Common methods use syntactic, lexical and discourse-based features to determine which words should be dropped or paraphrased (Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2007, 2008; Yoshikawa et al., 2012). More recently, neural language model has also been applied to the problem (Rush et al., 2015). For multi-document summarisation, sentence fusion (Barzilay and McKeown, 2005) allows the synthesis of common information across documents in one sentence, using grammatical dependencies as a substitute for a semantic representation. These methods have the ability to produce high-quality output, but their sentence generation is a local step in the summarisation pipeline, i.e. isolated from the content selection, which is essentially global. The only exceptions we know are Martins and Smith’s (2009) system, and Nishikawa’s (2014) system for Japanese text, both of which optimise sentence selection jointly with sentence compression. A different paradigm for abstractive sum"
C16-1055,D07-1008,0,0.398307,"till uncommon in current systems, mostly due to the problems with NLP text analysis and knowledge representation that would plague any “deeper” method. Current research into abstractive summarisation therefore mainly concerns compressing individual sentences or merging sentences of similar content. Sentence compression using machine learning and/or constraint-solving methods is an active area of research. Common methods use syntactic, lexical and discourse-based features to determine which words should be dropped or paraphrased (Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2007, 2008; Yoshikawa et al., 2012). More recently, neural language model has also been applied to the problem (Rush et al., 2015). For multi-document summarisation, sentence fusion (Barzilay and McKeown, 2005) allows the synthesis of common information across documents in one sentence, using grammatical dependencies as a substitute for a semantic representation. These methods have the ability to produce high-quality output, but their sentence generation is a local step in the summarisation pipeline, i.e. isolated from the content selection, which is essentially global. The only exceptions we know"
C16-1055,C08-1018,0,0.0428613,"Missing"
C16-1055,L16-1197,1,0.824745,"Missing"
C16-1055,E09-1001,0,0.422972,"syntactic parse with the Stanford Parser (Klein and Manning, 2003), in an attempt to create more meaningful semantic units similar to KvD’s original concept. This close-to-surface representation allowed us ease of operation and robustness, but the grammatical relations do not contain enough semantic information to regenerate from them. During our research, we decided to experiment with the ACE processor1 , allowing parsing and generation from Minimal Recursion Semantics (MRS) representations (Copestake et al., 2005) and related formalisms such as Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009). For sentences containing summary propositions, our generation module aligns Stanford grammatical dependencies with their corresponding DMRS components (called Elementary Predicates or EPs). New text is then generated by selecting the minimal set of EPs such that it fulfils two conditions: 1. it covers all the information in the desired proposition from our summariser and 2. it contains all the EPs necessary for successful generation with ACE. The second condition ensures grammaticality. How should one assess the quality of a summary sentence that minimally expresses the information in a prop"
C16-1055,E14-1077,1,0.931391,"and then compresses the summary sentences with two competitive sentence compression methods (Clarke and Lapata, 2008; Cohn and Lapata, 2007). This gives the extraction–compression route a fair chance because our system as a sentence extractor was found to be the best system evaluated in our previous work (Fang and Teufel, 2016). As a further comparison system, we additionally use the next best “generation” algorithm that can express the information in a proposition: one that simply extracts all words that gave rise to the proposition (cf. section 4.2). This is the output mechanism we used in (Fang and Teufel, 2014). Our results show that our system produces sentences of a good textual quality, and that the overall content selection (as measured in ROUGE) rivals current sentence-selection, while achieving much stronger compression than traditional sentence compressors do. We see two possible reasons why leaving out the middle-man of sentence selection is advantageous in abstractive summarisation. General sentence compression systems are trained to recognise material that tends to be dropped in a “general case”. But without information about global discourse effects in the overall text, such a compressor"
C16-1055,P16-2078,1,0.89297,"memory. While the text is processed incrementally, new propositions are attached to the tree This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 567 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 567–578, Osaka, Japan, December 11-17 2016. by argument overlap, and old propositions are pruned according to simulated memory limitations. The final summary is based on the best-connected propositions. We have presented in previous work (Fang and Teufel, 2016) an implementation of KvD’s core ideas. However, it did not have the ability to generate new text from the propositions, and we were thus forced to use sentence extraction for the creation of a compromise summary output. We were nevertheless able to show that the content selection of our summariser outperforms state-of-the-art extractive summarisers. The contribution in the current paper is the combination of our summariser with an existing NL generator that can verbalise the summary propositions. The propositions we use are created by aggregating grammatical dependencies gained from a syntact"
C16-1055,flickinger-etal-2014-towards,0,0.0616495,"Missing"
C16-1055,W11-1608,0,0.408215,"fic domains. (These operations could in principle be performed if NLP technologies such as robust inference and entailment recognition were in place.) Instead it defines the summary-worthiness of a proposition in the simplest possible way, as the number of cycles during which it is retained in memory. 2. Rather than using “concepts” as arguments in the propositions (which would require human intelligence), we create propositions based only on a syntactic parse, and only containing textual tokens, not intelligent “concepts”. Our propositions are comparable to the subject-verb-object triples in Genest and Lapalme (2011), but we allow more kinds of predication such as adjectival and prepositional modifications to be propositions in their own right, which means these pieces of information can be selected independently. For example, “The discovery of the element revolutionised fire-lighting” gives rise to the propositions revolutionised (the discovery, fire-lighting) and of (the discovery, the element). We use coreference resolution and models of semantic relatedness (such as lexical chains and cosine similarity in a vector space) to determine the overlap between these arguments. Starting from a list of summary"
C16-1055,P03-1054,0,0.0225855,"eas. However, it did not have the ability to generate new text from the propositions, and we were thus forced to use sentence extraction for the creation of a compromise summary output. We were nevertheless able to show that the content selection of our summariser outperforms state-of-the-art extractive summarisers. The contribution in the current paper is the combination of our summariser with an existing NL generator that can verbalise the summary propositions. The propositions we use are created by aggregating grammatical dependencies gained from a syntactic parse with the Stanford Parser (Klein and Manning, 2003), in an attempt to create more meaningful semantic units similar to KvD’s original concept. This close-to-surface representation allowed us ease of operation and robustness, but the grammatical relations do not contain enough semantic information to regenerate from them. During our research, we decided to experiment with the ACE processor1 , allowing parsing and generation from Minimal Recursion Semantics (MRS) representations (Copestake et al., 2005) and related formalisms such as Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009). For sentences containing summary propositions, o"
C16-1055,N03-1017,0,0.00680922,"tion method for sentence compression.11 It is a variation of the Noisy Channel model for sentence compression first introduced by Knight and Marcu (2000). Instead of the Broadcast News Corpus they used, we trained the tree rewriting model on the full Written News Corpus, which only became available later (Clarke and Lapata, 2008). Following them, we aligned the words between the original and the compressed sentences using GIZA++ (Och et al., 1999), with additional pairs added where each word in the lexicon is aligned with itself, and symmetrised the alignment using the intersection heuristic (Koehn et al., 2003). We used the Stanford PCFG Parser (Klein and Manning, 2003) to obtain the sentence parses, and the above-mentioned BNC language model for training and decoding. Both compressors were run on all input sentences.12 The sentence extractor (OurExt or LexRank) computes the summary-worthiness of the uncompressed sentences, and then the corresponding compressed sentences are output. 4.2 Textual quality evaluation We performed a human experiment to evaluate the textual quality of the sentences generated by our new system OurAbs, in comparison to OurTok. This evaluation is performed on individual sent"
C16-1055,W04-1013,0,0.0467182,"tion is a property that also needs to be evaluated carefully whenever sentential material is manipulated. It is easy to accidentally introduce effects that would distort the meaning of the new sentence, be it by inappropriate referring expressions, by dropping restrictive relative clauses and thus changing the truthconditional conditions of the sentence, or by many other subtle unintentional changes. In our evaluation, we ask humans to interpret the shortened sentence together with its original context in order to detect possible truth distortions. Our second evaluation uses the ROUGE metric (Lin, 2004) to evaluate the overall content selection of our abstractive end-to-end method. We cannot directly compare our output to that of an isolated sentence compressor; we first need to select the sentences. We therefore build a pipeline system that uses our summariser as the sentence extractor, and then compresses the summary sentences with two competitive sentence compression methods (Clarke and Lapata, 2008; Cohn and Lapata, 2007). This gives the extraction–compression route a fair chance because our system as a sentence extractor was found to be the best system evaluated in our previous work (Fa"
C16-1055,W09-1801,0,0.0515371,"Missing"
C16-1055,E06-1038,0,0.0122257,"end-to-end abstractive summarisation is still uncommon in current systems, mostly due to the problems with NLP text analysis and knowledge representation that would plague any “deeper” method. Current research into abstractive summarisation therefore mainly concerns compressing individual sentences or merging sentences of similar content. Sentence compression using machine learning and/or constraint-solving methods is an active area of research. Common methods use syntactic, lexical and discourse-based features to determine which words should be dropped or paraphrased (Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2007, 2008; Yoshikawa et al., 2012). More recently, neural language model has also been applied to the problem (Rush et al., 2015). For multi-document summarisation, sentence fusion (Barzilay and McKeown, 2005) allows the synthesis of common information across documents in one sentence, using grammatical dependencies as a substitute for a semantic representation. These methods have the ability to produce high-quality output, but their sentence generation is a local step in the summarisation pipeline, i.e. isolated from the content selection, which is"
C16-1055,C14-1156,0,0.06046,"Missing"
C16-1055,W99-0604,0,0.0751919,"d British National Corpus (BNC) with interpolated Kneser-Ney smoothing and the “unknown word” token. Cohn and Lapata (2007) present a supervised tree-to-tree transduction method for sentence compression.11 It is a variation of the Noisy Channel model for sentence compression first introduced by Knight and Marcu (2000). Instead of the Broadcast News Corpus they used, we trained the tree rewriting model on the full Written News Corpus, which only became available later (Clarke and Lapata, 2008). Following them, we aligned the words between the original and the compressed sentences using GIZA++ (Och et al., 1999), with additional pairs added where each word in the lexicon is aligned with itself, and symmetrised the alignment using the intersection heuristic (Koehn et al., 2003). We used the Stanford PCFG Parser (Klein and Manning, 2003) to obtain the sentence parses, and the above-mentioned BNC language model for training and decoding. Both compressors were run on all input sentences.12 The sentence extractor (OurExt or LexRank) computes the summary-worthiness of the uncompressed sentences, and then the corresponding compressed sentences are output. 4.2 Textual quality evaluation We performed a human"
C16-1055,D15-1044,0,0.0496576,"e any “deeper” method. Current research into abstractive summarisation therefore mainly concerns compressing individual sentences or merging sentences of similar content. Sentence compression using machine learning and/or constraint-solving methods is an active area of research. Common methods use syntactic, lexical and discourse-based features to determine which words should be dropped or paraphrased (Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2007, 2008; Yoshikawa et al., 2012). More recently, neural language model has also been applied to the problem (Rush et al., 2015). For multi-document summarisation, sentence fusion (Barzilay and McKeown, 2005) allows the synthesis of common information across documents in one sentence, using grammatical dependencies as a substitute for a semantic representation. These methods have the ability to produce high-quality output, but their sentence generation is a local step in the summarisation pipeline, i.e. isolated from the content selection, which is essentially global. The only exceptions we know are Martins and Smith’s (2009) system, and Nishikawa’s (2014) system for Japanese text, both of which optimise sentence selec"
C16-1055,P12-2068,0,0.0167591,"ems, mostly due to the problems with NLP text analysis and knowledge representation that would plague any “deeper” method. Current research into abstractive summarisation therefore mainly concerns compressing individual sentences or merging sentences of similar content. Sentence compression using machine learning and/or constraint-solving methods is an active area of research. Common methods use syntactic, lexical and discourse-based features to determine which words should be dropped or paraphrased (Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2007, 2008; Yoshikawa et al., 2012). More recently, neural language model has also been applied to the problem (Rush et al., 2015). For multi-document summarisation, sentence fusion (Barzilay and McKeown, 2005) allows the synthesis of common information across documents in one sentence, using grammatical dependencies as a substitute for a semantic representation. These methods have the ability to produce high-quality output, but their sentence generation is a local step in the summarisation pipeline, i.e. isolated from the content selection, which is essentially global. The only exceptions we know are Martins and Smith’s (2009)"
C16-1221,P14-5010,0,0.00336896,"ormance can be isolated – models employing formulae indexing and matching are not considered. 4.1 Computing a Type Embedding Space We use our dictionary of types (section 3.1) to construct a type embedding space – a word embedding space that includes embeddings for types as atomic lexical units. The type embedding space is used to assign meaning (or denotation) to types in the form of vector embeddings and to expand queries with new types (section 4.3.1). The type embedding space is constructed as follows. First, we apply sentence tokenisation over the MREC using the Stanford CoreNLP toolkit (Manning et al., 2014). Subsequently, we apply longest sequence matching on the words of each sentence and identify all instances of types in the the corpus. Once detected, word sequences belonging to types are replaced by a single token (a concatenation of the constituent words of the type). As we are interested in modelling the relatedness of meaningful linguistic tokens, rather than mathematical artefacts, we replace MathML blocks representing formulae in the sentence by a single token (“@@@”). Finally, the re-written sentences are passed on to word2vec in skipgram mode (Mikolov et al., 2013a; Mikolov et al., 20"
C16-1221,P15-2055,1,0.382416,"aced by a single token (a concatenation of the constituent words of the type). As we are interested in modelling the relatedness of meaningful linguistic tokens, rather than mathematical artefacts, we replace MathML blocks representing formulae in the sentence by a single token (“@@@”). Finally, the re-written sentences are passed on to word2vec in skipgram mode (Mikolov et al., 2013a; Mikolov et al., 2013b) with negative sampling and window size=10 to produce the type embedding space. 4.2 Test Collection Evaluation is carried out using the Cambridge University MathIR Test Collection (CUMTC) (Stathopoulos and Teufel, 2015), which is composed of 120 real-life MIR topics procured from the MathOverflow (MO) on-line community. As illustrated in Table 1, each MO thread in the CUMTC is sentence-tokenized, with sentences either being part of the “prelude” (introduction to the mathematical subject of interest) or part of a concrete sub-question. We produced 160 queries from the CUMTC by emitting one query per sub-question in the collection. The body of each query is obtained by concatenating the text of the sub-question to the text of the associated prelude. Prelude SQ-1 Let P be a parabolic subgroup of GL(n) with Levi"
D09-1155,J08-4004,0,0.0294529,"Missing"
D09-1155,J02-4002,1,0.838057,"an expert, a semi-expert, and a non-expert) is acceptable overall vindicate our task definition as domain-knowledge free (using the tools of justification and domain-specific generic knowledge). However, the agreements involving the semi-expert are higher than the agreement between expert and non-expert. This probably means that the chemistry generics were not fully adequate to ensure that the non-expert understood enough of the chemistry to achieve the highest-possible agreement. 1500 The automation of AZ-annotation is underway. This requires adaptation of the high-level features used in AZ (Teufel and Moens, 2002) to chemistry. We are also preparing an annotation experiment with naive annotators. Another research avenue is the expansion of the guidelines to other disciplines such as bio-medicine, and to longer journal articles, e.g., in computational linguistics. OWN FAIL Initial attempts to improve the dehydration of 4 via chemical or thermal means were unsuccessful; similarly, attempts to couple the chlorosilane (Me3Si)2 (Me2ClSi)CH with Ag2O failed. (b510692c) OWN FAIL When the ABL algorithms try to learn with two completely distinct sentences, nothing can be learned. (0104006) OWN R ES While the ac"
D09-1155,E99-1015,1,0.914116,"r so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics. 1 Introduction Teufel et al. (1999) define the task of Argumentative Zoning (AZ) as a sentence-by-sentence classification with mutually exclusive categories from the annotation scheme given in Fig. 1. The reasoning behind the categories is inspired by the notion of a knowledge claim (Myers, 1992; Luukkonen, 1992): the act of writing a paper corresponds to an attempt of claiming ownership for a new piece of knowledge, which is to be integrated into the repository of scientific knowledge in the authors’ field by the process of peer review and publication. In the cause of this process, the authors have to convince the reviewers th"
D09-1155,W06-1613,1,0.871442,"@cl.cam.ac.uk batchelorc@rsc.org Abstract divide the paper into zones, OTHER , OWN and BACKGROUND. These are defined on the basis of who owns the knowledge claim in the corresponding segment. There are also two categories which are defined by their relationship to existing work, BASIS and C ONTRAST . That means that parts of the AZ scheme are similar to citation function classification schemes from the area of citation content analysis (Garfield, 1965; Weinstock, 1971; Spiegel-R¨using, 1977), and to automatic citation function classification (Nanba and Okumura, 1999; Garzone and Mercer, 2000; Teufel et al., 2006). The remaining categories, A IM and T EXTUAL , fulfil different rhetorical functions for the presentation of the paper. A IM points out the paper’s main knowledge claim, a rhetorical move which may be repeated in the conclusion and the introduction. T EXTUAL explains the physical location of information, e.g., by giving a section overview or presenting a summary of a subsection. On the basis of human-annotated training material, AZ can be automatically classified using supervised machine learning. Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scie"
D09-1155,mizuta-collier-2004-annotation,0,0.0640423,"must have a way of distinguishing, for instance, between methods and results. Note that several of the applications based on AZ and AZ-II in general rely on the rare categories much more than they rely on the more frequent categories. OWN FAIL is an example of a rare but important category, and so is A IM , which is central to summarisation applications. The comparative and contrastive categories C O D I A NTISUPP and G AP W EAK , on the other hand, are particularly useful to citation-based search applications. Other AZ-like schemes for scientific discourse created for the biomedical domain (Mizuta and Collier, 2004) and for computer science (Feltrim et al., 2005) also made the decision to subdivide OWN , in similar ways to how we propose here. The current work, however, is the first experimental proof that humans can make this distinction – and the others encoded in AZ-II – reliably, and in two quite distinct disciplines. 3 Discipline-Independent Non-Expert Annotation An important principle of AZ is that its categories can be decided without domain knowledge. This rule is anchored in the guidelines: when choosing a category, no reasoning about the scientific facts is allowed. The avoidance of domain-know"
D09-1155,J96-2004,0,\N,Missing
D16-1259,P15-2137,1,0.810676,"le denoting whether the corresponding event ei has been selected. bij is a variable which is 1 if and only if both events i and j have been selected. pen(ei , ej ) is a penalty function that is 1 if the two events ei and ej have the same date, otherwise 0. Each event was linked to the preceding temporal expression identified by HeidelTime; this heuristic was found to work well. The last constraint ensures that not more than Lmax events are chosen, where Lmax is the desired timeline length for the article considered. 4 Evaluation For evaluating our algorithms, the methodology we introduced in (Bauer and Teufel, 2015) is used, along with the accompanying Cambridge SingleDocument Timeline Corpus (CSDTC, version 2.0), which has been made publicly available1 . 4.1 Cambridge Single-Document Timeline Corpus The CSDTC contains 10 articles from 3 subject areas: GPE (geo-political entities such as countries and cities), FIELD OF SCIENCE and INVENTION. To tune our algorithms, we constructed a development set of a further 30 annotated history articles from the subject areas in the CSDTC and one additional subject area (FOOD/DRINK). Due to the high annotation cost, only a single timeline creator was used. Important e"
D16-1259,J07-4004,0,0.0358708,"e a date attached to them, as authors tend to give the reader this information if it is available. This is true for all historical periods from prehistory onwards, since for most events at least an approximate date is known. We considered two alternatives: The simplest approach is to only use sentences that contain a date, regardless of where in the sentence the date is located. A more sophisticated alternative verifies that the date is syntactically attached to the event, such as in “Richelieu died in 1642”. To identify such cases, we constructed a parse tree using the C&C dependency parser (Clark and Curran, 2007) and only considered a TimeML event to be “dated” if it is at most two outgoing dependencies away from a temporal expression. We used HeidelTime (Str¨otgen and Gertz, 2013), a the state-of-theart temporal expression software package, to identify such temporal expressions. 3.2 Lexical cues The key component we use to judge the importance of any event are lexical cues about the input text’s subject area. Examples of such subject areas include INVENTION and FOOD/DRINK. The subject area of a text should give us prior knowledge about which types of events are likely to be important. For instance, w"
D16-1259,P12-1010,0,0.0211676,"ext and anchoring them in time. The creation of the TimeML specification language (Pustejovsky et al., 2003) laid the foundations for the TempEval series of shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), in which systems had to identify TimeML events and temporal expressions in free-form text. Further subtasks included the normalisation of temporal expressions and the creation of links between events and temporal expressions. A further shared task investigated the use of TimeML annotation for the downstream task of question answering (Llorens et al., 2015). Kolomiyets et al. (2012) created a connected timeline for a text based on TimeML annotations; a dependency parser infers dependency structures between events. Finally, a recent SemEval task (Minard et al., 2015) explored the related problem of cross-document event ordering. Here, relevant events and temporal expressions concerning a single target entity of interest have to be identified in more than one input document. Chasin et al. (2014) try to identify important events in single texts, but their approach is limited to articles on wars and battles, and the problem is not approached as a summarisation task. Their me"
D16-1259,S15-2134,0,0.0132271,"s from a single input text and anchoring them in time. The creation of the TimeML specification language (Pustejovsky et al., 2003) laid the foundations for the TempEval series of shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), in which systems had to identify TimeML events and temporal expressions in free-form text. Further subtasks included the normalisation of temporal expressions and the creation of links between events and temporal expressions. A further shared task investigated the use of TimeML annotation for the downstream task of question answering (Llorens et al., 2015). Kolomiyets et al. (2012) created a connected timeline for a text based on TimeML annotations; a dependency parser infers dependency structures between events. Finally, a recent SemEval task (Minard et al., 2015) explored the related problem of cross-document event ordering. Here, relevant events and temporal expressions concerning a single target entity of interest have to be identified in more than one input document. Chasin et al. (2014) try to identify important events in single texts, but their approach is limited to articles on wars and battles, and the problem is not approached as a su"
D16-1259,W04-3252,0,0.0100549,"events. Finally, a recent SemEval task (Minard et al., 2015) explored the related problem of cross-document event ordering. Here, relevant events and temporal expressions concerning a single target entity of interest have to be identified in more than one input document. Chasin et al. (2014) try to identify important events in single texts, but their approach is limited to articles on wars and battles, and the problem is not approached as a summarisation task. Their method is lightly supervised, using features such as the presence of negation or past tense verbs in the sentence, and TextRank (Mihalcea and Tarau, 2004) for identifying salient sentences. We use an improved version of this system as a baseline. 3 Overall approach Our problem is that of finding an optimal sequence of events (of a given maximum length) in a given input article. We follow the literature on event extraction and use TimeML events (Pustejovsky et al., 2003). Most TimeML events are verbs, but some are nominalisations such as “invasion” or other event-like words such as “war”. The use of TimeML events, aside from the practical advantage that commonly-available event extraction algo2344 rithms exist, allows us to evaluate content sele"
D16-1259,N04-1019,0,0.0336438,"d on abstract (“deep”) meaning units called Historical Content Units (HCUs). HCUs were derived on the basis of human-created timelines. Between 32 and 80 HCUs per article were annotated for the articles in the CSDTC. Each HCU is weighted by the number of timeline creators who expressed its semantic content in their timelines. Because HCUs are linked to TimeML events in the surface text, it is possible to perform automatic deep evaluation without requiring any manual annotation of system summaries. Algorithms are evaluated on a given input article using an adapted version of the pyramid score (Nenkova and Passonneau, 2004), which is calculated as the ratio between the sum of all rewards for HCUs chosen by the algorithm normalised by the maximum possible score scoremax : score = P h∈HCU s wh ·Cov(h,E,T ) scoremax where wh is the weight of HCU h (a number between 1 and the number of annotators), E is the set of events in the article, T are the events in the system timeline, and the coverage score Cov(h, E, T ) is a number between 0 and 1 that indicates to what extent the events chosen by the algorithm jointly express the semantic content of HCU h. The basic version of Cov(h, E, T ) is defined as follows: P Cov(h,"
D16-1259,P15-1154,0,0.047252,"Missing"
D16-1259,S13-2001,0,0.034313,", 2016. 2016 Association for Computational Linguistics Allan et al., 2001). This task definition allows the exploitation of features such as document creation times and headlines. The most important feature is redundancy between articles, which facilitates the identification of salient events. A second important strand of work focuses on extracting all events from a single input text and anchoring them in time. The creation of the TimeML specification language (Pustejovsky et al., 2003) laid the foundations for the TempEval series of shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), in which systems had to identify TimeML events and temporal expressions in free-form text. Further subtasks included the normalisation of temporal expressions and the creation of links between events and temporal expressions. A further shared task investigated the use of TimeML annotation for the downstream task of question answering (Llorens et al., 2015). Kolomiyets et al. (2012) created a connected timeline for a text based on TimeML annotations; a dependency parser infers dependency structures between events. Finally, a recent SemEval task (Minard et al., 2015) explored the related probl"
D16-1259,S07-1014,0,0.0486983,"pages 2343–2349, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Allan et al., 2001). This task definition allows the exploitation of features such as document creation times and headlines. The most important feature is redundancy between articles, which facilitates the identification of salient events. A second important strand of work focuses on extracting all events from a single input text and anchoring them in time. The creation of the TimeML specification language (Pustejovsky et al., 2003) laid the foundations for the TempEval series of shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), in which systems had to identify TimeML events and temporal expressions in free-form text. Further subtasks included the normalisation of temporal expressions and the creation of links between events and temporal expressions. A further shared task investigated the use of TimeML annotation for the downstream task of question answering (Llorens et al., 2015). Kolomiyets et al. (2012) created a connected timeline for a text based on TimeML annotations; a dependency parser infers dependency structures between events. Finally, a recent SemEval task (M"
D19-1530,Q14-1029,0,0.125379,"Missing"
D19-1530,W19-3621,1,0.676916,"nd two sets of attribute words A and B. We compute Cohen’s d (a measure of the difference in relative similarity of the word sets within each embedding; higher is more biased), and a one-sided p-value which indicates whether the bias detected by WEAT within each embedding is significant (the best outcome being that no such bias is detectable). We do this for three tests proposed by Nosek et al. (2002) which measure the strength of various gender stereotypes: art–maths, arts–sciences, and careers–family.10 Indirect bias To demonstrate indirect gender bias we adapt a pair of methods proposed by Gonen and Goldberg (2019). First, we test whether the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do this, we define a new subspace, ~btest , using the 23 word pairs used in the Google Analogy family test subset (Mikolov et al., 2013) following Bolukbasi et al.’s (2016) method, and determine 10 In the careers–family test the gender dimension is expressed by female and male first names, unlike in the other sets, where pronouns and typical gendered words are used. the 1000 most biased words in each corpus (the 500 words most similar to ~btest and ~btest ) in the unmitigated"
D19-1530,J15-4004,0,0.0791268,"llowing bias mitigation. Second, we test whether a classifier can be trained to reclassify the gender of debiased words. If it succeeds, this would indicate that biasinformation still remains in the embedding. We trained an RBF-kernel SVM classifier on a random sample of 1000 out of the 5000 most biased words from each corpus using ~btest (500 from each gender), then report the classifier’s accuracy when reclassifying the remaining 4000 words. Word similarity The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset (Hill et al., 2015) provides a ground-truth measure of similarity produced by 500 native English speakers.11 Similarity scores in an embedding are computed as the cosine angle between wordvector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at ↵ = 0.01. Sentiment classification Following Le and Mikolov (2014), we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input (Lau and Baldwin, 2016) to Doc2Vec on the Stanford Large Movie Re"
D19-1530,W16-1609,0,0.0222116,"s of word similarity. The SimLex-999 dataset (Hill et al., 2015) provides a ground-truth measure of similarity produced by 500 native English speakers.11 Similarity scores in an embedding are computed as the cosine angle between wordvector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at ↵ = 0.01. Sentiment classification Following Le and Mikolov (2014), we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input (Lau and Baldwin, 2016) to Doc2Vec on the Stanford Large Movie Review dataset. The classification is performed by an SVM classifier using the document embeddings as features, trained on 40,000 labelled reviews and tested on the remaining 10,000 documents, reported as error percentage. Non-biased gender analogies When proposing WED, Bolukbasi et al. (2016) use human raters to class gender-analogies as either biased (woman:housewife :: man:shopkeeper) or appropriate (woman:grandmother :: man::grandfather), and postulate that whilst biased analogies are undesirable, appropriate ones should remain. Our new analogy test"
D19-1530,W12-3018,0,0.0215335,"Missing"
D19-1530,D07-1043,0,0.060778,"the Google Analogy family test subset (Mikolov et al., 2013) following Bolukbasi et al.’s (2016) method, and determine 10 In the careers–family test the gender dimension is expressed by female and male first names, unlike in the other sets, where pronouns and typical gendered words are used. the 1000 most biased words in each corpus (the 500 words most similar to ~btest and ~btest ) in the unmitigated embedding. For each debiased embedding we then project these words into 2D space with tSNE (van der Maaten and Hinton, 2008), compute clusters with k-means, and calculate the clusters’ Vmeasure (Rosenberg and Hirschberg, 2007). Low values of cluster purity indicate that biased words are less clustered following bias mitigation. Second, we test whether a classifier can be trained to reclassify the gender of debiased words. If it succeeds, this would indicate that biasinformation still remains in the embedding. We trained an RBF-kernel SVM classifier on a random sample of 1000 out of the 5000 most biased words from each corpus using ~btest (500 from each gender), then report the classifier’s accuracy when reclassifying the remaining 4000 words. Word similarity The quality of a space is traditionally measured by how w"
D19-1530,N18-2002,0,0.208296,"Missing"
D19-1530,N18-2003,0,0.175913,"Missing"
E14-1053,C94-2167,0,0.13103,"Missing"
E14-1053,bird-etal-2008-acl,0,0.0190735,"haviour. Strohman et al. (2007) perform RLR with the paper text as a query to their recommendation system, using text similarity, citation counts, citation coupling, author information, and the citation graph. Their model achieves a mean-average precision of 0.102 against a corpus from the Rexa10 database. Bethard and Jurafsky (2010) improve on Strohman et al. by the use of a SVM with 19 features from 6 broad categories: similar terms; cited by others; recency; cited using similar terms; similar topics; and social habits. They achieve a MAP of 0.279 against the ACL Anthology Reference Corpus (Bird et al., 2008), with the following features performing best: publication age, citation counts, the terms in citation sentences, and the LDA topics of the citing documents. They also use (unchanged) PageRank authority counts as one of the features, but find that it provides little discriminative power to the SVM. A drawback of their method is the large amount of information that has to be provided to create their SVM features, and the expensive training routine, which is based on pairwise paper–paper comparisons in the corpus. Variations of the RLR tasks exist, which additionally determine the position in th"
E14-1053,D07-1109,0,0.0952127,"Missing"
E14-1053,S10-1055,0,0.0392395,"Missing"
E14-1053,W09-3607,0,0.0192338,"und 2,000, the number of citations ranging under 10,000, and the number of topics in their models ranging from eight to twenty. But LDA has been shown to scale to corpora of millions of terms (Newman et al., 2006), and PageRank to billions (Page et al., 1998) of documents. Our model, which advocates a pipelined approach, benefits from the fact that separate topic modelling is computationally tractable using LDA, and the fact that citation graph modelling is cheap using Personalised PageRank. 4 Evaluation 1: RLR We evaluate our authority-based search model using the 2010 ACL Anthology Network (Radev et al., 2009). We removed from it corrupted documents, i.e., those of less than 100 characters or containing only control characters. The ACL Anthology Network provides external meta-data about the articles, which was manually curated. We do not use this meta-data because we wanted to build as system that can be applied to any large collection of articles, where external meta-data would not normally exist. We therefore build an approximate citation graph from the paper text itself, as a one-off task when constructing the LDA space. We extract titles, dates and full-text from every article and perform a sea"
E14-1077,W97-0703,0,0.592328,"nge after reading one more sentence, by flipping the edge connecting #3 and #14. 3 2 14 4 14 3 2 16 4 16 17 17 18 The summariser presented here is a hybrid: its core algorithm is symbolic, but its limited powers of generalisation come from a semantic similarity metric that is defined via distributionally derived probabilities. Because its core processing is symbolic and based on a simple semantic representation, it is possible to derive an explanation based on the coherence tree and the propositions selected from it. There are some similarities to the idea of summarisation via lexical chains (Barzilay and Elhadad, 1997), as both methods trace concepts (as representatives of topics) across a document. The KvD model arguably uses more informative meaning units, as it is based on the combination of concepts within propositions, rather than on concept repetition alone. 18 Figure 5: Tree before and after a root change. 3 Related Work One of the dilemmas in summarisation research is how “deep”, i.e. semantics-oriented, a summariser should be. Shallow analysis of lexical similarity between sentences and/or the keywords contained in sentences has lead to summarisers that are robust and perform very well for most tex"
E14-1077,J08-1001,0,0.0507487,"attraction and repulsion of similar summary sentences (Carbonell and Goldstein, 1998). There are statistical models of sentence shortening (Knight and Marcu, 2002). While much work in summarisation has concentrated on multi-document summarisation, where the main challenge is the detection of redundant information, the summariser presented here is a single-document summariser. A different, related stream of research looked at the automatic detection of coherence in text. Graesser et al (2004) present a coherence checker based on over 200 coherence metrics, including argument overlap as in KvD. Barzilay and Lapata (2008) use a profiling of texts akin to Centering theory to rank texts according to their coherence. It would be interesting to combine their notion of entity-based coherence with KvD’s notion of argument overlap. 4 Experiments We now perform two experiments. The first tests the contribution of our concept matcher and root change strategy on a small document set we have collected, and compares against two research summarisers. In the second experiment, we test the performance of our summariser on a much larger and standard dataset. However, researchers have been attracted by deeper, more symbolic an"
E14-1077,P03-1054,0,0.00624508,"cept. Concept mapping is the task of forming equivalence classes of surface expressions; each concept then corresponds to one such equivalence class. The KvD model, because it simulates concept mapping and proposition attachment in parallel, conceals some of the choices that a fully automatic model has to make. meaningful arguments to ensure similar potential for overlap. To achieve suitable granularity of propositions, we aggregate information spread out over several grammatical dependencies, and exclude semantically empty words from participating in argument overlap. We use Stanford Parser (Klein and Manning, 2003), and aggregate subjects and complements of a predicate into a single proposition. Active and passive voices are unified; clauses are treated as embedded propositions; controlling subjects of open clausal complements are recovered. Given current technology, concept mapping can only be performed probabilistically. We use the Stanford coreference resolution, named-entity detection (to extend coreference detection to nonsame-head references, e.g. mapping “the tech giant” to “Apple Inc.”2 ); and to find synonymy or at least semantic relatedness, we use a wellknown measure of semantic similarity, n"
E14-1077,P84-1055,0,0.435918,"Experiments We now perform two experiments. The first tests the contribution of our concept matcher and root change strategy on a small document set we have collected, and compares against two research summarisers. In the second experiment, we test the performance of our summariser on a much larger and standard dataset. However, researchers have been attracted by deeper, more symbolic and thus more explanatory summarisation models that use semantic representations of some form (Radev and McKeown, 1998) and often rely on explicit discourse modelling (Lehnert, 1981; Kintsch and van Dijk, 1978; Cohen, 1984). The problem with templatebased summarisers is that they tend to be domaindependent; the problem with discourse structurebased summarisers is in general that they require knowledge modelling and reasoning far beyond the capability of today’s state of the art in artificial intelligence. Rhetorical Structure Theory (Mann and Thompson, 1987) provides a domainindependent framework that takes local discourse structure into account, which has lead to a successful prototype summariser (Marcu, 2000). This We will use the intrinsic evaluation strategy of comparison to a gold standard. Human judgements"
E14-1077,N03-1020,0,0.105796,"Missing"
E14-1077,P98-2127,0,0.0406652,"of a predicate into a single proposition. Active and passive voices are unified; clauses are treated as embedded propositions; controlling subjects of open clausal complements are recovered. Given current technology, concept mapping can only be performed probabilistically. We use the Stanford coreference resolution, named-entity detection (to extend coreference detection to nonsame-head references, e.g. mapping “the tech giant” to “Apple Inc.”2 ); and to find synonymy or at least semantic relatedness, we use a wellknown measure of semantic similarity, namely Lin’s Dependency-Based Thesaurus (Lin, 1998). We are not committed to this particular measure, but it empirically performed best out of the 11 we tried; especially it outperformed WordNet pathbased measures. Note however that only the 200 most similar words for each word are provided by this tool. The similarity measure is normalised by relative ranking to provide the probability that an expression refers to the same concept as another expression. We use WordNet (Miller, 1995) for derivationally related forms (to solve e.g. nominalisation). This establishes the first competition, the one between concept matches. Some predicates are not"
E14-1077,W03-0501,0,0.0396924,"concepts (as representatives of topics) across a document. The KvD model arguably uses more informative meaning units, as it is based on the combination of concepts within propositions, rather than on concept repetition alone. 18 Figure 5: Tree before and after a root change. 3 Related Work One of the dilemmas in summarisation research is how “deep”, i.e. semantics-oriented, a summariser should be. Shallow analysis of lexical similarity between sentences and/or the keywords contained in sentences has lead to summarisers that are robust and perform very well for most texts (Radev et al., 2004; Dorr and Zajic, 2003; Carbonell and Goldstein, 1998). The methods applied include a random-surfer model (Mihalcea and Tarau, 2004; Radev, 2004), a model of attraction and repulsion of similar summary sentences (Carbonell and Goldstein, 1998). There are statistical models of sentence shortening (Knight and Marcu, 2002). While much work in summarisation has concentrated on multi-document summarisation, where the main challenge is the detection of redundant information, the summariser presented here is a single-document summariser. A different, related stream of research looked at the automatic detection of coherenc"
E14-1077,W04-1013,0,0.0358381,"independent; the problem with discourse structurebased summarisers is in general that they require knowledge modelling and reasoning far beyond the capability of today’s state of the art in artificial intelligence. Rhetorical Structure Theory (Mann and Thompson, 1987) provides a domainindependent framework that takes local discourse structure into account, which has lead to a successful prototype summariser (Marcu, 2000). This We will use the intrinsic evaluation strategy of comparison to a gold standard. Human judgements would be the most credible, but as a cheap alternative, we use ROUGE-L (Lin, 2004), which has been shown to correlate well to human judgements. For each sentence, ROUGE-L treats it as a sequence of words, and finds the longest common subsequences (LCSs) with any sentence in a gold standard summary. The score is defined as the Fmeasure of the precision and recall of the LCSs. 737 Encounters between police and Black Panther members. Students to complain of harassment. Automobiles Panther Party signs glued to bumpers. Bloody encounters between police and Black Panther members punctuated the summer days of 1969. Students to complain of continuous harassment by law enforcement o"
E14-1077,J98-3005,0,0.0575525,". It would be interesting to combine their notion of entity-based coherence with KvD’s notion of argument overlap. 4 Experiments We now perform two experiments. The first tests the contribution of our concept matcher and root change strategy on a small document set we have collected, and compares against two research summarisers. In the second experiment, we test the performance of our summariser on a much larger and standard dataset. However, researchers have been attracted by deeper, more symbolic and thus more explanatory summarisation models that use semantic representations of some form (Radev and McKeown, 1998) and often rely on explicit discourse modelling (Lehnert, 1981; Kintsch and van Dijk, 1978; Cohen, 1984). The problem with templatebased summarisers is that they tend to be domaindependent; the problem with discourse structurebased summarisers is in general that they require knowledge modelling and reasoning far beyond the capability of today’s state of the art in artificial intelligence. Rhetorical Structure Theory (Mann and Thompson, 1987) provides a domainindependent framework that takes local discourse structure into account, which has lead to a successful prototype summariser (Marcu, 2000"
E14-1077,radev-etal-2004-mead,1,0.793726,"both methods trace concepts (as representatives of topics) across a document. The KvD model arguably uses more informative meaning units, as it is based on the combination of concepts within propositions, rather than on concept repetition alone. 18 Figure 5: Tree before and after a root change. 3 Related Work One of the dilemmas in summarisation research is how “deep”, i.e. semantics-oriented, a summariser should be. Shallow analysis of lexical similarity between sentences and/or the keywords contained in sentences has lead to summarisers that are robust and perform very well for most texts (Radev et al., 2004; Dorr and Zajic, 2003; Carbonell and Goldstein, 1998). The methods applied include a random-surfer model (Mihalcea and Tarau, 2004; Radev, 2004), a model of attraction and repulsion of similar summary sentences (Carbonell and Goldstein, 1998). There are statistical models of sentence shortening (Knight and Marcu, 2002). While much work in summarisation has concentrated on multi-document summarisation, where the main challenge is the detection of redundant information, the summariser presented here is a single-document summariser. A different, related stream of research looked at the automatic"
E14-1077,W04-3252,0,\N,Missing
E14-1077,C98-2122,0,\N,Missing
E14-3006,J08-1001,0,0.0445497,"ference resolver. Our results indicate that this integration, combined with domain-dependent training data, can outperform the performance of an out-of-the-box coreference resolver. For the (much harder) task of resolving associative anaphora, our preliminary results show the need for and the effect of semantic features. 1 (1) Xe-Ar was found to be in a layered structure with Ar on the surface1 . (2) We base our experiments on the Penn treebank. The corpus size is ... The resolution of associative links is important because it can help in tasks which use the concept of textual coherence, e.g. Barzilay and Lapata (2008)’s entity grid or Hearst (1994)’s text segmentation. They might also be of use in higherlevel text understanding tasks such as textual entailment (Mirkin et al., 2010) or summarisation based on argument overlap (Kintsch and van Dijk, 1978; Fang and Teufel, 2014). Gasperin (2009) showed that biological texts differ considerably from other text genres, such as news text or dialogue. In this respect, our results confirm that the proportion between non-referring and referring entities in scientific text differs from that reported for other genres. The same holds for the type and relative number of"
E14-3006,W11-0210,0,0.219461,"Missing"
E14-3006,N06-2015,0,0.0414906,"Navarro and Ananiadou, 2011; Cohen et al., 2010). Some work has been done for other disciplines, such as computational linguistics. Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-art coreference systems (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Chen and Ng, 2012) trained on news text and dialogue, as provided in the OntoNotes corpus (Hovy et al., 2006). Other state-of-the-art systems, such as Raghunathan et al. (2010) and Berkeley’s Coreference Resolution System (Durrett and Klein, 2013), also treat coreference as a task on news text and dialogue. We base our experiments on the IMS coreference resolver by Bj¨orkelund and Farkas (2012), one of the best publicly available English coreference systems. The resolver uses the decision of a cluster-based de46 Category Example C OREFERENCE LINKS G IVEN ( SPECIFIC ) G IVEN ( GENERIC ) A SSOCIATIVE LINKS A SSOCIATIVE Categories without links A SSOCIATIVE ( SELF - CONTAINING ) D ESCRIPTION U NUSED D E"
E14-3006,W12-4503,0,0.028898,"Missing"
E14-3006,W12-4504,0,0.0129651,"he research community and focuses mostly on the biomedical domain (Gasperin, 2009; Batista-Navarro and Ananiadou, 2011; Cohen et al., 2010). Some work has been done for other disciplines, such as computational linguistics. Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-art coreference systems (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Chen and Ng, 2012) trained on news text and dialogue, as provided in the OntoNotes corpus (Hovy et al., 2006). Other state-of-the-art systems, such as Raghunathan et al. (2010) and Berkeley’s Coreference Resolution System (Durrett and Klein, 2013), also treat coreference as a task on news text and dialogue. We base our experiments on the IMS coreference resolver by Bj¨orkelund and Farkas (2012), one of the best publicly available English coreference systems. The resolver uses the decision of a cluster-based de46 Category Example C OREFERENCE LINKS G IVEN ( SPECIFIC ) G IVEN ( GENERIC ) A SSOCIATIVE LINKS A SSOC"
E14-3006,P12-1084,0,0.445665,"gorisation frames and paths in the syntax tree as well as the semantic distance between the surface forms (e.g. edit distance). However, none of this work is concerned with associative anaphora. Hou et al. (2013) present a corpus of news text annotated with associative links that are not limited with respect to semantic relations between anaphor and antecedent. Their experiments focus on antecedent selection only, assuming that the recognition of associative entities has already been performed. Information status has been investigated extensively in different genres such as news text, e.g. in Markert et al. (2012). Poesio and Vieira (1998) performed an information status-based corpus study on news text, defining the following categories: coreferential, bridging, larger situation, unfamiliar and doubt. To the best of our knowledge, there is currently no study on information status in scientific text. In this paper, we propose a classification scheme for scientific text that is derived from Riester et al. (2010) and Poesio and Vieira (1998). We investigate the differences between news text and scientific text by analysing the distribution of information status categories. We hypothesise that the proporti"
E14-3006,P10-1123,0,0.0576351,"Missing"
E14-3006,D13-1203,0,0.0172927,"Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-art coreference systems (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Chen and Ng, 2012) trained on news text and dialogue, as provided in the OntoNotes corpus (Hovy et al., 2006). Other state-of-the-art systems, such as Raghunathan et al. (2010) and Berkeley’s Coreference Resolution System (Durrett and Klein, 2013), also treat coreference as a task on news text and dialogue. We base our experiments on the IMS coreference resolver by Bj¨orkelund and Farkas (2012), one of the best publicly available English coreference systems. The resolver uses the decision of a cluster-based de46 Category Example C OREFERENCE LINKS G IVEN ( SPECIFIC ) G IVEN ( GENERIC ) A SSOCIATIVE LINKS A SSOCIATIVE Categories without links A SSOCIATIVE ( SELF - CONTAINING ) D ESCRIPTION U NUSED D EICTIC P REDICATIVE I DIOM D OUBT We present the following experiment. It deals with ... We use the Jaccard similarity coefficient in our e"
E14-3006,P10-1142,0,0.093345,"omain adaptation for science and show how this improves an out-of-the-box coreference resolver, and (iv) experiments on the resolution of associative anaphora with a coreference resolver that is adapted to this new notion of “reference” by including semantic features. To the best of our knowledge, this is the first work on anaphora resolution in multi-discipline, full-text scientific papers that also deals with associative anaphora. 2 Related Work Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same real-world entities (Ng, 2010). Resolving anaphora in scientific text has only recently gained interest in the research community and focuses mostly on the biomedical domain (Gasperin, 2009; Batista-Navarro and Ananiadou, 2011; Cohen et al., 2010). Some work has been done for other disciplines, such as computational linguistics. Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-a"
E14-3006,E14-1077,1,0.798155,"s show the need for and the effect of semantic features. 1 (1) Xe-Ar was found to be in a layered structure with Ar on the surface1 . (2) We base our experiments on the Penn treebank. The corpus size is ... The resolution of associative links is important because it can help in tasks which use the concept of textual coherence, e.g. Barzilay and Lapata (2008)’s entity grid or Hearst (1994)’s text segmentation. They might also be of use in higherlevel text understanding tasks such as textual entailment (Mirkin et al., 2010) or summarisation based on argument overlap (Kintsch and van Dijk, 1978; Fang and Teufel, 2014). Gasperin (2009) showed that biological texts differ considerably from other text genres, such as news text or dialogue. In this respect, our results confirm that the proportion between non-referring and referring entities in scientific text differs from that reported for other genres. The same holds for the type and relative number of linguistic expressions used for reference. To address this issue, we decided to investigate information status (Nissim et al., 2004) of noun phrases. Information status tells us whether a noun phrase refers to an already Introduction Resolving anaphoric relatio"
E14-3006,nissim-etal-2004-annotation,0,0.418537,"tasks such as textual entailment (Mirkin et al., 2010) or summarisation based on argument overlap (Kintsch and van Dijk, 1978; Fang and Teufel, 2014). Gasperin (2009) showed that biological texts differ considerably from other text genres, such as news text or dialogue. In this respect, our results confirm that the proportion between non-referring and referring entities in scientific text differs from that reported for other genres. The same holds for the type and relative number of linguistic expressions used for reference. To address this issue, we decided to investigate information status (Nissim et al., 2004) of noun phrases. Information status tells us whether a noun phrase refers to an already Introduction Resolving anaphoric relations automatically requires annotated data for training and testing. Anaphora and coreference resolution systems have been tested and evaluated on different genres, mainly news articles and dialogue. However, for scientific text, annotated data are scarce and coreference resolution systems are lacking (Sch¨afer et al., 2012). We present a study of anaphora in scientific literature and show the difficulties that arise when resolving coreferent and associative entities i"
E14-3006,W12-4502,0,0.0383424,"Missing"
E14-3006,J98-2001,0,0.785711,"aths in the syntax tree as well as the semantic distance between the surface forms (e.g. edit distance). However, none of this work is concerned with associative anaphora. Hou et al. (2013) present a corpus of news text annotated with associative links that are not limited with respect to semantic relations between anaphor and antecedent. Their experiments focus on antecedent selection only, assuming that the recognition of associative entities has already been performed. Information status has been investigated extensively in different genres such as news text, e.g. in Markert et al. (2012). Poesio and Vieira (1998) performed an information status-based corpus study on news text, defining the following categories: coreferential, bridging, larger situation, unfamiliar and doubt. To the best of our knowledge, there is currently no study on information status in scientific text. In this paper, we propose a classification scheme for scientific text that is derived from Riester et al. (2010) and Poesio and Vieira (1998). We investigate the differences between news text and scientific text by analysing the distribution of information status categories. We hypothesise that the proportion of associative anaphora"
E14-3006,W12-4501,0,0.0223285,"ogue refer to the same real-world entities (Ng, 2010). Resolving anaphora in scientific text has only recently gained interest in the research community and focuses mostly on the biomedical domain (Gasperin, 2009; Batista-Navarro and Ananiadou, 2011; Cohen et al., 2010). Some work has been done for other disciplines, such as computational linguistics. Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-art coreference systems (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Chen and Ng, 2012) trained on news text and dialogue, as provided in the OntoNotes corpus (Hovy et al., 2006). Other state-of-the-art systems, such as Raghunathan et al. (2010) and Berkeley’s Coreference Resolution System (Durrett and Klein, 2013), also treat coreference as a task on news text and dialogue. We base our experiments on the IMS coreference resolver by Bj¨orkelund and Farkas (2012), one of the best publicly available English coreference systems. The resolver uses the decis"
E14-3006,gojun-etal-2012-adapting,0,0.0230149,"modifier than the antecedent, such as Table 5: Top five terms of local and global noncoreferring bias lists Collocation list. One of our hypotheses is that the NPs occurring in verb-object collocations are typically not part of any coreference chain. To test this, we use our collection of 2000 scientific papers to extract domain-specific verb-object collocations. We assume that for some collocations, this tendency is stronger (make use, take place) than for others that could potentially be coreferring (see figure, apply rule). The collocations have been identified with a term extraction tool (Gojun et al., 2012). Every collocation that occurs at least twice in the data is present on the list. Table 6 shows the most frequent terms. make + use give + rise derive + form parse + sentence sort + feature see + figure silence + efficiency embed + sentence focus + algorithm Adapting a Coreference Resolver for Associative Links in Science (8) the negative strain ... the positive strain; (9) three categories ... the first category; (10) siRNAs ... the most effective siRNAs. We assume that these associative relations can be identified with a coreference resolver without adding additional features. Other cases a"
E14-3006,D10-1048,0,0.0139782,"has been done for other disciplines, such as computational linguistics. Sch¨afer et al. (2012) present a large corpus of 266 full-text computational linguistics papers from the ACL Anthology, annotated with coreference links. The CoNLL shared task 2012 on modelling multilingual unrestricted coreference in OntoNotes (Pradhan et al., 2012) produced several state-of-the-art coreference systems (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Chen and Ng, 2012) trained on news text and dialogue, as provided in the OntoNotes corpus (Hovy et al., 2006). Other state-of-the-art systems, such as Raghunathan et al. (2010) and Berkeley’s Coreference Resolution System (Durrett and Klein, 2013), also treat coreference as a task on news text and dialogue. We base our experiments on the IMS coreference resolver by Bj¨orkelund and Farkas (2012), one of the best publicly available English coreference systems. The resolver uses the decision of a cluster-based de46 Category Example C OREFERENCE LINKS G IVEN ( SPECIFIC ) G IVEN ( GENERIC ) A SSOCIATIVE LINKS A SSOCIATIVE Categories without links A SSOCIATIVE ( SELF - CONTAINING ) D ESCRIPTION U NUSED D EICTIC P REDICATIVE I DIOM D OUBT We present the following experimen"
E14-3006,P94-1002,0,0.18486,"this integration, combined with domain-dependent training data, can outperform the performance of an out-of-the-box coreference resolver. For the (much harder) task of resolving associative anaphora, our preliminary results show the need for and the effect of semantic features. 1 (1) Xe-Ar was found to be in a layered structure with Ar on the surface1 . (2) We base our experiments on the Penn treebank. The corpus size is ... The resolution of associative links is important because it can help in tasks which use the concept of textual coherence, e.g. Barzilay and Lapata (2008)’s entity grid or Hearst (1994)’s text segmentation. They might also be of use in higherlevel text understanding tasks such as textual entailment (Mirkin et al., 2010) or summarisation based on argument overlap (Kintsch and van Dijk, 1978; Fang and Teufel, 2014). Gasperin (2009) showed that biological texts differ considerably from other text genres, such as news text or dialogue. In this respect, our results confirm that the proportion between non-referring and referring entities in scientific text differs from that reported for other genres. The same holds for the type and relative number of linguistic expressions used fo"
E14-3006,riester-etal-2010-recursive,0,0.628791,"lection only, assuming that the recognition of associative entities has already been performed. Information status has been investigated extensively in different genres such as news text, e.g. in Markert et al. (2012). Poesio and Vieira (1998) performed an information status-based corpus study on news text, defining the following categories: coreferential, bridging, larger situation, unfamiliar and doubt. To the best of our knowledge, there is currently no study on information status in scientific text. In this paper, we propose a classification scheme for scientific text that is derived from Riester et al. (2010) and Poesio and Vieira (1998). We investigate the differences between news text and scientific text by analysing the distribution of information status categories. We hypothesise that the proportion of associative anaphora in scientific text is higher than in news text, making it necessary to resolve them in some form. Our experiments on the resolution of coreferent anaphora concern the domain-adaptation of a coreference resolver to this new domain and examine the effect of domain-dependent training data and features aimed at capturing technical terminology. We also present an unusual setup wh"
E14-3006,N13-1111,0,0.19177,"ithm is encoded as a feature and fed to a pairwise classifier, which makes decisions about pairs of mentions rather than clusters. This stacked approach overcomes problems of previous systems that are based on the isolated pairwise decision. The features used are mostly taken from previous work on coreference resolution and encode a variety of information, i.e, surface forms and their POS tags, subcategorisation frames and paths in the syntax tree as well as the semantic distance between the surface forms (e.g. edit distance). However, none of this work is concerned with associative anaphora. Hou et al. (2013) present a corpus of news text annotated with associative links that are not limited with respect to semantic relations between anaphor and antecedent. Their experiments focus on antecedent selection only, assuming that the recognition of associative entities has already been performed. Information status has been investigated extensively in different genres such as news text, e.g. in Markert et al. (2012). Poesio and Vieira (1998) performed an information status-based corpus study on news text, defining the following categories: coreferential, bridging, larger situation, unfamiliar and doubt."
E14-3006,C12-2103,0,0.200417,"Missing"
E14-3006,C08-1033,0,\N,Missing
E95-1014,C92-1025,0,0.026803,"Missing"
E95-1014,A92-1018,0,0.0173797,"uns, and that this will be done by comparing syntactic structures attached to the verb and noun forms. In order to extract corpus evidence related to these phenomena, we proceed as follows: 1. We generate all the morphologically related forms of the word pair using a lexical transducer for English ( K a r t t u n e n et al., 1992). This list of words will be used as corpus filter. 2. The lines of the corpus are tokenized (Grefenstette and Tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained. 3. T h e corpus lines retained are part-of-speech tagged (Cutting et al., 1992). This allows us to divide the corpus evidence into verb evidence and noun evidence. 4. Using a robust surface parser (Grefenstette, 1994), we derive the local syntactic patterns involving the verbal form and the nominalized form. 5. Considering t h a t nominalized forms retain some of the verbal characteristics of the underlying predicate, we want to extract the most common a r g u m e n t / a d j u n c t structures found around verbal uses of the predicate. As an approximation, we extract here all the prepositional phrases found after the verb. 6. For nominal forms, we select only those uses"
E99-1015,E95-1026,0,0.0105514,"logy), limitations, and further work. AIM Sentences best portraying the particular (main) research goal of the article TEXTUAL Explicit statements about the textual section structure of the paper CONTRAST Sentences contrasting own work to other work; sentences pointing out weaknesses in other research; sentences stating that the research task of the current paper has never been done before; direct comparisons BASIS Statements that the own work uses some other work as its basis or starting point, or gets support from this other work FULL SCHEME Figure 1: Overview of the annotation scheme 1997; Alexandersson et al., 1995; Jurafsky et al., 1997), but our task is more difficult since it requires more subjective interpretation. 3 Annotation experiment Our annotation scheme is based on the intuition that its categories provide an adequate and intuitive description of scientific texts. But this intuition alone is not enough of a justification: we believe that our claims, like claims about any other descriptive account of textual interpretation, should be substantiated by demonstrating that other humans can apply this interpretation consistently to actual texts. We did three studies. Study I and II were designed to"
E99-1015,J97-1002,1,0.612409,"Missing"
E99-1015,J96-2004,1,0.212084,"will produce the same classifications at different times, is important because an instable annotation scheme can never be reproducible. Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators. We use the Kappa coefficient K (Siegel and Castellan, 1988) to measure stability and repro112 ducibility among k annotators on N items: In our experiment, the items are sentences. Kappa is a better measurement of agreement than raw percentage agreement (Carletta, 1996) because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders. No m a t t e r how many items or annotators, or how the categories are distributed, K--0 when there is no agreement other than what would be expected by chance, and K = I when agreement is perfect. We expect high random agreement for our annotation scheme because so many sentences fall into the OWN category. Studies I and II will determine how far we can trust in the human-annotated training material for both learning and evaluation of the aut"
E99-1015,P84-1055,0,0.0307613,"Missing"
E99-1015,W97-0713,0,0.0295132,"estricted domains to larger domains and unrestricted text. Sp~irck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection could be improved substantially if the global rhetorical context of the extracted material was taken into account more. Marcu (1997) makes a similar point based on rhetorical relations as defined by Rhetorical Structure Theory (RST, (Mann and Thompson, 1987)). 110 In contrast to this approach, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to local R S T - t y p e moves. For example, sentences which describe weaknesses of previous approaches can provide a good characterization of the scientific articles in which they occur, since they are likely to also be a description of the problem that paper is intending to solve. Take a sentence like &quot;Un]ortunately, this wor"
J02-4002,P99-1071,0,0.15372,"Missing"
J02-4002,J96-2004,0,0.0509803,"explained or linguistic example sentences? NO YES YES Does this sentence make explicit reference to the structure of the paper? AIM YES TEXTUAL BACKGROUND NO OWN NO Does it describe a negative aspect of other work, or a contrast or comparison of the own work to it? YES NO Does this sentence mention other work as basis of or support for the current paper? CONTRAST YES BASIS Figure 6 Decision tree for rhetorical annotation. 420 NO OTHER Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996). The kappa coefficient is defined as follows: K= P(A) − P(E) 1 − P(E) where P(A) is pairwise agreement and P(E) random agreement. K varies between 1 when agreement is perfect and −1 when there is a perfect negative correlation. K = 0 is defined as the level of agreement that would be reached by random annotation using the same distribution of categories as the actual annotators did. The main advantage of kappa as an annotation measure is that it factors out random agreement by numbers of categories and by their distribution. As kappa also abstracts over the number of annotators considered, it"
J02-4002,A00-2004,0,0.00847919,"nsidering each sentence as a “document.” The result was a classification performance of K = .30; the classifier nearly always chooses OWN and OTHER segments. The rare but important categories AIM, BACKGROUND, CONTRAST, and BASIS could be retrieved only with low precision and recall. Therefore, text classification methods do not provide a solution to our problem. This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations. Instead, we predict that other indicators apart from the simple words contained in the sentence could provide strong evidence for the modeling of rhetorical status. Also, the relatively small amount of training material we have at our disposal requires a machine learning method that makes optimal use of as many different kinds of features as possible. We predicted that this would increase precision and recall on the categories in which we are interest"
J02-4002,J93-1003,0,0.026449,"ual segments. There are four equivalence classes of 429 Computational Linguistics Volume 28, Number 4 agents with ambiguous reference (“this system”): REF AGENT, REF US AGENT, THEM PRONOUN AGENT, and AIM REF AGENT. Agent classes were created based on intuition, but subsequently each class was tested with corpus statistics to determine whether it should be removed or not. We wanted to find and exclude classes that had a distribution very similar to the overall distribution of the target categories, as such features are not distinctive. We measured associations using the log-likelihood measure (Dunning 1993) for each combination of target category and semantic class by converting each cell of the contingency into a 2×2 contingency table. We kept only classes of verbs in which at least one category showed a high association (gscore &gt; 5.0), as that means that in these cases the distribution was significantly different from the overall distribution. The last column in Table 6 shows that the classes THEM PRONOUN, GENERAL, SOLUTION, PROBLEM, and REF were removed; removal improved the performance of the Agent feature. SegAgent. SegAgent is a variant of the Agent feature that keeps track of previously r"
J02-4002,J97-1003,0,0.328164,"1997) and considering each sentence as a “document.” The result was a classification performance of K = .30; the classifier nearly always chooses OWN and OTHER segments. The rare but important categories AIM, BACKGROUND, CONTRAST, and BASIS could be retrieved only with low precision and recall. Therefore, text classification methods do not provide a solution to our problem. This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations. Instead, we predict that other indicators apart from the simple words contained in the sentence could provide strong evidence for the modeling of rhetorical status. Also, the relatively small amount of training material we have at our disposal requires a machine learning method that makes optimal use of as many different kinds of features as possible. We predicted that this would increase precision and recall on the categories in which we"
J02-4002,A00-2024,0,0.0159156,"al can then be extracted and displayed verbatim as “extracts” (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality. Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). The extent to which it is possible to do postprocessing is limited, however, by the fact that contentful material is extracted without information about the general discourse context in which the material occurred in the source text. For instance, a sentence describing the solution to a scientific problem might give the main contri∗ Simone Teufel, Computer Laboratory, Cambridge University, JJ Thomson Avenue, Cambridge, CB3 OFD, England. E-mail: Simone.Teufel@cl.cam.ac.uk † Marc Moens, Rhetorical Systems and University of Edinburgh, 2 Buccleuch Pl"
J02-4002,P98-1112,0,0.00581776,"Missing"
J02-4002,A97-1042,0,0.0826912,"Missing"
J02-4002,E99-1011,0,0.00470165,"ific field. 1. Introduction Summarization systems are often two-phased, consisting of a content selection step followed by a regeneration step. In the first step, text fragments (sentences or clauses) are assigned a score that reflects how important or contentful they are. The highestranking material can then be extracted and displayed verbatim as “extracts” (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality. Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). The extent to which it is possible to do postprocessing is limited, however, by the fact that contentful material is extracted without information about the general discourse context in which the material occurred in the source text. For instance, a"
J02-4002,P99-1072,0,0.0130862,"ific field. 1. Introduction Summarization systems are often two-phased, consisting of a content selection step followed by a regeneration step. In the first step, text fragments (sentences or clauses) are assigned a score that reflects how important or contentful they are. The highestranking material can then be extracted and displayed verbatim as “extracts” (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality. Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). The extent to which it is possible to do postprocessing is limited, however, by the fact that contentful material is extracted without information about the general discourse context in which the material occurred in the source text. For instance, a"
J02-4002,J91-1002,0,0.0764377,"F*IDF) method (McCallum 1997) and considering each sentence as a “document.” The result was a classification performance of K = .30; the classifier nearly always chooses OWN and OTHER segments. The rare but important categories AIM, BACKGROUND, CONTRAST, and BASIS could be retrieved only with low precision and recall. Therefore, text classification methods do not provide a solution to our problem. This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations. Instead, we predict that other indicators apart from the simple words contained in the sentence could provide strong evidence for the modeling of rhetorical status. Also, the relatively small amount of training material we have at our disposal requires a machine learning method that makes optimal use of as many different kinds of features as possible. We predicted that this would increase precision and recall on the categories"
J02-4002,J98-3005,0,0.0186156,"Missing"
J02-4002,W97-0710,1,0.356068,"Missing"
J02-4002,W00-1302,1,0.554852,"Missing"
J02-4002,J94-2004,0,0.228951,"Missing"
J02-4002,W98-1123,0,\N,Missing
J02-4002,E95-1030,0,\N,Missing
J02-4002,E99-1015,1,\N,Missing
J02-4002,H91-1061,0,\N,Missing
J02-4002,J02-4001,0,\N,Missing
J02-4002,P95-1004,0,\N,Missing
J02-4002,J97-1002,0,\N,Missing
J02-4002,C98-1108,0,\N,Missing
J02-4002,E95-1038,0,\N,Missing
J02-4002,J00-3003,0,\N,Missing
J13-2003,P10-1024,0,0.0334597,"Missing"
J13-2003,W03-1402,0,0.011637,"General-domain lexical resources often include information about metaphorical word senses, although unsystematically and without any accompanying semantic annotation. For example, WordNet2 (Fellbaum 1998) contains the comprehension sense of grasp, deﬁned as “get the meaning of something,” and the reading sense of skim, deﬁned as “read superﬁcially.” A great deal of metaphorical senses are absent from the current version of WordNet, however. A number of researchers have advocated the necessity of systematic inclusion and mark-up of metaphorical senses in such general¨ domain lexical resources (Alonge and Castelli 2003; Lonneker and Eilts 2004) and claim that this would be beneﬁcial for the computational modeling of metaphor. Metaphor processing systems could then either use this knowledge or be evaluated against it. ¨ Lonneker (2004) mapped the senses from EuroWordNet3 to the Hamburg Metaphor ¨ ¨ Database (Lonneker 2004; Reining and Lonneker-Rodman 2007) containing examples of metaphorical expressions in German and French. Currently no explicit information about metaphor is integrated into WordNet for English, however. Although consistent inclusion in WordNet is in principle possible for conventional metap"
J13-2003,andersen-etal-2008-bnc,0,0.0860829,"Missing"
J13-2003,N03-1003,0,0.0247979,"Missing"
J13-2003,P01-1008,0,0.111162,"Missing"
J13-2003,D08-1007,0,0.382018,"Missing"
J13-2003,E06-1042,0,0.245004,"Missing"
J13-2003,W02-1016,0,0.208896,"Missing"
J13-2003,P06-4020,0,0.335837,"Missing"
J13-2003,E03-1034,0,0.184107,"Missing"
J13-2003,N06-1003,0,0.0359736,"Missing"
J13-2003,E99-1042,0,0.140698,"Missing"
J13-2003,P06-2012,0,0.147165,"Missing"
J13-2003,J90-1003,0,0.263128,"Missing"
J13-2003,J07-4004,0,0.0410609,"Missing"
J13-2003,W09-1108,0,0.0380368,"Missing"
J13-2003,W09-1109,0,0.0249527,"Missing"
J13-2003,D09-1046,0,0.0183457,"Missing"
J13-2003,J91-1003,0,0.724298,"solving non-literal meanings via analogical comparisons, the development of a complete and computationally practical account of this phenomenon is a challenging and complex task. Despite the importance of metaphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davi"
J13-2003,J83-3004,0,0.520817,"given context. Selectional restrictions are the semantic constraints that a predicate places onto its arguments. Consider the following example. (17) a. My aunt always drinks her tea on the terrace. b. My car drinks gasoline. (Wilks 1978) The verb drink normally requires a grammatical subject of type ANIMATE and a grammatical object of type LIQUID, as in Example (17a). Therefore, drink taking a car as a subject in (17b) is an anomaly, which, according to Wilks, indicates a metaphorical use of drink. Although Wilks’s idea inspired a number of computational experiments on metaphor recognition (Fass and Wilks 1983; Fass 1991; Krishnakumaran and Zhu 2007), it is important to note that in practice this approach has a number of limitations. Firstly, there are other kinds of non-literalness or anomaly in language that cause a violation of semantic norm, such as metonymies. Thus the method would overgenerate. Secondly, there are kinds of metaphor that do not represent a violation of selectional restrictions (i.e., the approach may also undergenerate). This would happen, for example, when highly conventionalized metaphorical word senses are more frequent than the original literal senses. Due to their frequen"
J13-2003,W06-3506,0,0.716069,"e the importance of metaphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy ´ S´eaghdha 2010) open up 2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O many avenues for the creation of such a system. This is the n"
J13-2003,P93-1023,0,0.193506,"Missing"
J13-2003,W09-2905,0,0.0174715,"Missing"
J13-2003,J98-1002,0,0.203057,"Missing"
J13-2003,N06-1058,0,0.0156241,"Missing"
J13-2003,kingsbury-palmer-2002-treebank,0,0.530837,"Missing"
J13-2003,P03-1054,0,0.0074496,"Missing"
J13-2003,N10-1017,0,0.0115698,"Missing"
J13-2003,korhonen-etal-2006-large,1,0.429092,"Missing"
J13-2003,W03-1601,0,0.0832078,"Missing"
J13-2003,W07-0103,0,0.857218,"taphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy ´ S´eaghdha 2010) open up 2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O many avenues for the creation of such a system. This is the niche the presented work is int"
J13-2003,S01-1009,0,0.016863,"Missing"
J13-2003,P98-2127,0,0.122342,"Missing"
J13-2003,C88-1081,0,0.672595,"B3 0FD, UK. E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk. Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication: 31 May 2012. doi:10.1162/COLI a 00124 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 2 Metaphors arise when one concept is viewed in terms of the properties of another. Humans often use metaphor to describe abstract concepts through reference to more concrete or physical experiences. Some examples of metaphor include the following. (1) How can I kill a process? (Martin 1988) (2) Hillary brushed aside the accusations. (3) I invested myself fully in this research. (4) And then my heart with pleasure ﬁlls, And dances with the daffodils. (“I wandered lonely as a cloud,” William Wordsworth, 1804) Metaphorical expressions may take a great variety of forms, ranging from conventional metaphors, which we produce and comprehend every day, for example, those in Examples (1)–(3), to poetic and novel ones, such as Example (4). In metaphorical expressions, seemingly unrelated features of one concept are attributed to another concept. In Example (1), a computational process is"
J13-2003,J04-1002,0,0.937327,"but also those of nouns and adjectives. As opposed to previous approaches that modeled metaphorical reasoning starting from a hand-crafted description and applying it to explain the data, we aim to design a statistical model that captures regular patterns of metaphoricity in a large corpus and thus generalizes to unseen examples. Compared to labor-intensive manual efforts, this approach is more robust and, being nearly unsupervised, cost-effective. In contrast to previous statistical approaches, which addressed metaphors of a speciﬁc topic or did not consider linguistic metaphor at all (e.g., Mason 2004), the proposed method covers all metaphors in principle, can be applied to unrestricted text, and can be adapted to different domains and genres. 306 Shutova, Teufel, and Korhonen Statistical Metaphor Processing Our ﬁrst experiment is concerned with the identiﬁcation of metaphorical expressions in unrestricted text. Starting from a small set of metaphorical expressions, the system learns the analogies involved in their production in a minimally supervised way. It generalizes over the exempliﬁed analogies by means of verb and noun clustering (i.e., the identiﬁcation of groups of similar concept"
J13-2003,W02-0816,0,0.0401881,"Missing"
J13-2003,S07-1009,0,0.0372132,"Missing"
J13-2003,P79-1016,0,0.200731,"Missing"
J13-2003,C88-2088,0,0.631192,"Missing"
J13-2003,P08-1028,0,0.0173268,"Missing"
J13-2003,T87-1040,0,0.278653,"hrasing gold standard by asking human subjects (not previously exposed to system output) to produce their own literal paraphrases for metaphorical verbs. The system paraphrasing was then also evaluated against this gold standard. 2. Theoretical and Computational Background 2.1 Metaphor and Polysemy Theorists of metaphor distinguish between two kinds of metaphorical language: novel (or poetic) metaphors (i.e., those that are imaginative), and conventionalized metaphors 307 Computational Linguistics Volume 39, Number 2 (i.e., those that are used as a part of an ordinary discourse). According to Nunberg (1987), all metaphors emerge as novel, but over time they become part of general usage and their rhetorical effect vanishes, resulting in conventionalized metaphors. Following Orwell (1946), Nunberg calls such metaphors “dead” and claims that they are not psychologically distinct from literally used terms. The scheme described by Nunberg demonstrates how metaphorical associations capture patterns governing polysemy, namely, the capacity of a word to have multiple meanings. Over time some of the aspects of the target domain are added to the meaning of a term in the source domain, resulting in a (meta"
J13-2003,P10-1045,0,0.109223,"Missing"
J13-2003,P93-1024,0,0.173268,"Missing"
J13-2003,peters-peters-2000-lexicalised,0,0.461657,"Missing"
J13-2003,I05-5010,0,0.0684057,"Missing"
J13-2003,P07-1115,1,0.843382,"Missing"
J13-2003,W09-4204,0,0.0354842,"Missing"
J13-2003,W04-3219,0,0.100268,"Missing"
J13-2003,W07-0102,0,0.288064,"Missing"
J13-2003,D10-1114,0,0.0232497,"Missing"
J13-2003,P99-1014,0,0.479383,"Missing"
J13-2003,P10-1093,0,0.0529917,"Missing"
J13-2003,J06-2001,0,0.0415072,"Missing"
J13-2003,W03-1609,0,0.0627777,"Missing"
J13-2003,N10-1147,1,0.808786,"Missing"
J13-2003,C10-1113,1,0.830833,"Missing"
J13-2003,shutova-teufel-2010-metaphor,1,0.842088,"associated with the act of killing. In Example (2) Hillary is not literally cleaning the space by sweeping accusations. Instead, the accusations lose their validity in that situation, in other words Hillary rejects them. The verbs brush aside and reject both entail the resulting disappearance of their object, which is the shared salient property that makes it possible for this analogy to be lexically expressed as a metaphor. Characteristic of all areas of human activity (from poetic to ordinary to scientiﬁc) and thus of all types of discourse, metaphor becomes an important problem for NLP. As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional metaphor is ubiquitous in natural language text (according to their data, on average every third sentence in general-domain text contains a metaphorical expression). This makes metaphor processing essential for automatic text understanding. For example, an NLP application which is unaware that a “leaked report” is a “disclosed report” and not, for example, a “wet report,” would fail further semantic processing of the piece of discourse in which this phrase appears. A system capable of recognizing and interpreting metaphorical expressio"
J13-2003,D09-1067,1,0.593576,"Missing"
J13-2003,D11-1095,1,0.868196,"Missing"
J13-2003,D11-1094,1,0.352036,"Missing"
J13-2003,C08-1119,0,0.183734,"Missing"
J13-2003,P09-2019,0,0.0327936,"Missing"
J13-2003,P08-1089,0,0.0260576,"Missing"
J13-2003,P09-1094,0,0.02327,"Missing"
J13-2003,N06-1057,0,0.0107955,"Missing"
J13-2003,W09-0208,0,\N,Missing
J13-2003,C98-2122,0,\N,Missing
L16-1697,P11-3015,0,0.0204331,"possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richly annotated resource containing this kind of information. The goal of CiteNe"
L16-1697,W04-3202,0,0.104107,"on on the uses of the tool (Section 6.). 2. k∈K Our goal is to find XL that maximizes the utility U : arg maxXL U (XL ). 3. x∈X where L is a loss function, e.g. squared error loss (f (x) − y)2 . The task of active learning is then to find a set of points While in practice it may be sufficient to optimise a given model, and therefore mould a corpus using AL to fit it, this loses sight of the bigger aim, that of studying phenomena for understanding how something works; further, it has been shown to even be detrimental in some cases to apply data acquired for one model using AL to another model (Baldridge, 2004; Rubens and Sugiyama, 2006; Sugiyama and Rubens, 2008). 2 The tool is available for download at https://github. com/move-tool. Related Work We next introduce works related to our proposed modelfree AL method. 3.1. Method Formulation Let us formulate our proposal in the context of existing AL approaches. The majority of AL methods are designed for supervised learning settings, where given the training data {(x1 , y1 ), . . . , (xn , yn )}, the task is to learn a function f : X 7→ Y that accurately predicts the output values Y of the input X. For AL settings, it is assumed that we are able to s"
L16-1697,W09-1403,0,0.020287,"Missing"
L16-1697,W09-3611,1,0.806146,"d network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richly annotated resource c"
L16-1697,C08-1087,0,0.0137431,"papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richl"
L16-1697,J02-4001,0,0.0605987,"with citation function, within research papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunat"
L16-1697,P11-1080,0,0.0248296,"ed to models and corpus construction, and introduced our novel method for model-free AL to build phenomena-driven corpora, including the development of a tool, MOVE, and shown its use on a real world scenario of a citation-network based corpus we are developing, CiteNet. The case study introduced in Section 5. is typical of a wide range of phenomena in computational linguistics where entities are linked in some form and can be thus represented as a network. For instance, in the cross-document coreference task, systems must identify noun-phrases which corefer across document boundaries, e.g., (Singh et al., 2011). Our proposed method could be used to find an initial training and test set of such documents, based on obvious named entity (NE) coreferences (e.g., those which are long enough to be guaranteed to be unique if they are found in identical forms across documents). The point is not to find all coreferences in advance or we would not need to make the tool, but to insure enough variance in the documents to produce a subnetwork representing the coreferences adequately. These coreference links represent the equivalent of the citation links from the case study, but in the crossdocument NE task, an a"
L16-1697,W10-4121,0,0.0316975,"s will fail if they are fed poor quality data during training, i.e. “garbage in, garbage out”. On the other end, corpus construction (CC) typically faces the challenge of maximising the usefulness of annotation while keeping the costs within the allotted budget. Corpus annotation is usually undergone in one of three ways: (1) purely automatic, (2) automated annotation followed by manual correction, and (3) purely manual annotation (McEnery et al., 1995). As active learning (AL) for CC identifies the most “informative” data for annotation (Olsson, 2009; Tomanek and Olsson, 2009; Settles, 2009; Song and Yao, 2010), at first glance it seems like a perfect fit when by necessity you cannot annotate all documents. For example, consider Figure 1, which shows random input vs. input selected using AL to consider the network structure. However, this brings us to what we define as the chickenand-egg corpus and model conundrum, which refers to how AL often happens in a closed-loop process, the underlying model or models directly influencing which data is selected for annotation, which improves the model’s accuracy, and so on. For exploratory CC in which creators may be investigating hitherto under/unexplored phe"
L16-1697,W06-1613,1,0.640383,"n our case, we wanted to create an annotated corpus of novelty claims and citation spans, replete with citation function, within research papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the p"
L16-1697,W09-1906,0,0.109871,"mproving models), even the best algorithms will fail if they are fed poor quality data during training, i.e. “garbage in, garbage out”. On the other end, corpus construction (CC) typically faces the challenge of maximising the usefulness of annotation while keeping the costs within the allotted budget. Corpus annotation is usually undergone in one of three ways: (1) purely automatic, (2) automated annotation followed by manual correction, and (3) purely manual annotation (McEnery et al., 1995). As active learning (AL) for CC identifies the most “informative” data for annotation (Olsson, 2009; Tomanek and Olsson, 2009; Settles, 2009; Song and Yao, 2010), at first glance it seems like a perfect fit when by necessity you cannot annotate all documents. For example, consider Figure 1, which shows random input vs. input selected using AL to consider the network structure. However, this brings us to what we define as the chickenand-egg corpus and model conundrum, which refers to how AL often happens in a closed-loop process, the underlying model or models directly influencing which data is selected for annotation, which improves the model’s accuracy, and so on. For exploratory CC in which creators may be investi"
L16-1697,D07-1051,0,0.114612,"ning on Networks There is extensive work on utilising network structure for improving AL, e.g., using additional information provided by edges (Bilgic and Getoor, 2009), network topology (Hanneke and Xing, 2009) favouring nodes at centre of clusters (Macskassy, 2009), high connectivity (Shi and Zhao, 2010), and social network metrics (Macskassy, 2009; Kuwadekar, 2010; Ji, 2012). However, these works are model-centred, which as explained in the introduction, is not the case for our method, which is trying to liberate the corpus creator from needing a model. 3.6. (d) +Betweenness Corpus Utility Tomanek and Wermter (2007) state that AL-based corpora should be reusable for training with modified or improved classifiers to have true utility. In part, this is because it can be difficult to predict the best suited algorithm for a task, so swapping learning algorithms during experimentation may be needed (Busser and Morante, 2005). Not knowing which model will be applied to the constructed corpus may seem minor, but it has been shown both empirically (Baldridge, 2004) and methodologically (Rubens and Sugiyama, 2006; Sugiyama and Rubens, 2008) that samples obtained for one model are often detrimental to another, so"
liakata-etal-2010-corpora,W08-0606,0,\N,Missing
liakata-etal-2010-corpora,W09-1325,1,\N,Missing
liakata-etal-2010-corpora,D09-1155,1,\N,Missing
liakata-etal-2010-corpora,P07-1125,0,\N,Missing
liakata-etal-2010-corpora,I08-1050,0,\N,Missing
N06-1050,teufel-elhadad-2002-collection,1,0.810674,"istically extracting character information (similar to OCR) are necessary since many of the PDFs were created from scanned paper-copies and others do not contain character information in an accessible format. The OmniPage output describes a paper as text blocks with typesetting information such as font and positional information. A pre-processor (Lewin et al., 2005) filters and summarizes the OmniPage output into Intermediate XML (IXML), as well as correcting certain characteristic errors from that stage. A journal-specific template converts the IXML to a logical XML-based document structure (Teufel and Elhadad, 2002), by exploiting low-level, presentational, journal-specific information such as font size and positioning of text blocks. Subsequent stages incrementally add more detailed information to the logical representation. The paper’s reference list is annotated in more detail, marking up individual references, author names, titles and years of publication. Finally, a citation processor identifies and marks up citations in the document body and their constituent parts, e.g., author names and years. 5 Preliminary Experimentation We expect that our test collection, built for our citation experiments, wi"
N07-1040,1995.iwpt-1.8,0,0.0250204,"Missing"
N07-1040,grover-etal-2000-lt,0,0.099544,"Missing"
N07-1040,passonneau-2004-computing,0,0.0121771,"the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference class S in the model. Recall error is then RE(S) = m(S)/c(S) and Recall is . Recall for the enR(S) = 1 − RE = c(S)−m(S) c(S) tire file (or set of files) is calculated by summing over all co-reference classes in the model: R= P i c(Si ) P − m(Si ) i c(Si ) Precision (P ) is calculated by swapping the model and system and the f-measure (F = 2R × P/(R + P )) is symmetric with respect to both annotations. 5.2 Krippendorff ’s Alpha We follow Passonneau (2004) and Poesio and Artstein (2005) in using Krippendorff (1980)’s α metric to compute agreement between annotations. The advantage of α over the more commonly used κ metric is that α allows for partial agreement when annotators assign multiple labels to the same markable; in this case calculating agreement on a markable requires a more graded agreement calculation than the “1 if sets are identical and 0 otherwise” provided for by κ. Krippendorff’s α measures disagreement, and allows for the use of distance metrics to calculate partial disagreement. Following Passonneau, we present results using f"
N07-1040,J02-4002,1,0.738894,"just to some subpart of it. Kim and Webber (2006) solve the problem of distinguishing between these relations for one case. They decide whether the pronoun “they” anaphorically refers to the authors of a cited paper, or whether it refers to some entity that is discussed in (a subpart of) a paper (e.g., “galaxies”). In this paper, we tackle the other problem of scientific attribution. We do not distinguish between the two types of links stated above, but only identify which citation(s) a linguistic expression is attributable 1 We use a list of around 40 research methodology related nouns from Teufel and Moens (2002), such as e.g., “study, account, investigation, result” etc. These are nouns we are particularly interested in. 317 to. For tasks of interest to us, it is not enough to only consider anaphoric references to entire papers; authors often make statements comparing/using/criticising aspects or subparts of cited work. We therefore consider a far wider range of markables than Kim and Webber’s single pronoun “they”. Our attribution task differs from the traditional anaphora resolution task in that we have a fixed list of possible referents (the reference list items, Current-Paper or No-SpecificPaper)"
N07-1040,W06-1613,1,0.757387,"c data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but which are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun “they” in astronomy papers: it can either refer to a citation or to some entitie"
N07-1040,M95-1005,0,0.0685961,"arkable, rather than a citation and the markable; similarly, in 3(b,c) we count instances of first person pronoun/“this paper”; for 2(e), we now calculate the distance of the closest citation instance. In short, the same features are used, but current work and citations are swapped. 5 Evaluation Metrics We consider two evaluation metrics. The first is the scoring system used for the co-reference task in the Message Understanding Conferences MUC-6 and MUC-7. The second is Krippendorff’s α. We briefly discuss both below. 5.1 The MUC-6/MUC-7 Metric The MUC-6/MUC-7 Co-reference evaluation metric (Vilain et al., 1995) works by comparing co-reference classes across two annotated files. Calling one annotation the “model” and the other the “system”, for each co-reference class S in the model, c(S) is the minimal number of co-reference links needed to generate the class (this is one less than the cardinality of the class; c(S) = |S |− 1). m(S) is the number of “missing” links in the system annotation relative to the co-reference class as marked up in the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference clas"
N12-1073,P11-1051,0,0.0660016,"n using this context-enhanced gold standard definition. 1 Simone Teufel University of Cambridge Computer Laboratory 15 JJ Thomson Avenue Cambridge, CB3 0FD, U.K. simone.teufel@cl.cam.ac.uk Introduction Sentiment analysis of citations in scientific papers and articles is a new and interesting problem. It can open up many exciting new applications in bibliographic search and in bibliometrics, i.e., the automatic evaluation of the influence and impact of individuals and journals via citations. Automatic detection of citation sentiment can also be used as a first step to scientific summarisation (Abu-Jbara and Radev, 2011). Alternatively, it can help researchers during search, e.g., by identifying problems with a particular approach, or by helping to recognise unaddressed issues and possible gaps in the current research. Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–6"
N12-1073,P11-3015,1,0.824676,"en detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o). A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006). An example annotation for Smadja (1993) is given in Figure 2, where the first column shows the line number and the second one shows the class label. Figure 2: Example annotation of a citation context. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984). As is evident from Table 1, including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light"
N12-1073,bird-etal-2008-acl,0,0.198757,"Missing"
N12-1073,P07-1056,0,0.0794817,"only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to"
N12-1073,W05-0408,0,0.00965392,"nd Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is li"
N12-1073,P97-1023,0,0.0104322,". 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation ext"
N12-1073,W09-3611,0,0.0134128,"ed lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. The most relevant work is by Qazvinian and Radev (2010) who extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes. However, their dataset consists of only 10 papers and their annotation scheme differs from our four-cl"
N12-1073,W11-1902,0,0.0239925,"Missing"
N12-1073,W02-1011,0,0.0164117,"Missing"
N12-1073,P10-1057,0,0.0210627,"citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. The most relevant work is by Qazvinian and Radev (2010) who extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes. However, their dataset consists of only 10 papers and their annotation scheme differs from our four-class annotation as they do not deal with any sentiment. 5 Conclusion In this paper, we focus on automatic detection of citation sentiment using the citation context. We present a new corpus and show that ignoring the citation context would result in loss of a lot of sentiment, specially criticism t"
N12-1073,W06-1613,1,0.930598,"e 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentiment hidden in the text. Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it ever more important to recover what explicit sentiment there is from the context of the citation. However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction of praise or neutral se"
N12-1073,P02-1053,0,0.00601412,"emes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using"
N12-1073,J09-3003,0,0.0154054,"r, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Sentiment analysis of citations in scientific papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics. Current work on citation sentiment detection focuses on only the citation sentence. In this paper, we address the problem of context-enhanced citation sentiment detection. We present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account. We believe that this gold standard is closer to the truth than a"
N12-1073,W03-1017,0,0.0727218,"proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference re"
N12-1073,P84-1044,0,0.315784,"Missing"
N18-1028,P16-1112,1,0.821307,"res. For each edge e, let a be the position of its left-most component (variable or type) and b the position of its rightmost component (variable or symbol). the embedding vector of its supertype or “NONE” otherwise. These features are mapped to a separate embedding space and then concatenated with the word embedding to form a single task-specific word representation. This allows us to capture useful information about each word, and also designate which words to focus on when processing the sentence. We use a neural sequence labeling architecture, based on the work of Lample et al. (2016) and Rei and Yannakoudakis (2016). The constructed word representations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representations with word embeddings using a p"
N18-1028,P15-2055,1,0.847572,"ll return instances of the typed SLT in figure 1(b) where the variables are vectors, as opposed to integers. Therefore, a typed index can reduce the number of false positives and increase precision. Four MIR retrieval models are introduced in Section 7.3 designed to control for text indexing/retrieval so that the effects of type-aware vs type-agnostic formula indexing and scoring can be isolated. These models make use of the Tangent formula indexing and scoring functions (Pattaniyil and Zanibbi, 2014), which we have implemented. We use the Cambridge University Math IR Test Collection (CUMTC) (Stathopoulos and Teufel, 2015) which is composed of 120 research-level mathematical information needs and 160 queries. The CUMTC is ideal for our evaluation for two reasons. First, topics in the CUMTC are expressed in natural language and are rich in mathematical types. This allows us to directly apply our best performing variable typing model (BiLSTM) in our retrieval experiment in order to extract variable typings for documents and queries. Second, the CUMTC uses the MREC as its underlying document collection, which enables downstream evaluation in an optimal setting for variable typing. 7.1 m(d, e1 , . . . , en ) = n X"
N18-1028,C16-1221,1,0.862635,"performing model is evaluated on an extrinsic task: MIR, by producing a typed formula index. Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing. 1 the variables P and N in the symbolic context are assigned the meaning “parabolic subgroup” and “unipotent radical” by the textual context surrounding them respectively. We will refer to the task of assigning one mathematical type to each variable in a sentence as variable typing. We use mathematical types (Stathopoulos and Teufel, 2016) as variable denotation labels. Types are multi-word phrases drawn from the technical terminology of the mathematical discourse that label mathematical objects (e.g., “set”), algebraic structures (e.g., “monoid”) and instantiable notions (e.g., “cardinality of a set”). In the sentence presented earlier, the phrases “parabolic subgroup”, “Levi decomposition” and “unipotent radical” are examples of types. Typing variables may be beneficial to other natural language processing (NLP) tasks, such as topic modeling, to group documents that assign meaning to variables consistently (e.g., “E” is “ener"
N18-1028,N16-1030,0,0.0107615,"tence Table 4: SVM+ features. For each edge e, let a be the position of its left-most component (variable or type) and b the position of its rightmost component (variable or symbol). the embedding vector of its supertype or “NONE” otherwise. These features are mapped to a separate embedding space and then concatenated with the word embedding to form a single task-specific word representation. This allows us to capture useful information about each word, and also designate which words to focus on when processing the sentence. We use a neural sequence labeling architecture, based on the work of Lample et al. (2016) and Rei and Yannakoudakis (2016). The constructed word representations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representatio"
N18-1028,C16-1030,1,0.836001,"sentations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representations with word embeddings using a predictive gating operation (Rei et al., 2016). This allows our model to capture character-level patterns and estimate representations for previously unseen words. In this framework, an alternative word reprethe model). The filters are applied to the input text (i.e. convolutions), and then max-pooled, flattened, concatenated, and a dropout layer (p = 0.5) is then applied before being fed into a multilayer perceptron (MLP), with the number of hidden layers and their hidden units as hyperparameters. Finally, a softmax layer is used to output a binary decision. The model is implemented using the Keras library using binary cross-entropy as l"
P03-1048,J93-1004,0,0.0343641,"Missing"
P03-1048,W00-0403,1,0.724039,". The Kappa coefficient controls agreement P (A) by taking into account agreement by chance P (E) : K= P (A) − P (E) 1 − P (E) No matter how many items or annotators, or how the categories are distributed, K = 0 when there is no agreement other than what would be expected by chance, and K = 1 when agreement is perfect. If two annotators agree less than expected by chance, Kappa can also be negative. We report Kappa between three annotators in the case of human agreement, and between three humans and a system (i.e. four judges) in the next section. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al., 2000) is tested on a large corpus for the first time in this project. RU takes into account chance agreement as a lower bound and interjudge agreement as an upper bound of performance. RU allows judges and summarizers to pick different sentences with similar content in their summaries without penalizing them for doing so. Each judge is asked to indicate the importance of each sentence in a cluster on a scale from 0 to 10. Judges also specify which sentences subsume or paraphrase each other. In relative utility, the score of an automatic summary increases with the importance of the sentences that it"
P03-1048,W97-0710,1,0.831436,"Missing"
P03-1048,E99-1011,0,\N,Missing
P03-1048,W97-0704,0,\N,Missing
P03-1048,J96-2004,0,\N,Missing
P03-1048,I05-2047,0,\N,Missing
P06-1116,P90-1034,0,0.028519,"Missing"
P06-1116,X98-1026,0,0.0229924,"Missing"
P06-1116,P97-1004,0,0.0100586,"t checking. Unsupervised methods for similar tasks include Agichtein and Gravano’s (2000) work, which shows that clusters of vector-spacebased patterns can be successfully employed to detect specific IE relationships (companies and their headquarters), and Ravichandran and Hovy’s (2002) algorithm for finding patterns for a Question Answering (QA) task. Based on training material in the shape of pairs of question and answer terms – e.g., (e.g. {Mozart, 1756}), they learn the 2 Thus, our task shows some parallels to work in paraphrasing (Barzilay and Lee, 2002) and syntactic variant generation (Jacquemin et al., 1997), but the methods are very different. 922 Input: Tuples {A1 , A2 , . . . , Am } and {B1 , B2 , . . . , Bn }. Initialisation: Set the concept-A reference set to {A1 , A2 , . . . , Am } and the concept-B reference set to {B1 , B2 , . . . , Bn }. Set the concept-A active element to A1 and the concept-B active element to B1 . Recursion: 1. Concept B retrieval: (i) Hypothesize: Find terms in the corpus which are in the desired relationship with the concept-A active element (e.g. direct objects of a verb active element). This results in the concept-B candidate set. (ii) Rank: Rank the concept-B cand"
P06-1116,P99-1004,0,0.274495,"active-verb-element DETERMINER * POSTMOD&quot; DOs, Passive: &quot;DETERMINER * AUX active-verb-element element&quot; TVs, Active: &quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&quot; TVs, Passive:&quot;DET active-noun-element AUX * POSTMOD&quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1 , x2 )  =   newspaper articles for this relation. Second, in order to obtain larger coverage and more current data we also exp"
P06-1116,P03-1017,0,0.0218609,"Missing"
P06-1116,P93-1024,0,0.400531,"Active: &quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&quot; DOs, Passive: &quot;DETERMINER * AUX active-verb-element element&quot; TVs, Active: &quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&quot; TVs, Passive:&quot;DET active-noun-element AUX * POSTMOD&quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1 , x2 )  =   newspaper articles for this relation. Second, in order to obtain larger coverage and more current d"
P06-1116,P02-1006,0,0.0326431,"Missing"
P06-1116,W98-0307,1,0.668434,"ational Linguistics and 44th Annual Meeting of the ACL, pages 921–928, c Sydney, July 2006. 2006 Association for Computational Linguistics a) In this paper, we introduce a method for similaritybased estimation of . . . b) Here, we present a similarity-based approach for estimation of. . . c) In this paper, we propose an algorithm which is . . . d) We will here define a technique for similarity-based. . . Cases f)–i) in contrast are false matches: they do not express the authors’ goals, although they are superficially similar to the correct contexts. While string-based approaches (Paice, 1981; Teufel, 1998) are too restrictive to cover the wide variation within the correct contexts, bag-of-words approaches such as Agichtein and Gravano’s (2000) are too permissive and would miss many of the distinctions between correct and incorrect contexts. Figure 2: Context around cue phrases (lexical variants) semantics holding between these terms (“birth year”) via frequent string patterns occurring in the context, such as “A was born in B”, by considering n-grams of all repeated substrings. What is common to these three works is that bootstrapping relies on constraints between the context external to the ex"
P06-1116,P03-1016,0,0.0186174,"OSTMOD&quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1 , x2 )  =   newspaper articles for this relation. Second, in order to obtain larger coverage and more current data we also experiment with Google Scholar 3 , an automatic web-based indexer of scientific literature (mainly peer-reviewed papers, technical reports, books, pre-prints and abstracts). Google Scholar snippets are often incomplete fragments which"
P06-1116,W02-1022,0,0.0234855,"of semantic features for roughly 3000 nouns for constraint checking. Unsupervised methods for similar tasks include Agichtein and Gravano’s (2000) work, which shows that clusters of vector-spacebased patterns can be successfully employed to detect specific IE relationships (companies and their headquarters), and Ravichandran and Hovy’s (2002) algorithm for finding patterns for a Question Answering (QA) task. Based on training material in the shape of pairs of question and answer terms – e.g., (e.g. {Mozart, 1756}), they learn the 2 Thus, our task shows some parallels to work in paraphrasing (Barzilay and Lee, 2002) and syntactic variant generation (Jacquemin et al., 1997), but the methods are very different. 922 Input: Tuples {A1 , A2 , . . . , Am } and {B1 , B2 , . . . , Bn }. Initialisation: Set the concept-A reference set to {A1 , A2 , . . . , Am } and the concept-B reference set to {B1 , B2 , . . . , Bn }. Set the concept-A active element to A1 and the concept-B active element to B1 . Recursion: 1. Concept B retrieval: (i) Hypothesize: Find terms in the corpus which are in the desired relationship with the concept-A active element (e.g. direct objects of a verb active element). This results in the c"
P06-1116,briscoe-carroll-2002-robust,0,0.0119537,"ated verbs and nouns are applied in order to recover more and more variants. The full algorithm is given in Fig. 3. The algorithm requires corpus data for the steps Hypothesize (producing a list of potential candidates) and Rank (testing them for similarity). We Figure 3: Lexical variant bootstrapping algorithm estimate frequencies for the Rank step from the written portion of the British National Corpus (BNC, Burnard (1995)), 90 Million words. For the Hypothesize step, we experiment with two data sets: First, the scientific subsection of the BNC (24 Million words), which we parse using RASP (Briscoe and Carroll, 2002); we then examine the grammatical relations (GRs) for transitive verb constructions, both in active and passive voice. This method guarantees that we find almost all transitive verb constructions cleanly; Carroll et al. (1999) report an accuracy of .85 for 923 DOs, Active: &quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&quot; DOs, Passive: &quot;DETERMINER * AUX active-verb-element element&quot; TVs, Active: &quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&quot; TVs, Passive:&quot;DET active-noun-element AUX * POSTMOD&quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive ve"
P15-2055,W03-2003,0,0.112122,"Missing"
P15-2055,N06-1050,1,0.503428,"cuments. 3 Related Work Test collections over scientific publications were first introduced for the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b). Despite criticism for sourcing queries from collection documents, the Cranfield experiments highlighted the importance of jointly reporting recall and precision, pioneered the practice of using authors and citations for augmenting relevance judgements and established the test collection paradigm. Expert citations have already been exploited for procuring relevance judgements. For example, Ritchie et al. (2006) elicited relevance judgements for citations in papers accepted in a scientific conference from their authors and used these judgements as part of their test collection of scientific publications. In terms of domain, our work is related to the NTCIR-10 Math IR test collection (Aizawa et al., 2013). Furthermore, the topics in our collection are analogous to those in the NTCIR full-text search, in the sense that they take the form of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test coll"
P15-2055,fujii-etal-2006-test,0,0.0311466,"answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of sourcing relevance judgements from expert citations is an established practice in IR. In the context of patent search, for example, Graf and Azzopardi (2008) utilised citations in patent office expert reports as relevance judgements, while Fujii et al. (2006) automatically extracted patent office expert citations used to reject patent applications. 4 the beginning of the section is achieved since no overlap beyond the prelude is introduced between queries generated for micro-topics attached to a given topic. 4.3 Systems Using Lucene as the indexing and searching backend, we compare the performance of two retrieval methods. Underpinning both methods is Lucene’s default similarity (project, 2013), which is based on cosine similarity: sim(q, d) = where V (q) and V (d) are weighted vectors for the query and candidate document respectively. As a perfor"
P15-2137,S13-2001,0,0.0833346,"Missing"
P15-2137,W03-0508,1,0.749227,"Missing"
P15-2137,S07-1014,0,0.242985,"Missing"
P15-2137,W04-1013,0,0.0131673,"able on their own. Additionally, a good timeline satisfies conflicting constraints: it should contain only salient events, and the overall time period considered should be covered well by events. Timeline construction is not a new task. It has been performed, for example, in a multi-document summarisation (Chieu and Lee, 2004; Yan et al., 2011; Nguyen et al., 2014) or in a single-document classification context (Chasin et al., 2013). It is crucial to reliably evaluate algorithms that create such timelines automatically. Of course, any summary can be evaluated by surface methods such as ROUGE (Lin, 2004). But even for traditional summaries, ROUGE-based evaluation has been criticised for being too shallow, and it is even less adequate for timelines, because of their special properties described above. 1. Ask timeline writers to create timelines with a fixed number of date-event pairs. 2. An HCU creator (the first author) transforms these timelines into HCUs, historical content units, which are defined based on semantic overlap between timeline text. 3. We then create a mapping between HCUs and the source text, or more precisely, TimeML events in the source text. This mapping between HCUs and s"
P15-2137,S10-1063,0,0.0514916,"Missing"
P15-2137,N04-1019,0,0.612121,"imelines. Our evaluation relies on deep semantic units which we call historical content units. An advantage of our approach is that it does not require human annotation of new system summaries. 1 1997 June 1997 There was unrest in Albania. Fatos Nano was elected Prime Minister. Figure 1: Extract from a Wikipedia article and two lines of a corresponding timeline. We therefore opt for a “deep” method which attempts to measure to which degree a systemgenerated timeline contains semantic units found in gold-standard timelines. Our content units resemble those of van Halteren and Teufel (2003) and Nenkova and Passonneau (2004), but are larger in that they correspond to historical events. Traditional deep summarisation evaluation is expensive because it involves annotation of goldstandard summaries as well as annotation of each system summary. A major operational advantage of our approach is that we require human annotation only for gold-standard summaries, not for system summaries. After a one-time effort of creating semantic units and mapping them to the original text, the quality of a system’s content selection can be evaluated for infinitely many new system summaries for free. Our method is the following: Introd"
P15-2137,C14-1114,0,0.110096,"ed with a date (see Figure 1). A timeline is different from a standard single- or multi-document summary: Each event description is accompanied by a timestamp, and event descriptions themselves are independent linguistic units which should be understandable on their own. Additionally, a good timeline satisfies conflicting constraints: it should contain only salient events, and the overall time period considered should be covered well by events. Timeline construction is not a new task. It has been performed, for example, in a multi-document summarisation (Chieu and Lee, 2004; Yan et al., 2011; Nguyen et al., 2014) or in a single-document classification context (Chasin et al., 2013). It is crucial to reliably evaluate algorithms that create such timelines automatically. Of course, any summary can be evaluated by surface methods such as ROUGE (Lin, 2004). But even for traditional summaries, ROUGE-based evaluation has been criticised for being too shallow, and it is even less adequate for timelines, because of their special properties described above. 1. Ask timeline writers to create timelines with a fixed number of date-event pairs. 2. An HCU creator (the first author) transforms these timelines into HC"
P15-2137,S10-1010,0,\N,Missing
P16-2078,W97-0703,0,0.475938,"Missing"
P16-2078,P11-1050,0,0.0569299,"Missing"
P16-2078,W03-0510,0,0.0632737,"marisation. §6 presents experimental evidence that our model of argument overlap is superior to the earlier one. Our summariser additionally beats several extractive state-of-the-art summarisers. We show that this advantage does not come from our use of lexical chains alone, but also from KvD’s incremental processing. Our second contribution concerns a new corpus of educational texts, presented in §5. Part of the reason why we prefer a genre other than news is the vexingly good performance of the lead baseline in the news genre. Traditionally, many summarisers struggled to beat this baseline (Lin and Hovy, 2003). We believe that the problem is partly due to the journalistic style, which calls for an abstract-like lead. If we want to measure the content selection ability of summarisers, alternatInput Sentences Summary Sentences parse “generate” Memory Cycle Propositions ◦ ◦ ◦ ◦ Summary Propositions attach ◦ ◦◦ ◦◦ “recall” × × × × “forget” count × × × Forgotten Propositions Figure 1: The KvD-inspired incremental summarisation model. ive data sets are needed. Satisfyingly, we find that on our corpus the lead baseline is surpassable by intelligent summarisers. 2 The KvD Model The KvD model is a cognitive"
P16-2078,W04-1013,0,0.013062,"-known lexical similarity-based single document summarisers: MEAD (Radev et al., 2004, M), TextRank (Mihalcea and Tarau, 2004, TR), and LexRank (Erkan and Radev, 2004, LR). Because the evaluation tool we use is sensitive to text length, fair evaluation demands equal length of all summaries tested. We obtain output of exactly 100 ± 2 words from each summariser by iteratively requesting longer summaries, and unless this results in a sentence break within 2 tokens of the 100-word limit, we cut the immediately longer output to exactly 100 words. 6.2 Results For automated evaluation, we use ROUGE (Lin, 2004), which evaluates a summary by comparing it against several gold standard summaries. Table 2 shows our results in terms of ROUGE1, 2, L and SU4. The metrics are based on the co-occurrence of unigrams, bigrams, longest common subsequences, and skip-bigrams (within distance of 4 and including unigrams), respectively. Our summariser outperforms all other summarisers,3 and is the only summariser that beats the lead baseline. The fact that our summariser beats D, our KvD summariser using FT14-style distributional semantics for argument overlap, is clear evidence that our method of lexical chaining"
P16-2078,C08-1018,0,0.0668336,"cycles is beneficial in itself, by comparing it to a non-incremental algorithm using the same underlying information. 1 Introduction Automatic summarisation is one of the big artificial intelligence challenges in a world of information overload. Many summarisers, mostly extractive, have been developed in recent years (Radev et al., 2004; Mihalcea and Tarau, 2004; Wong et al., 2008; Celikyilmaz and Hakkani-T¨ur, 2011). Research is moving beyond extraction in various directions: One could perform text manipulation such as compression as a separate step after extraction (Knight and Marcu, 2000; Cohn and Lapata, 2008), or alternatively, one could base a summary on an internal semantic representation such as the proposition (Lehnert, 1981; McKeown and Radev, 1995). One summarisation model that allows manipulation of semantic structures of texts was proposed by Kintsch and van Dijk (1978, henceforth KvD). It is a model of human text processing, where the text is turned into propositions and processed incrementally, sentence by sentence. The final summary is based on those propositions whose semantic participants (arguments) are wellconnected to others in the text and hence likely to be remembered by a human"
P16-2078,E14-1077,1,0.90493,"human reading the text, under the assumption of memory limitations. Such a deep model is attractive because it provides the theoretical possibility of performing inference and generalisation over propositions, even if current NLP technology only supports shallow versions of such manipulations. This gives it a clear theoretical advantage over nonpropositional extraction systems whose information units are individual words and their connections, e.g. centroids or random-walk models. We present in this paper a new KvD-based summariser that is word sense-aware, unlike our earlier implementation (Fang and Teufel, 2014). §2 explains the KvD model with respect to summarisation. §3 and §4 explain why and how we use lexical chains to model argument overlap, a phenomenon which is central to KvD-style summarisation. §6 presents experimental evidence that our model of argument overlap is superior to the earlier one. Our summariser additionally beats several extractive state-of-the-art summarisers. We show that this advantage does not come from our use of lexical chains alone, but also from KvD’s incremental processing. Our second contribution concerns a new corpus of educational texts, presented in §5. Part of the"
P16-2078,radev-etal-2004-mead,1,0.719992,"e based on lexical chains. We evaluate on a new corpus of 124 summaries of educational texts, and show that our new system outperforms the old method and several stateof-the-art non-proposition-based summarisers. The experiment also verifies that the incremental nature of memory cycles is beneficial in itself, by comparing it to a non-incremental algorithm using the same underlying information. 1 Introduction Automatic summarisation is one of the big artificial intelligence challenges in a world of information overload. Many summarisers, mostly extractive, have been developed in recent years (Radev et al., 2004; Mihalcea and Tarau, 2004; Wong et al., 2008; Celikyilmaz and Hakkani-T¨ur, 2011). Research is moving beyond extraction in various directions: One could perform text manipulation such as compression as a separate step after extraction (Knight and Marcu, 2000; Cohn and Lapata, 2008), or alternatively, one could base a summary on an internal semantic representation such as the proposition (Lehnert, 1981; McKeown and Radev, 1995). One summarisation model that allows manipulation of semantic structures of texts was proposed by Kintsch and van Dijk (1978, henceforth KvD). It is a model of human te"
P16-2078,J02-4004,0,0.0514378,"Missing"
P16-2078,C08-1124,0,0.0269861,"w corpus of 124 summaries of educational texts, and show that our new system outperforms the old method and several stateof-the-art non-proposition-based summarisers. The experiment also verifies that the incremental nature of memory cycles is beneficial in itself, by comparing it to a non-incremental algorithm using the same underlying information. 1 Introduction Automatic summarisation is one of the big artificial intelligence challenges in a world of information overload. Many summarisers, mostly extractive, have been developed in recent years (Radev et al., 2004; Mihalcea and Tarau, 2004; Wong et al., 2008; Celikyilmaz and Hakkani-T¨ur, 2011). Research is moving beyond extraction in various directions: One could perform text manipulation such as compression as a separate step after extraction (Knight and Marcu, 2000; Cohn and Lapata, 2008), or alternatively, one could base a summary on an internal semantic representation such as the proposition (Lehnert, 1981; McKeown and Radev, 1995). One summarisation model that allows manipulation of semantic structures of texts was proposed by Kintsch and van Dijk (1978, henceforth KvD). It is a model of human text processing, where the text is turned into"
P16-2078,W04-3252,0,\N,Missing
P97-1072,J98-2001,1,\N,Missing
radev-etal-2004-mead,W02-0404,1,\N,Missing
radev-etal-2004-mead,H01-1056,1,\N,Missing
radev-etal-2004-mead,W00-1009,1,\N,Missing
radev-etal-2004-mead,P02-1040,0,\N,Missing
rupp-etal-2008-language,W07-1008,1,\N,Missing
rupp-etal-2008-language,W06-1613,1,\N,Missing
rupp-etal-2008-language,P07-2012,1,\N,Missing
rupp-etal-2008-language,P06-4020,0,\N,Missing
rupp-etal-2008-language,J96-2004,0,\N,Missing
rupp-etal-2008-language,N07-1040,1,\N,Missing
rupp-etal-2008-language,W06-2718,1,\N,Missing
saggion-etal-2002-developing,J93-1004,0,\N,Missing
saggion-etal-2002-developing,W97-0703,0,\N,Missing
saggion-etal-2002-developing,A00-2035,0,\N,Missing
saggion-etal-2002-developing,W00-0408,0,\N,Missing
saggion-etal-2002-developing,E99-1011,0,\N,Missing
saggion-etal-2002-developing,W97-0704,0,\N,Missing
saggion-etal-2002-developing,W00-0403,1,\N,Missing
saggion-etal-2002-developing,grover-etal-2000-lt,0,\N,Missing
saggion-etal-2002-developing,J96-2004,0,\N,Missing
saggion-etal-2002-developing,I05-2047,0,\N,Missing
saggion-etal-2002-developing,W01-0100,0,\N,Missing
shutova-teufel-2010-metaphor,J91-1003,0,\N,Missing
shutova-teufel-2010-metaphor,C88-1081,0,\N,Missing
shutova-teufel-2010-metaphor,T87-1040,0,\N,Missing
teufel-van-halteren-2004-agreement,saggion-etal-2002-developing,1,\N,Missing
teufel-van-halteren-2004-agreement,W03-0508,1,\N,Missing
teufel-van-halteren-2004-agreement,E99-1011,0,\N,Missing
teufel-van-halteren-2004-agreement,W02-0406,0,\N,Missing
teufel-van-halteren-2004-agreement,C96-2166,0,\N,Missing
W00-1302,J96-2004,0,0.0344084,"sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P ( A ) for chance agreement P(E): Automatic Zoning Argumentative • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. K = P{A)-P(E) 1-P(Z) • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &quot;method&quot; and &quot;introduction&quot;). We model this with syntactic features. Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement."
W00-1302,J97-1003,0,0.0622635,"In this paper, we will describe the algorithm of an argumentative zoner. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as trMning material in our statistical classifier. 2 As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e measured stabi"
W00-1302,W98-1123,0,0.0281008,"we will describe the algorithm of an argumentative zoner. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as trMning material in our statistical classifier. 2 As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e measured stability (the degree to"
W00-1302,P98-1112,0,0.0310699,"cture. For example, PRESENTATION..ACTIONS include communication verbs like &quot;present&quot;, &quot;report&quot;, &quot;state&quot; (Myers, 1992; Thompson and Yiyun, 1991), RESEARCH_ACTIONS include &quot;analyze&quot;, &quot;conduct&quot; and &quot;observe&quot;, and ARGUMENTATION_ACTIONS &quot;argue&quot;, &quot;disagree&quot;, &quot;object to&quot;. Domain-specific actions are contained in the classes indicating a problem ( &quot;.fail&quot;, &quot;degrade&quot;, &quot;overestimate&quot;), and solution-contributing actions (&quot; &quot;circumvent&apos;, solve&quot;, &quot;mitigate&quot;). The main reason for using a hand-crafted, genre-specific lexicon instead of a general resource such as WordNet or Levin&apos;s (1993) classes (as used in Klavans and Kan (1998)), was to avoid polysemy problems without having to perform word sense disambiguation. Verbs in our texts often have a specialized meaning in the domain of scientific argumentation, which our lexicon readily encodes. We did notice some ambiguity problems (e.g. &quot;follow&quot; can mean following another approach, or it can mean follow in a sense having nothing to do with presentation of research, e.g. following an arc in an algorithm). In a wider domain, however, ambiguity would be a much bigger problem. Processing of the articles includes transformation from I~TEX into XML format, recognition of form"
W00-1302,W97-0713,0,0.0831919,"Missing"
W00-1302,E99-1015,1,0.837382,"f Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P ( A ) for chance agreement P(E): Automatic Zoning Argumentative • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. K = P{A)-P(E) 1-P(Z) • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (19"
W00-1302,J94-2004,0,0.550987,"Missing"
W00-1302,J91-1002,0,0.130686,"our argumentative zones. In this paper, we will describe the algorithm of an argumentative zoner. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as trMning material in our statistical classifier. 2 As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e"
W00-1302,P99-1046,0,0.0136567,"algorithm of an argumentative zoner. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as trMning material in our statistical classifier. 2 As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: 3 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). W e measured stability (the degree to which the same an"
W00-1302,C98-1108,0,\N,Missing
W03-0508,W00-0408,0,0.0180862,"summaries are collected (in the range of at least 30-40 summaries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking. 1 Introduction It is an understatement to say that measuring the quality of summaries is hard. In fact, there is unanimous consensus in the summarisation community that evaluation of summaries is a monstrously difficult task. In the past years, there has been quite a lot of summarisation work that has effectively aimed at finding viable evaluation strategies (Sp¨arck Jones, 1999; Jing et al., 1998; Donaway et al., 2000). Largescale conferences like SUMMAC (Mani et al., 1999) and DUC (2002) have unfortunately shown weak results in that current evaluation measures could not distinguish between automatic summaries – though they are effective enough to distinguish them from human-written summaries. In principle, the best way to evaluate a summary is to try to perform the task for which the summary was meant in the first place, and measure the quality of the summary on the basis of degree of success in executing the task. However, such extrinsic evaluations are so time-consuming to set up that they cannot be used"
W03-0508,W02-0406,0,0.35916,"ting relevant documents. While relevance judgements between humans remain different, Voorhees (2000) shows that the relative rankings of systems are nevertheless stable across annotators, which means that meaningful IR measures have been found despite the inherent subjectivity of relevance judgements. Similarly, in MT, the recent Bleu measure also uses the idea that one gold standard is not enough. In an experiment, Papineni et al. (2001) based an evaluation on a collection of four reference translations of 40 general news stories and showed the evaluation to be comparable to human judgement. Lin and Hovy (2002) examine the use of a multiple gold standard for summarisation evaluation, and conclude “we need more than one model summary although we cannot estimate how many model summaries are required to achieve reliable automated summary evaluation”. We explore the differences and similarities between various human summaries in order to create a basis for such an estimate, and as a side-effect, also re-examine the degree of difference between the use of a single summary gold standard and the use of a compound gold standard. 1.2 Similarity measures The second aspect we examine is the similarity measure"
W03-0508,W01-0100,0,0.0431026,"upon at least two hard subtasks: selection of information and production of new text. Both tasks are known from various NLP fields (e.g. information retrieval and information extraction for selection; generation and machine translation (MT) for production) to be not only hard to execute, but also hard to evaluate. This is caused for a large part by the fact that in both cases there is no single “best” result, but rather various “good” results. It is hence no wonder that the evaluation of summarisation, combining these two, is even harder. The general approach for intrinsic evaluations, then (Mani, 2001), is to separate the evaluation of the form of the text (quality) and its information content (informativeness). In this paper, we will focus on the latter, the intrinsic evaluation of informativeness, and we will address two aspects: the (in)sufficiency of the single human summary to measure against, and the information unit on which similarity measures are based. 1.1 Gold standards In various NLP fields, such as POS tagging, systems are tested by way of comparison against a “gold standard”, a manually produced result which is supposed to be the “correct”, “true” or “best” result. This presup"
W03-0508,E99-1011,0,0.0625568,"maries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking. 1 Introduction It is an understatement to say that measuring the quality of summaries is hard. In fact, there is unanimous consensus in the summarisation community that evaluation of summaries is a monstrously difficult task. In the past years, there has been quite a lot of summarisation work that has effectively aimed at finding viable evaluation strategies (Sp¨arck Jones, 1999; Jing et al., 1998; Donaway et al., 2000). Largescale conferences like SUMMAC (Mani et al., 1999) and DUC (2002) have unfortunately shown weak results in that current evaluation measures could not distinguish between automatic summaries – though they are effective enough to distinguish them from human-written summaries. In principle, the best way to evaluate a summary is to try to perform the task for which the summary was meant in the first place, and measure the quality of the summary on the basis of degree of success in executing the task. However, such extrinsic evaluations are so time-consuming to set up that they cannot be used for the day-to-day evaluation needed during system deve"
W03-0508,2001.mtsummit-papers.68,0,0.138776,"ivity of relevance judgements is circumvented by extensive sampling: many different queries are collected to level out the difference humans have in suggesting queries and in selecting relevant documents. While relevance judgements between humans remain different, Voorhees (2000) shows that the relative rankings of systems are nevertheless stable across annotators, which means that meaningful IR measures have been found despite the inherent subjectivity of relevance judgements. Similarly, in MT, the recent Bleu measure also uses the idea that one gold standard is not enough. In an experiment, Papineni et al. (2001) based an evaluation on a collection of four reference translations of 40 general news stories and showed the evaluation to be comparable to human judgement. Lin and Hovy (2002) examine the use of a multiple gold standard for summarisation evaluation, and conclude “we need more than one model summary although we cannot estimate how many model summaries are required to achieve reliable automated summary evaluation”. We explore the differences and similarities between various human summaries in order to create a basis for such an estimate, and as a side-effect, also re-examine the degree of diff"
W03-0508,saggion-etal-2002-developing,1,0.885112,"Missing"
W03-0508,C96-2166,0,0.0128641,"Missing"
W03-0508,P02-1040,0,\N,Missing
W04-3254,W02-0406,0,0.0988307,"Missing"
W04-3254,W01-0100,0,0.0842462,"rformed in SUMMAC (Mani et al., 1999) followed that strategy. However, extrinsic evaluations are time-consuming to set up and can thus not be used for the day-to-day evaluation needed during system development. So in practice, a method for intrinsic evaluation is needed, where the properties of the summary itself are examined, independently of its application. Intrinsic evaluation of summary quality is undeniably hard, as there are two subtasks of summarisation which need to be evaluated, information selection and text production — in fact these two subtasks are often separated in evaluation (Mani, 2001). If we restrict our attention to information selection, systems are tested by way of comparison against a “gold standard”, a Hans van Halteren Language and Speech University of Nijmegen, The Netherlands manually produced result which is supposed to be the “correct”, “true” or “best” result. In summarisation there appears to be no “one truth”, but rather various “good” results. Human subjectivity in what counts as the most important information is high. This is evidenced by low agreement on sentence selection tasks (Rath et al., 1961; Jing et al., 1998), and low word overlap measures in the ta"
W04-3254,E99-1011,0,0.0246115,"s paper, we show that factoid annotation is highly reproducible, introduce a weighted factoid score, estimate how many summaries are required for stable system rankings, and show that the factoid scores cannot be sufficiently approximated by unigrams and the DUC information overlap measure. 1 Introduction Many researchers in summarisation believe that the best way to evaluate a summary is extrinsic evaluation (Sp¨arck Jones, 1999): to measure the quality of the summary on the basis of degree of success in executing a specific task with that summary. The summary evaluation performed in SUMMAC (Mani et al., 1999) followed that strategy. However, extrinsic evaluations are time-consuming to set up and can thus not be used for the day-to-day evaluation needed during system development. So in practice, a method for intrinsic evaluation is needed, where the properties of the summary itself are examined, independently of its application. Intrinsic evaluation of summary quality is undeniably hard, as there are two subtasks of summarisation which need to be evaluated, information selection and text production — in fact these two subtasks are often separated in evaluation (Mani, 2001). If we restrict our atten"
W04-3254,N04-1019,0,0.52991,"text k n P(A) 2 2 .91 2 2 .94 P(E) .69 .67 Figure 3: Agreement of factoid definition. One of us then used the Kuwait consensus agreement to annotate the 16 machine summaries for that text which were created by different participants in DUC-2002, an annotation which could be done rather quickly. However, a 150 100 Average number of factoids 50 0 to suggest such identities and subsumption relations. We then calculate Kappa at Phases 1 and 2. It is not trivial to define what an ’item’ in the Kappa calculation should be. Possibly the use of Krippendorff’s alpha will provide a better approach (cf. Nenkova and Passonneau (2004)), but for now we measure using the better-known kappa, in the following way: For each equivalence between factoids A and C, create items { A – C – s |s ∈ S } (where S is the set of all summaries). For each factoid A subsumed by a set B of factoids, create items { A ← b – s |b ∈ B, s ∈ S}. For example, given 5 summaries a, b, c, d, e, Annotator A1 assigns P30 to summaries a, c and e. Annotator A2 (who has split P30 into F9.21 and F9.22), assigns a to F9.21 and c and e to F9.22. This creates the 10 items for Kappa calculation given in Figure 2. Results for our data set are given in Figure 3. Fo"
W04-3254,2001.mtsummit-papers.68,0,0.0318577,"Missing"
W04-3254,W03-0508,1,0.752793,"Missing"
W04-3254,P02-1040,0,\N,Missing
W06-0804,N06-1050,1,0.654904,"Missing"
W06-1312,N06-1050,1,0.85381,"Missing"
W06-1312,E99-1015,1,0.884703,"Missing"
W06-1312,W06-1613,1,0.855471,"Missing"
W06-1312,P84-1044,0,0.676122,"Missing"
W06-1613,P06-1116,1,0.62025,"Missing"
W06-1613,J96-2004,0,0.0803218,"constrained to the paragraph boundary. In rare cases, paper-wide information is required (e.g., for PMot, we need to know that a praised approach is used by the authors, information which may not be local in the paragraph). Annotators are thus asked to skim-read the paper before annotation. One possible view on this annotation scheme could consider the first two sets of categories as “negative” and the third set of categories “positive”, in the sense of Pang et al. (2002) and Turney (2002). Authors need to make a point (namely, 3 As opposed to reference list items, which are fewer. Following Carletta (1996), we measure agreement in (E) Kappa, which follows the formula K = P (A)−P where 1−P (E) P(A) is observed, and P(E) expected agreement. Kappa ranges between -1 and 1. K=0 means agreement is only as expected by chance. Generally, Kappas of 0.8 are considered stable, and Kappas of .69 as marginally stable, according to the strictest scheme applied in the field. 4 2 Our citation processor can recognise these after parsing the citation list. 106 authors of the paper, and everybody else) are modelled by 185 patterns. For instance, in a paragraph describing related work, we expect to find references"
W06-1613,J02-4002,1,0.433414,": Distribution of citation categories Weak CoCoGM CoCoR0 P .78 .81 .77 R .49 .52 .46 F .60 .64 .57 Percentage Accuracy 0.77 Kappa (n=12; N=2829; k=2) 0.57 Macro-F 0.57 CoCo.56 .19 .28 CoCoXY .72 .54 .62 PBas .76 .46 .58 PUse .66 .61 .63 PModi .60 .27 .37 PMot .75 .64 .69 PSim .68 .38 .48 PSup .83 .32 .47 Neut .80 .92 .86 Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3). that recorded the presence of cues that our annotators associated with a particular class. 3.3 P R F Other features There are other features which we use for this task. We know from Teufel and Moens (2002) that verb tense and voice should be useful for recognizing statements of previous work, future work and work performed in the paper. We also recognise modality (whether or not a main verb is modified by an auxiliary, and which auxiliary it is). The overall location of a sentence containing a reference should be relevant. We observe that more PMot categories appear towards the beginning of the paper, as do Weak citations, whereas comparative results (CoCoR0, CoCoR-) appear towards the end of articles. More fine-grained location features, such as the location within the paragraph and the sectio"
W06-1613,W06-1312,1,0.843214,"Missing"
W06-1613,P02-1053,0,0.00484051,"Missing"
W06-1613,P84-1044,0,0.052741,"ve been created over the years, and the question has been studied in detail, even to the level of in-depth interviews with writers about each individual citation (Hodges, 1972). Part of this sustained interest in citations can be explained by the fact that bibliometric metrics are commonly used to measure the impact of a researcher’s work by how often they are cited (Borgman, 1990; Luukkonen, 1992). However, researchers from the field of discourse studies have long criticised purely quantitative citation analysis, pointing out that many citations are done out of “politeness, policy or piety” (Ziman, 1968), and that criticising citations or citations in pass103 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103–110, c Sydney, July 2006. 2006 Association for Computational Linguistics Li and Abe 96 Brown et al. 90a Resnik 95 Church and Gale 91 Rose et al. 90 Dagan et al. 94 Hindle 93 Hindle 90 Nitta and Niwa 94 Dagan et al 93 Pereira et al. 93 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Followi"
W06-1613,W02-1011,0,0.0180982,"Missing"
W06-1613,N06-1050,1,0.718252,"Missing"
W06-1613,W06-0804,1,0.742995,"Missing"
W06-1613,H91-1061,0,\N,Missing
W07-1008,W04-1213,0,0.0286041,"Kemp and Lynch, 1998; Sun et al., 2007) or classification (Wilbur et al., 1999) systems have been published. A plugin for the GATE system3 will also recognise a limited range of chemical entities. Other named entity recognition or classification systems (Narayanaswamy et al., 2003; Torii et al., 2004; Torii and Vijay-Shanker, 2002; Spasic and Ananiadou, 2004) sometimes include chemicals as well as genes, proteins and other biological entities. However, due to differences in corpora and the scope of the task, it is difficult to compare them. There has been no chemical equivalent of the JNLPBA (Kim et al., 2004) or BioCreAtIvE (Yeh et al., 2005) evaluations. Therefore, a corpus and a task definition are required. To find an upper bound on the levels of performance that are available for the task, it is necessary to study the inter-annotator agreement for the manual annotation of the texts. In particular, it is useful to see to what extent the guidelines can be applied by those not involved in their development. Producing guidelines that enable a highly consistent annotation may raise the quality of the results of any machinelearning techniques that use training data applied to the guidelines, and pro"
W07-1008,N04-2002,0,0.114919,"nd Murray-Rust, 2006). With information extraction techniques, chemicals could be linked to their properties, applications and reactions, and with traditional gene/protein NLP techniques, it could be pos1 2 http://pubchem.ncbi.nlm.nih.gov/ http://www.projectprospect.org/ 57 BioNLP 2007: Biological, translational, and clinical language processing, pages 57–64, c Prague, June 2007. 2007 Association for Computational Linguistics sible to discover new links between chemical data and bioinformatics data. A few chemical named entity recognition (Corbett and Murray-Rust, 2006; Townsend et al., 2005; Vasserman, 2004; Kemp and Lynch, 1998; Sun et al., 2007) or classification (Wilbur et al., 1999) systems have been published. A plugin for the GATE system3 will also recognise a limited range of chemical entities. Other named entity recognition or classification systems (Narayanaswamy et al., 2003; Torii et al., 2004; Torii and Vijay-Shanker, 2002; Spasic and Ananiadou, 2004) sometimes include chemicals as well as genes, proteins and other biological entities. However, due to differences in corpora and the scope of the task, it is difficult to compare them. There has been no chemical equivalent of the JNLPBA"
W07-1008,W06-3328,0,0.0259304,"olysemous words are less likely to be a problem here, the problems with tokenisation and large numbers of unknown words remain just as pressing. As with biomedical text (Yeh et al., 2005), cases of conjunctive and disjunctive nomenclature, such as ‘benzoic and thiophenic acids’ and ‘bromo- or chlorobenzene’ exist in the corpus. However, these only accounted for 27 CM entities. 5 Named-Entity Recognition To establish some baseline measures of performance, we applied the named-entity modules from the toolkit LingPipe,4 which has been successfully applied to NER of D. melanogaster genes (e.g. by Vlachos and Gasperin (2006)). LingPipe uses a first-order HMM, using an enriched tagset that marks not only the positions of the named entities, but the tokens in front of and behind them. Two different strategies are employed for handling unknown tokens. The first (the TokenShapeChunker) replaces unknown or rare tokens with a morphologicallybased classification. The second, newer module (the CharLmHmmChunker) estimates the probability of an observed word given a tag using language models based on character-level grams. The LingPipe developers suggest that the TokenShapeChunker typically outperforms the 4 http://www.ali"
W07-1530,W05-0305,1,0.887762,"Missing"
W07-1530,J93-2004,0,0.0279621,"Missing"
W07-1530,W04-2703,1,0.832896,"Missing"
W07-1530,W04-0212,1,0.90064,"Missing"
W07-1530,J05-2005,0,0.0850358,"Missing"
W07-1530,W04-0213,1,0.81694,"d projects/muc/. 2 The Automated Content Extraction program, www.nist.gov/speech/tests/ace/. 192 genre-specific corpus of German newspaper commentaries, taken from the daily papers M¨arkische Allgemeine Zeitung and Tagesspiegel. One central aim is to provide a tool for studying mechanisms of argumentation and how they are reflected on the linguistic surface. The corpus on the one hand is a collection of “raw” data, which is used for genreoriented statistical explorations. On the other hand, we have identified two sub-corpora that are subject to a rich multi-level annotation (MLA). The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (1215 sentences), with 33.000 tokens in total. The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate3 tool. In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007)) and rhetorical structure according to RST (Mann and Thompson, 1988). Our annotation software architecture consists of a variety of standard, external tools that can be u"
W07-1530,J02-4002,1,0.73024,"a single, very complex annotation step; • end up with less ambiguity in the annotations, since the reasons for specific decisions can be made explicit (by annotations on “simpler” levels); • be more explicit than a single tree can be: if a discourse fulfills, for example, a function both for thematic development and for the writer’s intention, they can both be accounted for; • provide the central information that a “traditional” rhetorical tree conveys, without loosing essential information. 5 AZ Corpus (Simone Teufel, Cambridge) The Argumentative Zoning (AZ) annotation scheme (Teufel, 2000; Teufel and Moens, 2002) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teu"
W07-1530,E99-1015,1,0.662031,"02) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function"
W07-1530,W06-1312,1,0.772883,"sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function would be “motivate that the method used is sound”. The annotation scheme contains 12 functions, clustered into “superiority”, “neutral comparison/contrast”, “praise or usage” and “neutral”. One type of research we hope to do in the future is to study the relationship between these rhetorical phonemena with more tradi"
W12-4303,P11-1051,0,0.309055,"c it and the lexical hook METEOR. Most current techniques, with the exception of Qazvinian and Radev (2010), are not able to detect linguistic mentions of citations in such forms. Ignoring such mentions and examining only the sentences contain18 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18–26, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics ing an explicit citation results in loss of information about the cited paper. While this phenomenon is problematic for applications like scientific summarisation (Abu-Jbara and Radev, 2011), it has a particular relevance for citation sentiment detection (Athar, 2011). Citation sentiment detection is an attractive task. Availability of citation polarity information can help researchers in understanding the evolution of a field on the basis of research papers and their critiques. It can also help expert researchers who are in the process of preparing opinion based summaries for survey papers by providing them with motivations behind as well as positive and negative comments about different approaches (Qazvinian and Radev, 2008). Current work on citation sentiment detection works u"
W12-4303,P11-3015,1,0.927547,"nd Radev (2010), are not able to detect linguistic mentions of citations in such forms. Ignoring such mentions and examining only the sentences contain18 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18–26, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics ing an explicit citation results in loss of information about the cited paper. While this phenomenon is problematic for applications like scientific summarisation (Abu-Jbara and Radev, 2011), it has a particular relevance for citation sentiment detection (Athar, 2011). Citation sentiment detection is an attractive task. Availability of citation polarity information can help researchers in understanding the evolution of a field on the basis of research papers and their critiques. It can also help expert researchers who are in the process of preparing opinion based summaries for survey papers by providing them with motivations behind as well as positive and negative comments about different approaches (Qazvinian and Radev, 2008). Current work on citation sentiment detection works under the assumption that the sentiment present in the citation sentence repres"
W12-4303,W97-0703,0,0.391237,"Missing"
W12-4303,J08-1001,0,0.0135882,"Missing"
W12-4303,P10-1089,0,0.0236024,"Missing"
W12-4303,bird-etal-2008-acl,0,0.0277387,"Missing"
W12-4303,P07-1056,0,0.0184051,"cation include work by Wilbur et al. (2006), who annotated a 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources and NLP tools. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007), which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analysis. Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010). Kaplan et al. (2009) explor"
W12-4303,councill-etal-2008-parscit,0,0.0939749,"the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment. 1 Introduction The idea of using citations as a source of information has been explored extensively in the field of bibliometrics, and more recently in the field of computational linguistics. State-of-the-art citations identification mechanisms focus either on detecting explicit citations i.e. those that consist of either the author names and the year of publication or bracketed numbers only, or include a small sentence window around the explicit citation as input text (Councill et al., 2008; Radev et al., 2009; Ritchie et al., 2008). The assumption behind this approach is that all related mentions of the paper would be concentrated in the immediate vicinity of the anchor text. However, this assumption does not generally hold true (Teufel, 2010; Sugiyama et al., 2010). The phenomenon of trying to determine a citations’s citation context has a long tradition in library sciences Figure 1: Example of the use of anaphora While the first sentence cites the target paper explicitly using the name of the primary author along with the year of publication of the paper, the remaining senten"
W12-4303,W05-0408,0,0.0166236,"to citation classification include work by Wilbur et al. (2006), who annotated a 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources and NLP tools. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007), which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analysis. Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010). Kapl"
W12-4303,D08-1038,0,0.00916337,"t author’s name within a 4-word window of the year of publication. Our dataset is skewed as there are many more objective sentences than subjective ones. In such scenarios, average micro-F scores tend to be slightly higher as they are a weighted measure. To avoid this bias, we also report the macro-F scores. Furthermore, to ensure there is enough data for training each class, we use 10-fold cross-validation (Lewis, 1991) in all our experiments. We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework. The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001). For each ith sentence Si , we use the following binary features. • Si−1 contains the last name of the primary author, followed by the year of publication within a four-word window. (a) Sentence Text (b) Paper metadata Figure 3: Different views of an annotated paper. This feature is meant to capture the fact that the sentence immediately after an explicit citation is more likely to continue talking about the same work. • Si contains the last name of the primary author followed by the year of publication within a"
W12-4303,P97-1023,0,0.0699654,"tarting with a pronoun (e.g. they, their, he, she, etc.) are more likely to describe the subject citation of the previous sentence in detail. For example: “Because Daume III (2007) views the adaptation as merely augmenting the feature space, each of his features has the same prior mean and variance, regardless of whether it is domain specific or independent. He could have set these parameters differently, but he did not.” • Si starts with a connector. This feature also focuses on detecting the topic continuity. Connectors have been shown to be effective in other context related works as well (Hatzivassiloglou and McKeown, 1997; Polanyi and Zaenen, 2006). A list of 23 connectors (e.g. however, although, moreover, etc.) has been compiled by examining the high frequency connectors from a separate set of papers from the same domain. An example is: “An additional consistent edge of a linearchain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al. , 2005). However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistenc"
W12-4303,W09-3611,0,0.0341971,"topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007), which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analysis. Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010). Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (Soon et al., 2001; Ng and Cardie, 2002). However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative. For citation extraction, the most relevant work is by Qazvinian and Radev (2010) who proposed a framework of Markov Random Fields to extract only the non-explicit citations for a given paper. They 24 model each sentence as a node in a graph and experiment with various"
W12-4303,W11-1902,0,0.0221554,"re likely to be restricted to the description of an approach which the authors include in their solution, or further develop. Another aspect concerns which features might help in detecting coherent citation blocks. We have here addressed coherence of citation contexts via certain referring expressions, lexical hooks and also coherence-indicating conjunctions (amongst others). The reintroduction of citation contexts was addressed via lexical hooks. Much more could be done to explore this very interesting question. A more fine-grained model of coherence might include proper anaphora resolution (Lee et al., 2011), which is still an unsolved task for scientific texts, and also include models of lexical coherence such as lexical chains (Barzilay and Elhadad, 1997) and entity coherence (Barzilay and Lapata, 2008). 7 Conclusion In this paper, we focus on automatic detection of citation sentiment using citation context. We annotate a new large corpus and show that ignoring the citation context would result in loss of a lot of sentiment, specially criticism. We also report the results of the state-of-the-art citation sentiment detection systems on this corpus and when using this context-enhanced gold standa"
W12-4303,H91-1061,0,0.0555722,"n in any form (o n p). To make sure that the easily detectable explicit citations do not influence the results, we change the class label of all those sentences to x which contain the first author’s name within a 4-word window of the year of publication. Our dataset is skewed as there are many more objective sentences than subjective ones. In such scenarios, average micro-F scores tend to be slightly higher as they are a weighted measure. To avoid this bias, we also report the macro-F scores. Furthermore, to ensure there is enough data for training each class, we use 10-fold cross-validation (Lewis, 1991) in all our experiments. We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework. The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001). For each ith sentence Si , we use the following binary features. • Si−1 contains the last name of the primary author, followed by the year of publication within a four-word window. (a) Sentence Text (b) Paper metadata Figure 3: Different views of an annotated paper. This feature is meant to capture the fact that the sent"
W12-4303,P02-1014,0,0.0462912,"ce citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analysis. Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010). Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (Soon et al., 2001; Ng and Cardie, 2002). However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative. For citation extraction, the most relevant work is by Qazvinian and Radev (2010) who proposed a framework of Markov Random Fields to extract only the non-explicit citations for a given paper. They 24 model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes weighted by lexical similarity between nodes. However, their dataset consists of only 569 citations from 10 papers and their annotation sche"
W12-4303,C08-1087,0,0.161014,"matic for applications like scientific summarisation (Abu-Jbara and Radev, 2011), it has a particular relevance for citation sentiment detection (Athar, 2011). Citation sentiment detection is an attractive task. Availability of citation polarity information can help researchers in understanding the evolution of a field on the basis of research papers and their critiques. It can also help expert researchers who are in the process of preparing opinion based summaries for survey papers by providing them with motivations behind as well as positive and negative comments about different approaches (Qazvinian and Radev, 2008). Current work on citation sentiment detection works under the assumption that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper (Athar, 2011; Piao et al., 2007; Pham and Hoffmann, 2004). This assumption is so dominant because current citation identification methods (Councill et al., 2008; Ritchie et al., 2008; Radev et al., 2009) can readily identify the citation sentence, whereas it is much harder to determine the relevant context. However, this assumption most certainly does not hold true when the citation context spans more t"
W12-4303,P10-1057,0,0.550755,"2010). The phenomenon of trying to determine a citations’s citation context has a long tradition in library sciences Figure 1: Example of the use of anaphora While the first sentence cites the target paper explicitly using the name of the primary author along with the year of publication of the paper, the remaining sentences mentioning the same paper appear after a gap and contain an indirect and implicit reference to that paper. These mentions occur two sentences after the formal citation in the form of anaphoric it and the lexical hook METEOR. Most current techniques, with the exception of Qazvinian and Radev (2010), are not able to detect linguistic mentions of citations in such forms. Ignoring such mentions and examining only the sentences contain18 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18–26, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics ing an explicit citation results in loss of information about the cited paper. While this phenomenon is problematic for applications like scientific summarisation (Abu-Jbara and Radev, 2011), it has a particular relevance for citation sentiment detection (Athar, 2011). C"
W12-4303,J01-4004,0,0.0285019,"d on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analysis. Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010). Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (Soon et al., 2001; Ng and Cardie, 2002). However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative. For citation extraction, the most relevant work is by Qazvinian and Radev (2010) who proposed a framework of Markov Random Fields to extract only the non-explicit citations for a given paper. They 24 model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes weighted by lexical similarity between nodes. However, their dataset consists of only 569 citations from 10 papers and"
W12-4303,W06-1613,1,0.749655,"the citation context spans more than one sentence. Concerning the sentiment aspect of the citation context from Figure 1, we see that the citation sentence does not contain any sentiment towards the cited paper, whereas the following sentences act as a critique and list its shortcomings. It is clear that criticism is the intended sentiment, but if the gold standard is defined by looking at the citation sentence in isolation, a significant amount of sentiment expressed in the text is lost. Given that overall most citations in a text are neutral with respect to sentiment (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it even more important to recover what explicit sentiment there is in the article, wherever it is to be found. In this paper, we examine methods to extract all opinionated sentences from research papers which mention a given paper in as many forms as we can identify, not just as explicit citations. We present a new corpus in which all mentions of a cited paper 19 have been manually annotated, and show that our annotation treatment increases citation sentiment coverage, particularly for negative sentiment. We then explore methods to automatically identify all mentions of a paper in"
W12-4303,P02-1053,0,0.00235906,"nt detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Other approaches to citation classification include work by Wilbur et al. (2006), who annotated a 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources and NLP tools. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007), which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author’s reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a “lower level” of analy"
W12-4303,W03-1017,0,\N,Missing
W17-5103,C16-1324,0,0.052201,"Missing"
W17-5103,W15-0501,0,0.102812,"ing was measured at K=0.69 (N =9879; n=4, k=2) and source agreement is given in Table 5. The baseline results are given in Table 6. The Near29 non-agreed links still share linking structure. We observe that often the same set of source spans are linked to some destination span, although the destination itself is different across annotators. Our agreement metrics are thus underestimating the degree of shared linkage structure. 6 of a text. There has also been some recent work on agreement metrics for argument relations. As far as agreement on detection of argumentative components is concerned, Kirschner et al. (2015) point out that measures such as kappa and F1 score may cause some inappropriate penalty for slight differences of annotation between annotators, and proposed a graph-based metric based on pair-wise comparison of predefined argument components. This particular metric, while addressing some of the problems of kappa and F1, is not directly applicable to our annotation where annotators can freely chose the beginnings and ends of spans. Duthie et al. (2007) introduce CASS, a further, very recent adaptation of the metric by Kirschner et al. that can deal with disagreement in segmentation. However,"
W17-5103,C14-1142,0,0.0602475,"tation detection, achieving interannotator agreement of K=0.75 in Cohen’s Kappa (ECHR). On a genre other than legal text, Faulkner (2014) annotated student essays using three tags (“for”, “against” and “neither”), reaching interannotator agreement of K=0.70 (Cohen). As far as the rhetorical status classification part of our scheme is concerned, the closest approach to ours is Al Khatib et al. (2016), but they do not employ any explicit links, and they work on a different genre (news editorials). A task related to our linking steps is the determination of relations between argument components. Stab and Gurevych (2014) annotated argumentative relations (support and attack) in essays; they reported inter-annotator agreement of K=0.81 (Fleiss) for both support and attack. Hua and Wang (2017) proposed an annotation scheme for labeling sentence-level supporting arguments relations with four types (STUDY, FACTUAL, OPINION, REASONING). Their results for argument type classification are as follows: K=0.61 for STUDY, K=0.75 for FACTUAL, K=0.71 for OPINION, and K=0.29 for REASONING. However, these two relation-based studies discover only one aspect of argument structure, whereas our combination of linking tasks and"
W17-5103,J02-4002,1,0.549369,"egligent or not”. While full Issue Topic and FRAMING linking annotation would allow very sophisticated summaries, our fall-back strategy is to create simpler summaries using only the final conclusion and the main supporting text such as judicial decisions. For these simple summaries, performing only automatic rhetorical status classification is sufficient. 2.3 Rhetorical structure To exploit the idea that argument structure is a crucial aspect for legal summarization, we take a rhetorical status based approach. This method was originally defined for the summarization of scientific articles by Teufel and Moens (2002), but later studies such as Hachey and Grover (2006) applied the rhetorical status approach to the legal text domain for the context of English law. Hachey and Grover defined the following seven labels: In English law, the judgment first states the facts and events, corresponding to category “FACT”. “PROCEEDINGS” labels sentences which restate the details of previous judgments in lower courts. “BACKGROUND” is the label for quotations or citations of law materials which Law Lords use to discuss precedents and legislation. “FRAMING” is a rhetorical role that captures all aspects of the Law Lord’"
W17-5103,P17-2032,0,0.0197756,"(“for”, “against” and “neither”), reaching interannotator agreement of K=0.70 (Cohen). As far as the rhetorical status classification part of our scheme is concerned, the closest approach to ours is Al Khatib et al. (2016), but they do not employ any explicit links, and they work on a different genre (news editorials). A task related to our linking steps is the determination of relations between argument components. Stab and Gurevych (2014) annotated argumentative relations (support and attack) in essays; they reported inter-annotator agreement of K=0.81 (Fleiss) for both support and attack. Hua and Wang (2017) proposed an annotation scheme for labeling sentence-level supporting arguments relations with four types (STUDY, FACTUAL, OPINION, REASONING). Their results for argument type classification are as follows: K=0.61 for STUDY, K=0.75 for FACTUAL, K=0.71 for OPINION, and K=0.29 for REASONING. However, these two relation-based studies discover only one aspect of argument structure, whereas our combination of linking tasks and a rhetorical status classification task means that we address the global hierarchical argument structure 7 Discussion and future work It is hard to evaluate a newly defined,"
W17-5103,W16-2805,0,\N,Missing
W97-1301,J86-3001,0,0.127727,"Missing"
W97-1301,P97-1072,1,0.3529,"Missing"
W98-0307,W97-0704,0,0.0449097,"Missing"
W98-0307,W97-0710,1,0.714813,"ymbol [REF], which is a signal for mentions of other researchers&apos; solutions, tasks or problems). The computational linguistics corpus was drawn from the computation and language archive (h~p://xxx.lanl.gov/cmp-lg) and contains 123 articles; the 129 articles of the cardiology corpus appeared in the American Heart Journal. The medical corpus is smaller in overall size (436,909 words vs. 654,477; 14,770 sentences vs. 23,072). For the computational linguistics corpus, we additionally had a collection of 948 sentences that had been identified as relevant by a human annotator in a prior experiment (Teufel & Moens 1997). A human judge annotated these with respect to the 23 rhetorical moves introduced in Figure 8. 4.1 Filtering First, we compiled the two corpora into those bigrams, trigrams, 4-grams, 5-grams and 6-grams which did not cross sentence boundaries. We worked with a short stop-list compiled from the corpus (60 highest-frequent words) from which we had excluded those which we expected to be important in an argumentative domain, e.g. personal and demonstrative pronouns. We lowercased all words and counted punctuation (including brackets) as a full word. We then filtered the n-grams through our seed l"
W99-0311,J96-2004,0,0.0969676,"Missing"
W99-0311,W97-0713,0,0.0345083,"ed or carelessannotation. W e therefore needed an annotation scheme which is simple enough to be usable in a stable and intuitive way for several annotators. This paper also reports on how we tested the stabilityof the annotation scheme we developed. A second design criterion for our annotation scheme was that we wanted the roles to be annotated automatically. This paper reports on preliminary results which show that the annotation process can indeed be automated. To summarise, we have argued that discourse structure information will improve summarisation. Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly,although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987). In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. Our categories are not hierarchical, and they are much less fine-grained than RST-relations. As mentioned above, we wanted them to a) provide context information for flexiblesummarisation, b) provide a higher degree of comparability between papers, and c)"
W99-0311,C94-1056,0,0.0132545,"tilllead to a biased or carelessannotation. W e therefore needed an annotation scheme which is simple enough to be usable in a stable and intuitive way for several annotators. This paper also reports on how we tested the stabilityof the annotation scheme we developed. A second design criterion for our annotation scheme was that we wanted the roles to be annotated automatically. This paper reports on preliminary results which show that the annotation process can indeed be automated. To summarise, we have argued that discourse structure information will improve summarisation. Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly,although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987). In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. Our categories are not hierarchical, and they are much less fine-grained than RST-relations. As mentioned above, we wanted them to a) provide context information for flexiblesummarisation, b) provide a higher degree of comparability between"
W99-0311,J97-1002,0,\N,Missing
