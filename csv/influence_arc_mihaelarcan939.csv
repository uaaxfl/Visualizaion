2013.mtsummit-posters.1,W11-2107,0,0.0170933,"guee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), w"
2013.mtsummit-posters.1,W11-1204,0,0.0469957,"Missing"
2013.mtsummit-posters.1,2005.mtsummit-papers.11,0,0.0204063,"a collection of legislative texts of the European Union written between 1950 and now and is available in more than twenty official European languages (Steinberger et al., 2006). The EnglishGerman parallel corpus consists of 1.2 million aligned sentences, and 32 million English and 30 million German tokens. A similar corpus to JRC-Acquis is the Europarl parallel corpus (version 7),8 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 47 million English and 45 million German tokens (Koehn, 2005). Repurchase agreements, Guarantees given, Equity instruments . . . Provisions, Securities, Assets . . . Table 1: Examples of the longest and shortest financial labels in FINREP taxonomy hold 569 monolingual labels in English. FINREP labels are mostly noun phrases, many of which are quite long as can be seen in Figure 1. The longer labels are the product of nominalizing and condensing descriptions of the meaning of the corresponding reporting concept. Each reporting concept has, in addition to its labels, a unique cluster of XBRL identifiers, which are used to tag instances of the concept, e.g"
2013.mtsummit-posters.1,P07-2045,0,0.00944041,"NREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), which uses the domain-specific translation models described.19 Figure 3: Translation GUI for the financial domain Figure 3 illustrates the options of the GUI. The interface allows differen"
2013.mtsummit-posters.1,J03-1002,0,0.00432263,"ranslations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), which uses the domain-specific translation models described.19 Figure 3: Translation GUI for the financial domain Figure 3 illustrates the options of the GUI. The interface allows different language pairs and different size n-best lists when the translations are generated. Further an output option is available, which generates a downloadable .csv file. The ”Upload dictionary” option allows the user to"
2013.mtsummit-posters.1,P02-1040,0,0.0864417,"es and unigram expressions. Thanks to the extensive multilingual data of Wikipedia and Linguee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In com"
2013.mtsummit-posters.1,J03-3002,0,0.0189339,"pedia article titles with their multilingual equivalents, Wikipedia holds much more information in the articles themselves. Therefore, exploiting these non-parallel resources in future, as shown by Fiˇser et al. (2011), would clearly help to improve the performance of the translation system. Besides Wikipedia/DBpedia, which can be used for lexicon generation and WSD, the Web itself stores an enormous amount of data, which is often represented in a multilingual way. Therefore a major part of the future work needs to be focused on extraction and alignment on multilingual websites and documents (Resnik and Smith, 2003). In addition to exploiting new resources for statistical machine translation, the manual evaluation for translated labels needs to become the focus of our future work. Although such manual evaluation is time consuming, it provides a closer look into the translation errors. Even through the small manual evaluation of 100 FINREP labels we learned that fine-grained translation error classes have to be formulated. We observed that we have to distinguish between translations with ”one grammatical error” or ”several grammatical errors”. It might also be interesting to classify the types of grammati"
2013.mtsummit-posters.1,2006.amta-papers.25,0,0.0181138,"ltilingual data of Wikipedia and Linguee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely ac"
2013.mtsummit-posters.1,steinberger-etal-2006-jrc,0,0.0679597,".. Unpaid capital which has been called up Figure 1: The financial label Demand deposits and cash equivalents and its ancestors in the financial taxonomy FINREP Length Count 30 1 2 ... 110 1 36 Examples 3.2 Financial assets pledged as collateral, financial assets pledged as collateral for which the transferre has the right to sell or repledge in the absence of default by the reporting institution The parallel corpus JRC-Acquis7 (version 3.0) is a collection of legislative texts of the European Union written between 1950 and now and is available in more than twenty official European languages (Steinberger et al., 2006). The EnglishGerman parallel corpus consists of 1.2 million aligned sentences, and 32 million English and 30 million German tokens. A similar corpus to JRC-Acquis is the Europarl parallel corpus (version 7),8 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 47 million English and 45 million German tokens (Koehn, 2005). Repurchase agreements, Guarantees given, Equity instruments . . . Provisions, Securities, Assets . . . Table 1: Examples of the longest and shortest financial lab"
2013.mtsummit-posters.1,C08-1125,0,0.180485,"d to train a translation and language model. For our current research we used the JRC-Acquis, Europarl and the European Central Bank (ECB) corpora. Finally, in Section 3.4 we describe the procedure to obtain a domain-specific corpus from Linguee and Wikipedia/DBpedia. The results of the translations produced by an SMT trained on these domain-specific resources were compared to SMT results from a system trained on more general resources. Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this existing corpora to illustrate any improvements gained by modelling a new domain-specific corpus for the financial domain. 3.1 The Financial taxonomy - FINREP Under EU law financial institutions such as banks, credit institutions and investment firms must submit periodic reports to national supervisory bodies. The content of these reports is guided by the European Banking Authority4 (EBA) by means of two complementary reporting frameworks: financial reporting (FINREP) and COREP5 (COmmon solvency ratio REPorting) common reporting. These"
2014.amta-researchers.5,P13-1040,0,0.221652,"-linear weights on this specific data. Bilingual term extraction is performed in two steps. First, the source and the target sides of the data are processed by a keyword extractor to identify the most relevant terms in each language. Taking advantage of the parallel data, each monolingual term in the source language is paired with a term in the target language. We perform this step by comparing different techniques, showing that simple approaches based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add"
2014.amta-researchers.5,C12-1005,1,0.865188,"ir approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disambiguated term extraction using the rich lexical and semantic knowledge of Wikipedia. 8 Conclusion In this paper, we propose a framework to enhance translation quality by exploiting bilingual terms extracted from the parallel sentences daily produced by professional translators. The results show that an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU"
2014.amta-researchers.5,W14-4803,1,0.785943,"d. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disambiguated term extraction using the rich lexical and semantic knowledge of Wikipedia. 8 Conclusion In this paper, we propose a framework to enhance translation quality by exploiting bilingual terms extracted from the parallel sentences daily produced by professional translators. The results show that an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU score over a generic baseline system. Furthermore, we investigate the integration of the extracted bilingual terms into the SMT system. For the f"
2014.amta-researchers.5,2013.mtsummit-papers.5,0,0.459578,"based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models with a recently developed technique, namely the Realtime Adaptive Translation Systems with cdec (Denkowski et al., 2014), that, based on lexicalized synchronous context-free grammars, takes as input the whole source and post-edited sentences and automatically updates the models. The evaluation of our framework on two different"
2014.amta-researchers.5,W09-0432,0,0.0612499,"passing the new weights to Moses through XML tags for each incoming sentence, which required to extend Moses with this new option. An issue with incremental tuning is the risk of over-fitting of the model on a small development set, when it differs from the test set. In our scenario, this is prevented by the fact that all the sets come from the same document, or from different documents on similar topic in the same project. Although it is important to tune an SMT system on a sufficiently large development set, reasonably good weights can be obtained even if such data are very few, as shown in Bertoldi and Federico (2009). In our framework, it is not possible to concatenate all the previous partitions to enlarge the development set, because the presence of already extracted bilingual terms in the cache-based models would artificially favour the cache-based components during the tuning. 4 Experimental Setting In this Section, we propose a set of experiments aimed at showing the capability of our framework to extract high quality domain-specific bilingual terms from a small amount of parallel data and to integrate them in the translation task. The translation direction considered is from English to Italian. To i"
2014.amta-researchers.5,bouamor-etal-2012-identifying,0,0.221065,"ides of the data are processed by a keyword extractor to identify the most relevant terms in each language. Taking advantage of the parallel data, each monolingual term in the source language is paired with a term in the target language. We perform this step by comparing different techniques, showing that simple approaches based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models wit"
2014.amta-researchers.5,P11-2031,0,0.0255575,"slations. Furthermore, XML markup cannot handle overlaps between dictionary entries. In our experiments, we found only 15 cases where the entries overlap, whereby we give preference to longer source terms. For each set of partitions, the incremental tuning was run to update the log-linear weights. For a comparison, we also run MERT on each partition starting with flat weights (nonincremental tuning). In Table 3, we report BLEU scores for each partition separately (columns “Part #”), as well as the evaluation on the whole corpus (column “Document level”). The approximate randomization approach Clark et al. (2011) is used to test whether differences among system performances are statistically significant at document level. Results in the table marked with * are statistically significantly better than the baseline with a p-value &lt; 0.05. Comparing the baseline XML markup and the cache-based methods, we notice that the translation performance of cache-based models always outperforms significantly all the other methods in both domains. This is also confirmed at partition level, with few exceptions for the initial partitions. The XML markup performs better than the baseline in both domains, but statistical"
2014.amta-researchers.5,E14-1042,0,0.0177783,"rio we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models with a recently developed technique, namely the Realtime Adaptive Translation Systems with cdec (Denkowski et al., 2014), that, based on lexicalized synchronous context-free grammars, takes as input the whole source and post-edited sentences and automatically updates the models. The evaluation of our framework on two different domains (IT and medical) suggests that: (i) an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU score over a generic SMT system; (ii) strategies to integrate terminology need to take into consideration also the surrounding context of a translated term; (iii) in order to take advantage of the continuous appending of new info"
2014.amta-researchers.5,N13-1073,0,0.0382693,"altime cdec) an online model adaptation system. Differently from the cache-based approach, it automatically extracts new translation rules from the whole source and post-edited sentences and adds them to the translation grammar. This system takes advantage of cdec (Dyer et al., 2010), a standalone decoder, aligner, and learning framework for SMT. cdec allows us to train word-based and phrase-based models, as well as models based on lexicalized synchronous content-free grammars (SCFG), which was used in our experiment. The adaptation of cdec to work in real time requires the use of Fast Align (Dyer et al., 2013) to perform on-the-fly word alignment between source and post-edited sentences. This makes possible the incremental addition of information to the translation models after a sentence is translated. Furthermore, Realtime cdec adapts the Bayesian language model using the hierarchical Pitman-Yor process approach, whereby MIRA (Chiang, 2012) is used to optimize the discriminative parameters of the decoder. In our experiments we use the Realtime cdec similarly to the scenario described in Section 3.2. Each sentence pair (source, post-edition) from partitionn−1 is added to the model and used by MIRA"
2014.amta-researchers.5,P10-4002,0,0.0287658,"ed also by the results reported in Table 3, showing that translation quality is generally lower than for the IT domain. 6 Cache-Based Model vs. Online Adaptation Model with cdec To complete our evaluation, we compare the XML markup and the cache-based approach with the Realtime Adaptive Translation Systems with cdec,9 (henceforth Realtime cdec) an online model adaptation system. Differently from the cache-based approach, it automatically extracts new translation rules from the whole source and post-edited sentences and adds them to the translation grammar. This system takes advantage of cdec (Dyer et al., 2010), a standalone decoder, aligner, and learning framework for SMT. cdec allows us to train word-based and phrase-based models, as well as models based on lexicalized synchronous content-free grammars (SCFG), which was used in our experiment. The adaptation of cdec to work in real time requires the use of Fast Align (Dyer et al., 2013) to perform on-the-fly word alignment between source and post-edited sentences. This makes possible the incremental addition of information to the translation models after a sentence is translated. Furthermore, Realtime cdec adapts the Bayesian language model using"
2014.amta-researchers.5,R11-1017,1,0.891393,"Missing"
2014.amta-researchers.5,C14-2028,1,0.840113,"monolingual term extraction tool as well as the most suitable bilingual alignment approach, we use freely available data, which were manually annotated to better evaluate all the intermediate steps of the experiment. Two datasets belonging to the IT domain, namely a portion of GNOME project data (4,3K tokens)5 and KDE Data (9,5K),6 are used for domain-specific term extraction. The whole framework, including the machine translation part, is tested on a subset of the EMEA corpus (Tiedemann, 2009) for the medical domain (18K tokens) and an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann"
2014.amta-researchers.5,2012.amta-papers.22,0,0.10194,"Missing"
2014.amta-researchers.5,W12-3154,0,0.0264004,"i´c (2012) extract terms and lexicon entries from SMT phrase tables. In their approach they apply linguistic, lexicon and frequency filters to obtain good lexicon entries. Similarly, we also access the phrase table to build our bilingual terminology, whereby our filter relies on the term and sentence lookup approach. Furthermore, there has been research done on the integration of domain-specific parallel data into SMT, e.g. dictionaries or bilingual terminology, either by retraining new and general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with us"
2014.amta-researchers.5,itagaki-aikawa-2008-post,0,0.0258275,"d general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disam"
2014.amta-researchers.5,S10-1004,0,0.0174807,"where no or little training data are available, we chose three unsupervised terminology extractors supporting different languages. 4 http://www.alchemyapi.com/products/features/keyword-extraction/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 55 KX is a terminology extractor, which combines frequency information and part-of-speech patterns of n-grams to identify the most relevant terms in a corpus. It is freely available for English and Italian and was the first-ranked unsupervised system in the Semeval2010 task on keyword extraction (Kim et al., 2010). TWSC follows an approach which is very similar to KX, integrating morpho-syntactic patterns with statistical features. One of the main differences w.r.t. KX is the implementation of different co-occurrence statistics to rank term candidates, and the treatment of nested terms. Nevertheless, we expect the performance of these two tools to be very similar. A third system considered is AlchemyAPI. This commercial tool employs sophisticated statistical algorithms and linguistic approaches to analyse textual content and extract topic keywords, but no further implementation details are given. 2.2 B"
2014.amta-researchers.5,2005.mtsummit-papers.11,0,0.0114654,"software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domains, this reflects a real CAT scenario, where only a small quantity of domain-specific data is available. Manual Terminology Annotation In order to evaluate the quality of the bilingual terms, we create a terminological gold st"
2014.amta-researchers.5,P07-2045,0,0.0124687,"shown significant productivity gains when human translators post-edit machine translation output rather than translating documents from scratch. This evidence has raised interest in the integration of machine translation systems within CAT software. In this context, an important open issue is how to support translators with domain-specific information when dealing with highly specific texts, i.e. manuals coming from different domains (information technology (IT), legal, agriculture, etc.). Translation tools such as Google Translate,1 Bing Translator2 or open source SMT systems such as Moses (Koehn et al., 2007) trained on generic data are the most common solutions, but they often result in unsatisfactory translations. A valuable alternative to support professional translators is represented by online terminology resources, e.g. IATE,3 which are continuously updated and can be easily queried. However, the manual use of these services can be very time demanding when working with a CAT tool. For these reasons, the automatic identification and integration of bilingual domain-specific terms into an SMT system is a crucial step towards increasing translation quality of high-specific texts in a CAT environ"
2014.amta-researchers.5,W02-1405,0,0.154471,"al terminology. Thurmair and Aleksi´c (2012) extract terms and lexicon entries from SMT phrase tables. In their approach they apply linguistic, lexicon and frequency filters to obtain good lexicon entries. Similarly, we also access the phrase table to build our bilingual terminology, whereby our filter relies on the term and sentence lookup approach. Furthermore, there has been research done on the integration of domain-specific parallel data into SMT, e.g. dictionaries or bilingual terminology, either by retraining new and general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT"
2014.amta-researchers.5,2013.mtsummit-wptp.10,0,0.0652808,"Missing"
2014.amta-researchers.5,J03-1002,0,0.00565761,"or domain-specific term extraction. The whole framework, including the machine translation part, is tested on a subset of the EMEA corpus (Tiedemann, 2009) for the medical domain (18K tokens) and an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domai"
2014.amta-researchers.5,P02-1040,0,0.092848,"reement following Landis and Koch (1977). This annotation effort resulted in the identification of 874 domain-specific bilingual terms in the two datasets.7 5 Evaluation In this Section, we report the quality of monolingual term extraction and the bilingual alignment. For each domain we evaluate the performance obtained by applying different approaches to the integration of bilingual terms into an SMT system. Evaluation of the extracted monolingual and bilingual terms is performed on the manually annotated KDE and GNOME datasets by calculating precision, recall and f-measure. The BLEU metric (Papineni et al., 2002) is used to automatically evaluate the translation quality of the EMEA and the IT manual datasets. 5.1 Monolingual Term Extraction Our first evaluation concerns monolingual term extraction from English and Italian documents provided by the KX, AlchemyAPI and TWSC extraction tools. As shown in Table 1, KX tends to overgenerate when extracting English terms. It extracts the highest number of expressions, which results in a high recall, but low precision. On the other hand, TWSC extracts the least English terms. Based on F1, we observe that AlchemyAPI is the best performing tool when extracting E"
2014.amta-researchers.5,S10-1036,0,0.0136457,"lel data), while the second is the creation of bilingual terminology starting from the monolingual ones. In order to obtain the best possible performance, we compare different approaches in both steps. At the monolingual level, we test the extraction using three unsupervised term extraction tools. For bilingual alignment, we compare different alignment strategies. The two steps are detailed in the following subsections. 2.1 Monolingual Terminology Extraction In order to find the best performing approach to identify monolingual terms, we compare three available term extractors: the KX toolkit (Pianta and Tonelli, 2010), TWSC (Pinnis et al., 2012) and AlchemyAPI.4 Given our experimental scenario, where no or little training data are available, we chose three unsupervised terminology extractors supporting different languages. 4 http://www.alchemyapi.com/products/features/keyword-extraction/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 55 KX is a terminology extractor, which combines frequency information and part-of-speech patterns of n-grams to identify the most relevant terms in a corpus. It is freely available for English and Italian and was the fi"
2014.amta-researchers.5,W09-2907,0,0.0426805,"Missing"
2014.amta-researchers.5,steinberger-etal-2006-jrc,0,0.0430484,"an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domains, this reflects a real CAT scenario, where only a small quantity of domain-specific data is available. Manual Terminology Annotation In order to evaluate the quality of the bilingual terms, we create"
2014.amta-researchers.5,2012.eamt-1.59,0,0.0531867,"Missing"
2014.amta-researchers.5,W10-2602,0,0.0457622,"Missing"
2014.amta-researchers.5,vintar-fiser-2008-harvesting,0,0.0420124,"Missing"
2014.amta-researchers.5,O03-1003,0,0.20189,"Missing"
2014.amta-researchers.5,O04-2001,0,\N,Missing
2014.amta-researchers.5,C94-1084,0,\N,Missing
2014.amta-researchers.5,N10-1062,0,\N,Missing
2015.eamt-1.14,W05-0814,0,0.0246803,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.14,E03-1076,0,0.060003,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.14,2005.mtsummit-papers.11,0,0.0154666,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.14,J11-4002,1,0.90191,"Missing"
2015.eamt-1.14,2011.eamt-1.12,0,0.0137748,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.14,E09-1087,0,0.0737483,"Missing"
2015.eamt-1.14,tiedemann-2012-parallel,0,0.0135084,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.14,steinberger-etal-2012-dgt,0,\N,Missing
2015.eamt-1.14,P02-1040,0,\N,Missing
2015.eamt-1.14,P07-2045,0,\N,Missing
2015.eamt-1.14,J03-1002,0,\N,Missing
2015.eamt-1.14,W14-4210,1,\N,Missing
2015.eamt-1.14,etchegoyhen-etal-2014-machine,0,\N,Missing
2015.eamt-1.15,W05-0814,0,0.0385293,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.15,E03-1076,0,0.0450815,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.15,2005.mtsummit-papers.11,0,0.0120371,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.15,J11-4002,1,0.893851,"Missing"
2015.eamt-1.15,2011.eamt-1.12,0,0.0238497,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.15,E09-1087,0,0.0700919,"Missing"
2015.eamt-1.15,tiedemann-2012-parallel,0,0.0242552,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.15,W11-2103,0,\N,Missing
2016.amta-researchers.1,2011.mtsummit-papers.35,0,0.0720755,"Missing"
2016.amta-researchers.1,2013.mtsummit-papers.5,0,0.142336,"atedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting"
2016.amta-researchers.1,W15-3001,1,0.887759,"Missing"
2016.amta-researchers.1,2011.mtsummit-papers.1,0,0.0211613,"e data selected by the instance selection module are randomly split in training and development sets three times. A minimum number of selected sentence pairs is required to trigger the parameter optimisation process. If this minimum value is not reached, the optimization step is skipped because having few sentences might not yield to reliable weights. In this case, the weights computed on the previous input segment are used. In our experiments, we observed that this solution is more reliable and efﬁcient than the feature weights obtained with a single tuning, as it was previously proposed in (Cettolo et al., 2011). We believe this procedure to optimize the feature weights over a development set that closely resembles the test segment can help to obtain weights more suitable to the segment to be post-edited. Decode Test Segment. To decode the input segments, all the local models (language, translation, reordering) are built with all the selected instances. The log-linear feature weights are computed by taking the arithmetic mean of the tuned weights for the three data splits. The decoding process is performed with the Moses toolkit recalling that the input segment is kept untouched when no reliable info"
2016.amta-researchers.1,W16-2377,1,0.844916,"Missing"
2016.amta-researchers.1,W15-3025,1,0.897722,"parallel data consisting of MT output on one side and its corrected version on the other side. This data can be leveraged to develop automatic post-editing (APE) systems capable not only to spot recurring MT errors, but also to correct them. Thus, integrating an APE system inside the CAT framework can further improve the 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S quality of the suggested segments, reduce the workload of human post-editors and increase the productivity of the translation industry. As pointed out in (Parton et al., 2012) and (Chatterjee et al., 2015b), from the application point of view APE components would make it possible to: • Improve the MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of"
2016.amta-researchers.1,P15-2026,1,0.871001,"parallel data consisting of MT output on one side and its corrected version on the other side. This data can be leveraged to develop automatic post-editing (APE) systems capable not only to spot recurring MT errors, but also to correct them. Thus, integrating an APE system inside the CAT framework can further improve the 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S quality of the suggested segments, reduce the workload of human post-editors and increase the productivity of the translation industry. As pointed out in (Parton et al., 2012) and (Chatterjee et al., 2015b), from the application point of view APE components would make it possible to: • Improve the MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of"
2016.amta-researchers.1,P11-2031,0,0.0597909,"Missing"
2016.amta-researchers.1,E14-1042,0,0.24108,"ost-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning mechanism. To perform post-edit propagation, this system was trained incrementally us"
2016.amta-researchers.1,W07-0732,0,0.02438,"ailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned parameters. Although the"
2016.amta-researchers.1,W08-0509,0,0.015528,"tain a jointrepresentation that links each source word with a MT word (mt#src). This representation has been proposed in the context-aware APE approach by B´echara et al. (2011) and leverages the source information to disambiguate post-editing rules. Recently, Chatterjee et al. (2015b) also conﬁrmed this approach to work better than translating from raw MT segments over multiple language pairs. The joint-representation is used as a source corpus to train all the APE systems reported in this paper and it is obtained by ﬁrst aligning the words of source (src) and MT (mt) segments using MGIZA++ (Gao and Vogel, 2008), and then each mt word is concatenated with its corresponding src words. The Autodesk training, development, and test sets consist of 12,238, 1,948, and 1,956 segments respectively, while the WMT2016 data contains 12,000, 1,000, and 2,000 segments. Table 1 provides some additional statistics of the source (mt#src) and target (pe) training corpus, the repetition rate (RR) to measure the repetitiveness inside a text (Bertoldi et al., 2013), and the average TER score for both the data sets (computed between MT and PE). It is interesting to note that the Autodesk data set has on average shorter s"
2016.amta-researchers.1,2010.amta-papers.21,0,0.0339723,"the following steps repeatedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system,"
2016.amta-researchers.1,2005.eamt-1.19,0,0.0357856,"one proposed in real-time cdec (Denkowski et al., 2014), which uses sufﬁx-arrays to select the top k instances. In our approach the sample size is in fact dynamically set in order to select only the most similar ones. This allows us to build more reliable models (since the underlying data better resembles the test segment), and to gain speed 2 https://lucene.apache.org/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S when the sample size is small. The use of a tf-idf similarity measure was proposed before in the context of machine translation by Hildebrand et al. (2005) to create a pseudo in-domain corpus from a big out-of-domain corpus. Our work is the ﬁrst to investigate it for the APE task in an online learning scenario. Model Creation. From the selected instances we build several local models. The ﬁrst is the language model: A tri-gram local language model is built over the target side of the training corpus with the IRSTLM toolkit (Federico et al., 2008). Since the selected training data closely resembles the input segment, we believe that the local LM can capture the peculiarities of the domain to which the input segment belongs. Along with the local L"
2016.amta-researchers.1,W04-3250,0,0.0393172,"Missing"
2016.amta-researchers.1,P07-2045,0,0.00402914,"lieve that the local LM can capture the peculiarities of the domain to which the input segment belongs. Along with the local LM we always use a trigram global LM, which is updated whenever a human post-edition (pe) is received. The other local models are the translation and the reordering models: these local models are built over the training instances retrieved from the knowledge base. Since the training instances are very similar to the input segment, the post-editing rules learned from these local models are more reliable for the test segment. These models are build with the Moses toolkit (Koehn et al., 2007) and the word alignment of each sentence pair is computed using the incremental GIZA++ software.3 Parameter Optimization. The parameters are optimized over a section of the selected instances (development set). The size of this development set is critical: if it is too large, then the parameter optimization will be expensive. On the other hand, if it is too small the tuned weights might not be reliable. To achieve fast optimization with reliably-tuned weights, multiple instances of MIRA are run in parallel on several small development sets and all the resulting weights are then averaged. For t"
2016.amta-researchers.1,W13-2237,0,0.0186495,"eceives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning"
2016.amta-researchers.1,E14-2012,0,0.0200912,"thin the context of a single document. For every new document the APE system begins with an “empty” model. Since the post-editing rules are learned for a given document they can be more precise and useful for that document, but the limitation is that knowledge gained after processing one document is not utilized for other similar documents. This limitation can be addressed by our system (Section 3), in which we maintain one global knowledge base to store all the processed documents, still being able to retrieve post-editing rules speciﬁc to a document to be translated. Thot: The Thot toolkit (Ortiz-Martınez and Casacuberta, 2014) is developed to support fully automatic and interactive statistical machine translation.1 It was also used by Lagarda et al. (2015) in an online setting for the APE task, to perform large-scale experiments with several 1 https://github.com/daormar/thot 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S data sets for multiple language pairs, with base MT systems built using different technologies (rule-based MT, statistical MT). In the majority of their experiments online APE successfully improved the quality of the translations obtained from the bas"
2016.amta-researchers.1,P02-1040,0,0.098317,"Missing"
2016.amta-researchers.1,2012.eamt-1.34,0,0.0529399,"Missing"
2016.amta-researchers.1,2013.mtsummit-papers.24,0,0.245164,"he ability to incorporate human feedback in a real-time translation workﬂow. This led to the development of online learning algorithms that can leverage the continuous streams of data arriving in the form of human post-editing feedback to dynamically update the models and tune the parameters on-the-ﬂy within the CAT framework. In recent years, several online systems have been proposed in MT (see Section 2 for more details) to address the problem of incremental training of the models or on-the-ﬂy optimization of feature weights. Few online MT systems have also been applied to the APE scenario (Simard and Foster, 2013; Lagarda et al., 2015) in a controlled working environment in which the systems are trained and evaluated on homogeneous/coherent data where the training and test sets share similar characteristics. Moving from this controlled lab environment to real-world translation workﬂow, where training and test data can be produced by different MT systems, post-edited by various translators and belong to several text genres, makes the task more challenging, because the APE systems have to adapt to all these diversities in real-time. We deﬁne this scenario as a multi-domain translation environment (MDTE)"
2016.amta-researchers.1,N07-1064,0,0.222063,"iting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned pa"
2016.amta-researchers.1,W07-0728,0,0.21759,"iting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned pa"
2016.amta-researchers.1,2006.amta-papers.25,0,0.0327463,"Missing"
2016.amta-researchers.1,2009.mtsummit-posters.20,0,0.0160345,"dits with signiﬁcantly better performance than the existing online APE systems. 2 Online Translation Systems Online translation systems aim to incorporate human post-editing feedback (or the corrected version of the MT output) into their models in real-time, as soon as it becomes available. This feedback helps the system to learn from the mistakes made in the past translations and to avoid repeating them in future translations. This continuous learning capability will eventually improve the quality of the translations and consequently increase the productivity of the translators/post-editors (Tatsumi, 2009) working with MT suggestions in a CAT environment. The basic workﬂow of an online translation system goes through the following steps repeatedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx er"
2016.amta-researchers.1,2007.mtsummit-wpt.4,0,0.0535562,"r, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned parameters. Although these standard appr"
2016.amta-researchers.1,D15-1123,0,0.0194726,"ors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning mechanism. To perform post-edit propagation, this system was trained incrementally using pairs of machine-t"
2016.amta-researchers.1,2005.eamt-1.39,0,0.0218908,"Gaussian distributions. However, the feature weights of the log-linear model are static throughout the online learning process, as opposed to our method that updates the weights on-the-ﬂy. Also, this method learns post-editing rules from all the data processed in real-time, whereas, our approach learns from the most relevant data points. Realtime cdec: Denkowski et al. (2014) proposed an online model adaptation method to leverage human post-edited feedback to improve the quality of an MT system in a real-time translation workﬂow. To build the translation models they use a static sufﬁx array (Zhang and Vogel, 2005) to index initial data (or a seed corpus), and a dynamic lookup table to store information from the post-edited feedback. To decode a sentence, the statistics of the translation options are computed both from the sufﬁx array and from the lookup table. An incremental language model is maintained and updated with each incoming human post-edit. To update the feature weights they used an extended version of the margin-infused relaxed algorithm (MIRA) (Chiang, 2012). The decoding is treated as simply the next iteration of MIRA, where a segment is ﬁrst translated and then its corresponding reference"
2018.gwc-1.10,W14-0101,0,0.0195947,"anguages to form the IndoWordNet. Mohanty et al. (2017) built SentiWordNet for the Odia language, which is one of the official languages of India. Being an under-resourced language, Odia lacks proper machine translation system to translate the vocabulary of the available resource from English into Odia. The authors have created SentiWordNet for Odia using resources of other Indian languages and the IndoWordNet. Although the IndoWordNet structure does not map directly to the SentiWordNet, instead synsets are matched. The authors used these for translation from source lexicon to target lexicon. Aliabadi et al. (2014) have created a wordnet for the Kurdish language, one of the under-resourced languages in western Iranian language family. They have created Kurdish translation for the “core” wordnet synsets (Vossen, 1997), which is a set of 5,000 essential concepts. They used a dictionary to translate its literals (words), adopted an indirect evaluation alternative in which they look at the effectiveness of using KurdNet for rewriting Information Retrieval queries. Similarly, the work by Horv´ath et al. (2016) focuses on the semiautomatic construction of wordnet for the Mansi language, which is spoken by Man"
2018.gwc-1.10,C16-1010,1,0.934104,"xt from scratch, we use the available parallel corpora from multiple sources, like OPUS,2 to create a machine translation system to translate the wordnet senses in the Princeton WordNet into the mentioned under-resourced languages. Translation tools such as Google Translate,3 or open source SMT systems such as Moses (Koehn et 1 http://globalwordnet.org/ http://opus.lingfil.uu.se/ 3 http://translate.google.com/ 2 al., 2007) trained on generic data are the most common solutions, but they often result in unsatisfactory translations of domain-specific expressions. Therefore, we follow the idea of Arcan et al. (2016b), where the authors automatically identify relevant sentences in English containing the WordNet senses and translate them within the context, which showed translation quality improvement of the targeted entries. The effectiveness of our approach is evaluated by comparing the generated translations with the IndoWordNet entries, automatically and manually, respectively. This paper reports our first outcomes in improving wordnet for under-resourced Dravidian languages such as Tamil(ISO 639-2: tam), Telugu (ISO 639-2: tel) and Kannada (ISO 639-2: kan). 2 Related work Scannell (2007) describes th"
2018.gwc-1.10,W14-3902,0,0.0477905,"ll corpora used for training lead to incomplete word coverage, which may cause the out-of-vocabulary (OOV) issues. Besides the resource scarceness, another issue observed with the corpus for Dravidian languages was code-switching contents in the data. Codeswitching is an act of alternating between elements of two or more languages, which is prevalent in English→Tamil English→Telugu English→Kannada Original Non-Code mixing 20.29 28.81 14.64 20.61 28.25 14.45 Table 2: Automatic translation evaluation of the of 1000 randomly selected sentences in terms of the BLEU metric. multilingual countries (Barman et al., 2014). With English being the most used language in the digital world, people tend to mix English words with their native languages. That might be the case in other languages as well. 4 Methodology The principle approaches for constructing wordnets are the merge approach or the expand approach. In the merge approach, the synsets and relations are built independently and then aligned with WordNet. The drawbacks of the merge approach are that it is time-consuming and requires a lot of manual effort to build. On the contrary in the expand model, wordnet can be created automatically by translating syns"
2018.gwc-1.10,eisele-chen-2010-multiun,0,0.0245692,"languages than English were used to select the most relevant sentences for wordnet senses from a large set of generic parallel corpora. The goal is to identify sentences that share the same semantic information in respect to the synset of the WordNet entry that we want to translate. To ensure a broad lexical and domain coverage of English sentences, existing parallel corpora for various language pairs were merged into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, wordnets in a variety of languages, provided by the Open Multilingual Wordnet web page,9 were used. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i6083310 ‘a craft designed for water transportation’. For the second sense, i65336 ‘a tube in which a body fluid circulates’, we assume that we know the German tran"
2018.gwc-1.10,2012.amta-papers.22,0,0.0188799,"e followed for different languages. The IndoWordNet (Bhattacharyya, 2010) was compiled for eighteen out of the twenty-two official languages of India and made available for public use. It is based on the expand approach like EuroWordNet, but from the Hindi wordnet, which is then linked to English. On the Global WordNet Association website,1 a comprehensive list of wordnets available for different languages can be found, including IndoWordNet and EuroWordNet etc. This paper describes the effort towards generating and improving wordnets for the underresourced Dravidian languages. Since studies (Federico et al., 2012; L¨aubli et al., 2013; Green et al., 2013) have shown significant productivity gains when human translators post-edit machine translation output rather than translating text from scratch, we use the available parallel corpora from multiple sources, like OPUS,2 to create a machine translation system to translate the wordnet senses in the Princeton WordNet into the mentioned under-resourced languages. Translation tools such as Google Translate,3 or open source SMT systems such as Moses (Koehn et 1 http://globalwordnet.org/ http://opus.lingfil.uu.se/ 3 http://translate.google.com/ 2 al., 2007) t"
2018.gwc-1.10,W11-2123,0,0.0208039,"anslating synsets using different strategies, whereby the synsets are built in correspondence with the existing wordnet synsets. We followed the expand approach and created a machine translation systems to translate the sentences, which contained the WordNet senses in English to the target language 4.1 Training Machine Translation parameters In the following section, we takes as a baseline a parallel text, that has been aligned at the sentence level. To obtain the translations, we use Moses SMT toolkit with of baseline setup with 5-gram language model created using the training data by KenLM (Heafield, 2011). The baseline SMT system was built for three language pairs, English-Tamil, English-Telugu, and English-Kannada. The test set mentioned in Section 3.3 was used to evaluate our system. From Table 1 and Table 2 we can see that size of the parallel corpus has an impact on the BLEU score for test set which is evaluation criteria for the translation model. 4.2 Context Identification Since manual translation of wordnets using the extend approach is a very time consuming and expensive process, we apply SMT to automatically translate WordNet entries into the targeted Dravidian languages. While an dom"
2018.gwc-1.10,2016.gwc-1.20,0,0.102139,"Missing"
2018.gwc-1.10,P07-2045,0,0.00600865,"aximizing separately a language model p(t) and the inverse translation model p(s|t). A language model assigns a probability p(t) for any sentence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation This section describes the data collection"
2018.gwc-1.10,2005.mtsummit-papers.11,0,0.0150759,"disambiguated context of a sentence (Arcan et al., 2016a; Arcan et al., 2016b). Therefore existing translations of WordNet senses in other languages than English were used to select the most relevant sentences for wordnet senses from a large set of generic parallel corpora. The goal is to identify sentences that share the same semantic information in respect to the synset of the WordNet entry that we want to translate. To ensure a broad lexical and domain coverage of English sentences, existing parallel corpora for various language pairs were merged into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, wordnets in a variety of languages, provided by the Open Multilingual Wordnet web page,9 were used. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i6083310 ‘a craf"
2018.gwc-1.10,2016.gwc-1.9,1,0.878429,"Missing"
2018.gwc-1.10,J10-4005,0,0.0238948,"t given a source sentence s by maximizing separately a language model p(t) and the inverse translation model p(s|t). A language model assigns a probability p(t) for any sentence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation Thi"
2018.gwc-1.10,W16-5814,0,0.0573178,"languages with in the same sentence or between sentences. In many bilingual or multilingual communities like India, Hong Kong, Malaysia or Singapore, language interaction often happens in which two or more languages are mixed. Furthermore, it increasingly occurs in monolingual cultures due to globalization. In many contexts and domains, English is mixed with native languages within their utterance than in the past due to Internet boom. Due to the history and popularity of the English language, on the Internet Indian languages are more frequently mixed with English than other native languages (Chanda et al., 2016). A major part of our corpora comes from movie subtitles and technical documents, which makes it even more prone to code-mixing of English in the Dravidian languages. In our corpus, movie speeches are transcribed to text and they differ from that in other written genres: the vocabulary is informal, non-linguistics sounds like ah, and mixing of scripts in case of English and native languages (Tiedemann, 2008). Two example of codeswitching are demonstrated in Figure 1.The parallel corpus is initially segregated into English script and native script. All of the annotations are done using an autom"
2018.gwc-1.10,2013.mtsummit-wptp.10,0,0.0213978,"Missing"
2018.gwc-1.10,W17-5219,0,0.0211052,". The IndoWordNet entries are updated frequently. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. 4 http://www.cfilt.iitb.ac.in/ indowordnet/index.jsp In their further work (Rajendran et al., 2010), they emphasize the need for an independent wordnet for the Dravidian languages, based on EuroWordNet. This is due the observation that the morphology and lexical concepts of these languages are different compared to other Indian languages. The authors have combined the Tamil wordnet and wordnets in other Dravidian languages to form the IndoWordNet. Mohanty et al. (2017) built SentiWordNet for the Odia language, which is one of the official languages of India. Being an under-resourced language, Odia lacks proper machine translation system to translate the vocabulary of the available resource from English into Odia. The authors have created SentiWordNet for Odia using resources of other Indian languages and the IndoWordNet. Although the IndoWordNet structure does not map directly to the SentiWordNet, instead synsets are matched. The authors used these for translation from source lexicon to target lexicon. Aliabadi et al. (2014) have created a wordnet for the K"
2018.gwc-1.10,P02-1040,0,0.102499,"entence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation This section describes the data collection and the pre-processing process steps. The English-Tamil parallel corpus, which we used to train our SMT system is collected from various sour"
2018.gwc-1.10,W12-3152,0,0.0252879,".8 13,543 Table 1: Statistics of the parallel corpora used to train the translation systems. contains text from the news domain,5 sentences from the Tamil cinema articles6 and the Bible.7 For the news corpus, the authors downloaded web pages that have matching file names in both English and Tamil. For the cinema corpus, all the English articles had a link to the corresponding Tamil translation. The collection of the Bible corpus followed a similar pattern. We also took the English-Tamil parallel corpora for six Indian languages created with the help of Mechanical Turk for Wikipedia documents (Post et al., 2012). Since the data was created by non-expert translators hired over the Mechanical Turk, it is of mixed quality. From the OPUS website, we have collected the Gnome, KDE, Ubuntu and movie subtitles (Tiedemann, 2012). We furthermore manually aligned Tamil text Tirukkural,8 and combined all the parallel corpora into a single corpus. We first tokenized sentences in English and Tamil and then true-cased only the English side of the parallel corpus, since the Tamil language does not have a casing. Finally, we cleaned up the data by eliminating the sentences whose length is above 80 words. To obtain th"
2018.gwc-1.10,W12-5611,0,0.0802512,"Missing"
2018.gwc-1.10,tiedemann-2008-synchronizing,0,0.0423089,"st due to Internet boom. Due to the history and popularity of the English language, on the Internet Indian languages are more frequently mixed with English than other native languages (Chanda et al., 2016). A major part of our corpora comes from movie subtitles and technical documents, which makes it even more prone to code-mixing of English in the Dravidian languages. In our corpus, movie speeches are transcribed to text and they differ from that in other written genres: the vocabulary is informal, non-linguistics sounds like ah, and mixing of scripts in case of English and native languages (Tiedemann, 2008). Two example of codeswitching are demonstrated in Figure 1.The parallel corpus is initially segregated into English script and native script. All of the annotations are done using an automatic process. All words from a language other than the native script of our experiment are taken out on both sides of corpus if it occurs in native language side of the parallel corpus. The sentences are removed from both sides if the target language side does not contain native script words in it. Table 3 show the percentage of code-mixed text removed from original corpus. The goal of this approach is to in"
2018.gwc-1.10,tiedemann-2012-parallel,0,0.18819,"uthors downloaded web pages that have matching file names in both English and Tamil. For the cinema corpus, all the English articles had a link to the corresponding Tamil translation. The collection of the Bible corpus followed a similar pattern. We also took the English-Tamil parallel corpora for six Indian languages created with the help of Mechanical Turk for Wikipedia documents (Post et al., 2012). Since the data was created by non-expert translators hired over the Mechanical Turk, it is of mixed quality. From the OPUS website, we have collected the Gnome, KDE, Ubuntu and movie subtitles (Tiedemann, 2012). We furthermore manually aligned Tamil text Tirukkural,8 and combined all the parallel corpora into a single corpus. We first tokenized sentences in English and Tamil and then true-cased only the English side of the parallel corpus, since the Tamil language does not have a casing. Finally, we cleaned up the data by eliminating the sentences whose length is above 80 words. To obtain the parallel corpora for Telugu and Kannada, we used the corpora available on the OPUS website. The same pre-processing procedure was followed for Telugu and Kannada language, since both languages are close to the"
2018.gwc-1.10,W17-2911,0,0.0429524,"Missing"
2020.coling-main.369,D19-1522,0,0.0556029,"Missing"
2020.coling-main.369,D19-1189,0,0.170871,"ges 4179–4189 Barcelona, Spain (Online), December 8-13, 2020 KGs, there is a need for constructing appropriate subgraphs for the targeted domain, thereby reducing the amount of redundant information made available to the system. Subgraphs that are rich in domain information containing a low amount of noise are desirable. We, therefore, study the benefits of subgraphs created using N -hop and PageRank (Page et al., 1999) approaches. Furthermore, we analyse which subgraphs are best suited for the task at hand. In this work, we build upon the work of Knowledge-Based Recommendation Dialog (KBRD) (Chen et al., 2019). The authors extract movie entities from dialogues and utilise information from a KG to suggest movies to users. We incorporate pre-trained entity embeddings and make use of positional embeddings to improve the performance of the system. The main contributions of our work are as follows: • We conduct extensive experiments on different subgraph extracted from DBpedia(Auer et al., 2007) to show that the entire information contained in this resource is not beneficial and that there is a need for optimal subgraph creation technique. • We show that using pre-trained entity embeddings supplemented"
2020.coling-main.369,N19-1423,0,0.00571995,"d probability of a relation φ(es , r, eo ) over known relations. 4.2.2 Positional Embeddings During the dialogue flow, entities are mentioned sequentially. By virtue of this, entities mentioned later have a larger effect on future recommendations. Wu et al. (2019) constructed a session graph to represent the sequential nature of items belonging to a user. By leveraging the properties of such graphs, they learned latent item embeddings using graph neural networks. To avoid the construction of session graph and increasing the complexity of the model, we use positional embeddings as described in Devlin et al. (2019), which allow us to infuse sequence information into the entity embeddings. The pattern of embeddings induced by Equation 3 and 4 encodes positional information of entities in the model. POS(pos,2i+1) = cos(pos/100002i/dmodel ) (3) POS(pos,2i) = sin(pos/100002i/dmodel ) (4) The subscript pos in Equation 3 and Equation 4 refers to the index position of the entity in the dialogue, while dmodel refers to the dimensionality of the entity embeddings. The subscript i refers to the index in the vector representing the positional embedding. The final entity representation of the entity ei seen at posi"
2020.globalex-1.15,D13-1179,1,0.872474,"Missing"
2020.globalex-1.15,P16-1162,0,0.0241246,"geted Romance language family, but excluded the English-Spanish, EnglishFrench, English-Portuguese and Portuguese-French language pair. 3. 3.1. Neural Machine Translation Setup We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. To overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We used the following default neural network training parameters: two hidden layers, 500 hidden LSTM (long short term memory) units per layer, input feeding enabled, 13 epochs, batch size of 64, 0.3 dropout probability, dynamic learning rate decay, 500 dimension embeddings. 1 Results Results on Apertium In order to develop and train our system, we used the available Apertium data as a gold standard. In this case, we held out the English-Spanish translation d"
2020.globalex-1.15,P17-4012,0,0.0214073,"set of parallel data, i.e. for less-resourced languages, we trained a multi-source and multi-target NMT model (Ha et al., 2016) with wellresourced language pairs. In our work, we have chosen parallel corpora in the Romance language family, i.e. Spanish, Italian, French, Portuguese, Romanian, as well as English. To train the multi-way NMT system, we used all possible language combinations within the targeted Romance language family, but excluded the English-Spanish, EnglishFrench, English-Portuguese and Portuguese-French language pair. 3. 3.1. Neural Machine Translation Setup We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. To overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We used the following default neural network training"
2020.globalex-1.15,steinberger-etal-2012-dgt,0,0.0234989,"Missing"
2020.globalex-1.15,C94-1048,0,0.0244137,"aluations. Table 3: Performance of our system on predicting EnglishSpanish Apertium data 3.2. Task Results The official results from the organizers are reproduced in Table 4. We can see from this that in all evaluations, the system described in this paper (labelled ‘NUIG’), produced the highest precision in its results. However, as we saw in the Apertium analysis we had a significant drop in recall compared to the baselines and these overall meant that the system was 2nd or 3rd in terms of F-Measure. We also note that the systems to beat ours were those based on one-time inverse consultation (Tanaka and Umemura, 1994), and it should be relatively easy to combine these results into our architecture, suggesting that we could easily obtain a much stronger result. 3.3. Conclusion Acknowledgements This publication has emanated from research supported in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 P2, co-funded by the European Regional Development Fund, as well as by the H2020 project Prˆet-`a-LLOD under Grant Agreement number 825182. Bibliographical References Discussion Arcan, M., Torregrosa, D., Ahmadi, S., and McCrae, J. P. (2019). Inferring translation ca"
2020.semeval-1.208,Q17-1010,0,0.0160571,"Missing"
2020.semeval-1.208,2020.sltu-1.25,1,0.784813,"tatseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than tra"
2020.semeval-1.208,2020.sltu-1.28,0,0.0231748,"tatseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than tra"
2020.semeval-1.208,N19-1423,0,0.0118949,"e better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embedding (DME) and Contextualised DME (CDME) introduced by Kiela et al. (2018) has shown significant improvement in a variety of natural language processing tasks such as natural language inference, sentiment classification and image-caption retrieval. Pre-trained Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) based models (BERT) have performed exceptionally well in many natural language processing tasks such as text classification, natural language generation and machine translation. We designed our experiments based on CNN + BiLSTM, CDME, DME and BERT. 3 Methodology In this section, we are giving insights about the methodology followed to process and pseudo-label the data in Subsection 3.1 and Subsection 3.2 respectively. 3.1 Data Statistics and Pre-processing Steps The data statistics in Table 1 show that on average, a high number of functional words3 per tweet are present in the dataset. We dec"
2020.semeval-1.208,S19-2107,0,0.0132446,"been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) cla"
2020.semeval-1.208,D18-1176,0,0.0142837,"short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embedding (DME) and Contextualised DME (CDME) introduced by Kiela et al. (2018) has shown significant improvement in a variety of natural language processing tasks such as natural language inference, sentiment classification and image-caption retrieval. Pre-trained Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) based models (BERT) have performed exceptionally well in many natural language processing tasks such as text classification, natural language generation and machine translation. We designed our experiments based on CNN + BiLSTM, CDME, DME and BERT. 3 Methodology In this section, we are giving insights about the methodology foll"
2020.semeval-1.208,D14-1162,0,0.0834896,"OLID dataset to select the classifier which can be used to label the OffensEval 2020 dataset. CNN+BiLSTM: In this architecture, we extracted the abstract features from the text vector using a one dimensional (1D) convolution network and 1D maxpooling, which later has been fed to the BiLSTM to form a vector representation of the tweet. This vectorised text represents the word with its long and short term context in a vector space. Abstract and prominent features captured by the CNN have been contextualised with time steps with the RNN. We used pre-trained GloVe 50d Twitter crawled embeddings (Pennington et al., 2014). 1600 DME and CDME: We used DME and CDME as an ensemble of embeddings to study if it gives better results with the OLID dataset. This architecture takes advantage of both Word2vec (Mikolov et al., 2013) and FastText (Bojanowski et al., 2017) embeddings to learn the vector representation of the word. The Word2vec and FastText embedding of the same words, later on, are projected on the embedding matrix space. These projected vectors are later concatenated. Weights of each embedding have been learned as a hyperparameter using the self-attention model. Unlike DME, CDME has BiLSTM incorporated in"
2020.semeval-1.208,S19-2091,0,0.0286542,"Missing"
2020.semeval-1.208,S19-2136,0,0.0156389,"0), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for senti"
2020.semeval-1.208,2020.trac-1.7,1,0.75126,"its label prediction performance (offensive OFF, not-offensive NOT) and not on these scores, it was up to the participant to decide how to best map the scores to labels. We trained different text classifiers on the OLID (Zampieri et al., 2019a) dataset to predict the labels of the OffensEval 2020. This pseudo-labelling approach avoids the efforts involved in the manual adjustment of threshold values of average confidence (Avg conf ) and average standard deviation (Avg std). 2 Related work There has been significant research on aggression detection, hate speech detection (Ranjan et al., 2016; Rani et al., 2020; Jose et al., 2020) and cyberbullying (Schmidt and Wiegand, 2017). Based on this This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-statseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The"
2020.semeval-1.208,W17-1101,0,0.0277812,"ensive NOT) and not on these scores, it was up to the participant to decide how to best map the scores to labels. We trained different text classifiers on the OLID (Zampieri et al., 2019a) dataset to predict the labels of the OffensEval 2020. This pseudo-labelling approach avoids the efforts involved in the manual adjustment of threshold values of average confidence (Avg conf ) and average standard deviation (Avg std). 2 Related work There has been significant research on aggression detection, hate speech detection (Ranjan et al., 2016; Rani et al., 2020; Jose et al., 2020) and cyberbullying (Schmidt and Wiegand, 2017). Based on this This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-statseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average fun"
2020.semeval-1.208,S19-2128,0,0.0104818,"support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embeddi"
2020.semeval-1.208,2020.trac-1.6,1,0.769081,"Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an"
2020.semeval-1.208,N19-1144,0,0.0360412,"Missing"
2020.semeval-1.208,S19-2010,0,0.0417452,"Missing"
2020.trac-1.6,W18-4411,0,0.0114961,"for offensive content detection, consisting of 743 memes which are annotated with an offensive or not-offensive label. II We used this dataset to implement a multimodal offensive content classifier for memes. III We addressed issues associated with multimodal classification and data collection for memes. 2. Offensive Content Offensive content intends to upset or embarrasses people by being rude or insulting (Drakett et al., 2018). Past work on offensive content detection focused on hate speech detection (Schmidt and Wiegand, 2017; Ranjan et al., 2016; Jose et al., 2020), aggression detection (Aroyehun and Gelbukh, 2018), trolling (Mojica de la Vega and Ng, 2018), and cyberbullying (Arroyo-Fern´andez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in identifying offensive content in text and image. It also describes the research done in the"
2020.trac-1.6,W18-4417,0,0.0412797,"Missing"
2020.trac-1.6,W19-6809,1,0.680023,"ch as sadness, anger, disgust would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to c"
2020.trac-1.6,2020.sltu-1.25,1,0.820212,"st would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to classify the observations. I"
2020.trac-1.6,2020.sltu-1.28,1,0.771907,"st would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to classify the observations. I"
2020.trac-1.6,L18-1585,0,0.113528,"Missing"
2020.trac-1.6,D14-1162,0,0.0821,"Missing"
2020.trac-1.6,2020.trac-1.7,1,0.842073,"to understand the content from a single modality (He et al., 2016). Therefore, it is important to consider both modalities to understand the meaning or intention of the meme. Unfortunately, memes are responsible for spreading hatred in society, because of which there is a requirement to automatically identify memes with offensive content. But due to its multimodal nature, memes which often are the combination of text and image are difficult to regulate by automatic filtering. Offensive or abusive content on social media can be explicit or implicit (Waseem et al., 2017; Watanabe et al., 2018; Rani et al., 2020) and could be classified as explicitly offensive or abusive if it is unambiguously identified as such. As an example, it might contain racial, homophobic, or other offending slurs. In the case of implicit offensive or abusive content, the actual meaning is often obscured by the use of ambiguous terms, sarcasm, lack of profanity, hateful terms, or other means. As they fall under this criterion, memes can be categorized as implicit offensive content. Hence it is difficult to classify them as offensive for human annotators 1 I We created the MultiOFF dataset for offensive content detection, consi"
2020.trac-1.6,W17-1101,0,0.0330318,"lt to classify them as offensive for human annotators 1 I We created the MultiOFF dataset for offensive content detection, consisting of 743 memes which are annotated with an offensive or not-offensive label. II We used this dataset to implement a multimodal offensive content classifier for memes. III We addressed issues associated with multimodal classification and data collection for memes. 2. Offensive Content Offensive content intends to upset or embarrasses people by being rude or insulting (Drakett et al., 2018). Past work on offensive content detection focused on hate speech detection (Schmidt and Wiegand, 2017; Ranjan et al., 2016; Jose et al., 2020), aggression detection (Aroyehun and Gelbukh, 2018), trolling (Mojica de la Vega and Ng, 2018), and cyberbullying (Arroyo-Fern´andez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in"
2020.trac-1.6,2020.semeval-1.99,0,0.131682,"Missing"
2020.trac-1.6,W12-2103,0,0.0836443,"ez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in identifying offensive content in text and image. It also describes the research done in the area of meme analysis as well as multimodality. 3.1. Offensive Content in Text Warner and Hirschberg (2012) model offensive language by developing a Support Vector Machine (SVM) classifier, which takes in features manually derived from the text and classifies if the given text is abusive or not. Djuric et al. (2015) have used n-gram features to classify if the speech is abusive or not. There are many text-based datasets available for aggression identification (Watanabe et al., 2018), hate speech identification (Davidson et al., 2017) and Offensive language detection (Wiegand et al., 2018; Zampieri et al., 2019). Amongst the work mentioned, Watanabe et al. (2018) relies on unigrams and pattern of th"
2020.trac-1.6,W17-3012,0,0.0188561,"al nature of the meme, it is often difficult to understand the content from a single modality (He et al., 2016). Therefore, it is important to consider both modalities to understand the meaning or intention of the meme. Unfortunately, memes are responsible for spreading hatred in society, because of which there is a requirement to automatically identify memes with offensive content. But due to its multimodal nature, memes which often are the combination of text and image are difficult to regulate by automatic filtering. Offensive or abusive content on social media can be explicit or implicit (Waseem et al., 2017; Watanabe et al., 2018; Rani et al., 2020) and could be classified as explicitly offensive or abusive if it is unambiguously identified as such. As an example, it might contain racial, homophobic, or other offending slurs. In the case of implicit offensive or abusive content, the actual meaning is often obscured by the use of ambiguous terms, sarcasm, lack of profanity, hateful terms, or other means. As they fall under this criterion, memes can be categorized as implicit offensive content. Hence it is difficult to classify them as offensive for human annotators 1 I We created the MultiOFF dat"
2020.trac-1.6,S19-2010,0,0.0232044,"the area of meme analysis as well as multimodality. 3.1. Offensive Content in Text Warner and Hirschberg (2012) model offensive language by developing a Support Vector Machine (SVM) classifier, which takes in features manually derived from the text and classifies if the given text is abusive or not. Djuric et al. (2015) have used n-gram features to classify if the speech is abusive or not. There are many text-based datasets available for aggression identification (Watanabe et al., 2018), hate speech identification (Davidson et al., 2017) and Offensive language detection (Wiegand et al., 2018; Zampieri et al., 2019). Amongst the work mentioned, Watanabe et al. (2018) relies on unigrams and pattern of the text for detecting hate speech. These patterns are carefully crafted manually and then provided to machine learning models for further classification. Wiegand et al. (2018; Zampieri et al. (2019) deals with the classification of hateful tweets in the German language and addresses some of the issues in identifying offensive content. All this research puts more weight on features of single modality i.e. text and manual feature extraction. We work on memes which have more than one modality, i.e. image and t"
2020.vardial-1.6,D16-1250,0,0.0266056,"he induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1"
2020.vardial-1.6,P17-1042,0,0.391423,"ems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:  W ∗ = arg max Tr XW"
2020.vardial-1.6,P18-1073,0,0.122315,"re languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results could be improved considerably. This task is further complicated by the fact that many languages use distinct scripts, and as such learning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. F"
2020.vardial-1.6,P19-1494,0,0.0166961,"s by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:  W ∗ = arg max Tr XW Z T DT , (2) W where Tr(·) is the trace operator (the sum of all diag"
2020.vardial-1.6,P19-1302,0,0.0747503,"a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il , i2 , . . . , ik ] of indices of X such that for all j = 1, 2, . . . , k, where xij = zj . Given two sequences X and Y , the LCS of X and Y is a common subsequence with maximum length. More formally:  if i = 0 or j = 0  ∅ LCS (Xi−1"
2020.vardial-1.6,N10-1083,0,0.0778924,"Missing"
2020.vardial-1.6,Q17-1010,0,0.0329446,"acted from wikimedia.org for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into a single script, we transcribed the Wik"
2020.vardial-1.6,2018.gwc-1.10,1,0.779128,"transliteration, and we demonstrate that it is an effective and necessary step which yields more isomorphic embeddings and obtains more robust BLI. Second, we show that the use of the longest common subsequence (LCS) is a superior method of assessing the cognate similarity. 2 Dravidian Languages Dravidian languages are the common terminology (Caldwell, 1856) used to represent the South Indian languages, which consist of around 26 languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written docume"
2020.vardial-1.6,W19-7101,1,0.544357,"The Tamil, Malayalam and Telugu languages have their own written script symbols whereas Telugu and Kannada have significant similarities in their script symbols. Though Telugu and Kannada have these similarities, they are not readily intelligible for speakers of the language. The study of languages suggests that these languages formed a single language around late 4000 BCE and then started evolving on their own (Steever, 2015). Since the languages evolved sharing geographical, etymological and political borders, the cognates may have evolved similar meanings or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translate"
2020.vardial-1.6,W19-6809,1,0.894503,"The Tamil, Malayalam and Telugu languages have their own written script symbols whereas Telugu and Kannada have significant similarities in their script symbols. Though Telugu and Kannada have these similarities, they are not readily intelligible for speakers of the language. The study of languages suggests that these languages formed a single language around late 4000 BCE and then started evolving on their own (Steever, 2015). Since the languages evolved sharing geographical, etymological and political borders, the cognates may have evolved similar meanings or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translate"
2020.vardial-1.6,2020.sltu-1.25,1,0.768298,"languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written documents, as well as well-defined lexicons and grammar for the selected languages (Chakravarthi et al., 2020c). Hence in this work, we will focus on the four chosen Dravidian languages Tamil, Malayalam, Kannada and Telugu which are spoken by approximately 210 million people (Steever, 2015) across the world either as their first or second language. Figure 1: Example of cognate words for the Dravidian language Even though these language"
2020.vardial-1.6,2020.sltu-1.28,1,0.746996,"languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written documents, as well as well-defined lexicons and grammar for the selected languages (Chakravarthi et al., 2020c). Hence in this work, we will focus on the four chosen Dravidian languages Tamil, Malayalam, Kannada and Telugu which are spoken by approximately 210 million people (Steever, 2015) across the world either as their first or second language. Figure 1: Example of cognate words for the Dravidian language Even though these language"
2020.vardial-1.6,P11-1042,0,0.019873,"t the usage of Latin script outperforms the IPA for Multilingual NMT for Dravidian languages. This was proven with a cosine similarity of the corpus showing that transcribing the text into the Latin script retain more similarity. Inspired by this, we used the Indic-trans library by Bhat et al. (2015) to transliterate. We show the example of a comparison of NLCS and NL between languages for examples of cognate words in Table 1. Previous methods based on edit-distance and orthographic similarity are proposed for using linguist features for word alignments by supervised and unsupervised methods (Dyer et al., 2011; Berg-Kirkpatrick et al., 2010; Hauer et al., 2017). Hauer et al. (2017) created a seed dictionary based on the cognates of 61 Language Pairs kan-mal kan-mal kan-tam kan-tam kan-tel kan-tel mal-tam mal-tam mal-tel mal-tel tam-tel tam-tel Word Pair English Translation NLCS NL hajaradant-hajarulla rahasyadaan-rahasyadan navratn-navmani tandeilladant-tacoppanillat poojaniyavadantah-poojyaniyulu atyagatyavadant-atyavasaramin navaratnam-navmani tatanillat-tacoppanillat navaratnam-navratnalu tatanillat-tandriless tacoppanillat-tandriless sammadam-samardhinchada arriving secret nine gems having no l"
2020.vardial-1.6,W95-0114,0,0.299749,"gs or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source"
2020.vardial-1.6,C88-1042,0,0.579926,"and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Ar"
2020.vardial-1.6,L18-1550,0,0.0137973,"for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into a single script, we transcribed the Wikidump corpus before c"
2020.vardial-1.6,P08-1088,0,0.203984,"equence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However"
2020.vardial-1.6,E17-2098,0,0.0223466,"r Multilingual NMT for Dravidian languages. This was proven with a cosine similarity of the corpus showing that transcribing the text into the Latin script retain more similarity. Inspired by this, we used the Indic-trans library by Bhat et al. (2015) to transliterate. We show the example of a comparison of NLCS and NL between languages for examples of cognate words in Table 1. Previous methods based on edit-distance and orthographic similarity are proposed for using linguist features for word alignments by supervised and unsupervised methods (Dyer et al., 2011; Berg-Kirkpatrick et al., 2010; Hauer et al., 2017). Hauer et al. (2017) created a seed dictionary based on the cognates of 61 Language Pairs kan-mal kan-mal kan-tam kan-tam kan-tel kan-tel mal-tam mal-tam mal-tel mal-tel tam-tel tam-tel Word Pair English Translation NLCS NL hajaradant-hajarulla rahasyadaan-rahasyadan navratn-navmani tandeilladant-tacoppanillat poojaniyavadantah-poojyaniyulu atyagatyavadant-atyavasaramin navaratnam-navmani tatanillat-tacoppanillat navaratnam-navratnalu tatanillat-tandriless tacoppanillat-tandriless sammadam-samardhinchada arriving secret nine gems having no living father worthy of adoration primary nine gems h"
2020.vardial-1.6,P18-4003,0,0.0247781,"s a phonetic alphabet, such as the International Phonetic Alphabet (IPA); however, transcribing into the Latin script or non-native script is prevalent due to the ubiquity of the US/UK keyboard. The IPA is an evolving standard initially developed by the International Phonetic Association in 1888 with the goal of transcribing the sounds of all human languages. Transliteration is used to help language learners to read words written in foreign scripts, by writing the sound of the word using the equivalent letters. Romanisation remains a popular technique for transliteration of various languages (Hermjakob et al., 2018). The use of the Latin script for text entry of South Asian languages is common, even though there is no standard orthography for these languages in the script (Wolf-Sonkin et al., 2019). The 107 symbols used for writing the IPA are taken primarily from the Latin and Greek scripts some are novel creations. Diacritics are used for subtle distinctions in sounds and to show nasalisation of vowels, length, stress, and tones. Using IPA symbols, one can represent the pronunciation of words. Nevertheless, the study by Chakravarthi et al. (2019a) and Chakravarthi et al. (2019c) shows that transliterat"
2020.vardial-1.6,E14-1060,0,0.0242626,"e to 0. 3.2 Longest Common Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we u"
2020.vardial-1.6,J17-2001,0,0.132888,"used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method cou"
2020.vardial-1.6,N03-2016,0,0.480793,"e to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-related languages has shown to perform better than unrelated languages (Irvine and Callison-Burch, 2017), since closely related languages often share similar linguistics properties and cognates (Nasution et al., 2016). Cognates are words that have a similar meaning and similar orthography based on etymological relationships (Kondrak et al., 2003). Computational models of monolingual embeddings also exhibit isomorphism across closely related languages (Mikolov et al., 2013b; Ormazabal This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 57 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 57–69 Barcelona, Spain (Online), December 13, 2020 et al., 2019) based on the assumption that word embeddings in different languages have approximately the same structure. This isomorphic property was exploi"
2020.vardial-1.6,P04-1077,0,0.0187852,"This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il , i2 , . . . , ik ] of indices of X such that for all j = 1, 2, . . . , k, where xij = zj . Given t"
2020.vardial-1.6,W95-0115,0,0.299926,"edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il"
2020.vardial-1.6,J99-1003,0,0.210571,"w2 )) . The edit distance for a subset of possible word pairs is just considered as by how far most of word sets are orthographically unique, resulting in a normalised edit distance close to 1 and an orthographic similarity close to 0. 3.2 Longest Common Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced"
2020.vardial-1.6,L16-1524,0,0.0276039,"earning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-related languages has shown to perform better than unrelated languages (Irvine and Callison-Burch, 2017), since closely related languages often share similar linguistics properties and cognates (Nasution et al., 2016). Cognates are words that have a similar meaning and similar orthography based on etymological relationships (Kondrak et al., 2003). Computational models of monolingual embeddings also exhibit isomorphism across closely related languages (Mikolov et al., 2013b; Ormazabal This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 57 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 57–69 Barcelona, Spain (Online), December 13, 2020 et al., 2019) based on th"
2020.vardial-1.6,P19-1492,0,0.0277965,"Missing"
2020.vardial-1.6,P19-1018,0,0.0596185,"comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results could be improved considerably. This task is further complicated by the fact that many languages use distinct scripts, and as such learning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-rel"
2020.vardial-1.6,D14-1162,0,0.0832826,"r of sentences and number of tokens extracted from wikimedia.org for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into"
2020.vardial-1.6,W15-5910,0,0.0190922,"pts, and used Levenshtein distance without considering morphological properties. 3.4 Data Lexicons such as WordNet (Miller, 1995; Miller, 1998) for English or EuroWordNet for European languages are lexical resources which were used to improve MT quality. EuroWordNet is a cross-lingual synonym resource that linked WordNet synsets across European languages (Vossen, 1997). Similarly, IndoWordNet (Bhattacharyya, 2010) links WordNet synsets across major Indian languages from the Indo-Aryan, Dravidian and Sino-Tibetan families. An online multilingual dictionary for Indian languages was developed by Redkar et al. (2015) from IndoWordNet. However, this dictionary is not publicly accessible. To train and evaluate the quality of BLI, a seed dictionary and a test set of the bilingual lexicon is required. For the Dravidian languages, there is no existing seed dictionary, so we used the IndoWordNet. To create a seed dictionary, we used the IndoWordNet ID to link the WordNet entries for Tamil, Telugu, Malayalam and Kannada. We map the one-to-many word mapping from IndoWordNet to one-to-one word mapping by replicating the source word. Table 2 shows the seed dictionary statistics and Table 3 shows the statistics for"
2020.vardial-1.6,P18-2062,0,0.115953,"This isomorphic property was exploited by Artetxe et al. (2018) and Lample et al. (2018) to map monolingual word embeddings in different languages to a shared space through a linear transformation. For closely-related languages, it follows that cognates can be used as a form of alignment as words that have a similar form are quite likely to be cognates and therefore could be used as a weak seed dictionary. Given previous work on the use of seed dictionaries (Patra et al., 2019), the usage of such alignments is likely to improve performance of BLI. Previous works used the Levenshtein distance (Riley and Gildea, 2018); however, this is not linguistically well-motivated as it allows for multiple changes that are not consistent with the kinds of changes seen etymologically. The goal of this work is to exploit the orthographic information between languages that use a different script. For that purpose, we bring the languages into a single script, which allows us to take advantage of the cognate properties of closely related languages. This paper has two principal contributions: first, we study the use of transliteration, and we demonstrate that it is an effective and necessary step which yields more isomorphi"
2020.vardial-1.6,rosner-sultana-2014-automatic,0,0.0296233,"lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results c"
2020.vardial-1.6,W16-2009,0,0.0248562,"ommon Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our w"
2020.vardial-1.6,D12-1003,0,0.020776,"ed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language w"
2020.vardial-1.6,P98-2212,0,0.352374,"roves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that wi"
2020.vardial-1.6,P16-1024,0,0.0671408,"Missing"
2020.vardial-1.6,P15-2118,0,0.0592429,"Missing"
2020.vardial-1.6,W19-3114,0,0.0285754,"keyboard. The IPA is an evolving standard initially developed by the International Phonetic Association in 1888 with the goal of transcribing the sounds of all human languages. Transliteration is used to help language learners to read words written in foreign scripts, by writing the sound of the word using the equivalent letters. Romanisation remains a popular technique for transliteration of various languages (Hermjakob et al., 2018). The use of the Latin script for text entry of South Asian languages is common, even though there is no standard orthography for these languages in the script (Wolf-Sonkin et al., 2019). The 107 symbols used for writing the IPA are taken primarily from the Latin and Greek scripts some are novel creations. Diacritics are used for subtle distinctions in sounds and to show nasalisation of vowels, length, stress, and tones. Using IPA symbols, one can represent the pronunciation of words. Nevertheless, the study by Chakravarthi et al. (2019a) and Chakravarthi et al. (2019c) shows that transliteration into the Latin Script is best suited to take advantage of cognate information from closely related languages. For example, LCS of the input sequence “AABCDH” and “AABHEDE” is “AAB” o"
2020.vardial-1.6,P17-1179,0,0.0231617,"can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:"
2020.webnlg-1.15,W14-3348,0,0.0312581,"sing the Adam optimizer with a learning rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains over the baselines right out of the b"
2020.webnlg-1.15,2020.webnlg-1.7,0,0.227841,"Missing"
2020.webnlg-1.15,W17-3518,0,0.147703,"e (Gatt and Krahmer, 2018). However, there has been a shift recently towards end-to-end architectures using neural networks to convert data in the input to text in a natural language in the output. In our submission, we employ an end-to-end approach using the T5 model architecture (Raffel et al., 2020) which is pre-trained on a large corpus of text scraped from the Web. We fine-tune the T5 model on the WebNLG+ corpus and explore 1 RDF - Resource Description Framework various pre-training and pre-processing strategies to improve the performance of our system. 2 Background The WebNLG challenge (Gardent et al., 2017) was created with the goal of producing a common benchmark to compare “microplanners”, i.e, generation systems that verbalise non-linguistic content to text in some human language. In 2017, the challenge received a mix of submissions built using template or grammar-based pipeline, statistical machine translation (SMT) and neural machine translation (NMT) frameworks. The test set used for final evaluation was split into two subsets, seen and unseen. The first subset contained data from the categories that were also present in the training set while the second included new data from unseen categ"
2020.webnlg-1.15,2020.inlg-1.14,0,0.018246,"NMT and SMT-based systems mostly outperformed the rule-based pipeline sytems in terms of BLEU and TER score. However, the scores for the NMT-based systems dropped significantly on the unseen categories while the rule-based systems were able to generalise better on the new and unseen domains. Further work by Castro Ferreira et al. (2019) compared pipeline-based and end-to-end architectures and their findings also suggest that the systems which are trained end-to-end are comparable to pipeline methods on the seen categories but do not generalise to new and unseen domains of data. More recently, Kale (2020) have shown that applying transfer learning using an end-to-end pretrained model such as T5 achieves state-of-the-art results on three benchmark datasets for data-totext generation and performs well even on out-ofdomain inputs in the unseen categories of data. The T5 model (Raffel et al., 2020) follows a transformer-based encoder-decoder architecture 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 137–143, 2020 Association for Computational Linguistics Attribution 4.0 International. Tripleset 200 Pu"
2020.webnlg-1.15,P17-4012,0,0.02376,"F-triples given in the input. 139 5 Experimental Setup We adopt the WebNLG baseline system (Gardent et al., 2017) as one of the baseline architectures for our experiments, which is a vanilla sequence-tosequence LSTM model with attention (Bahdanau et al., 2015) where the RDF-triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture (Vaswani et al., 2017) similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). These baseline models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for two baseline models. Two hidden layers and 500 units per hidden layer with input feeding (Luong et al., 2015) enabled and word embeddings of size 500-dimensions are used for the LSTM neural model. Dropout is applied with value 0.3 and the LSTM model is trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-h"
2020.webnlg-1.15,D18-2012,0,0.0119781,"s not appear to be any significant improvements with the pre-trained model. The addition of &lt;SUB>, &lt;PRED> and &lt;OBJ> tags in the T5+tags model improves the BLEU score for unseen categories by more than 2 points from 37.4 to 39.5. However, for seen categories, there is a drop of about of 0.9. Information about entity types from DBpedia also appears to be useful for the unseen categories, improving the BLEU score from 37.4 to 38.9 for the T5+types model. However, it also leads to a performance drop by about 4 points for each metric in the case of seen categories. The T5 model uses SentencePiece (Kudo and Richardson, 2018) for subword tokenisation to handle unknown and rare tokens, such as the multi-word predicates in this corpus. However, Data Coverage Relevance Correctness Text Structure Fluency 92.892 (0.17) 92.066 (0.127) 92.063 (0.116) 95.442 (0.251) 93.784 (0.161) 92.588 (0.113) 94.061 (0.161) 94.392 (0.139) 91.794 (0.19) 90.138 (0.13) 92.053 (0.189) 94.149 (0.256) 87.4 (0.039) 85.737 (-0.064) 91.588 (0.258) 92.105 (0.254) 82.43 (0.011) 80.941 (-0.143) 88.898 (0.233) 89.846 (0.279) 95.296 (0.28) 90.253 (0.065) 91.253 (0.059) 95.491 (0.264) 94.568 (0.153) 89.568 (-0.043) 94.512 (0.178) 94.142 (0.135) 93.59"
2020.webnlg-1.15,D15-1166,0,0.0122312,"tures for our experiments, which is a vanilla sequence-tosequence LSTM model with attention (Bahdanau et al., 2015) where the RDF-triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture (Vaswani et al., 2017) similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). These baseline models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for two baseline models. Two hidden layers and 500 units per hidden layer with input feeding (Luong et al., 2015) enabled and word embeddings of size 500-dimensions are used for the LSTM neural model. Dropout is applied with value 0.3 and the LSTM model is trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model is trained using Adam optimizer (Kingma and"
2020.webnlg-1.15,P02-1040,0,0.109128,"ctively with a batchsize of 32 using the Adam optimizer with a learning rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains o"
2020.webnlg-1.15,W17-4770,0,0.0903171,"Missing"
2020.webnlg-1.15,2020.acl-main.704,0,0.0407051,"with a pretraining strategy relevant for this task. For our final submission to the WebNLG+ challenge 2020, we train a “base” variant of the T5 model using data from the entire training set of the WebNLG+ corpus. Before fine-tuning the T5base model, we split the multi-word predicates and add &lt;SUB>, &lt;PRED> and &lt;OBJ> tags for subjects, predicates and objects respectively. Table 3 shows the automatic evaluation results for our submission using the GERBIL NLG framework (Moussalem et al., 2020) on the WebNLG+ test set in terms of chrf++ (Popovi´c, 2017), BERT score (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) along with BLEU, METEOR and TER scores. Our 141 system ranks among the top 5 for most of these evaluation metrics across all categories. In terms of BLEU score, our submission achieves scores of 58.26 for seen categories and 45.52 for the unseen categories. For the test set containing unseen entities, our system achieves the highest BLEU score of 52.76 and ranks among the top two for most of the automatic evaluation metrics. Table 4 shows results of human evaluation on the WebNLG+ test set for our submission along with two baselines and the reference lexicalisation. For the evaluation, human"
2020.webnlg-1.15,2006.amta-papers.25,0,0.0404603,"ing rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains over the baselines right out of the box. Even though the baseline t"
2020.webnlg-1.15,2020.emnlp-demos.6,0,0.0329574,"Missing"
2020.webnlg-1.6,W14-3348,0,0.0112758,"models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model 2 6 Results and Discussion In this section, we report the results of our experiments in terms of two commonly used evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We also report scores in terms of precision and recall of the input entities covered in the output generations. For statistical significance, we use MultEval (Clark et al., 2011) to perform bootstrap resampling and report the results on three test sets consisting of instances from seen, unseen and all categories in Table 3. For the test set consisting of seen categories, using KGEs shows consistent improvement over the baseline models for both LSTM and transformer architectures. This improvement is observed in both cases whether the delexicalisation step is performed or not. For the models t"
2020.webnlg-1.6,N18-1029,0,0.0447332,"Missing"
2020.webnlg-1.6,W17-3518,0,0.269612,"tput. This trend is largely inspired by the success of the end-to-end approaches in the related task of machine translation as well as the availability of large corpora for data-to-text generation such as the WikiBio (Lebret et al., 2016) or the ROTOWIRE (Wiseman et al., 2017) datasets, which contain input data in the form of a table consisting of rows and columns. However, the structure and representation of the input data can vary significantly depending on the task at hand. For example, the input can also be a knowledge graph (KG) represented as a set of RDF-triples like the WebNLG corpus (Gardent et al., 2017) or a dialogue-act-based meaning representation like the E2E dataset (Novikova et al., 2017). In this work, we employ pre-trained knowledge graph embeddings (KGEs) for data-to-text generation with a model which is trained in an end-toend fashion using an encoder-decoder style neural network architecture. These embeddings have been shown to be useful in similar end-to-end architectures especially in domain-specific and underresourced scenarios for machine translation (Moussallem et al., 2019). We focus on the WebNLG corpus which contains RDF-triples paired with verbalisations in English. We com"
2020.webnlg-1.6,E17-2068,0,0.0462123,"semantically-enriched also take into account the associated semantic information. Approaches where relationships are interpreted as displacements operating on the lowdimensional embeddings of the entities, have been implemented within the TransE toolkit (Bordes et al., 2013). RDF2Vec (Ristoski and Paulheim, 2016) uses language modelling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. Cochez et al. (2017) exploited the Global Vectors algorithm in RDF2Vec to compute embeddings from the co-occurrence matrix of entities and relations. However, Joulin et al. (2017b) showed that a BoW based approach with the fastText algorithm (Joulin et al., 2017a) generates state-of-the-art results in KGEs. For data-to-text generation, Chen et al. (2019) have shown that leveraging external knowledge is useful in generating text from Wikipedia infoboxes. In our work, we incorporate pre-trained KGEs based on the fastText model with an end-toend approach for the data-to-text generation. Table 2: Example of an input tripleset paired with reference text in the output (top) and corresponding delexicalised version (bottom). input paired with 42,873 lexicalisations in the out"
2020.webnlg-1.6,D19-1052,0,0.0348328,"Missing"
2020.webnlg-1.6,P17-4012,0,0.0126405,"ation on the test set we take the checkpoint with the best BLEU score on the validation set. Experimental Setup We follow the WebNLG baseline system (Gardent et al., 2017) as one of the baseline architectures for our experiments, which is a vanilla sequenceto-sequence LSTM model with attention where the RDF triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). The models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for the baseline model: two hidden layers with 500 LSTM units per hidden layer and word embeddings of 500 dimensions. Dropout is applied with value 0.3 and the LSTM models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with val"
2020.webnlg-1.6,D19-1299,0,0.0180416,"mbeddings of the entities, have been implemented within the TransE toolkit (Bordes et al., 2013). RDF2Vec (Ristoski and Paulheim, 2016) uses language modelling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. Cochez et al. (2017) exploited the Global Vectors algorithm in RDF2Vec to compute embeddings from the co-occurrence matrix of entities and relations. However, Joulin et al. (2017b) showed that a BoW based approach with the fastText algorithm (Joulin et al., 2017a) generates state-of-the-art results in KGEs. For data-to-text generation, Chen et al. (2019) have shown that leveraging external knowledge is useful in generating text from Wikipedia infoboxes. In our work, we incorporate pre-trained KGEs based on the fastText model with an end-toend approach for the data-to-text generation. Table 2: Example of an input tripleset paired with reference text in the output (top) and corresponding delexicalised version (bottom). input paired with 42,873 lexicalisations in the output. We follow the same structure for splitting the dataset into training and test sets as defined in the challenge. The final evaluation is done on a test set split into seen an"
2020.webnlg-1.6,D16-1128,0,0.0617342,"Missing"
2020.webnlg-1.6,N19-1236,0,0.0270887,"Missing"
2020.webnlg-1.6,W17-5525,0,0.054177,"Missing"
2020.webnlg-1.6,P02-1040,0,0.106413,"pplied with value 0.3 and the LSTM models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model 2 6 Results and Discussion In this section, we report the results of our experiments in terms of two commonly used evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We also report scores in terms of precision and recall of the input entities covered in the output generations. For statistical significance, we use MultEval (Clark et al., 2011) to perform bootstrap resampling and report the results on three test sets consisting of instances from seen, unseen and all categories in Table 3. For the test set consisting of seen categories, using KGEs shows consistent improvement over the baseline models for both LSTM and transformer architectures. This improvement is observed in both cases whether the delexicalisation ste"
2020.webnlg-1.6,D14-1162,0,0.0860599,"fashion using an encoder-decoder style neural network architecture. These embeddings have been shown to be useful in similar end-to-end architectures especially in domain-specific and underresourced scenarios for machine translation (Moussallem et al., 2019). We focus on the WebNLG corpus which contains RDF-triples paired with verbalisations in English. We compare the use of KGEs to two baseline models – the standard sequenceto-sequence model with attention (Bahdanau et al., 2015) and the transformer model (Vaswani et al., 2017). We also do a comparison with pre-trained GloVe word-embeddings (Pennington et al., 2014). 2 Related Work Castro Ferreira et al. (2019) have compared pipeline-based and end-to-end architectures for data-to-text generation on the WebNLG corpus. Their findings suggest that the systems which are trained end-to-end are comparable to pipeline methods on seen data categories but do not generalise to new and unseen domains of data. Marcheggiani and Perez-Beltrachini (2018) proposed an encoder based on graph convolutional networks to exploit the structure in the input for an end-to-end system which showed a slight improvement over the standard LSTM encoder. However, their test set did not"
2020.wildre-1.2,K19-1096,0,0.0110666,"015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the image. The majority of memes do not"
2020.wildre-1.2,2018.gwc-1.10,1,0.835805,"Missing"
2020.wildre-1.2,W19-7101,1,0.674614,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,W19-6809,1,0.756237,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,2020.sltu-1.25,1,0.812156,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,2020.sltu-1.28,1,0.822974,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,W17-3001,0,0.0240583,"ga and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and the"
2020.wildre-1.2,W15-5948,0,0.024406,"memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that tend to be offensive, provocative, or menacing (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018)”. Their main function"
2020.wildre-1.2,W18-4409,0,0.0195459,"pinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the i"
2020.wildre-1.2,W18-4401,0,0.126908,"g in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is fi"
2020.wildre-1.2,malmasi-zampieri-2017-detecting,0,0.0213379,"4 would be “Sorry my friend (girl)”. As this example does not contain any provoking or offensive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 201"
2020.wildre-1.2,W18-3504,0,0.0321038,"and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to"
2020.wildre-1.2,P16-2065,0,0.0169659,"troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer"
2020.wildre-1.2,K15-1032,0,0.0178824,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,R15-1058,0,0.0223515,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,L18-1585,0,0.0292017,"Missing"
2020.wildre-1.2,P18-2031,0,0.0198473,"2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in dete"
2020.wildre-1.2,2020.trac-1.7,1,0.686215,"o be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is five Rupees”, whereby"
2020.wildre-1.2,2020.trac-1.6,1,0.747165,"ial networks. Due to this freedom, people are becoming creative in their jokes by making memes. Although memes are meant to be humorous, sometimes it becomes threatening and offensive to specific people or community. On the Internet, a troll is a person who upsets or starts a hatred towards people or community. Trolling is the activity of posting a message via social media that is intended to be offensive, provocative, or menacing to distract which often has a digressive or off-topic content with the intent of provoking the audience (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Suryawanshi et al., 2020). Despite this growing body of research in natural language processing, identifying trolling in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a seri"
2020.wildre-1.2,Q14-1006,0,0.0309954,"r balance male and female annotators). Based on Landis and Koch (1977) and given the inherent obscure nature of memes, we got fair agreement amongst the annotators. K= 4.5. Data Statistics We collected 2,969 memes, of which most are images with text embedded on them. After the annotation, we learned that the majority (1,951) of these were annotated as troll memes, and 1,018 as not-troll memes. Furthermore, we observed that memes, which have more than one image have a high probability of being a troll, whereas those with only one image are likely to be not-troll. We included Flickr30K2 images (Young et al., 2014) to the not-troll category to address the class imbalance. Flickr30K is only added to training, while the test set is randomly chosen from our dataset. In all our experiments the test set remains the same. 5. 6. We experimented with ResNet and MobileNet. The variation in experiments comes in terms of the data on which the models have been trained on, while the test set (300 memes) remained the same for all experiments. In the first variation, TamilMemes in Table 1, we trained the ResNet and MobileNet models on our Tamil meme dataset(2,669 memes). The second variation, i.e. TamilMemes + ImageNe"
2021.deelio-1.8,S12-1052,0,0.255205,", we explore if causal knowledge is useful for question answering and present strategies on how to enhance a pretrained language model with causal knowledge. There is limited work on incorporating external causal knowledge to improve question answering and no prior work on using causal knowledge to improve multiple-choice question answering. The task of causal question answering aims to reason about cause and effects over a provided real or hypothetical premise. Specifically, we explore the multiple-choice formulation of this task in the context of the COPA (Choice of Plausible Alternatives) (Gordon et al., 2012b) and WIQA (What If Reasoning over Procedural Text) (Tandon et al., 2019) benchmark tasks. COPA and WIQA are both challenging causal reasoning tasks. WIQA requires reasoning on hypothetical perturbations to procedural descriptions of events. Consider the example in Figure 1. To answer the hypothetical question about the downstream effect of an increase of ash and cloud on the environment, the model must be able to causally link Event 3 (about ash clouds) to Event 5 (erupted materials disturb the environment). If provided a causal fact such as (ash clouds, cause-effect, environmental disturban"
2021.deelio-1.8,D19-6004,0,0.0213172,"COPA was first introduced as a SemEval 2012 shared task (Gordon et al., 2012a). COPA consists of a premise and two alternatives. The task is to identify which alternative is most likely the cause or effect of the provided premise. Background commonsense causal knowledge is required to successfully answer questions as there is limited lexical overlap between the premise and alternatives. The COPA dataset consists of 1,000 questions, broken into 500 development and 500 test questions. Recent pretrained models such as BERT and RoBERTa have seen improved performance on the COPA dataset. However, Kavumba et al. (2019) found that these models exploited superficial cues such as the token frequency in the correct answers. To mitigate this effect, Kavumba et al. expanded the development set to include mirror instances to balance the lexical distribution between correct and incorrect answers. For each set of alternatives, the mirror instance introduces a new premise, where the previous correct alternative is now incorrect. This new dataset, called COPA-Balanced, also categorized the test set into easy and hard groups. The easy group consists of 190 questions where RoBERTa-Large and BERT-Large could answer corre"
2021.deelio-1.8,P19-1470,0,0.0291252,"d causal facts. CauseNet consists of about 12 million concepts and 11.5 million relations extracted from Wikipedia and ClueWeb12 1 . ConceptNet (Speer et al., 2017), a public knowledge graph, consists of 36 relations and includes a causes relation. The ATOMIC (Sap et al., 2019) knowledge base consists of 877k textual descriptions of inferential knowledge organized around event prompts and agent-centric activities. ATOMIC describes the social and commonsense knowledge of these events along nine if-then relations which describe the event’s causes and effects on other agents/participants. COMET (Bosselut et al., 2019) is a language model adaptation framework that is trained on ATOMIC and ConceptNet to generate novel commonsense facts and construct robust commonsense knowledge bases. This paper uses CauseNet as its primary source for causal knowledge as it contains a broad and deep set of causal facts (including descriptions of physical processes relevant to WIQA). Next we provide a summary of the question answering tasks which require causal reasoning. The task of binary causal question answering poses questions of cause and effect as yes/no questions (i.e. Could X cause Y?). Hassanzadeh et al. evaluate th"
2021.deelio-1.8,N19-1112,0,0.156096,"bout causality. Causal facts are generally extracted from natural language descriptions. For example, the statement Global warming is caused primarily by human activities such as coal-burning power plants would yield the causal fact factories cause global warming. These causal facts can also be described explicitly in a knowledge base or expressed formally as triples with an explicit cause-effect relation. For exRecent model-based approaches for question answering tasks have primarily focused on finetuning pretrained transformer-based language models, such as BERT (Devlin et al.) and RoBERTa (Liu et al., 2019c), on task-specific datasets. These language models have been found to contain transferable linguistic knowledge (Liu et al., 2019a) and general knowledge (Petroni et al., 2019) that are effective for most downstream natural language processing (NLP) tasks. For more complex tasks, such as causal reasoning, pretrained language models are often limited as they lack the specific external background knowledge required to effectively reason about causality. 70 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"
2021.deelio-1.8,2021.ccl-1.108,0,0.0745963,"Missing"
2021.deelio-1.8,D19-1005,0,0.0291255,"nowledge improves RoBERTa’s performance to nearly match the current state-of-the-art (SOTA) and improve upon the SOTA in specific sub-categories such as in-paragraph and out-of-paragraph reasoning. • Premise: Air pollution in the city worsened. What was the CAUSE of this? 2 • Alternative 1: Factories increased their production. Related Work Enhancing language models with external knowledge (in the form of a knowledge graph or knowledge base) remains an open problem. Several promising strategies have emerged for injecting knowledge into large language models as part of the pretraining process. Peters et al. (2019) present the Knowledge Attention and Recontextualization • Alternative 2: Factories shut down. Lexically, there is limited information in the premise and alternatives that the model can exploit to answer the question. To successfully answer this question, the model requires both background 71 (KAR) layer which can be inserted into a neural language model architecture and used to train knowledge enhanced contextual embeddings. Liu et al. (2019b) introduce the K-BERT model which learns knowledge enabled representations from sentence trees that consist of inputs augmented with knowledge triples."
2021.deelio-1.8,D16-1014,0,0.0390732,"Missing"
2021.gem-1.13,W05-0909,0,0.443849,"hich consists of 6 layers each in the encoder and decoder with a multi-head attention sub-layer consisting of 8 attention heads. The word embeddings have a dimension of 512 and the fully-connected feed-forward sublayers are 2048dimensional. Pre-training on DBpedia abstracts is done on a single Nvidia GeForce GTX 1080 Ti GPU for 10 epochs with a batch size of 8 using the Adam optimizer with a learning rate of 0.001. All the other hyperparameter values are set to their default values. Table 1 shows scores for the output generations on the validation set for BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). We find random masking to perform the best in terms of automatic evaluation metrics compared to specifically masking entities or predicates, though the results are not statistically significantly different. Furthermore, in our experiments we compare the results when additional tags are added to the input either as entity types from DBpedia or NER tags from spaCy or just the <SUB>, <PRED> and <OBJ> tags. For this, we use the T5-base model with approximately 220 million parameters. This model consists of 12 layers each in the encoder and decoder with 12 attention heads"
2021.gem-1.13,2020.webnlg-1.7,0,0.0545537,"Missing"
2021.gem-1.13,N19-1423,0,0.0305047,"onGen, hence scores on some subsets are not shown. The evaluation metrics are divided into different categories measuring lexical similarity, semantic equivalence, diversity and system characteristics. Popular metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-1/2/L (Lin, 2004) are used for lexical similarity, while recently proposed metrics such as 2 https://github.com/GEM-benchmark/ GEM-metrics BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) which rely on sentence embeddings from pre-trained contextualised embedding models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are used for evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) is calculated (Li et al., 2018). The size of the output vocabulary (|V|) and the mean length of the generated output te"
2021.gem-1.13,W19-8652,0,0.0360191,"Missing"
2021.gem-1.13,W17-3518,0,0.37912,"ions for NLG have relied on rulebased systems designed using a modular pipeline approach (Gatt and Krahmer, 2018). However, recently approaches based on neutral networks with an encoder-decoder architecture trained in an endto-end fashion have gained popularity. These typically follow the paradigm of pre-training on a large corpus followed by fine-tuning on a task specific dataset and have been shown to achieve state-of-theart results on many natural language tasks (Raffel In our participation in the GEM benchmark, we submit outputs for four datasets including DART (Nan et al., 2021), WebNLG (Gardent et al., 2017; Castro Ferreira et al., 2020), E2E (Novikova et al., 2017; Duˇsek et al., 2019) and CommonGen (Lin et al., 2020). We use the pre-trained T5-base model architecture (Raffel et al., 2020) for our submission implemented using the transformers library from Hugging Face (Wolf et al., 2020). We first train on monolingual data before fine-tuning on the task-specific dataset. For DART and WebNLG, we use abstracts from DBpedia (Auer et al., 2007) for training while for the other two datasets, we use monolingual target-side references for pre-training with a masked language modeling objective. We expe"
2021.gem-1.13,2020.acl-main.703,0,0.0996048,"Missing"
2021.gem-1.13,2020.findings-emnlp.165,0,0.0344977,"wever, recently approaches based on neutral networks with an encoder-decoder architecture trained in an endto-end fashion have gained popularity. These typically follow the paradigm of pre-training on a large corpus followed by fine-tuning on a task specific dataset and have been shown to achieve state-of-theart results on many natural language tasks (Raffel In our participation in the GEM benchmark, we submit outputs for four datasets including DART (Nan et al., 2021), WebNLG (Gardent et al., 2017; Castro Ferreira et al., 2020), E2E (Novikova et al., 2017; Duˇsek et al., 2019) and CommonGen (Lin et al., 2020). We use the pre-trained T5-base model architecture (Raffel et al., 2020) for our submission implemented using the transformers library from Hugging Face (Wolf et al., 2020). We first train on monolingual data before fine-tuning on the task-specific dataset. For DART and WebNLG, we use abstracts from DBpedia (Auer et al., 2007) for training while for the other two datasets, we use monolingual target-side references for pre-training with a masked language modeling objective. We experiment with different masking strategies where we mask entities and predicates (for DART), meaning representation"
2021.gem-1.13,W04-1013,0,0.0287175,"Missing"
2021.gem-1.13,2021.ccl-1.108,0,0.0645923,"Missing"
2021.gem-1.13,W17-5525,0,0.0544369,"Missing"
2021.gem-1.13,P02-1040,0,0.110933,"rs library (Wolf et al., 2020) which consists of 6 layers each in the encoder and decoder with a multi-head attention sub-layer consisting of 8 attention heads. The word embeddings have a dimension of 512 and the fully-connected feed-forward sublayers are 2048dimensional. Pre-training on DBpedia abstracts is done on a single Nvidia GeForce GTX 1080 Ti GPU for 10 epochs with a batch size of 8 using the Adam optimizer with a learning rate of 0.001. All the other hyperparameter values are set to their default values. Table 1 shows scores for the output generations on the validation set for BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). We find random masking to perform the best in terms of automatic evaluation metrics compared to specifically masking entities or predicates, though the results are not statistically significantly different. Furthermore, in our experiments we compare the results when additional tags are added to the input either as entity types from DBpedia or NER tags from spaCy or just the <SUB>, <PRED> and <OBJ> tags. For this, we use the T5-base model with approximately 220 million parameters. This model consists of 12 layers each in the encoder a"
2021.gem-1.13,2020.webnlg-1.15,1,0.807329,"dom masking Table 3: Results from automatic evaluation on the E2E validation set with different masking strategies on monolingual data for pre-training using the T5-base model. Table 4: Results from automatic evaluation on the CommonGen validation set with different masking strategies on monolingual data for pre-training using the T5base model. Since the entire WebNLG (en) corpus is already included the DART dataset without any modifications, we use the same model as defined in §2.1 without any further fine-tuning to generate outputs on the WebNLG (en) dataset. Our overall approach is same as Pasricha et al. (2020) for the WebNLG+ challenge 2020 except here we use additional 6,678 DBpedia abstracts for pre-training and the larger DART dataset for fine-tuning which results in a higher scores for automatic evaluation metrics. masking appears to perform better though the differences in terms of automatic evaluation metrics are not significantly different. For our submission to the GEM benchmark, we use the same model architecture and hyperparameter values as described previously for DART to generate the output submissions on the E2E test set and challenge sets. This model is first pre-trained on the monoli"
2021.gem-1.13,2020.acl-main.704,0,0.0137475,"of writing we do not have access to all the references in the test set as well as the challenge sets for DART and CommonGen, hence scores on some subsets are not shown. The evaluation metrics are divided into different categories measuring lexical similarity, semantic equivalence, diversity and system characteristics. Popular metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-1/2/L (Lin, 2004) are used for lexical similarity, while recently proposed metrics such as 2 https://github.com/GEM-benchmark/ GEM-metrics BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) which rely on sentence embeddings from pre-trained contextualised embedding models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are used for evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) i"
2021.gem-1.13,W19-2303,0,0.0120652,"evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) is calculated (Li et al., 2018). The size of the output vocabulary (|V|) and the mean length of the generated output texts are reported as system characteristics (Sun et al., 2019). Compared to the baselines described in the GEM benchmark (Gehrmann et al., 2021), we observe higher scores in our submissions for automatic metrics on the CommonGen and DART datasets while scoring lower on the cleaned E2E and WebNLG (en) datasets especially on the test and challenge subsets for both E2E and WebNLG. 152 4 Conclusion We presented a description of the system submitted by NUIG-DSI to the GEM benchmark 2021. We participated in the modeling shared task and submitted outputs on four datasets for data-to-text generation including DART, WebNLG (en), E2E and CommonGen using the T5-bas"
C12-1005,W11-2107,0,0.0157244,"ted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators"
C12-1005,W11-1204,0,0.0405268,"Missing"
C12-1005,P07-2045,0,0.0139662,"translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English o"
C12-1005,J03-1002,0,0.00499596,"and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English ones, stored in the UK GAAP. This allowed us to do a small automatic evaluation, regardless of the low number of labels to be translated. Scoring Metric Source # correct BLEU-2 BLEU-4 NIST TER Meteor JRC-Acquis ECB Linguee+Wikipe"
C12-1005,P02-1040,0,0.0881958,"rman one. These translation pairs were used to suggest the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to"
C12-1005,2006.amta-papers.25,0,0.0395008,"the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cro"
C12-1005,steinberger-etal-2006-jrc,0,0.328224,"Missing"
C12-1005,C08-1125,0,0.0916576,"ties, Charges, Balance, Capital, Reserves . . . 2 1 Table 1: Examples for financial labels in the UK GAAP # of labels 30 20 10 0 1 2 3 4 5 7 6 Length of a label 8 9 10 11 Figure 1: Label length of the UK GAAP ontology 3.2 JRC-Acquis The general parallel corpus JRC-Acquis3 was used as baseline training data. This corpus is available in almost every EU official language (except Irish), and is a collection of legislative texts written between 1950 and now. Although previous research showed, that a training model built by using a general resource cannot be used to translate domain-specific terms (Wu et al., 2008), we decided to evaluate the translations on these resources to illustrate any improvement steps from a general resource to specialised domain resources. 3.3 European Central Bank Corpus For comparison with JRC-Acquis, we also did experiments using the European Central Bank Corpus4 , which contains a financial vocabulary. The multilingual corpus is generated by extracting the website and documentation from the European Central Bank and is aligned among 19 European languages. For our research we used the English-German language pair, which consists of 113,171 sentence pairs or 2.8 million Engli"
C12-1005,zesch-etal-2008-extracting,0,0.0118582,"a, they used thresholds to avoid storing undesirable categories. Müller and Gurevych (2008) used 68 Wikipedia and Wiktionary as knowledge bases to integrate semantic knowledge into Information Retrieval. Their models, text semantic relatedness (for Wikipedia) and word semantic relatedness (for Wiktionary), are compared to a statistical model implemented in Lucene. In their approach to bilingual retrieval, they use the cross-language links in Wikipedia, which improved the retrieval performance in their experiment, especially when the machine translation system generated incorrect translations. Zesch et al. (2008) address the issues in accessing the largest collaborative resources: Wikipedia and Wiktionary. They describe several modules and APIs for converting a Wikipedia XML Dump into a more suitable format. Instead of parsing the large Wikipedia XML Dump, they suggest to store the Dump into a database, which significantly increases the performance in retrieval time of queries. 3 Experimental Data We are investigating the problem of translating a domain-specific vocabulary, therefore our experiments started with an analysis of the financial terms stored in the investigated ontology. With these extract"
C16-1010,2014.amta-researchers.5,1,0.855557,"Translations The SMT system is configured to return the t highest scoring translations, according to its model, and we select the translation as the most frequent translation of the context among this t-best list. In our experiments, we combined this with m disambiguations to give tm candidate translations from which the candidate is chosen. Target Side Lookup (TSL) We can also utilize the translation of our context into the target language xliT from the parallel corpus, however this cannot be applied directly as we do not know which word(s) in xliT correspond to the input and previous work (Arcan et al., 2014) has shown that automatic inference of this alignment (e.g., with GIZA++) can seriously affect performance. Instead we filter contexts to those that generate a translation candidate, wklT , such that wklT ∈ xliT , i.e., the machine translation agrees with the gold-standard translation for this context. 4 Experimental Setting This section gives an overview on the multilingual resources and the translation toolkit used in our experiment. Furthermore, we give insights into SMT evaluation techniques, considering the translation direction of the English WordNet entries into Italian, Slovene, Spanis"
C16-1010,P15-1069,1,0.524332,"ingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word expressions (MWE). In contrast, our approach use an SMT system trained on a large amount of parallel sentences, which allows us to align possible MWEs, such as commercial loan or take a breath, between source and target language. Furthermore, we engage the idea of identifying relevant contextual information to support an SMT system translating short expressions, which showed better performance compared to approaches without a context. Arcan et al. (2015) built small domainspecific translation models for ontology translation from relevant sentence pairs that were identified in a parallel corpus based on the ontology labels to be translated. With this approach they improve the translation quality over the usage of large generic translation models. Since the generation of translation models can be computational expensive, Arcan et al. (2016) use large generic translation models to translate ontology labels, which were placed into a disambiguated context. With this approach the authors demonstrate translation quality improvement over commercial s"
C16-1010,P13-1133,0,0.0310493,"orm of concepts, where new concepts may be added even if they are not represented (yet) in the Princeton WordNet or even lexicalized in English (e.g., many languages have distinct gendered role words, such as ‘male teacher’ and ‘female teacher’, but these meanings are not distinguished in English). Previous studies of generating non-English wordnets combined Wiktionary knowledge with existing wordnets to extend them or to create new ones (de Melo and Weikum, 2009). Bond and Paik (2012) describe in their work the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are"
C16-1010,2016.gwc-1.9,1,0.90782,"f a sentence. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i608331 ‘a craft designed for water transportation’. For the second sense, i65336 ‘a tube in which a body fluid circulates’, we assume that we know the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We use the CILI identifiers for synsets (Bond et al., 2016) 97 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 97–108, Osaka, Japan, December 11-17 2016. German translation for this sense is Gefäß. In our approach we look for sentences in a parallel corpus, where the words vessel and Gefäß both occur and obtain a context such as ‘blood vessel’ that allows the SMT system to translate this sense correctly. This alone is not sufficient as Gefäß is also a translation of i60834 ‘an object used as a container’, however in Italian these two senses are distinct (vaso and recipiente respective"
C16-1010,S07-1054,0,0.0243835,"kipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of parallel corpora in combination with wordnets in languages other than English for sense disambiguation, which will help us to improve automatic translations of English WordNet entries. We assume that we have a m"
C16-1010,P11-2031,0,0.0293983,"c produces good correlation with human judgement at the sentence or segment level. chrF3 is a character n-gram metric, which has shown very good correlations with human judgements on the WMT2015 shared metric task (Stanojevi´c et al., 2015), especially when translating from English into morphologically rich(er) languages. As there are multiple translations available for each sense in the target wordnet we use all translations as multiple references for BLEU, for the other two metrics we compare only to the most frequent member of the synset. The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value < 0.05. 5 Evaluation In this section we present the evaluation of the translated English WordNet words into Italian, Slovene, Spanish and Croatian. We evaluate the quality of translations of the WordNet entries based on the provided contextual information as well as the impact on the number of languages and their effect on wordsense disambiguation. 5.1 Translation Quality Evaluation Based on Contextual Information Our main evaluation focuses on the importance of identifying relevant cont"
C16-1010,W14-3348,0,0.0558521,"corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics. METEOR (Metric for Evaluation of T"
C16-1010,eisele-chen-2010-multiun,0,0.142157,"word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and"
C16-1010,W11-2123,0,0.0256256,"37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for ide"
C16-1010,W02-0808,0,0.183729,"Missing"
C16-1010,W09-4202,0,0.0213779,"the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are obtained by comparing aligned words in parallel corpora in several languages. Similarly, the sloWNet for Slovene (Fišer, 2007) and Wolf for French (Sagot and Fišer, 2008) are constructed using a multilingual corpus and word alignment techniques in combination with other existing lexical resources. 2 The Polylingual WordNet is available at http://polylingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word e"
C16-1010,N03-1017,0,0.0314089,"Missing"
C16-1010,2005.mtsummit-papers.11,0,0.191404,"del learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and"
C16-1010,P03-1058,0,0.073926,"ge of Wikipedia. This is done by assigning WordNet synsets to Wikipedia entries, and making these relations multilingual through the interlingual links. For languages, which do not have the corresponding Wikipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of par"
C16-1010,J03-1002,0,0.00552686,"7M 296M 377M 130M 378M 302M 34M 33M 13M 37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we"
C16-1010,P02-1040,0,0.103798,"inberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics"
C16-1010,W15-3049,0,0.0522913,"Missing"
C16-1010,2016.gwc-1.43,0,0.0636712,"Missing"
C16-1010,W15-3031,0,0.0649935,"Missing"
C16-1010,E12-1015,0,0.17713,"ZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calcu"
C16-1010,C04-1192,0,0.101426,"Missing"
L16-1090,2007.mtsummit-papers.5,0,0.0604561,"em for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish language was shown in Scannell (2006). The rule-based system was developed for translations of closely related languages, Irish (Gaeilge) and"
L16-1090,2003.mtsummit-papers.9,0,0.117195,"Missing"
L16-1090,W14-3348,0,0.0162316,"was extracted from corpora annotated with *). In addition to the publicly available parallel corpora, the Acadamh na hOllscola´ıochta Gaeilge12 at the National University of Ireland, Galway (NUIG) provided us with translations of second level textbooks (Cuimhne na dT´eacsleabhar) in the domain of economics and geography. The data resource, funded by An Chomhairle um Oideachas Gaeltachta agus Gaelscola´ıochta (COGG), holds around 350,000 parallel sentences or 6M English and 6.5M Irish words, respectively. 5. Evaluation Here, we report results based on the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and the chrF3 (Popovi´c, 2015) metric for automatic evaluation of translations. Additionally, we perform a manual evaluation of the translations into Irish. BLEU is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with standard exact word (or phrase"
L16-1090,W11-2123,0,0.0576013,"he development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the evaluation metric BLEU. Furthermore, it gives detailed information about the used data for the translation models accessed by IRI"
L16-1090,N03-1017,0,0.0503569,"atures are preserved in such a standardization process (Scannell, 2014). For example, it may be desirable to standardize spelling and orthography, but to preserve dialectal vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the ta"
L16-1090,P07-2045,0,0.0189343,"tandardize spelling and orthography, but to preserve dialectal vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the eva"
L16-1090,W11-2164,0,0.0274951,"nslation improvement or using a pivot language to overcome the data sparseness. With the aim of language preservation, Lewis and Yang (2012) show an SMT system for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with t"
L16-1090,2010.eamt-1.37,0,0.0195681,"using a pivot language to overcome the data sparseness. With the aim of language preservation, Lewis and Yang (2012) show an SMT system for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish"
L16-1090,J03-1002,0,0.00563437,"l vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the evaluation metric BLEU. Furthermore, it gives detailed informat"
L16-1090,P02-1040,0,0.104513,"corpora (the evaluation data set was extracted from corpora annotated with *). In addition to the publicly available parallel corpora, the Acadamh na hOllscola´ıochta Gaeilge12 at the National University of Ireland, Galway (NUIG) provided us with translations of second level textbooks (Cuimhne na dT´eacsleabhar) in the domain of economics and geography. The data resource, funded by An Chomhairle um Oideachas Gaeltachta agus Gaelscola´ıochta (COGG), holds around 350,000 parallel sentences or 6M English and 6.5M Irish words, respectively. 5. Evaluation Here, we report results based on the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and the chrF3 (Popovi´c, 2015) metric for automatic evaluation of translations. Additionally, we perform a manual evaluation of the translations into Irish. BLEU is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along"
L16-1090,W15-3049,0,0.0598333,"Missing"
L16-1090,W14-4605,0,0.166414,"language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish language was shown in Scannell (2006). The rule-based system was developed for translations of closely related languages, Irish (Gaeilge) and Scottish Gaelic (G`aidhlig), respectively. The translation system is based on a bilingual lexicon, which performs part-of-speech tagging, word sense disambiguation and a syntactic/lexical transfer. This work was expanded in Scannell (2014), focusing on overcoming the orthographical differences between the languages. As an additional task, the author casts the text normalisation problem as an SMT problem and applies the statistical models for normalisation of historical Irish text. The most recent work on a domain-specific English-Irish SMT is shown in Dowling et al. (2015), aiming to help Irish government with their translation tasks. 1 http://www.meta-net.eu/whitepapers/ key-results-and-cross-language-comparison 2 http://server1.nlp.insight-centre.org/ iris/ 3. Irish language Irish is a VSO language on the Celtic branch of the"
L16-1090,skadins-etal-2014-billions,0,0.0331127,"Missing"
L16-1090,W15-3031,0,0.0620098,"Missing"
L16-1090,tiedemann-2012-parallel,0,0.0905896,"Missing"
L18-1149,P15-1069,1,0.827864,"ord-alignment and machine translation approaches, and compared the results with the proposed semantics transfer approach, focusing on the semantic coherence of the generated translations between Spanish, French and German. Sajous et al. (2010) enriched Wiktionary by relying on similarity measures based on random walks through the graphs extracted from its lexical networks. In their final step they engaged users in collaborative editing in order to validate the resource. A different approach for translation and disambiguation of domainspecific expressions stored in knowledge bases was shown in Arcan et al. (2015), where the authors identified relevant in-domain parallel sentences and used them to train a small but domain-aware SMT system. Ordan et al. (2017) demonstrated an approach for bilingual dictionary creation using different translation directions within a loop. In contrast, de Melo and Weikum (2012) did not match concepts with SMT, but showed a machine learning approach which determines the best translation for English WordNet synsets by taking bilingual dictionaries, structural information of WordNet and corpus frequency information into account. Similarly, the multilingual disambiguation of"
L18-1149,C16-1010,1,0.797131,"idge the gap between language-specific information and the language-independent semantic content (Gracia et al., 2012). Since manual multilingual translation and evaluation of knowledge bases is a very time-consuming and expensive process, we apply SMT to automatically translate domain-specific expressions and demonstrate its validity by translating the IATE entries. While an SMT system can only return the most frequent or dominant translation when given a term by itself, it has been showed that SMT provides strong word sense disambiguation when the word is given in the context of a sentence (Arcan et al., 2016a; Arcan et al., 2016b). As a motivating example, we consider the word vessel, which appears several times in the IATE repository, whereby the most frequent translation into German is Schiff, with the meaning of ‘a craft designed for water transportation’, e.g., as given by Google Translate.3 To overcome the issue of obtaining translations for vessel in other languages, and also in different domains (in the sense of blood vessel, for instance), we aim to identify (several) parallel sentences, which hold the terminological entries in the targeted domain, and use their context to translate them"
L18-1149,P11-2031,0,0.0290034,"luation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significant. 5. Results In this section, we present the evaluation of the translated IATE entries into several languages not initially included in this resource, and how existing IATE terms have been exploited for our purposes in the parallel corpora used in this work.7 In addition to this, we illustrate the enhancing of IATE RDF resource with additional contextual information 7 We randomly selected 2,000 terms, although not all target terms are represented in each language for evaluation. 933 # of Terms Bulgarian"
L18-1149,declerck-etal-2006-multilingual,0,0.117658,"Missing"
L18-1149,W14-3348,0,0.0177264,"3M 163M 938k 1M 687k 561k 1M 1M 640k 826k 1M 180k 626k 1M 934k 421k 389k 218k 976k 1M 1M 1M 543k 631k 687k 1M 1M 1M 1M 1M 1M 1M 2M 1M 271k 1M 3M 1M 833k 653k 380k 1M 1M 1M 1M 1M 1M 1M Table 3: Statistics on parallel data for translation model training and word-sense disambiguation. data set of reference translations. Considering the shortness of the entries in IATE, we report scores based on the unigram overlap (BLEU-1). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significan"
L18-1149,eisele-chen-2010-multiun,0,0.0159357,"nts, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them wi"
L18-1149,W11-2123,0,0.0166436,"the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel c"
L18-1149,N03-1017,0,0.00891204,"uation techniques. 4.1. IATE - Inter-Active Terminology for Europe IATE is the terminology database of the EU with its objective of supporting the EU translators and creating a terminology resource to ensure standardisation throughout all institutions. It incorporates the various terminology databases into one database containing approximately one million multilingual entries in English (Table 2).5 Recent domains that have been extensively covered include the financial crisis, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 20"
L18-1149,P07-2045,0,0.0132526,"s, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 201"
L18-1149,2005.mtsummit-papers.11,0,0.0830539,"learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and r"
L18-1149,W11-1013,1,0.819266,"Missing"
L18-1149,J03-1002,0,0.00757023,"n et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). S"
L18-1149,P02-1040,0,0.108804,"Missing"
L18-1149,W15-3049,0,0.0629611,"Missing"
L18-1149,W15-3031,0,0.0609754,"Missing"
L18-1149,E12-1015,0,0.0499086,"(Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them with a 5 6 Based on IATE TBX file - IATE export 16032017.tbx http://opus.nlpl.eu/"
N13-2006,S12-1095,1,0.710787,"Missing"
N13-2006,W12-4201,0,0.0119607,", the corpus was used to generate feature vectors on the basis of the contextual information provided by surrounding words. Finally we calculate the semantic similarity between the extracted information from the parallel corpus and the ontology vocabulary. Related Work Word sense disambiguation (WSD) systems generally perform on the word level, for an input word they generate the most probable meaning. On the other hand, state of the art translation systems operate on sequences of words. This discrepancy between unigrams versus n-grams was first described in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of o"
N13-2006,P05-1048,0,0.0285816,"table and language model. Further, the corpus was used to generate feature vectors on the basis of the contextual information provided by surrounding words. Finally we calculate the semantic similarity between the extracted information from the parallel corpus and the ontology vocabulary. Related Work Word sense disambiguation (WSD) systems generally perform on the word level, for an input word they generate the most probable meaning. On the other hand, state of the art translation systems operate on sequences of words. This discrepancy between unigrams versus n-grams was first described in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate"
N13-2006,W11-2107,0,0.0138112,"780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated five training sets, based on n-grams of the ontology vocabulary (from unigram to 5-gram) appea"
N13-2006,P03-1054,0,0.00365331,"logy German GAAP targeted label: Equity-equivalent partner loans contextual information: capital (6), reserve (3), loss (3), balance sheet (2) . . . currency translation (1), negative consolidation difference (1), profit (1) Table 3: Contextual information for the financial label Equity-equivalent partner loans 4.4 To compare the contextual information extracted from Europarl a similar approach was applied to the vocabulary in the German GAAP ontology. First, to avoid unnecessary segments, e.g. provisions for or losses from executory, we parsed the financial ontology with the Stanford parser (Klein and Manning, 2003) and extracted meaningful segments from the ontology labels. This step was done primarily to avoid comparing all possible n-gram segments with the filtered segments extracted from the Europarl corpus (cf. Subsection 4.2). With the syntactical information given by the Stanford parser we extracted a set of noun segments for the ontology labels, which we defined by the rules shown in Table 2. # 1 2 3 4 5 executory contracts (pattern 3), provisions for expected losses and expected losses from executory contracts (pattern 5). In the next step, for all 2794 labels from the financial ontology, a uniq"
N13-2006,2005.mtsummit-papers.11,0,0.0128442,"rom a linguistic point of view. They are used in financial or accounting reports as unique financial expressions or identifiers to organise and retrieve the reported information automatically. Therefore it is important to translate these financial labels with exact meaning preservation. 3 http://www.xbrl.de/ 3.2 Europarl As a baseline approach we used the Europarl parallel corpus,4 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 40 million English and 43 million German tokens (Koehn, 2005). Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this general corpus to illustrate any improvement steps gained by enriching the standard approach with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromt"
N13-2006,P07-2045,0,0.00540875,"h with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and vocabulary (Section 4.3). 4.1 Moses toolkit To translate the English financial labels into German, we used the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. 4.2 Building the contextual-semantic resource from the parallel corpus Europarl To enhance the baseline approach with additional semantic information, we built a new resource of contextual information from Europarl. From the original phrase table, which was generated from the Europarl corpus, we used the subphrase table, which was generated to translate the German GAAP financial ontology in the baseline approach. Although this sub-ph"
N13-2006,D09-1022,0,0.030141,"bed in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of output words from the bag of input words. In their research, no alignment between input and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses b"
N13-2006,W11-1013,0,0.271968,"nput and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses bag-of-word information to enrich the missing contextual information that arises from translating ontology labels in isolation. (McCrae et al., 2011) exploit in their research 41 3 Data sets 3.1 Financial ontology For our experiment we used the financial ontology German GAAP (Generally Accepted Accounting Practice),3 which holds 2794 concepts with labels in German and English. Balance sheet ... Total equity and liabilities ... Equity Equity-equivalent partner loans Revenue reserves Legal reserve ... Legal reserve, of which transferred from prior year net retained profits Figure 1: The financial label Equity-equivalent partner loans and its neighbours in the German GAAP ontology As seen in Figure 1 the financial labels do not correspond to"
N13-2006,J03-1002,0,0.00307123,"ent Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and vocabulary (Section 4.3). 4.1 Moses toolkit To translate the English financial labels into German, we used the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. 4.2 Building the contextual-semantic resource from the parallel corpus Europarl To enhance the baseline approach with additional semantic information, we built a new resource of contextual information from Europarl. From the original phrase table, which was generated from the Europarl corpus, we used the subphrase table, which was generated to translate the German GAAP financial ontology in the baseline approach. Although this sub-phrase table holds only segments necessary to translate the financial labels, it sti"
N13-2006,P02-1040,0,0.0858052,"Target label Eigenkapital Equity Kapitalbeteiligung Gleichheit Gerechtigkeit Jaccard 0.0780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated f"
N13-2006,I11-1074,0,0.0123468,"e translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of output words from the bag of input words. In their research, no alignment between input and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses bag-of-word information to enrich the missing contextual information that arises from translating ontology labels in isolation. (McCrae et al., 2011) exploit in their research 41 3 Data sets 3.1 Financial ontology For our experiment we used the financial ontology German GAAP (Generally Accepted Accounting Practice),3 which holds 2794 concepts with labels in German and English. Balance sheet ... Total equity and liabilities ... Equity Equity-equivalent partner loans Revenue reserves Legal reserve ... Legal"
N13-2006,2006.amta-papers.25,0,0.0348069,"leichheit Gerechtigkeit Jaccard 0.0780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated five training sets, based on n-grams of the ontology"
N13-2006,C08-1125,0,0.0214557,"tion automatically. Therefore it is important to translate these financial labels with exact meaning preservation. 3 http://www.xbrl.de/ 3.2 Europarl As a baseline approach we used the Europarl parallel corpus,4 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 40 million English and 43 million German tokens (Koehn, 2005). Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this general corpus to illustrate any improvement steps gained by enriching the standard approach with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and"
P15-1069,C12-1005,1,0.864122,"ds of the current label are combined with the related words of its direct parent in the ontology. The usage of the ontology hierarchy allows us to take advantage of the specific vocabulary of the related words in the computation of the cosine similarity. Given a label and a source sentence from the generic corpus, related words and their weights are extracted from both of them and used as entries of the vectors passed to the cosine similarity. The most similar source sentence and the label should share the largest number of related words (largest cosine similarity). 3.2 We engage the idea of (Arcan et al., 2012) where the authors provide to the SMT system unambiguous terminology identified in Wikipedia to improve the translations of labels in the financial domain. To disambiguate Wikipedia entries with translations into different domains, they query the repository for analysing the n-gram overlap between the financial labels and the Wikipedia entries and store the frequency of categories which are associated with the matched entry. In a final step they extract only bilingual Wikipedia entries, which are associated with the most frequent Wikipedia categories identified in the previous step. Since the"
P15-1069,W14-4803,1,0.799714,"Missing"
P15-1069,W14-3348,0,0.0275309,"domainspecific models to evaluate different approaches when combining generic and domain-specific data together. We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA12 (Tiedemann, 2009), which holds specific medical parallel data extracted from the European Medicines Agency documents and websites. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recal"
P15-1069,2014.amta-researchers.5,1,0.895111,"its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual 710 this reason, we improve it by extending the semantic information of labels using a technique for computing vector representations of words. The technique is based on a neural network that analyses the textual data provided as input and provides as output a list of semantically related words (Mikolov et al., 2013). Each input string is vectorized using the surrounding context and compared to other vectorized sets of words (from the training data) in a multi-dimensi"
P15-1069,eck-etal-2004-language,0,0.0142663,"t labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wi"
P15-1069,D11-1033,0,0.0218699,"tions of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown i"
P15-1069,2013.mtsummit-papers.5,0,0.0612,"l corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource. 4 Wikipedia dump id enwiki-20141106 https://code.google.com/p/word2vec/ 6 http://wiki.dbpedia.org/Downloads2014 5 711 Model (Bertoldi et al., 2013) approach. 4 Experimental Setting The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-specific data can be used to build a translation model. Its goal is to leverage the large coverage of the background model, while preserving the domain-specific knowledge coming from the domain-specific data. For this purpose the generic and the domain-specific translation models are merged. For those translation candidates that appear in both models, only one instance is reported in the Fill-Up model with the lar"
P15-1069,W11-2131,0,0.013545,"4) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Different from these approaches, we do not embed any specific in-domain knowledge to the generic corpus, from which sentence selection is performed. Furthermore, none of these methods explicitly exploit the ontological hierarchy for label disambiguation and are not specifically designed to deal with the characteristics of ontology labels. 3 Methodology We propose an approach that uses the ontology labels to be translated to select the most relevant parallel sentences from a generic parallel corpus. Since onto"
P15-1069,E12-1016,0,0.0314405,"Missing"
P15-1069,2011.iwslt-evaluation.18,0,0.0125056,"rated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource. 4 Wikipedia dump id enwiki-20141106 https://code.google.com/p/word2vec/ 6 http://wiki.dbpedia.org/Downloads2014 5 711 Model (Bertoldi et al., 2013) approach. 4 Experimental Setting The Fill-Up model has been developed to address a common scenario where a large generic background m"
P15-1069,bouamor-etal-2012-identifying,0,0.0126692,"dia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource."
P15-1069,W12-3154,0,0.0682286,"e translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge. This research showed tha"
P15-1069,P11-2031,0,0.0431554,"perplexity score gives a notion of how well the probability model based on the ontology vocabulary predicts a sample, which is in our case each sentence in the generic corpus. Second, we use the method shown in (Hildebrand et al., 2005), where the authors use a method 13 11 tf-idf – term frequency-inverse document frequency http://iate.europa.eu/downloadTbx.do 15 METEOR configuration: exact, stem, paraphrase For reproducibility and future evaluation we take the first one-third part of each corpus. 12 http://opus.lingfil.uu.se/EMEA.php 14 713 The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value < 0.05. 5 Dataset Type Evaluation of Ontology Labels In this Section, we report the translation quality of ontology labels based on translation systems learned from different sentence selection methods. Additionally, we perform experiments training an SMT system on the combination of in- and outdomain knowledge. The final approach enhances a domain-specific translation system with lexical knowledge identified in IATE or DBpedia. 5.1 Size BLEU-2 BLEU-4 METEOR Generic dataset 1.9M EMEA dat"
P15-1069,2005.eamt-1.19,0,0.0505213,"Missing"
P15-1069,C14-1182,0,0.0444323,"Missing"
P15-1069,declerck-etal-2006-multilingual,0,0.649534,"Missing"
P15-1069,D14-1014,0,0.0128301,"that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Differen"
P15-1069,P02-1040,0,0.0948144,"tem in combination with the smaller domainspecific models to evaluate different approaches when combining generic and domain-specific data together. We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA12 (Tiedemann, 2009), which holds specific medical parallel data extracted from the European Medicines Agency documents and websites. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on t"
P15-1069,2005.mtsummit-papers.11,0,0.0478815,"age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German based on tf-idf 13 to select the most relevant sentences. This widely-used method in information retrieval tells us how important a word is to a document, whereby each sentence from the generic corpus is treated as a document. Finally, we compare our approach wi"
P15-1069,P07-2045,0,0.00491823,"uses an ageing of the existing translation candidates and hence their re-scoring; in case of re-insertion of a phrase pair, the old value is set to the initial value. Similarly to the CBTM, the local language model is built to give preference to the provided target expressions. Each entry stored in CBLM is associated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lin"
P15-1069,W09-2907,0,0.0250648,"Missing"
P15-1069,W02-1405,0,0.0100846,"ous entries, the cosine similarity gives more weight to the Wikipedia entry, which is closer to our preferred domain. Finally, if the Wikipedia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilin"
P15-1069,steinberger-etal-2006-jrc,0,0.0179494,"sociated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German based on tf-idf 13 to select the most relevant sentences. This widely-used method in information retrieval tells us how important a word is to a document, whereby each sentence from the generic corpus is treated as a document. Finally"
P15-1069,D07-1036,0,0.0550878,"Missing"
P15-1069,W11-1013,0,0.59252,"Missing"
P15-1069,tiedemann-2012-parallel,0,0.0488733,"Missing"
P15-1069,P10-2041,0,0.0156483,"sing the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target sid"
P15-1069,2011.iwslt-papers.6,0,0.0139458,"resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual 710 this reason, we improve it by extending the semantic information of labels using a technique for computing vector representations of words. The technique is based on a neural network that analyses the textual data provided as input and provides as output a list of semantically related words (Mikolov et al., 2013). Each input string is vectorized using the surrounding context and compared to other vectorized sets of words (from the training data"
P15-1069,C08-1125,0,0.0116254,"w to gain adequate translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge."
P15-1069,J03-1002,0,0.00748084,"n case of re-insertion of a phrase pair, the old value is set to the initial value. Similarly to the CBTM, the local language model is built to give preference to the provided target expressions. Each entry stored in CBLM is associated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German"
P15-1069,2014.amta-researchers.15,0,0.0265574,"Missing"
W12-4210,W11-1204,0,0.043027,"Missing"
W12-4210,P07-2045,0,0.00794763,"ailable knowledge5 . With the heavily interlinked information base, Wikipedia forms a rich lexical and semantic resource. Besides a large amount of articles, it also holds a hierarchy of Categories that Wikipedia Articles are tagged with. It includes knowledge about named entities, domain-specific terms and word senses. Furthermore, the redirect system of Wikipedia articles can be used as a dictionary for synonyms, spelling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of buil"
W12-4210,2005.mtsummit-papers.11,0,0.00794548,"esults of Google Translate. This finding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms. 1 Introduction Our research on translation of ontology vocabularies is motivated by the challenge of translating domainspecific terms with restricted or no additional textual context that in other cases can be used for translation improvement. For our experiment we started by translating financial terms with baseline systems trained on the EuroParl (Koehn, 2005) corpus and the JRC-Acquis (Steinberger et al., 2006) corpus. Although both resources contain a large amount of parallel data, the translations were not satisfying. To improve the translations of the financial ontology vocabulary we built a new parallel resource, which was generated using Linguee1 , an online translation query service. With this data, we could train a small system, which produced better translations than the baseline model using only general resources. Since the manual development of terminological resources is a time intensive and expensive task, we used Wikipedia as a backgr"
W12-4210,W05-0909,0,0.0424273,". As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0.364 0.006 0.348 4.830 2.458 1.307 3.93"
W12-4210,J03-1002,0,0.00271255,"lling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of building them will be presented. Section 4.1 gives an overview of generating a parallel resource from Linguee, which was used in generating a new domain-specific training model. In Section 4.2 a detailed description is given how we extracted terms from Wikipedia for generating a domain-specific lexicon. 4.1 Domain-specific parallel corpus generation To build a new training model that is specialised on our xEBR ontology, w"
W12-4210,P02-1040,0,0.0827818,"ikipedia knowledge base also existed. As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0."
W12-4210,C08-1125,0,0.0513713,"Missing"
W12-4210,zesch-etal-2008-extracting,0,0.0196027,"Missing"
W12-4210,steinberger-etal-2006-jrc,0,\N,Missing
W12-4210,W07-0734,0,\N,Missing
W13-5502,baccianella-etal-2010-sentiwordnet,0,0.0652912,"Missing"
W13-5502,strapparava-valitutti-2004-wordnet,1,\N,Missing
W14-4803,P13-1040,0,0.100306,"Missing"
W14-4803,2013.mtsummit-posters.1,1,0.807058,"Missing"
W14-4803,W04-2214,0,0.0190318,"o each linked term in a text, after that we obtain the most frequent domain and filter out the terms that are out of scope. In the example above, the term mouse is accepted because it belongs to the domain computer science, as the majority of terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected. The large number of languages and domains to cover prevents us from using standard text classification techniques to categorize the document. For this reason, we implemented an approach based on the mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia categories are created and assigned by different human editors, and are therefore less rigorous, coherent and consistent than usual ontologies. In addition, the Wikipedia’s category hierarchy forms a cyclic graph (Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity, allows us reducing the number of domains to few tens instead of"
W14-4803,2011.iwslt-evaluation.18,0,0.0335625,"d identify their equivalents using Wikipedia cross-lingual links. For this purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding two more components able to first identify domain-specific terms, and to find their translations in a target language. The identified bilingual terms are then compared with those obtained by TaaS (Skadinˇs et al., 2013). The embedding of the domain-specific terms into an SMT system is performed by use of the XML markup approach, which uses the terms as preferred translation candidates at run time, and the Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms. Our results show that the performance of our technique and TaaS are comparable in the identification of monolingual and bilingual domain-specific terms. From the machine translation point of view, our experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach. Thiswork workisis licensed licenced under Attribution 4.0 International License. Page numbers and proceedings footer This under aaCrea"
W14-4803,bouamor-etal-2012-identifying,0,0.139922,"erwise, we return the most frequent alternative form of the term in the target language. From the previous example, the system is able to return the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in computer science. Using this information, the term mouse is paired with its translation into Italian. 2.2 Integration of Bilingual Terms into SMT A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the training data and the terms. Although it has been shown to perform better than more complex techniques (Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications. In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms with ambiguous translations are penalised, because the most frequent and general translations often receive the highest probability, which drives the SMT system to ignore specific translations. In this paper, we focus on two techniques that give more priority to specific translations than generic ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to address a comm"
W14-4803,P11-2031,0,0.0368264,"search path” in English) is translated with a completely wrong translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve the performance of an SMT system and if it is robust to the aforementioned errors. 4.3 Embedding Terminology into SMT Our further experiments focused on the automatic evaluation of the translation quality of the EMEA, GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Machine was embedded through the Fill-Up and XML markup approaches. The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value &lt; 0.05. The parameters of the baseline method and the Fill-Up models were optimized on the development set. Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are statistically significant (GNOME En→It, KDE anno En↔It). Embedding the same bilingual terminology into the Fill-Up model helped to outperform the baseline approach for all test sets, wh"
W14-4803,W07-0717,0,0.0906092,"Missing"
W14-4803,J09-4007,1,0.827322,"roach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages. The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense, a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The third step enriches the linked terms using information extracted from Wikipedia and LOD resources. The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e., orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross language links, etc. For example, in the text “click right mouse key to pop up menu and Gnome panel”, The"
W14-4803,W12-3154,0,0.180428,"formance is similar to the scores obtained using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En→It and 27.02 for It→En, which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of the two term identification methods and suggests a novel research direction. 5 Related Work The main focus of our research is on bilingual term identification and the embedding of this knowledge into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that an SMT system built by using a large general resource cannot be used to translate domain-specific terms, we have to provide the system domain-specific lexical knowledge. Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string 28 matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Bes"
W14-4803,W07-0733,0,0.227517,"Missing"
W14-4803,P07-2045,0,0.00742184,"translation by the resource it was provided. In our case, we favour first the translation provided by ETB. If no translation is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting the term extraction approach, TaaS requires manual specification of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system"
W14-4803,2005.mtsummit-papers.11,0,0.123656,"uages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term em"
W14-4803,W02-1405,0,0.803922,"Missing"
W14-4803,W13-5630,0,0.0323917,"Missing"
W14-4803,N07-1025,0,0.0357079,"age for the specific language and domain. The first step identifies and ranks the terms by relevance using a simple statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages. The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense, a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The third step enriches the linked terms using information extracted from Wikipedia and LOD resources. The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e., orthographical and morphological variants, synonyms, and related te"
W14-4803,2011.iwslt-papers.6,0,0.197482,"Missing"
W14-4803,J03-1002,0,0.00531712,"slation provided by ETB. If no translation is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting the term extraction approach, TaaS requires manual specification of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating rem"
W14-4803,P02-1040,0,0.0894061,"approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term embedding methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identified terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of the translations. The metric calculates the overlap of n-grams between the SMT system output and a reference translation, provided by a professional translator. 4.1 Monolingual Term Identification In Table 1, the column ’Ident.’ represents the number of identified terms for each tool, whereby we observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms, TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower 5 6 In the rest of the paper, we refer to the annotated p"
W14-4803,W09-2907,0,0.30525,"Missing"
W14-4803,steinberger-etal-2006-jrc,0,0.177866,"ication of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identifi"
W14-4803,tiedemann-2012-parallel,0,0.0212487,"cument. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term embedding methods for the two domains: IT"
W14-4803,C08-1125,0,0.0692236,"or It→En. This performance is similar to the scores obtained using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En→It and 27.02 for It→En, which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of the two term identification methods and suggests a novel research direction. 5 Related Work The main focus of our research is on bilingual term identification and the embedding of this knowledge into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that an SMT system built by using a large general resource cannot be used to translate domain-specific terms, we have to provide the system domain-specific lexical knowledge. Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string 28 matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the"
W14-4803,W07-0201,0,0.0128413,"e, pop up menu and Gnome panel), while the term key in the domain music is rejected. The large number of languages and domains to cover prevents us from using standard text classification techniques to categorize the document. For this reason, we implemented an approach based on the mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia categories are created and assigned by different human editors, and are therefore less rigorous, coherent and consistent than usual ontologies. In addition, the Wikipedia’s category hierarchy forms a cyclic graph (Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity, allows us reducing the number of domains to few tens instead of some hundred thousands (800,000 23 categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia categories that contain more pages (∼1,000) have been manually mapped to WordNet domains. The domain for a term is"
W19-3205,D18-2029,0,0.0197621,"work, e.g. Delahunty et al. (2018), we did not employ separate algorithms for the four symptoms, but considered that all four symptoms are intrinsically interconnected. Within the machine learning literate, multilabel and multi-class approaches have been shown to outperform indi3.2 Feature extraction Three methods of feature extraction were employed. Text representation was employed using the Universal Sentence Encoder (USE), specifically developed for longer than word representations. The model is trained using a deep learning transformer neural network architecture on a variety of datasets (Cer et al., 2018). Each of our patient statements was passed into their pretrained model and a statement level representation vector of shape 512 was returned. 41 LIWC is a psycholinguistic dictionary containing 94 psychological trait dimensions and over 2,000 words related to these dimensions (Pennebaker et al., 2001). A percentage count of the number of words in the text related to each dimension is computed. To identify an optimal subset of the number of relevant dimensions, we reviewed all proceedings from the CLEF eRisk workshop 2017 and 2018 (Losada and Crestani, 2016). For each proceeding that employed"
W19-3205,gratch-etal-2014-distress,0,0.0289252,"suffers from the limitation of viewing these disorders as binary occurrences, whether a disorder is present or not. Although this approach makes sense given the nature of machine learning classifiers, from the perspective of medical professionals, however, individuals can rarely be placed into binary classes. Different combinations of symptoms can dramatically affect the diagnosis (American Psychiatric Association, 2013). System Description Data Our initial dataset is the DAIC-WOZ, which is composed of transcribed clinical interviews collected through a Wizard-of-Oz approach for 142 patients (Gratch et al., 2014). The topic of the interviews are general conversations and were all collected within the United States. For each patient, a transcript of their interview is provided along with PHQ-8 scores, where bot statements were removed leaving only patient statements. PHQ-8 scores can be mapped to PHQ-2 scores, and GAD-2 scores were inferred from data provided by Johansson et al. (2013). The final dataset was composed of 23,726 text statements. To evaluate our system on social media data, we employed the Reddit depression dataset (Losada and Crestani, 2016). This dataset we gained access to contained Re"
W19-6725,agerri-etal-2014-ixa,0,0.0127965,"possible analysis for a given surface word; and using external information (in the form of a monolingual POS tagger) to disambiguate. For the former approach, we used a unique tag for each possible CAT and CL values concatenation; e.g. the categories NST and VST and all the inflection classes (CL) for snake (Figure 1). For the latter, we used the Stanford POS tagger (Toutanova et al., 2003), that uses the Penn Treebank (Marcus et al., 1994) tag Proceedings of MT Summit XVII, volume 2 set for English, and the AnCora (Civit and Martí, 2004) tag set for Spanish, and the IXA pipeline POS tagger (Agerri et al., 2014) with the Universal Dependencies POS tag set (Nivre et al., 2018) for Basque. All POS tag sets were mapped to the tag set used by Lucy LT. If the tagger provided POS tag was equivalent to one or more Lucy LT tags, then the non matching Lucy LT tags were removed. Otherwise, we kept the set of tags; e.g. if the POS tag emits noun as the most likely tag, then only NST and the concatenation of all the inflection classes for the corresponding entry would be used as additional information. As a comparison, we also evaluated NMT models trained with Stanford or IXA POS tags as additional information."
W19-6725,P17-2021,0,0.0145642,"approaches for Basque-Spanish. The authors focus on different subword unit representations, i.e. linguistically-motivated or frequency-based word Proceedings of MT Summit XVII, volume 2 segmentation method. Shi et al. (2016) investigated whether an encoder-decoder translation system learns syntactic information on the source side as a side affect of training the neural models. Several syntactic labels of the source sentence were created and logistic regression models using the learned sentence encoding vectors or learned word by word hidden vectors were used to predict these syntactic labels. Aharoni and Goldberg (2017) presented a method to incorporate syntactic information of the target language in an NMT system, showing improved word reordering compared to their baseline system. Eriguchi et al. (2016) proposed an NMT model leveraging syntactic information to improve the accuracy for English→Japanese translation. The phrase structure of the source sentence was recursively encoded in a bottom-up fashion to first produce a vector representation of the sentence, then decode it while aligning the input phrases and words with the output. Bastings et al. (2017) relied on graph-convolutional networks primarily de"
W19-6725,2003.mtsummit-systems.1,1,0.537683,"is as effective as using subword units in this particular setting. 1 Introduction In rule-based machine translation (RBMT), a linguist formalises linguistic knowledge into lexicons and grammar rules. This knowledge is used by the system to analyse sentences in the source language and translate them. While this approach does not require any training corpora and grants control over the translations created by the system, the process of encoding linguistic knowledge requires great amounts of expert time. Notable examples of RBMT systems are the original, rule-based Systran (Toma, 1977), Lucy LT (Alonso and Thurmair, 2003) and Apertium (Forcada et al., 2011). Instead, corpus-based machine translation systems learn to translate from examples, usually in © 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 the form of sentence-level aligned corpora. On the one hand, this approach is generally more computationally expensive and offers limited control over the generated translations. Furthermore, it is not feasible for language pairs that have little to no available parallel resources. On the other hand,"
W19-6725,L16-1090,1,0.855644,"y to train a neural model. On the other hand, an under-resourced scenario can be a specific domain, e.g. medical, where a significant amount of data exists, but does not cover the targeted domain. The Table 1 shows the statistics on the used datasets. For Basque and Irish, we used the available corpora stored on the OPUS webpage.2 We used OpenSubtitles2018 (Lison and Tiedemann, 2016),3 Gnome and KDE4 datasets (Tiedemann, 2012). Additionally, the English-Irish parallel corpus is augmented with second level education textbooks (Cuimhne na dTéacsleabhar) in the domain of economics and geography (Arcan et al., 2016). In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences. To ensure a broad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 3 opus.nlpl.eu www.opensubtitles.org Proceedings of MT Summit XVII, volume 2 corpora from the OPUS web page into one parallel data set and randomly extracted the sentences. In addition to the previous corpora, we added Europarl (Koehn, 2005), DGT (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2"
W19-6725,P18-2049,0,0.0159776,"ertags within the target word sequence as syntactic information, processed by the decoder of their NMT system. Their evaluation showed translation quality improvements for the German→English and Romanian→English translation directions. Similarly, their approach outperformed multi-tasking approach for the same language pairs. Garcıa-Martınez et al. (2016) trained their NMT model to simultaneously generate the lemma and its corresponding factors, i.e. POS, gender, and number, demonstrating that factored architecture increases the vocabulary coverage while decreasing the number of unknown words. Ataman and Federico (2018) described the addition of a recurrent neural network to generate compositional representations of the input words, obtaining better results than systems using byte-pair encoding when translating from morphologically rich languages to English. Banerjee and Bhattacharyya (2018) compared two different approaches for subword units when translating from English to Hindi and Bengali, byte pair encoding and morpheme-based segmentation, showing that the latter approach improves the translations, and further improvements can be achieved by combining both. Dublin, Aug. 19-23, 2019 |p. 126 (&quot;snake&quot; NST"
W19-6725,W18-1207,0,0.0132257,"rmed multi-tasking approach for the same language pairs. Garcıa-Martınez et al. (2016) trained their NMT model to simultaneously generate the lemma and its corresponding factors, i.e. POS, gender, and number, demonstrating that factored architecture increases the vocabulary coverage while decreasing the number of unknown words. Ataman and Federico (2018) described the addition of a recurrent neural network to generate compositional representations of the input words, obtaining better results than systems using byte-pair encoding when translating from morphologically rich languages to English. Banerjee and Bhattacharyya (2018) compared two different approaches for subword units when translating from English to Hindi and Bengali, byte pair encoding and morpheme-based segmentation, showing that the latter approach improves the translations, and further improvements can be achieved by combining both. Dublin, Aug. 19-23, 2019 |p. 126 (&quot;snake&quot; NST ALO &quot;snake&quot; CL (P-S S-01) KN CNT ON CO SX (N) TYN (ANI)) (&quot;snake&quot; VST ALO &quot;snak&quot; ARGS ((($SUBJ N1 (TYN CNC LOC C-POT)) ($ADV DIR))) CL (G-ING I-E P-ED PA-ED PR-ES1) ON CO PLC (NF)) Figure 1: The word snake as a noun (NST) and a verb (VST) in Lucy LT dictionaries. Each entry is"
W19-6725,D17-1209,0,0.0681733,"n standard Arabic and Arabic dialects, i.e. Levantine Arabic and Maghrebi Arabic. The work demonstrated that the POS information for the low resourced Arabic dialects was beneficial for the translation quality, specifically if pre-trained FastText models were injected during the NMT training step. Niehues and Cho (2017) jointly trained several English-German natural language processing tasks in one system with shared encoder and one attention model and decoder per task. By integrating additional linguistic resources via multitask learning, the performance of each individual task was improved. Bastings et al. (2017) showed that incorporating syntactic structure such as dependency tree using graph convolutional encoders was beneficial for neural machine translation. Their work focused on exploiting structural information on the source side by adding a second encoder. The goal of their work was to provide the encoder with access to rich syntactic information without placing rigid constraints on the interaction between syntax and the translation task Etchegoyhen et al. (2018) studied NMT, RBMT, and phrase-based statistical machine translation approaches for Basque-Spanish. The authors focus on different sub"
W19-6725,eisele-chen-2010-multiun,0,0.013301,"rcan et al., 2016). In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences. To ensure a broad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 3 opus.nlpl.eu www.opensubtitles.org Proceedings of MT Summit XVII, volume 2 corpora from the OPUS web page into one parallel data set and randomly extracted the sentences. In addition to the previous corpora, we added Europarl (Koehn, 2005), DGT (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA and OpenOffice (Tiedemann, 2009). To evaluate the targeted under-resourced scenario within medical domain, we exclusively used the EMEA corpus. For Simplified Chinese, we used a parallel corpus provided by the industry partner, which was collected from bilingual English-Simplified Chinese news portals. The corpora were tokenised using the OpenNMT toolkit, with the exception of Simplified Chinese, that was tokenized using Jieba,4 and lowercased. 4.2 NMT framework We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models cove"
W19-6725,P16-1078,0,0.015579,"ation method. Shi et al. (2016) investigated whether an encoder-decoder translation system learns syntactic information on the source side as a side affect of training the neural models. Several syntactic labels of the source sentence were created and logistic regression models using the learned sentence encoding vectors or learned word by word hidden vectors were used to predict these syntactic labels. Aharoni and Goldberg (2017) presented a method to incorporate syntactic information of the target language in an NMT system, showing improved word reordering compared to their baseline system. Eriguchi et al. (2016) proposed an NMT model leveraging syntactic information to improve the accuracy for English→Japanese translation. The phrase structure of the source sentence was recursively encoded in a bottom-up fashion to first produce a vector representation of the sentence, then decode it while aligning the input phrases and words with the output. Bastings et al. (2017) relied on graph-convolutional networks primarily developed for modelling graph-structured data. These networks used predicted syntactic dependency trees of source sentences to produce representations of words that are sensitive to their sy"
W19-6725,P17-4012,0,0.0328824,"pora, we added Europarl (Koehn, 2005), DGT (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA and OpenOffice (Tiedemann, 2009). To evaluate the targeted under-resourced scenario within medical domain, we exclusively used the EMEA corpus. For Simplified Chinese, we used a parallel corpus provided by the industry partner, which was collected from bilingual English-Simplified Chinese news portals. The corpora were tokenised using the OpenNMT toolkit, with the exception of Simplified Chinese, that was tokenized using Jieba,4 and lowercased. 4.2 NMT framework We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. In order to overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We also added the different morphological and"
W19-6725,N03-1017,0,0.0442239,"e, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 the form of sentence-level aligned corpora. On the one hand, this approach is generally more computationally expensive and offers limited control over the generated translations. Furthermore, it is not feasible for language pairs that have little to no available parallel resources. On the other hand, it boasts a much higher coverage of the targeted language pair, depending on the availability of parallel corpora. Examples of corpus-based machine translation paradigms are statistical phrase-based translation (Koehn et al., 2003) and neural machine translation (NMT) models (Bahdanau et al., 2015). In this work, we focused on leveraging RBMT knowledge for improving the performance of NMT systems in an under-resourced scenario. Namely, we used information contained in Lucy LT, an RBMT system where the linguistic knowledge is formalised by human linguists as computational grammars, monolingual and bilingual lexicons. Monolingual lexicons are collections of lexical entries; each lexical entry is a set of feature-value pairs containing morphological, syntactic and semantic information. Bilingual lexicon entries include sou"
W19-6725,W04-3250,0,0.0756403,"dings, maximum vocabulary size of 50,000 subwords, maximum of 32,000 unique BPE merge operations, unlimited different values for the word features and between 11 and 23 dimension embeddings for word features.5 4.3 Evaluation In order to evaluate the performance of the different systems, we used BLEU (Papineni et al., 2002), an automatic evaluation that boasts high correlation with human judgements, and translation error rate (TER) (Snover et al., 2006), a metric that represents the cost of editing the output of the MT systems to match the reference. Additionally, we used bootstrap resampling (Koehn, 2004) with a sample size of 1,000 and 1,000 iterations, and reported statistical significance with ? < 0.05. We also presented a box-and-whisker plot with the first, second and third quartiles as a box, and the first (<0.025) and last (≥0.975) 40-quantiles as whiskers, corresponding to ? < 0.05. In addition, we compared the performance of our NMT systems with the NMTbased Google Translate,6 and the translations performed using Lucy LT RBMT; for the latter, only English-Spanish and English-Basque models are available. 5 Results In this section we describe the quantitative and qualitative evaluation"
W19-6725,2005.mtsummit-papers.11,0,0.0748735,"na dTéacsleabhar) in the domain of economics and geography (Arcan et al., 2016). In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences. To ensure a broad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 3 opus.nlpl.eu www.opensubtitles.org Proceedings of MT Summit XVII, volume 2 corpora from the OPUS web page into one parallel data set and randomly extracted the sentences. In addition to the previous corpora, we added Europarl (Koehn, 2005), DGT (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA and OpenOffice (Tiedemann, 2009). To evaluate the targeted under-resourced scenario within medical domain, we exclusively used the EMEA corpus. For Simplified Chinese, we used a parallel corpus provided by the industry partner, which was collected from bilingual English-Simplified Chinese news portals. The corpora were tokenised using the OpenNMT toolkit, with the exception of Simplified Chinese, that was tokenized using Jieba,4 and lowercased. 4.2 NMT framework We used OpenNMT (Klein et al., 2017), a generic deep l"
W19-6725,L16-1147,0,0.0154508,"ework used. 4.1 Training and Evaluation Datasets In this work, we focused on NMT for underresourced scenarios. On the one hand, we consider languages, such as Basque or Irish, which do not have a significant amount of parallel data necessary to train a neural model. On the other hand, an under-resourced scenario can be a specific domain, e.g. medical, where a significant amount of data exists, but does not cover the targeted domain. The Table 1 shows the statistics on the used datasets. For Basque and Irish, we used the available corpora stored on the OPUS webpage.2 We used OpenSubtitles2018 (Lison and Tiedemann, 2016),3 Gnome and KDE4 datasets (Tiedemann, 2012). Additionally, the English-Irish parallel corpus is augmented with second level education textbooks (Cuimhne na dTéacsleabhar) in the domain of economics and geography (Arcan et al., 2016). In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences. To ensure a broad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 3 opus.nlpl.eu www.opensubtitles.org Proceedings of MT Summit XVII, volume 2"
W19-6725,H94-1020,0,0.0811835,"orm; e.g. the word snake can be both a noun and a verb (Figure 1). For addressing this problem, we took two different approaches: using ambiguity classes that describe all the possible analysis for a given surface word; and using external information (in the form of a monolingual POS tagger) to disambiguate. For the former approach, we used a unique tag for each possible CAT and CL values concatenation; e.g. the categories NST and VST and all the inflection classes (CL) for snake (Figure 1). For the latter, we used the Stanford POS tagger (Toutanova et al., 2003), that uses the Penn Treebank (Marcus et al., 1994) tag Proceedings of MT Summit XVII, volume 2 set for English, and the AnCora (Civit and Martí, 2004) tag set for Spanish, and the IXA pipeline POS tagger (Agerri et al., 2014) with the Universal Dependencies POS tag set (Nivre et al., 2018) for Basque. All POS tag sets were mapped to the tag set used by Lucy LT. If the tagger provided POS tag was equivalent to one or more Lucy LT tags, then the non matching Lucy LT tags were removed. Otherwise, we kept the set of tags; e.g. if the POS tag emits noun as the most likely tag, then only NST and the concatenation of all the inflection classes for t"
W19-6725,W17-4707,0,0.140899,"everaging syntactic information to improve the accuracy for English→Japanese translation. The phrase structure of the source sentence was recursively encoded in a bottom-up fashion to first produce a vector representation of the sentence, then decode it while aligning the input phrases and words with the output. Bastings et al. (2017) relied on graph-convolutional networks primarily developed for modelling graph-structured data. These networks used predicted syntactic dependency trees of source sentences to produce representations of words that are sensitive to their syntactic neighbourhoods. Nadejde et al. (2017) introduced CCG supertags within the target word sequence as syntactic information, processed by the decoder of their NMT system. Their evaluation showed translation quality improvements for the German→English and Romanian→English translation directions. Similarly, their approach outperformed multi-tasking approach for the same language pairs. Garcıa-Martınez et al. (2016) trained their NMT model to simultaneously generate the lemma and its corresponding factors, i.e. POS, gender, and number, demonstrating that factored architecture increases the vocabulary coverage while decreasing the number"
W19-6725,W17-4708,0,0.0191264,"stic knowledge, such as morphological features, part of speech (POS) tags and syntactic dependency labels, as input features for the English-German and English-Romanian NMT systems. Baniata et al. (2018) proposed a multitask-based NMT system with POS information for translation between English, modern standard Arabic and Arabic dialects, i.e. Levantine Arabic and Maghrebi Arabic. The work demonstrated that the POS information for the low resourced Arabic dialects was beneficial for the translation quality, specifically if pre-trained FastText models were injected during the NMT training step. Niehues and Cho (2017) jointly trained several English-German natural language processing tasks in one system with shared encoder and one attention model and decoder per task. By integrating additional linguistic resources via multitask learning, the performance of each individual task was improved. Bastings et al. (2017) showed that incorporating syntactic structure such as dependency tree using graph convolutional encoders was beneficial for neural machine translation. Their work focused on exploiting structural information on the source side by adding a second encoder. The goal of their work was to provide the e"
W19-6725,P02-1040,0,0.104429,"t neural network training parameters: two hidden layers, 500 hidden 4 github.com/fxsjy/jieba Dublin, Aug. 19-23, 2019 |p. 128 LSTM (long short term memory) units per layer, input feeding enabled, 13 epochs, batch size of 64, 0.3 dropout probability, dynamic learning rate decay, 500 dimension embeddings, maximum vocabulary size of 50,000 subwords, maximum of 32,000 unique BPE merge operations, unlimited different values for the word features and between 11 and 23 dimension embeddings for word features.5 4.3 Evaluation In order to evaluate the performance of the different systems, we used BLEU (Papineni et al., 2002), an automatic evaluation that boasts high correlation with human judgements, and translation error rate (TER) (Snover et al., 2006), a metric that represents the cost of editing the output of the MT systems to match the reference. Additionally, we used bootstrap resampling (Koehn, 2004) with a sample size of 1,000 and 1,000 iterations, and reported statistical significance with ? < 0.05. We also presented a box-and-whisker plot with the first, second and third quartiles as a box, and the first (<0.025) and last (≥0.975) 40-quantiles as whiskers, corresponding to ? < 0.05. In addition, we comp"
W19-6725,W16-2209,0,0.044847,"sentence. We focused on the analysis phase, with special interest for two of the features used: the morphological category (CAT) and the inflection class (CL) or classes of the lexical entries. In order to test this approach, we focused on English-Spanish (both generic and medical domain), English-Basque, English-Irish and EnglishSimplified Chinese in an under-resourced scenario, using corpora with around one million parallel entries. Results suggested that adding morphological information to the source language is as effective as using subword units in this particular setting. 2 Related work Sennrich and Haddow (2016) demonstrated the inclusion of various linguistic knowledge, such as morphological features, part of speech (POS) tags and syntactic dependency labels, as input features for the English-German and English-Romanian NMT systems. Baniata et al. (2018) proposed a multitask-based NMT system with POS information for translation between English, modern standard Arabic and Arabic dialects, i.e. Levantine Arabic and Maghrebi Arabic. The work demonstrated that the POS information for the low resourced Arabic dialects was beneficial for the translation quality, specifically if pre-trained FastText models"
W19-6725,tiedemann-2012-parallel,0,0.0159866,"is work, we focused on NMT for underresourced scenarios. On the one hand, we consider languages, such as Basque or Irish, which do not have a significant amount of parallel data necessary to train a neural model. On the other hand, an under-resourced scenario can be a specific domain, e.g. medical, where a significant amount of data exists, but does not cover the targeted domain. The Table 1 shows the statistics on the used datasets. For Basque and Irish, we used the available corpora stored on the OPUS webpage.2 We used OpenSubtitles2018 (Lison and Tiedemann, 2016),3 Gnome and KDE4 datasets (Tiedemann, 2012). Additionally, the English-Irish parallel corpus is augmented with second level education textbooks (Cuimhne na dTéacsleabhar) in the domain of economics and geography (Arcan et al., 2016). In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences. To ensure a broad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 3 opus.nlpl.eu www.opensubtitles.org Proceedings of MT Summit XVII, volume 2 corpora from the OPUS web page into one par"
W19-6725,N03-1033,0,0.0460273,"classified as verbs, as they share the same surface form; e.g. the word snake can be both a noun and a verb (Figure 1). For addressing this problem, we took two different approaches: using ambiguity classes that describe all the possible analysis for a given surface word; and using external information (in the form of a monolingual POS tagger) to disambiguate. For the former approach, we used a unique tag for each possible CAT and CL values concatenation; e.g. the categories NST and VST and all the inflection classes (CL) for snake (Figure 1). For the latter, we used the Stanford POS tagger (Toutanova et al., 2003), that uses the Penn Treebank (Marcus et al., 1994) tag Proceedings of MT Summit XVII, volume 2 set for English, and the AnCora (Civit and Martí, 2004) tag set for Spanish, and the IXA pipeline POS tagger (Agerri et al., 2014) with the Universal Dependencies POS tag set (Nivre et al., 2018) for Basque. All POS tag sets were mapped to the tag set used by Lucy LT. If the tagger provided POS tag was equivalent to one or more Lucy LT tags, then the non matching Lucy LT tags were removed. Otherwise, we kept the set of tags; e.g. if the POS tag emits noun as the most likely tag, then only NST and th"
W19-6725,N16-1004,0,0.0974332,"2008, the prices keep being sky-high. Although the price increases were lower in the second half of 2008, prices are still very high. 47.48 47.48 44.50 48.25 47.48 45.51 0.00 41.81 0.35 0.35 0.45 0.35 0.35 0.40 0.70 0.40 Table 2: Qualitative analysis of a sentence translated by all models for Spanish to English translation. Fragments in bold face are translation mistakes, and fragments in italics are translation alternatives that, while being penalised by TER and BLEU, can be considered correct. path would be using multiple encoders. This approach has been used to improve the performance NMT (Zoph and Knight, 2016), but, in our scenario, one of the inputs would be the output of the RBMT system. As previously mentioned, corpus-based machine translation gives limited control over the output to the user, specially when dealing with homographs and terminology; instead, RBMT gives total control. Combining the source sentence with Proceedings of MT Summit XVII, volume 2 the RBMT output that contains the user-selected translations might lead to improvements in domainspecific or low resource scenarios. Finally, we also plan to leverage information contained in other freely available RBMT systems, such as Aperti"
W19-6725,P16-1162,0,0.0370184,"als. The corpora were tokenised using the OpenNMT toolkit, with the exception of Simplified Chinese, that was tokenized using Jieba,4 and lowercased. 4.2 NMT framework We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. In order to overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We also added the different morphological and syntactic information as word features. We used the following default neural network training parameters: two hidden layers, 500 hidden 4 github.com/fxsjy/jieba Dublin, Aug. 19-23, 2019 |p. 128 LSTM (long short term memory) units per layer, input feeding enabled, 13 epochs, batch size of 64, 0.3 dropout probability, dynamic learning rate decay, 500 dimension embeddings, maximum vocabulary size of 50,000 subwords, m"
W19-6725,D16-1159,0,0.0255064,"anslation. Their work focused on exploiting structural information on the source side by adding a second encoder. The goal of their work was to provide the encoder with access to rich syntactic information without placing rigid constraints on the interaction between syntax and the translation task Etchegoyhen et al. (2018) studied NMT, RBMT, and phrase-based statistical machine translation approaches for Basque-Spanish. The authors focus on different subword unit representations, i.e. linguistically-motivated or frequency-based word Proceedings of MT Summit XVII, volume 2 segmentation method. Shi et al. (2016) investigated whether an encoder-decoder translation system learns syntactic information on the source side as a side affect of training the neural models. Several syntactic labels of the source sentence were created and logistic regression models using the learned sentence encoding vectors or learned word by word hidden vectors were used to predict these syntactic labels. Aharoni and Goldberg (2017) presented a method to incorporate syntactic information of the target language in an NMT system, showing improved word reordering compared to their baseline system. Eriguchi et al. (2016) proposed"
W19-6725,2006.amta-papers.25,0,0.0715443,"ng short term memory) units per layer, input feeding enabled, 13 epochs, batch size of 64, 0.3 dropout probability, dynamic learning rate decay, 500 dimension embeddings, maximum vocabulary size of 50,000 subwords, maximum of 32,000 unique BPE merge operations, unlimited different values for the word features and between 11 and 23 dimension embeddings for word features.5 4.3 Evaluation In order to evaluate the performance of the different systems, we used BLEU (Papineni et al., 2002), an automatic evaluation that boasts high correlation with human judgements, and translation error rate (TER) (Snover et al., 2006), a metric that represents the cost of editing the output of the MT systems to match the reference. Additionally, we used bootstrap resampling (Koehn, 2004) with a sample size of 1,000 and 1,000 iterations, and reported statistical significance with ? < 0.05. We also presented a box-and-whisker plot with the first, second and third quartiles as a box, and the first (<0.025) and last (≥0.975) 40-quantiles as whiskers, corresponding to ? < 0.05. In addition, we compared the performance of our NMT systems with the NMTbased Google Translate,6 and the translations performed using Lucy LT RBMT; for"
W19-6809,I08-2113,0,0.0302845,"e Dravidian language family to exploit the similar syntax and semantic structures by phonetic transcription of the corpora into Latin script along with image feature to improve the translation quality. 3 3.1 Background Dravidian Languages Dravidian languages have individual writing scripts and have been assigned a unique block in the Unicode computing industry standard. The similarity of these languages is that they are all written from left to right, consist of sequences of simple or complex characters and follow an alpha-syllabic writing system in which the individual symbols are syllables (Bhanuprasad and Svenson, 2008). The languages also have different sets of vowels and consonants. Vowels and consonants are atomic but when they are combined with each other they form consonant ligatures. Dravidian languages such as Tamil do not represent differences between aspirated and unaspirated stops, while other Dravidian languages such as Kannada Dublin, Aug. 19-23, 2019 |p. 57 and Malayalam have a large number of loan words from Indo-Aryan languages and support a large number of compound characters resulting from the combination of two consonants symbols (Kumar et al., 2015). The attention model calculates ct as th"
W19-6809,D17-1105,0,0.0119647,"odal NMT (MNMT) was introduced by Specia et al. (2016b) to generate image descriptions for a target language, given an image and/or a description in the source language. In previous works on MNMT, the researchers utilized visual context by involving both NMT and Image Description Generation (IDG) features that explicitly uses an encoder-decoder (Cho et al., 2014). However, the encoder-decoder architecture encodes the source sentence into a fixed-length vector. To overcome this drawback (Bahdanau et al., 2015) introduced attention mechanism to focus on parts of the source sentence. The work by Calixto and Liu (2017), carried out different experiments to incorporate visual features into NMT by projecting an image feature vector as words into the source sentence, using the image to initialize the encoder hidden state, and using image features to initialize the decoder hidden state. In Calixto et al. (2017), the author incorporated features through a separate encoder and doublyattentive attention of the decoder to depend on the LoResMT 2019 image feature. This allowed them to predict the next word and showed that the image feature improved the translation quality. Although all these approaches have demonstr"
W19-6809,P17-1175,0,0.0396914,"Missing"
W19-6809,2018.gwc-1.10,1,0.442141,"Missing"
W19-6809,2003.mtsummit-papers.9,0,0.059995,"Missing"
W19-6809,W18-3405,0,0.064437,"e aligned with the parallel sentences for training. The largest existing dataset containing captions, images, and translations for English, German, French and Czech is the WMT shared task Multi30K dataset which is derived from the Flickr30k dataset (Plummer et al., 2015; Plummer et al., 2017). Typically this data is manually created with the help of bilingual annotators (Elliott et al., 2016), however, for many languages, such resources are not available. In those cases, machine translation can be a useful tool for the quick expansion to new languages by producing candidate translation (Dutta Chowdhury et al., 2018). In order to reduce the amount of time, we pose translation as a post-editing task. We automatically translated the English sentences from the WMT corpus using a pre-trained general domain Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). Multilingual NMT models (Firat et al., 2016) have been shown to increase the translation quality for under-resourced languages. Closely related Dravidian languages such as Tamil (ISO-639-1: ta), Kannada (ISO-639-1: kn), and Malayalam (ISO-639-1: ml) exhibit a large overlap in their vocabulary and strong syntactic and lexical similar"
W19-6809,N16-1101,0,0.0247052,"is manually created with the help of bilingual annotators (Elliott et al., 2016), however, for many languages, such resources are not available. In those cases, machine translation can be a useful tool for the quick expansion to new languages by producing candidate translation (Dutta Chowdhury et al., 2018). In order to reduce the amount of time, we pose translation as a post-editing task. We automatically translated the English sentences from the WMT corpus using a pre-trained general domain Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). Multilingual NMT models (Firat et al., 2016) have been shown to increase the translation quality for under-resourced languages. Closely related Dravidian languages such as Tamil (ISO-639-1: ta), Kannada (ISO-639-1: kn), and Malayalam (ISO-639-1: ml) exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Dravidian languages are a family of languages spoken primarily in the southern part of India and spread over South Asia and are considered as under-resourced languages. However, the scripts used to write these languages are different and they differ in their morphology. Recently Chakravarthi et al. (20"
W19-6809,Q17-1024,0,0.075519,"Missing"
W19-6809,P17-4012,0,0.0113752,"data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table 3: Results are expressed in BLEU score: Baseline is Multimodal NMT, MMNMT is trained on native script, and MMNMT-T is trained utilizing phonetic transcription. Lang pair En-Ta En-Ml En-Kn Ta-En Ml-En Kn-En Figure 2: Example of sentence and image with candidate tran"
W19-6809,P07-2045,0,0.00476175,"ription pairs for each image. Synthetic data or back-transliterated data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table 3: Results are expressed in BLEU score: Baseline is Multimodal NMT, MMNMT is trained on native script, and MMNMT-T is trained utilizing phonetic transcription. Lang pair En-Ta En-Ml En-Kn Ta-En Ml-E"
W19-6809,W15-5404,0,0.115764,"Missing"
W19-6809,P16-1162,0,0.00778975,"slation for the training set of MMDravi. For our tasks, all descriptions in English were converted to lowercase and tokenized, while we LoResMT 2019 Baseline 50.2 35.6 44.5 45.2 34.3 50.0 BLEUscore MMNMT MMNMT-T 51.0 52.3 36.0 36.5 45.1 45.9 47.4 48.9 36.2 37.6 50.2 50.8 did not have to bother about the case correction for Dravidian languages (as they do not have cases). We tokenized the Dravidian language using the OpenNMT tokenizer with segment alphabet options for Tamil, Kannada, and Malayalam. For the sub-word level representation, we chose the 10,000 most frequent units to train the BPE (Sennrich et al., 2016) model. We used this model for the sub-word level segmentation for the training, development, and evaluation set. We trained the MMNMT model to translate from English into Dravidian languages as well as from Dravidian languages into English. Visual features were extracted from publicly available pre-trained CNN’s. Specifically, we extract spatial image features using the VGG-19 network (Simonyan and Zisserman, 2014). In our experiment, we pass all the images in our dataset through the pre-trained VGG19 layered network to extract global information and use them in a separate visual attention me"
W19-6809,W16-2346,0,0.059033,"Missing"
W19-6809,tiedemann-nygaard-2004-opus,0,0.103665,"ed by Multi30K dataset (Elliott et al., 2016). The first one is an English description for each image and its German translation. The second is a corpus of five independently collected English and German description pairs for each image. Synthetic data or back-transliterated data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table"
W19-7101,C16-1010,1,0.937123,"m, 1998) was built from scratch. The taxonomies of the languages, synsets, relations among synset are built first in the merge approach. Popular wordnets like EuroWordNet (Vossen, 1997) and IndoWordNet (Bhattacharyya, 2010) are developed by the expand approach whereby the synsets are built in correspondence with the existing wordnet synsets by translation. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. To evaluate and improve the wordnets for the targeted under-resourced Dravidian languages, Chakravarthi et al. (2018) followed the approach of Arcan et al. (2016), which uses the existing translations of wordnets in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. They use this contextual information to improve the translation quality of WordNet senses. They showed that their approach can help overcome the drawbacks of simple translations of words without context. Chakravarthi et al. (2018) removed the codemixing based on the script of the parallel corpus to reduce the noise in translation. The authors used the SMT to create bilingual MT for three Dravidian languages. In our work, we us"
W19-7101,W05-0909,0,0.097147,"n in different scripts, they must be converted to some common representation before training the MNMT to take advantage of closely related language resources. A phonetic transcription is an approach where a word in one script is transformed into a different script by maintaining phonetic correspondence. Phonetic transcribing to Latin script and International Phonetic Alphabet (IPA) was studied by (Chakravarthi et al., 2019) and showed that Latin script outperforms IPA for the MNMT Dravidian languages. The improvements in results were shown in terms of the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) metric. To evaluate the similarity of the corpus the authors used cosine similarity and shown that transcribing to Latin script retain more similarity. We used Indic-trans library by Bhat et al. (2015), which bring all the languages into a single representation by phoneme matching algorithm. The same library can also back2 3 http://opus.nlpl.eu/ https://github.com/libindic/indic-trans MomenT-2019 transliterate from English (Latin script) to Indian languages. 3.5 Code-Mixing Code-mixing is a phenomenon which occurs commonly in most multilingual societies where the spe"
W19-7101,I08-2113,0,0.0393287,"orpora from three languages. In the second one removed code-mixing, phonetically transcribed the corpora and then compiled the multilingual corpora by concatenating the parallel corpora from three languages. These two experiments are contribution to this work compared to the previous works. 3 3.1 Experiment Setup Dravidian Languages For our study, we perform experiments on Tamil (ISO 639-1: ta), Telugu (ISO 639-1: te) and Kannada (ISO 639-1: kn). The targeted languages for this work differ in their orthographies due to historical reasons and whether they adopted the Sanskrit tradition or not (Bhanuprasad and Svenson, 2008). Each of these has been assigned a unique block in Unicode, and thus from an MNMT perspective are completely distinct. 3.2 Multilingual Neural Machine Translation Johnson et al. (2017) and Ha et al. (2016) extended the architecture of Bahdanau et al. (2015) to use a universal model to handle multiple source and target languages with a special tag in the encoder to determine which target language to translate. The idea is to use the unified vocabulary and training corpus without modification in the architecture to take advantage of the shared embedding. The goal of this approach is to improve"
W19-7101,2018.gwc-1.10,1,0.486668,"and Slavic share words from a common root (cognates), which are highly semantically and phonologically similar. In the scope of the wordnet creation for underresourced languages, combining parallel corpus from closely related languages, phonetic transcription of the corpus and creating multilingual neural machine translation has been shown to improve the results in this paper. The evaluation results ob1 http://globalwordnet.org/ Dublin, Aug. 19-23, 2019 |p. 1 tained from MNMT with transliterated corpus are better than the results of Statistical Machine Translation (SMT) from the recent work (Chakravarthi et al., 2018). 2 Related Work The Princeton WordNet (Miller, 1995; Fellbaum, 1998) was built from scratch. The taxonomies of the languages, synsets, relations among synset are built first in the merge approach. Popular wordnets like EuroWordNet (Vossen, 1997) and IndoWordNet (Bhattacharyya, 2010) are developed by the expand approach whereby the synsets are built in correspondence with the existing wordnet synsets by translation. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. To evaluate and improve the wordnets for the targeted under-resourced Dravidian la"
W19-7101,O09-5003,0,0.0184181,"created by voluntary annotators or align automatically. The technical documents translation such as KDE, GNOME, and Ubuntu translations have code-mixing data since some of the technical terms may not be known to voluntary annotators for translation. But the code-mixing from OpenSubtitle are due to bilingual and historical reasons of Indian speakers (Chanda et al., 2016; Parshad et al., 2016). Different combinations of languages may occur while code-mixing for example GermanItalian and French-Italian in Switzerland, HindiTelugu in state of Telangana, India, TaiwaneseMandarin Chinese in Taiwan (Chan et al., 2009). Since the Internet era, English become the international language of the younger generation. Hence, English words are frequently embedded in Indians’ speech. For our work, only intra-sentential code-mixing was taken into account. In this case, Dravidian languages as the primary language, and English as secondary languages. We removed the English words considering only the English as a foreign word based on the script. Statistics of the removal of code-mixing is shown in Table 2. 3.6 WordNet creation Using contextual information to improve the translation quality of wordnet senses was shown t"
W19-7101,W16-5814,0,0.0295609,"societies where the speaker or writer alternate between two or more languages in a sentence (Ayeomoni, 2006; Ranjan et al., 2016; Yoder et al., 2017; Parshad et al., 2016). Since most of our corpus came from publicly available parallel corpus are created by voluntary annotators or align automatically. The technical documents translation such as KDE, GNOME, and Ubuntu translations have code-mixing data since some of the technical terms may not be known to voluntary annotators for translation. But the code-mixing from OpenSubtitle are due to bilingual and historical reasons of Indian speakers (Chanda et al., 2016; Parshad et al., 2016). Different combinations of languages may occur while code-mixing for example GermanItalian and French-Italian in Switzerland, HindiTelugu in state of Telangana, India, TaiwaneseMandarin Chinese in Taiwan (Chan et al., 2009). Since the Internet era, English become the international language of the younger generation. Hence, English words are frequently embedded in Indians’ speech. For our work, only intra-sentential code-mixing was taken into account. In this case, Dravidian languages as the primary language, and English as secondary languages. We removed the English wor"
W19-7101,P17-1176,0,0.0191449,"bilingual MT for three Dravidian languages. In our work, we use MNMT system and we transliterate the closely related language corpus into a single script to take advantage of MNMT systems. Neural Machine Translation achieved rapid development in recent years, however, conventional NMT (Bahdanau et al., 2015) creates a separate machine translation system for each pair of languages. Creating individual machine translation system for many languages is resource consuming, considering there are around 7000 languages in the world. Recent work on NMT, specifically on lowresource (Zoph et al., 2016; Chen et al., 2017) or zero-resource machine translation (Johnson et al., 2017; Firat et al., 2016) uses third languages as pivots and showed that translation quality is significantly improved. Ha et al. (2016) proposed an approach to extend the Bahdanau et al. (2015) architecture to multilingual translation by sharing the MomenT-2019 entire model. The approach of shared vocabulary across multiple languages resulted in a shared embedding space. Although the results were promising, the result of the experiments was reported in highly resourced languages such as English, German, and French but many under-resourced"
W19-7101,D16-1026,0,0.0629126,"Missing"
W19-7101,Q17-1024,0,0.067806,"Missing"
W19-7101,P17-4012,0,0.0462817,"the common semantics across languages and reduce the number of translation systems needed. The sentence of different languages are distinguished through languages codes. 3.3 Data We used datasets from Chakravarthi et al. (2018) in our experiment. The authors collected three Dravidian languages ↔ English pairs from OPUS2 web-page (Tiedemann and Nygaard, 2004). Corpus statistics are shown in Table 1. More descriptions about the three datasets can be found in Chakravarthi et al. (2018). We transliterated this corpus using Indic-trans library3 . All the sentences are first tokenized with OpenNMT (Klein et al., 2017) tokenizer and then segmented into subword symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merge operations across all the languages. Following Ha et al. (2016), we indicate the language by prepending two tokens to indicate the desired source and target language. An example of a sentence in English to be translated into Tamil would be: src__en tgt_ta I like ice-cream 3.4 Transliteration As the Indian languages under our study are written in different scripts, they must be converted to some common representation before training the MNMT to take advantage of clos"
W19-7101,P02-1040,0,0.110524,"uages under our study are written in different scripts, they must be converted to some common representation before training the MNMT to take advantage of closely related language resources. A phonetic transcription is an approach where a word in one script is transformed into a different script by maintaining phonetic correspondence. Phonetic transcribing to Latin script and International Phonetic Alphabet (IPA) was studied by (Chakravarthi et al., 2019) and showed that Latin script outperforms IPA for the MNMT Dravidian languages. The improvements in results were shown in terms of the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) metric. To evaluate the similarity of the corpus the authors used cosine similarity and shown that transcribing to Latin script retain more similarity. We used Indic-trans library by Bhat et al. (2015), which bring all the languages into a single representation by phoneme matching algorithm. The same library can also back2 3 http://opus.nlpl.eu/ https://github.com/libindic/indic-trans MomenT-2019 transliterate from English (Latin script) to Indian languages. 3.5 Code-Mixing Code-mixing is a phenomenon which occurs commonly in most m"
W19-7101,W16-4806,0,0.0565569,"Missing"
W19-7101,tiedemann-nygaard-2004-opus,0,0.0459346,"fication in the architecture to take advantage of the shared embedding. The goal of this approach is to improve the transDublin, Aug. 19-23, 2019 |p. 2 lation quality for individual languages pairs, for which parallel corpus data is scarce by letting the NMT to learn the common semantics across languages and reduce the number of translation systems needed. The sentence of different languages are distinguished through languages codes. 3.3 Data We used datasets from Chakravarthi et al. (2018) in our experiment. The authors collected three Dravidian languages ↔ English pairs from OPUS2 web-page (Tiedemann and Nygaard, 2004). Corpus statistics are shown in Table 1. More descriptions about the three datasets can be found in Chakravarthi et al. (2018). We transliterated this corpus using Indic-trans library3 . All the sentences are first tokenized with OpenNMT (Klein et al., 2017) tokenizer and then segmented into subword symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merge operations across all the languages. Following Ha et al. (2016), we indicate the language by prepending two tokens to indicate the desired source and target language. An example of a sentence in English to be tr"
W19-7101,W17-2911,0,0.06079,"Missing"
W19-7101,D16-1163,0,0.0322041,"d the SMT to create bilingual MT for three Dravidian languages. In our work, we use MNMT system and we transliterate the closely related language corpus into a single script to take advantage of MNMT systems. Neural Machine Translation achieved rapid development in recent years, however, conventional NMT (Bahdanau et al., 2015) creates a separate machine translation system for each pair of languages. Creating individual machine translation system for many languages is resource consuming, considering there are around 7000 languages in the world. Recent work on NMT, specifically on lowresource (Zoph et al., 2016; Chen et al., 2017) or zero-resource machine translation (Johnson et al., 2017; Firat et al., 2016) uses third languages as pivots and showed that translation quality is significantly improved. Ha et al. (2016) proposed an approach to extend the Bahdanau et al. (2015) architecture to multilingual translation by sharing the MomenT-2019 entire model. The approach of shared vocabulary across multiple languages resulted in a shared embedding space. Although the results were promising, the result of the experiments was reported in highly resourced languages such as English, German, and French but"
