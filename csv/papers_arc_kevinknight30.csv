2021.emnlp-demo.23,{M}eet{D}ot: Videoconferencing with Live Translation Captions,2021,-1,-1,8,0,10361,arkady arkhangorodsky,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes."
2021.blackboxnlp-1.30,Learning Mathematical Properties of Integers,2021,-1,-1,2,0,1349,maria ryskina,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Embedding words in high-dimensional vector spaces has proven valuable in many natural language applications. In this work, we investigate whether similarly-trained embeddings of integers can capture concepts that are useful for mathematical applications. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora."
2020.wmt-1.7,{D}i{D}i{'}s Machine Translation System for {WMT}2020,2020,-1,-1,7,0,13786,tanfang chen,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the DiDi AI Labs{'} submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese-{\textgreater}English. In this direction, we use the Transformer as our baseline model and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese-{\textgreater}English."
2020.iwslt-1.1,{FINDINGS} {OF} {THE} {IWSLT} 2020 {EVALUATION} {CAMPAIGN},2020,-1,-1,12,0,11026,ebrahim ansari,Proceedings of the 17th International Conference on Spoken Language Translation,0,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track{'}s goal, data and evaluation metrics, and reports the results of the received submissions."
2020.inlg-1.44,{R}eview{R}obot: Explainable Paper Review Generation based on Knowledge Synthesis,2020,-1,-1,4,1,4844,qingyun wang,Proceedings of the 13th International Conference on Natural Language Generation,0,"To assist human review process, we build a novel ReviewRobot to automatically assign a review score and write comments for multiple categories such as novelty and meaningful comparison. A good review needs to be knowledgeable, namely that the comments should be constructive and informative to help improve the paper; and explainable by providing detailed evidence. ReviewRobot achieves these goals via three steps: (1) We perform domain-specific Information Extraction to construct a knowledge graph (KG) from the target paper under review, a related work KG from the papers cited by the target paper, and a background KG from a large collection of previous papers in the domain. (2) By comparing these three KGs, we predict a review score and detailed structured knowledge as evidence for each review category. (3) We carefully select and generalize human review sentences into templates, and apply these templates to transform the review scores and evidence into natural language comments. Experimental results show that our review score predictor reaches 71.4{\%}-100{\%} accuracy. Human assessment by domain experts shows that 41.7{\%}-70.5{\%} of the comments generated by ReviewRobot are valid and constructive, and better than human-written ones for 20{\%} of the time. Thus, ReviewRobot can serve as an assistant for paper reviewers, program chairs and authors."
2020.emnlp-main.458,{L}earning to {P}ronounce {C}hinese {W}ithout a {P}ronunciation {D}ictionary,2020,-1,-1,3,1,10362,christopher chu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89{\%}, which significantly exceeds the 22{\%} accuracy of prior work."
2020.emnlp-main.471,{S}olving {H}istorical {D}ictionary {C}odes with a {N}eural {L}anguage {M}odel,2020,-1,-1,3,1,10362,christopher chu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1{\%} of the cipher-word tokens correctly."
2020.acl-main.756,Parallel Corpus Filtering via Pre-trained Language Models,2020,25,0,3,1,10367,boliang zhang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new state-of-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available."
P19-1191,{P}aper{R}obot: Incremental Draft Generation of Scientific Ideas,2019,58,0,4,1,4844,qingyun wang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30{\%}, 24{\%} and 12{\%} of the time, respectively."
P19-1293,Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation,2019,0,1,4,1,20456,nima pourdamghani,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then {`}translating{'} the resulting pseudo-translation, or {`}Translationese{'} into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario."
W18-6502,Describing a Knowledge Base,2018,55,2,7,1,4844,qingyun wang,Proceedings of the 11th International Conference on Natural Language Generation,0,"We aim to automatically generate natural language descriptions about an input structured knowledge base (KB). We build our generation framework based on a pointer network which can copy facts from the input KB, and add two attention mechanisms: (i) \textit{slot-aware attention} to capture the association between a slot type and its corresponding slot value; and (ii) a new \textit{table position self-attention} to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we propose a \textit{KB reconstruction} based metric by extracting a KB from the generation output and comparing it with the input KB. We also create a new data set which includes 106,216 pairs of structured KBs and their corresponding natural language descriptions for two distinct entity types. Experiments show that our approach significantly outperforms state-of-the-art methods. The reconstructed KB achieves 68.8{\%} - 72.6{\%} F-score."
W18-4203,Creative Language Encoding under Censorship,2018,0,2,2,0,716,heng ji,Proceedings of the First Workshop on Natural Language Processing for {I}nternet Freedom,0,"People often create obfuscated language for online communication to avoid Internet censorship, share sensitive information, express strong sentiment or emotion, plan for secret actions, trade illegal products, or simply hold interesting conversations. In this position paper we systematically categorize human-created obfuscated language on various levels, investigate their basic mechanisms, give an overview on automated techniques needed to simulate human encoding. These encoders have potential to frustrate and evade, co-evolve with dynamic human or automated decoders, and produce interesting and adoptable code words. We also summarize remaining challenges for future research on the interaction between Natural Language Processing (NLP) and encryption, and leveraging NLP techniques for encoding and decoding."
W18-1505,Towards Controllable Story Generation,2018,0,24,4,0,1132,nanyun peng,Proceedings of the First Workshop on Storytelling,0,"We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation."
P18-4003,Out-of-the-box Universal {R}omanization Tool uroman,2018,0,3,3,0.765726,28971,ulf hermjakob,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present uroman, a tool for converting text in myriads of languages and scripts such as Chinese, Arabic and Cyrillic into a common Latin-script representation. The tool relies on Unicode data and other tables, and handles nearly all character sets, including some that are quite obscure such as Tibetan and Tifinagh. uroman converts digital numbers in various scripts to Western Arabic numerals. Romanization enables the application of string-similarity metrics to texts from different scripts without the need and complexity of an intermediate phonetic representation. The tool is freely and publicly available as a Perl script suitable for inclusion in data processing pipelines and as an interactive demo web page."
P18-4011,Translating a Language You Don{'}t Know In the {C}hinese Room,2018,0,1,4,0.765726,28971,ulf hermjakob,"Proceedings of {ACL} 2018, System Demonstrations",0,"In a corruption of John Searle{'}s famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don{'}t have any prior knowledge of the source language. Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine. We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language. The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages."
P18-2042,Paper Abstract Writing through Editing Mechanism,2018,21,3,7,1,4844,qingyun wang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30{\%} and by non-expert at a rate up to 80{\%}."
P18-1213,Modeling Naive Psychology of Characters in Simple Commonsense Stories,2018,33,2,4,0,12776,hannah rashkin,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people{'}s mental states {---} a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research."
N18-5009,"{ELISA}-{EDL}: A Cross-lingual Entity Extraction, Linking and Localization System",2018,0,2,6,1,10367,boliang zhang,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We demonstrate ELISA-EDL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets, resources and system training and testing APIs publicly available for research purpose."
N18-2011,Neural Poetry Translation,2018,0,1,3,1,3128,marjan ghazvininejad,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We present the first neural poetry translation system. Unlike previous works that often fail to produce any translation for fixed rhyme and rhythm patterns, our system always translates a source text to an English poem. Human evaluation of the translations ranks the quality as acceptable 78.2{\%} of the time."
N18-2083,Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation,2018,0,5,3,1,20456,nima pourdamghani,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,We present a method for improving word alignments using word similarities. This method is based on encouraging common alignment links between semantically similar words. We use word vectors trained on monolingual data to estimate similarity. Our experiments on translating fifteen languages into English show consistent BLEU score improvements across the languages.
N18-1205,Recurrent Neural Networks as Weighted Language Recognizers,2018,0,11,5,0,9883,yining chen,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs."
L18-1266,"{A}bstract {M}eaning {R}epresentation of Constructions: The More We Include, the Better the Representation",2018,0,1,5,0.169981,5184,claire bonial,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1023,Multi-lingual Common Semantic Space Construction via Cluster-consistent Word Embedding,2018,31,1,5,0.458559,9579,lifu huang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We construct a multilingual common semantic space based on distributional semantics, where words from multiple languages are projected into a shared space via which all available resources and knowledge can be shared across multiple languages. Beyond word alignment, we introduce multiple cluster-level alignments and enforce the word clusters to be consistently distributed across multiple languages. We exploit three signals for clustering: (1) neighbor words in the monolingual word embedding space; (2) character-level information; and (3) linguistic properties (e.g., apposition, locative suffix) derived from linguistic structure knowledge bases available for thousands of languages. We introduce a new cluster-consistent correlational neural network to construct the common semantic space by aligning words as well as clusters. Intrinsic evaluation on monolingual and multilingual QVEC tasks shows our approach achieves significantly higher correlation with linguistic features which are extracted from manually crafted lexical resources than state-of-the-art multi-lingual embedding learning methods do. Using low-resource language name tagging as a case study for extrinsic evaluation, our approach achieves up to 14.6{\%} absolute F-score gain over the state of the art on cross-lingual direct transfer. Our approach is also shown to be robust even when the size of bilingual dictionary is small."
C18-1313,{AMR} Beyond the Sentence: the Multi-sentence {AMR} corpus,2018,0,2,5,0,5428,tim ogorman,Proceedings of the 27th International Conference on Computational Linguistics,0,"There are few corpora that endeavor to represent the semantic content of entire documents. We present a corpus that accomplishes one way of capturing document level semantics, by annotating coreference and similar phenomena (bridging and implicit roles) on top of gold Abstract Meaning Representations of sentence-level semantics. We present a new corpus of this annotation, with analysis of its quality, alongside a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) may become a feasible method for developing rich representations of document meaning, useful for tasks such as information extraction and question answering."
W17-2315,Biomedical Event Extraction using {A}bstract {M}eaning {R}epresentation,2017,32,15,3,0,4267,sudha rao,{B}io{NLP} 2017,0,"We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results."
P17-4008,{H}afez: an Interactive Poetry Generation System,2017,6,22,4,1,3128,marjan ghazvininejad,"Proceedings of {ACL} 2017, System Demonstrations",0,None
P17-2091,Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary,2017,19,5,2,1,13788,xing shi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We speed up Neural Machine Translation (NMT) decoding by shrinking run-time target vocabulary. We experiment with two shrinking approaches: Locality Sensitive Hashing (LSH) and word alignments. Using the latter method, we get a 2x overall speed-up over a highly-optimized GPU implementation, without hurting BLEU. On certain low-resource language pairs, the same methods improve BLEU by 0.5 points. We also report a negative result for LSH on GPUs, due to relatively large overhead, though it was successful on CPUs. Compared with Locality Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs."
P17-1178,Cross-lingual Name Tagging and Linking for 282 Languages,2017,30,14,5,1,4904,xiaoman pan,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data."
I17-1037,Embracing Non-Traditional Linguistic Resources for Low-resource Language Name Tagging,2017,0,2,7,1,10367,boliang zhang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Current supervised name tagging approaches are inadequate for most low-resource languages due to the lack of annotated data and actionable linguistic knowledge. All supervised learning methods (including deep neural networks (DNN)) are sensitive to noise and thus they are not quite portable without massive clean annotations. We found that the F-scores of DNN-based name taggers drop rapidly (20{\%}-30{\%}) when we replace clean manual annotations with noisy annotations in the training data. We propose a new solution to incorporate many non-traditional language universal resources that are readily available but rarely explored in the Natural Language Processing (NLP) community, such as the World Atlas of Linguistic Structure, CIA names, PanLex and survival guides. We acquire and encode various types of non-traditional linguistic resources into a DNN name tagger. Experiments on three low-resource languages show that feeding linguistic knowledge can make DNN significantly more robust to noise, achieving 8{\%}-22{\%} absolute F-score gains on name tagging without using any human annotation"
D17-1266,Deciphering Related Languages,2017,19,13,2,1,20456,nima pourdamghani,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for translating texts between close language pairs. The method does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs: Afrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian, Malaysian/Indonesian, and Polish/Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data."
W16-6603,Generating {E}nglish from {A}bstract {M}eaning {R}epresentations,2016,9,25,2,1,20456,nima pourdamghani,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-5907,Unsupervised Neural Hidden {M}arkov Models,2016,26,7,5,0,9984,ke tran,Proceedings of the Workshop on Structured Prediction for {NLP},0,"In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context."
W16-5603,Obfuscating Gender in Social Media Writing,2016,29,29,2,1,10644,sravana reddy,Proceedings of the First Workshop on {NLP} and Computational Social Science,0,None
W16-2701,Leveraging Entity Linking and Related Language Projection to Improve Name Transliteration,2016,0,2,5,0.555556,4847,ying lin,Proceedings of the Sixth Named Entity Workshop,0,None
P16-1006,A Multi-media Approach to Cross-lingual Entity Knowledge Transfer,2016,38,4,6,1,4012,di lu,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
P16-1038,Grapheme-to-Phoneme Models for (Almost) Any Language,2016,13,10,2,1,33847,aliya deri,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1004,Multi-Source Neural Translation,2016,20,12,2,1,34666,barret zoph,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to 4.8 Bleu increases on top of a very strong attention-based neural translation model."
N16-1029,Name Tagging for Low-resource Incident Languages based on Expectation-driven Learning,2016,46,14,6,1,10367,boliang zhang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper we tackle a challenging name tagging problem in an emergent setting the tagger needs to be complete within a few hours for a new incident language (IL) using very few resources. Inspired by observing how human annotators attack this challenge, we propose a new expectation-driven learning framework. In this framework we rapidly acquire, categorize, structure and zoom in on ILspecific expectations (rules, features, patterns, gazetteers, etc.) from various non-traditional sources: consulting and encoding linguistic knowledge from native speakers, mining and projecting patterns from both mono-lingual and cross-lingual corpora, and typing based on cross-lingual entity linking. We also propose a cost-aware combination approach to compose expectations. Experiments on seven low-resource languages demonstrate the effectiveness and generality of this framework: we are able to setup a name tagger for a new IL within two hours, and achieve 33.8%-65.1% F-score 1."
N16-1145,"Simple, Fast Noise-Contrastive Estimation for Large {RNN} Vocabularies",2016,16,17,4,1,34666,barret zoph,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs."
L16-1067,Extracting Structured Scholarly Information from the Machine Translation Literature,2016,8,1,4,0,3377,eunsol choi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Understanding the experimental results of a scientific paper is crucial to understanding its contribution and to comparing it with related work. We introduce a structured, queryable representation for experimental results and a baseline system that automatically populates this representation. The representation can answer compositional questions such as: {``}Which are the best published results reported on the NIST 09 Chinese to English dataset?{''} and {``}What are the most important methods for speeding up phrase-based decoding?{''} Answering such questions usually involves lengthy literature surveys. Current machine reading for academic papers does not usually consider the actual experiments, but mostly focuses on understanding abstracts. We describe annotation work to create an initial hscientific paper; experimental results representationi corpus. The corpus is composed of 67 papers which were manually annotated with a structured representation of experimental results by domain experts. Additionally, we present a baseline algorithm that characterizes the difficulty of the inference task."
D16-1126,Generating Topical Poetry,2016,24,37,4,1,3128,marjan ghazvininejad,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1159,Does String-Based Neural {MT} Learn Source Syntax?,2016,0,98,3,1,13788,xing shi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1163,Transfer Learning for Low-Resource Neural Machine Translation,2016,23,96,4,1,34666,barret zoph,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation."
D16-1248,Why Neural Translations are the Right Length,2016,9,23,2,1,13788,xing shi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-1801,Invited Talk: How Much Information Does a Human Translator Add to the Original?,2015,0,0,1,1,10368,kevin knight,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"It is well-known that natural language has built-in redundancy. By using context, we can often guess the next word or character in a text. Two practical communities have independently exploited this fact. First, automatic speech and translation researchers build language models to distinguish fluent from non-fluent outputs. Second, text compression researchers convert predictions into short encodings, to save disk space and bandwidth. I will explore what these two communities can learn from each othersxe2x80x99 (interestingly different) solutions. Then I will look at the less-studied question of redundancy in bilingual text, addressing questions like How well can we predict human translator behavior? and How much information does a human translator add to the original? (This is joint work with Barret Zoph and Marjan Ghazvininejad.)"
P15-1057,Context-aware Entity Morph Decoding,2015,36,4,7,1,10367,boliang zhang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, xe2x80x9cBlack Mambaxe2x80x9d, the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing basketball games. This paper presents the first end-to-end context-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute xe2x80x9ccold-startxe2x80x9d it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1"
P15-1081,Unifying {B}ayesian Inference and Vector Space Models for Improved Decipherment,2015,23,12,3,1,30565,qing dou,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We introduce into Bayesian decipherment a base distribution derived from similarities of word embeddings. We use Dirichlet multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word embeddings from non-parallel data. Experimental results show that the base distribution is highly beneficial to decipherment, improving state-of-the-art decipherment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English."
N15-1021,How to Make a Frenemy: Multitape {FST}s for Portmanteau Generation,2015,4,6,2,1,33847,aliya deri,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"A portmanteau is a type of compound word that fuses the sounds and meanings of two component words; for example, xe2x80x9cfrenemyxe2x80x9d (friend  enemy) or xe2x80x9csmogxe2x80x9d (smoke  fog). We develop a system, including a novel multitape FST, that takes an input of two words and outputs possible portmanteaux. Our system is trained on a list of known portmanteaux and their component words, and achieves 45% exact matches in cross-validated experiments."
N15-1119,Unsupervised Entity Linking with {A}bstract {M}eaning {R}epresentation,2015,23,42,5,1,4904,xiaoman pan,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Most successful Entity Linking (EL) methods aim to link mentions to their referent entities in a structured Knowledge Base (KB) by comparing their respective contexts, often using similarity measures. While the KB structure is given, current methods have suffered from impoverished information representations on the mention side. In this paper, we demonstrate the effectiveness of Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to select high quality sets of entity xe2x80x9ccollaboratorsxe2x80x9d to feed a simple similarity measure (Jaccard) to link entity mentions. Experimental results show that AMR captures contextual properties discriminative enough to make linking decisions, without the need for EL training data, and that system with AMR parsing output outperforms hand labeled traditional semantic roles as context representation for EL. Finally, we show promising preliminary results for using AMR to select sets of xe2x80x9ccoherentxe2x80x9d entity mentions for collective entity linking 1 ."
N15-1180,How to Memorize a Random 60-Bit String,2015,11,3,2,1,3128,marjan ghazvininejad,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"User-generated passwords tend to be memorable, but not secure. A random, computergenerated 60-bit string is much more secure. However, users cannot memorize random 60bit strings. In this paper, we investigate methods for converting arbitrary bit strings into English word sequences (both prose and poetry), and we study their memorability and other properties."
D15-1105,How Much Information Does a Human Translator Add to the Original?,2015,31,3,3,1,34666,barret zoph,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We ask how much information a human translator adds to an original text, and we provide a bound. We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator? We develop new compression algorithms and establish a benchmark task."
D15-1136,Parsing {E}nglish into {A}bstract {M}eaning {R}epresentation Using Syntax-Based Machine Translation,2015,38,27,3,1,25437,michael pust,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results."
P14-2046,How to Speak a Language without Knowing It,2014,5,2,2,1,13788,xing shi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We develop a system that lets people overcome language barriers by letting them speak a language they do not know. Our system accepts text entered by a user, translates the text, then converts the translation into a phonetic spelling in the userxe2x80x99s own orthography. We trained the system on phonetic spellings in travel phrasebooks."
P14-2115,Be Appropriate and Funny: Automatic Entity Morph Encoding,2014,17,8,5,1,10367,boliang zhang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Internet users are keen on creating different kinds of morphs to avoid censorship, express strong sentiment or humor. For example, in Chinese social media, users often use the entity morph xe2x80x9cxc2xb9 ? b (Instant Noodles)xe2x80x9d to refer to xe2x80x9ch 8 xc2xb7 (Zhou Yongkang)xe2x80x9d because it shares one character xe2x80x9cxc2xb7 (Kang)xe2x80x9d with the well-known brand of instant noodles xe2x80x9cxc2xb7xefxbfxbd (Master Kang)xe2x80x9d. We developed a wide variety of novel approaches to automatically encode proper and interesting morphs, which can effectively pass decoding tests 1 ."
braune-etal-2014-mapping,Mapping Between {E}nglish Strings and Reentrant Semantic Graphs,2014,23,5,3,0,10142,fabienne braune,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We investigate formalisms for capturing the relation between semantic graphs and English strings. Semantic graph corpora have spurred recent interest in graph transduction formalisms, but it is not yet clear whether such formalisms are a good fit for natural language dataâin particular, for describing how semantic reentrancies correspond to English pronouns, zero pronouns, reflexives, passives, nominalizations, etc. We introduce a data set that focuses on these problems, we build grammars to capture the graph/string relation in this data, and we evaluate those grammars for conciseness and accuracy."
D14-1030,Aligning context-based statistical models of language with brain activity during reading,2014,19,30,3,0,26203,leila wehbe,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neural networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity. Our novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secondly, the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brainxe2x80x99s own representation of word i. Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brainxe2x80x99s own assessment of the probability of word i, as it can be used to predict the brain activity after the word ixe2x80x99s properties have been fetched from memory and the brain is in the process of integrating it into the context."
D14-1048,Aligning {E}nglish Strings with {A}bstract {M}eaning {R}epresentation Graphs,2014,7,32,4,1,20456,nima pourdamghani,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We align pairs of English sentences and corresponding Abstract Meaning Representations (AMR), at the token level. Such alignments will be useful for downstream extraction of semantic interpretation and generation rules. Our method involves linearizing AMR structures and performing symmetrized EM training. We obtain 86.5% and 83.1% alignment F score on development and test sets."
D14-1061,Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation,2014,35,14,3,1,30565,qing dou,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Inspired by previous work, where decipherment is used to improve machine translation, we propose a new idea to combine word alignment and decipherment into a single learning process. We use EM to estimate the model parameters, not only to maximize the probability of parallel corpus, but also the monolingual corpus. We apply our approach to improve Malagasy-English machine translation, where only a small amount of parallel data is available. In our experiments, we observe gains of 0.9 to 2.1 Bleu over a strong baseline."
D14-1185,Cipher Type Detection,2014,5,6,2,0,37443,malte nuhn,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Manual analysis and decryption of enciphered documents is a tedious and error prone work. Oftenxe2x80x94even after spending large amounts of time on a particular cipherxe2x80x94no decipherment can be found. Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives, and to focus human effort only on a small but potentially interesting subset of them. In this work, we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext. We are able to distinguish 50 different cipher types (specified by the American Cryptogram Association) with an accuracy of 58.5%. This is a 11.2% absolute improvement over the best previously published classifier."
W13-2322,{A}bstract {M}eaning {R}epresentation for Sembanking,2013,23,386,7,0,40939,laura banarescu,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it."
P13-5003,Decipherment,2013,3,1,1,1,10368,kevin knight,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials),0,None
P13-2131,{S}match: an Evaluation Metric for Semantic Feature Structures,2013,14,97,2,0,36751,shu cai,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study."
P13-1091,Parsing Graphs with Hyperedge Replacement Grammars,2013,13,47,6,0,3180,david chiang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithmxe2x80x99s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing."
D13-1173,Dependency-Based Decipherment for Resource-Limited Machine Translation,2013,25,15,2,1,30565,qing dou,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets."
W12-6207,{DAGGER}: A Toolkit for Automata on Directed Acyclic Graphs,2012,22,6,2,0,36857,daniel quernheim,Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing,0,"This paper presents DAGGER, a toolkit for finite-state automata that operate on directed acyclic graphs (dags). The work is based on a model introduced by (Kamimura and Slutzki, 1981; Kamimura and Slutzki, 1982), with a few changes to make the automata more applicable to natural language processing. Available algorithms include membership checking in bottom-up dag acceptors, transduction of dags to trees (bottom-up dag-to-tree transducers), k-best generation and basic operations such as union and intersection."
W12-4209,Towards Probabilistic Acceptors and Transducers for Feature Structures,2012,35,15,2,0,36857,daniel quernheim,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Weighted finite-state acceptors and transducers (Pereira and Riley, 1997) are a critical technology for NLP and speech systems. They flexibly capture many kinds of stateful left-to-right substitution, simple transducers can be composed into more complex ones, and they are EM- trainable. They are unable to handle long-range syntactic movement, but tree acceptors and transducers address this weakness (Knight and Graehl, 2005). Tree automata have been profitably used in syntax-based MT systems. Still, strings and trees are both weak at representing linguistic structure involving semantics and reference (who did what to whom). Feature structures provide an attractive, well-studied, standard format (Shieber, 1986; Rounds and Kasper, 1986), which we can view computationally as directed acyclic graphs. In this paper, we develop probabilistic acceptors and transducers for feature structures, demonstrate them on linguistic problems, and lay down a foundation for semantics-based MT."
P12-2016,Decoding Running Key Ciphers,2012,10,0,2,1,10644,sravana reddy,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic cipher-texts of different lengths, and find that it outperforms previous work that employs Viterbi decoding with character-based models."
D12-1025,Large Scale Decipherment for Out-of-Domain Machine Translation,2012,19,43,2,1,30565,qing dou,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points."
C12-1083,Semantics-Based Machine Translation with Hyperedge Replacement Grammars,2012,51,82,5,0,41034,bevan jones,Proceedings of {COLING} 2012,0,"We present an approach to semantics-based statistical machine translation that uses synchronous hyperedge replacement grammars to translate into and from graph-shaped intermediate meaning representations, to our knowledge the first work in NLP to make use of synchronous context free graph grammars. We present algorithms for each step of the semantics-based translation pipeline, including a novel graph-to-word alignment algorithm and two algorithms for synchronous grammar rule extraction. We investigate the influence of syntactic annotations on semantics-based translation by presenting two alternative rule extraction algorithms, one that requires only semantic annotations and another that additionally relies on syntactic annotations, and explore the effect of syntax and language bias in meaning representation structures by running experiments with two different meaning representations, one biased toward an English syntax-like structure and another that is language neutral. While preliminary work, these experiments show promise for semantically-informed machine translation."
W11-1511,What We Know About The Voynich Manuscript,2011,13,13,2,1,10644,sravana reddy,"Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,The Voynich Manuscript is an undeciphered document from medieval Europe. We present current knowledge about the manuscript's text through a series of questions about its linguistic properties.
W11-1201,Putting a Value on Comparable Data,2011,0,0,1,1,10368,kevin knight,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"Machine translation began in 1947 with an influential memo by Warren Weaver. In that memo, Weaver noted that human code-breakers could transform ciphers into natural language (e.g., into Turkish)n n xe2x80xa2 without access to parallel ciphertext/plaintext data, andn n xe2x80xa2 without knowing the plaintext language's syntax and semantics.n n Simple word- and letter-statistics seemed to be enough for the task. Weaver then predicted that such statistical methods could also solve a tougher problem, namely language translation.n n This raises the question: can sufficient translation knowledge be derived from comparable (non-parallel) data?n n In this talk, I will discuss initial work in treating foreign language as a code for English, where we assume the code to involve both word substitutions and word transpositions. In doing so, I will quantitatively estimate the value of non-parallel data, versus parallel data, in terms of end-to-end accuracy of trained translation systems. Because we still know very little about solving word-based codes, I will also describe successful techniques and lessons from the realm of letter-based ciphers, where the non-parallel resources are (1) enciphered text, and (2) unrelated plaintext. As an example, I will describe how we decoded the Copiale cipher with limited computer-like knowledge of the plaintext language.n n The talk will wrap up with challenges in exploiting comparable data at all levels: letters, words, phrases, syntax, and semantics."
W11-1202,The Copiale Cipher,2011,1,12,1,1,10368,kevin knight,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,The Copiale cipher is a 105-page enciphered book dated 1866. We describe the features of the book and the method by which we deciphered it.
P11-2014,Unsupervised Discovery of Rhyme Schemes,2011,11,20,2,1,10644,sravana reddy,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation."
P11-1002,Deciphering Foreign Language,2011,23,73,2,1,1424,sujith ravi,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from non-parallel text."
P11-1025,{B}ayesian Inference for Zodiac and Other Homophonic Ciphers,2011,18,18,2,1,1424,sujith ravi,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before."
W10-2502,A Decoder for Probabilistic Synchronous Tree Insertion Grammars,2010,28,3,2,1,11017,steve deneefe,Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing,0,Synchronous tree insertion grammars (STIG) are formal models for syntax-based machine translation. We formalize a decoder for probabilistic STIG; the decoder transforms every source-language string into a target-language tree and calculates the probability of this transformation.
P10-1051,Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons,2010,19,10,3,1,1424,sujith ravi,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization."
P10-1107,A Statistical Model for Lost Language Decipherment,2010,24,31,3,0,37815,benjamin snyder,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and high-level morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew."
P10-1108,Efficient Inference through Cascades of Weighted Tree Transducers,2010,19,20,2,1,3516,jonathan may,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001)."
N10-1014,Unsupervised Syntactic Alignment with Inversion Transduction Grammars,2010,34,18,4,0,3930,adam pauls,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline."
N10-1068,{B}ayesian Inference for Finite-State Transducers,2010,25,15,3,0.305638,3180,david chiang,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a Bayesian inference algorithm that can be used to train any cascade of weighted finite-state transducers on end-to-end data. We also investigate the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches."
J10-3001,{S}quibs: Does {GIZA}++ Make Search Errors?,2010,5,8,2,1,1424,sujith ravi,Computational Linguistics,0,"Word alignment is a critical procedure within statistical machine translation (SMT). Brown et al. (1993) have provided the most popular word alignment algorithm to date, one that has been implemented in the GIZA (Al-Onaizan et al., 1999) and GIZA (Och and Ney 2003) software and adopted by nearly every SMT project. In this article, we investigate whether this algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns alignments that are sub-optimal according to a trained model."
J10-2004,"Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation",2010,43,44,3,1,4596,wei wang,Computational Linguistics,0,"This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. We present three modifications to the MT training data to improve the accuracy of a state-of-the-art syntax MT system: re-structuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and re-aligning unifies word alignment across sentences to remove bad word alignments and refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm. We show that each individual technique leads to improvement as measured by BLEU, and we also show that the greatest improvement is achieved by combining them. We report an overall 1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English translation."
D10-1051,Automatic Analysis of Rhythmic Poetry with Applications to Generation and Translation,2010,12,75,3,0,46396,erica greene,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We employ statistical methods to analyze, generate, and translate rhythmic poetry. We first apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns."
C10-1106,"Fast, Greedy Model Minimization for Unsupervised Tagging",2010,15,10,3,1,1424,sujith ravi,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian."
W09-3503,Automata for Transliteration and Machine Translation,2009,0,1,1,1,10368,kevin knight,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"Automata theory, transliteration, and machine translation (MT) have an interesting and intertwined history.n n Finite-state string automata theory became a powerful tool for speech and language after the introduction of the ATT furthermore, these machines can be pipelined to attack complex problems like speech recognition. Likewise, n-gram models can be captured by finite-state acceptors, which can be reused across applications.n n It is possible to mix, match, and compose transducers to flexibly solve all kinds of problems. One such problem is transliteration, which can be modeled as a pipeline of string transformations. MT has also been modeled with transducers, and descendants of the FSM toolkit are now used to implement phrase-based machine translation. Even speech recognizers and MT systems can themselves be composed to deliver speech-to-speech MT.n n The main rub with finite-state string MT is word re-ordering. Tree transducers offer a natural mechanism to solve this problem, and they have recently been employed with some success.n n In this talk, we will survey these ideas (and their origins), and we will finish with a discussion of how transliteration and MT can work together."
W09-1804,A New Objective Function for Word Alignment,2009,23,16,2,0,46397,tugba bodrumlu,Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,0,"We develop a new objective function for word alignment that measures the size of the bilingual dictionary induced by an alignment. A word alignment that results in a small dictionary is preferred over one that results in a large dictionary. In order to search for the alignment that minimizes this objective, we cast the problem as an integer linear program. We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus."
P09-5002,Topics in Statistical Machine Translation,2009,0,1,1,1,10368,kevin knight,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,"In the past, we presented tutorials called Introduction to Statistical Machine Translation, aimed at people who know little or nothing about the field and want to get acquainted with the basic concepts. This tutorial, by contrast, goes more deeply into selected topics of intense current interest. We aim at two types of participants:n n 1. People who understand the basic idea of statistical machine translation and want to get a survey of hot-topic current research, in terms that they can understand.n n 2. People associated with statistical machine translation work, who have not had time to study the most current topics in depth."
P09-1057,Minimized Models for Unsupervised Part-of-Speech Tagging,2009,9,77,2,1,1424,sujith ravi,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings."
P09-1064,Fast Consensus Decoding over Translation Forests,2009,26,39,3,0,817,john denero,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The minimum Bayes risk (MBR) decoding objective improves BLEU scores for machine translation output relative to the standard Viterbi objective of maximizing model score. However, MBR targeting BLEU is prohibitively slow to optimize over k-best lists for large k. In this paper, we introduce and analyze an alternative to MBR that is equally effective at improving performance, yet is asymptotically faster --- running 80 times faster than MBR in experiments with 1000-best lists. Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists. We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems. Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU."
N09-4008,"Writing Systems, Transliteration and Decipherment",2009,0,5,1,1,10368,kevin knight,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts",0,"Nearly all of the core data that computational linguists deal with is in the form of text, which is to say that it consists of language data written (usually) in the standard writing system for the language in question. Yet surprisingly little is generally understood about how writing systems work. This tutorial will be divided into three parts. In the first part we discuss the history of writing and introduce a wide variety of writing systems, explaining their structure and how they encode language. We end this section with a brief review of how some of the properties of writing systems are handled in modern encoding systems, such as Unicode, and some of the continued pitfalls that can occur despite the best intentions of standardization. The second section of the tutorial will focus on the problem of transcription between scripts (often termed transliteration), and how this problem--which is important both for machine translation and named entity recognition--has been addressed. The third section is more theoretical and, at the same time we hope, more fun. We will discuss the problem of decipherment and how computational methods might be brought to bear on the problem of unlocking the mysteries of as yet undeciphered ancient scripts. We start with a brief review of three famous cases of decipherment. We then discuss how techniques that have been used in speech recognition and machine translation might be applied to the problem of decipherment. We end with a survey of the as-yet undeciphered ancient scripts and give some sense of the prospects of deciphering them given currently available data."
N09-2036,Faster {MT} Decoding Through Pervasive Laziness,2009,4,8,2,1,25437,michael pust,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Syntax-based MT systems have proven effective---the models are compelling and show good room for improvement. However, decoding involves a slow search. We present a new lazy-search method that obtains significant speedups over a strong baseline, with no loss in Bleu."
N09-2064,Combining Constituent Parsers,2009,9,25,2,1,41829,victoria fossum,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006). We propose three ways to improve upon existing methods for parser combination. First, we propose a method of parse hybridization that recombines context-free productions instead of constituents, thereby preserving the structure of the output of the individual parsers to a greater extent. Second, we propose an efficient linear-time algorithm for computing expected f-score using Minimum Bayes Risk parse selection. Third, we extend these parser combination methods from multiple 1-best outputs to multiple n-best outputs. We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus."
N09-1005,Learning Phoneme Mappings for Transliteration without Parallel Data,2009,29,24,2,1,1424,sujith ravi,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task.
N09-1025,"11,001 New Features for Statistical Machine Translation",2009,32,223,2,0.305638,3180,david chiang,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system. On a large-scale Chinese-English translation task, we obtain statistically significant improvements of 1.5 Bleu and  1.1 Bleu, respectively. We analyze the impact of the new features and the performance of the learning algorithm."
J09-4009,Binarization of Synchronous Context-Free Grammars,2009,32,54,4,0.421301,8438,liang huang,Computational Linguistics,0,"Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages. We develop a theory of binarization for synchronous context-free grammars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem."
D09-1076,Synchronous Tree Adjoining Machine Translation,2009,21,50,2,1,11017,steve deneefe,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Tree Adjoining Grammars have well-known advantages, but are typically considered too difficult for practical systems. We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable. Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material. The appropriate modifiers can later be spliced in as needed.n n In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data. We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding. Finally, we show that adjoining results in an end-to-end improvement of 0.8 Bleu over a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task."
W08-0306,Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation,2008,33,44,2,1,41829,victoria fossum,Proceedings of the Third Workshop on Statistical Machine Translation,0,"Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntax-based machine translation. We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules. We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA union baseline."
P08-1045,Name Translation in Statistical Machine Translation - Learning When to Transliterate,2008,14,94,2,1,28971,ulf hermjakob,Proceedings of ACL-08: HLT,1,"We present a method to transliterate names in the framework of end-to-end statistical machine translation. The system is trained to learn when to transliterate. For Arabic to English MT, we developed and trained a transliterator on a bitext of 7 million sentences and Googlexe2x80x99s English terabyte ngrams and achieved better name translation accuracy than 3 out of 4 professional translators. The paper also includes a discussion of challenges in name translation evaluation."
J08-3004,Training Tree Transducers,2008,114,170,2,1,37595,jonathan graehl,Computational Linguistics,0,"Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers."
D08-1085,Attacking Decipherment Problems Optimally with Low-Order {N}-gram Models,2008,12,29,2,1,1424,sujith ravi,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We introduce a method for solving substitution ciphers using low-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon's (1949) theory of uncertainty in decipherment."
D08-1093,Automatic Prediction of Parser Accuracy,2008,29,37,2,1,1424,sujith ravi,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees.n n In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain."
2008.amta-srw.2,Using Bilingual {C}hinese-{E}nglish Word Alignments to Resolve {PP}-attachment Ambiguity in {E}nglish,2008,-1,-1,2,1,41829,victoria fossum,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Student Research Workshop,0,"Errors in English parse trees impact the quality of syntax-based MT systems trained using those parses. Frequent sources of error for English parsers include PP-attachment ambiguity, NP-bracketing ambiguity, and coordination ambiguity. Not all ambiguities are preserved across languages. We examine a common type of ambiguity in English that is not preserved in Chinese: given a sequence {``}VP NP PP{''}, should the PP be attached to the main verb, or to the object noun phrase? We present a discriminative method for exploiting bilingual Chinese-English word alignments to resolve this ambiguity in English. On a held-out test set of Chinese-English parallel sentences, our method achieves 86.3{\%} accuracy on this PP-attachment disambiguation task, an improvement of 4{\%} over the accuracy of the baseline Collins parser (82.3{\%})."
2008.amta-papers.7,Overcoming Vocabulary Sparsity in {MT} Using Lattices,2008,-1,-1,3,1,11017,steve deneefe,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Source languages with complex word-formation rules present a challenge for statistical machine translation (SMT). In this paper, we take on three facets of this challenge: (1) common stems are fragmented into many different forms in training data, (2) rare and unknown words are frequent in test data, and (3) spelling variation creates additional sparseness problems. We present a novel, lightweight technique for dealing with this fragmentation, based on bilingual data, and we also present a combination of linguistic and statistical techniques for dealing with rare and unknown words. Taking these techniques together, we demonstrate +1.3 and +1.6 BLEU increases on top of strong baselines for Arabic-English machine translation."
D07-1038,Syntactic Re-Alignment Models for Machine Translation,2007,22,35,2,1,3516,jonathan may,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We present a method for improving word alignment for statistical syntax-based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly-used word alignment models. This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic.
D07-1078,Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy,2007,16,69,2,1,4596,wei wang,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.
D07-1079,What Can Syntax-Based {MT} Learn from Phrase-Based {MT}?,2007,22,79,2,1,11017,steve deneefe,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We compare and contrast the strengths and weaknesses of a syntax-based machine translation model with a phrase-based machine translation model on several levels. We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations."
2007.mtsummit-ucnlg.1,Automatic language translation generation help needs badly,2007,-1,-1,1,1,10368,kevin knight,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-tutorials.1,Statistical machine translation,2007,2,17,1,1,10368,kevin knight,Proceedings of Machine Translation Summit XI: Tutorials,0,"A method of statistical machine translation (SMT) is provided. The method comprises generating reordering knowledge based on the syntax of a source language (SL) and a number of alignment matrices that map sample SL sentences with sample target language (TL) sentences. The method further comprises receiving a SL word string and parsing the SL word string into a parse tree that represents the syntactic properties of the SL word string. The nodes on the parse tree are reordered based on the generated reordering knowledge in order to provide reordered word strings. The method further comprises translating a number of reordered word strings to create a number of TL word strings, and identifying a statistically preferred TL word string as a preferred translation of the SL word string."
W06-3601,A Syntax-Directed Translator with Extended Domain of Locality,2006,32,63,2,0.421301,8438,liang huang,Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,0,"A syntax-directed translator first parses the source-language input into a parse-tree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended tree-to-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented."
W06-1606,{SPMT}: Statistical Machine Translation with Syntactified Target Language Phrases,2006,16,206,4,0.0534535,31937,daniel marcu,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5."
P06-2065,Unsupervised Analysis for Decipherment Problems,2006,10,50,1,1,10368,kevin knight,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We study a number of natural language decipherment problems using unsupervised learning. These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most often fail on the first try, so we describe techniques for understanding errors and significantly increasing performance."
P06-1121,Scalable Inference and Training of Context-Rich Syntactic Translation Models,2006,10,416,3,0.833333,4268,michel galley,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules."
N06-1001,Capitalizing Machine Translation,2006,13,50,2,1,4596,wei wang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs."
N06-1031,Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality,2006,25,44,2,0,50088,bryant huang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,We identify problems with the Penn Tree-bank that render it imperfect for syntax-based machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.
N06-1033,Synchronous Binarization for Machine Translation,2006,12,142,4,0.279783,7671,hao zhang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system."
N06-1045,A Better N-Best List: Practical Determinization of Weighted Finite Tree Automata,2006,20,31,2,1,3516,jonathan may,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries. This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. It is chiefly due to nondeterminism in the weighted automata that produce the results. We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees. We also demonstrate our algorithm's effectiveness on two large-scale tasks."
2006.amta-papers.8,Statistical Syntax-Directed Translation with Extended Domain of Locality,2006,30,183,2,0.421301,8438,liang huang,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"In syntax-directed translation, the source-language input is first parsed into a parse-tree, which is then recursively converted into a string in the target-language. We model this conversion by an extended tree-to-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear frame-work in order to incorporate other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-the- art phrase-based system."
P05-3023,{T}ransonics: A Practical Speech-to-Speech Translator for {E}nglish-{F}arsi Medical Dialogs,2005,5,15,5,0,50887,robert belvin,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We briefly describe a two-way speech-to-speech English-Farsi translation system prototype developed for use in doctor-patient interactions. The overarching philosophy of the developers has been to create a system that enables effective communication, rather than focusing on maximizing component-level performance. The discussion focuses on the general approach and evaluation of the system by an independent government evaluation team."
P05-3025,Interactively Exploring a Machine Translation Model,2005,8,16,2,1,11017,steve deneefe,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"This paper describes a method of interactively visualizing and directing the process of translating a sentence. The method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the model's strengths and weaknesses, and to compare it to other MT systems. Using this visualization method, we can find and address conceptual and practical problems in an MT system. In our demonstration at ACL, new users of our tool will drive a syntax-based decoder for themselves."
2005.iwslt-1.10,{ISI}{'}s 2005 Statistical Machine Translation Entries,2005,10,2,2,1,11017,steve deneefe,Proceedings of the Second International Workshop on Spoken Language Translation,0,"ISI entered two statistical machine translation systems in the IWSLT evaluation this year: one was phrase-based and the other syntax-based. The syntax-based system represents the results of a current research effort, while the phrase-based system is representative of the current techniques in state-ofthe-art machine translation. This paper primarily describes the syntax-based system and its comparison to the phrasebased system. We will give a brief overview of the components of the systems and discuss the performance on the IWSLT development data, the evaluation results, and some post-evaluation results."
W04-1617,Language Weaver {A}rabic-{\\textgreater}{E}nglish {MT},2004,0,1,4,0.095644,31937,daniel marcu,Proceedings of the Workshop on Computational Approaches to {A}rabic Script-based Languages,0,This presentation is primarily a demonstration of a working statistical machine translation system which translates Modern Standard Arabic into English.
N04-1014,Training Tree Transducers,2004,28,157,2,1,37595,jonathan graehl,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
N04-1035,What{'}s in a translation rule?,2004,9,450,3,0.833333,4268,michel galley,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."
2004.iwslt-evaluation.9,The {ISI}/{USC} {MT} system,2004,2,8,2,1,47735,emil ettelaie,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2004.amta-tutorials.3,Introduction to statistical machine translation,2004,-1,-1,2,1,4417,philipp koehn,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions,0,None
P03-1040,Feature-Rich Statistical Translation of Noun Phrases,2003,16,96,2,1,4417,philipp koehn,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,We define noun phrase translation as a subtask of machine translation. This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features. We achieved 65.5% translation accuracy in a German-English translation task vs. 53.2% with IBM Model 4.
N03-5005,What{'}s New in Statistical Machine Translation,2003,0,17,1,1,10368,kevin knight,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Tutorial Abstracts,0,"Automatic translation from one human language to another using computers, better known as machine translation (MT), is a long-standing goal of computer science. Accurate translation requires a great deal of knowledge about the usage and meaning of words, the structure of phrases, the meaning of sentences, and which real-life situations are plausible. For general-purpose translation, the amount of required knowledge is staggering, and it is not clear how to prioritize knowledge acquisition efforts.Recently, there has been a fair amount of research into extracting translation-relevant knowledge automatically from bilingual texts. In the early 1990s, IBM pioneered automatic bilingual-text analysis. A 1999 workshop at Johns Hopkins University saw a re-implementation of many of the core components of this work, aimed at attracting more researchers into the field. Over the past years, several statistical MT projects have appeared in North America, Europe, and Asia, and the literature is growing substantially. We will provide a technical overview of the state-of-the-art."
N03-2016,Cognates Can Improve Statistical Translation Models,2003,7,86,3,0,1894,grzegorz kondrak,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,We report results of experiments aimed at improving the translation quality by incorporating the cognate information into translation models. The results confirm that the cognate identification approach can improve the quality of word alignment in bitexts without the need for extra resources.
N03-2026,Desparately Seeking {C}ebuano,2003,2,7,12,0,12879,douglas oard,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,This paper describes an effort to rapidly develop language resources and component technology to support searching Cebuano news stories using English queries. Results from the first 60 hours of the exercise are presented.
N03-1024,Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences,2003,16,207,2,0,4447,bo pang,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations."
E03-1076,Empirical Methods for Compound Splitting,2003,11,210,2,1,4417,philipp koehn,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
2003.mtsummit-tttt.3,Teaching statistical machine translation,2003,0,0,1,1,10368,kevin knight,Workshop on Teaching Translation Technologies and Tools,0,This paper describes some resources for introducing concepts of statistical machine translation. Students using these resources are not required to have any particular background in computational linguistics or mathematics.
2003.mtsummit-systems.2,Language Weaver: the next generation of machine translation,2003,2,1,3,0,52985,bryce benjamin,Proceedings of Machine Translation Summit IX: System Presentations,0,"We introduce a new generation of commercial translation software, based primarily on statistical learning and statistical language models."
2003.mtsummit-papers.6,Syntax-based language models for statistical machine translation,2003,-1,-1,2,0,35621,eugene charniak,Proceedings of Machine Translation Summit IX: Papers,0,"We present a syntax-based language model for use in noisy-channel machine translation. In particular, a language model based upon that described in (Cha01) is combined with the syntax based translation-model described in (YK01). The resulting system was used to translate 347 sentences from Chinese to English and compared with the results of an IBM-model-4-based system, as well as that of (YK02), all trained on the same data. The translations were sorted into four groups: good/bad syntax crossed with good/bad meaning. While the total number of translations that preserved meaning were the same for (YK02) and the syntax-based system (and both higher than the IBM-model-4-based system), the syntax based system had 45{\%} more translations that also had good syntax than did (YK02) (and approximately 70{\%} more than IBM Model 4). The number of translations that did not preserve meaning, but at least had good grammar, also increased, though to less avail."
W02-2102,The Importance of Lexicalized Syntax Models for Natural Language Generation Tasks,2002,15,15,2,0,4346,hal iii,Proceedings of the International Natural Language Generation Conference,0,"Abstract : The parsing community has long recognized the importance of lexicalized models of syntax. By contrast, these models do not appear to have had an impact on the statistical NLG community. To prove their importance in NLG, we show that a lexicalized model of syntax improves the performance of a statistical text compression system, and show results that suggest it would also improve the performances of an MT application and a pure natural language generation system."
W02-0902,Learning a Translation Lexicon from Monolingual Corpora,2002,13,204,2,1,4417,philipp koehn,Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition,0,"This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved."
W02-0505,Machine Transliteration of Names in {A}rabic Texts,2002,4,172,2,0,8332,yaser alonaizan,Proceedings of the {ACL}-02 Workshop on Computational Approaches to {S}emitic Languages,0,We present a transliteration algorithm based on sound and spelling mappings using finite state machines. The transliteration models can be trained on relatively small lists of names. We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-to-obtain training data. We apply our transliteration algorithm to the transliteration of names from Arabic into English. We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation. We also compare the accuracy of our system to the accuracy of human translators.
P02-1039,A Decoder for Syntax-based Statistical {MT},2002,10,168,2,1,49937,kenji yamada,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model."
P02-1051,Translating Named Entities Using Monolingual and Bilingual Resources,2002,5,233,2,0,8332,yaser alonaizan,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. We report on the application and evaluation of this algorithm in translating Arabic named entities to English. We also compare our results with the results obtained from human translations and a commercial system for the same task."
2002.tmi-tutorials.2,Statistical machine translation,2002,10,34,1,1,10368,kevin knight,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Tutorials,0,"This paper describes the right-to-left decoding method, which translates an input string by generating in right-to-left direction. In addition, presented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions. The experimental results on Japanese and English translation showed that the right-to-left was better for Englith-to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation."
benjamin-etal-2002-translation,Translation by the numbers: Language Weaver,2002,17,3,2,0,52985,bryce benjamin,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: System Descriptions,0,Pre-market prototype - to be available commercially in the second or third quarter of 2003.
soricut-etal-2002-using,Using a large monolingual corpus to improve translation accuracy,2002,6,45,2,0,4002,radu soricut,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The existence of a phrase in a large monolingual corpus is very useful information, and so is its frequency. We introduce an alternative approach to automatic translation of phrases/sentences that operationalizes this observation. We use a statistical machine translation system to produce alternative translations and a large monolingual corpus to (re)rank these translations. Our results show that this combination yields better translations, especially when translating out-of-domain phrases/sentences. Our approach can be also used to automatically construct parallel corpora from monolingual resources."
W01-0504,Knowledge Sources for Word-Level Translation Models,2001,0,82,2,1,4417,philipp koehn,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
P01-1030,Fast Decoding and Optimal Decoding for Machine Translation,2001,11,250,3,0,5732,ulrich germann,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."
P01-1067,A Syntax-based Statistical Translation Model,2001,320,733,2,1,49937,kenji yamada,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.
2000.amta-tutorials.4,Statistical machine translation,2000,-1,-1,1,1,10368,kevin knight,Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions,0,None
W99-0906,A Computational Approach to Deciphering Unknown Scripts,1999,6,62,1,1,10368,kevin knight,Unsupervised Learning in Natural Language Processing,0,"We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time."
J99-4005,Decoding complexity in word-replacement translation models,1999,3,276,1,1,10368,kevin knight,Computational Linguistics,0,"Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer. Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts. The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel). In order to translate (or decode) a French string, we look for the most likely English source. We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence. We trace this complexity to factors not present in other decoding problems."
W98-1426,The Practical Value of N-Grams Is in Generation,1998,3,121,2,0,54706,irene langkilde,Natural Language Generation,0,None
W98-1005,Translating Names and Technical Terms in {A}rabic Text,1998,3,173,2,0,55177,bonnie glover,Computational Approaches to {S}emitic Languages,0,"It is challenging to translate names and technical terms from English into Arabic. Translation is usually done phonetically: different alphabets and sound inventories force various compromises. For example, Peter Streams may come out as [Abstract contained text which could not be captured.] bytr strymz. This process is called transliteration. We address here the reverse problem: given a foreign name or loanword in Arabic text, we want to recover the original in Roman script. For example, an input like [Abstract contained text which could not be captured.] bytr strymz should yield an output like Peter Streams. Arabic presents special challenges due to unwritten vowels and phonetic-context effects. We present results and examples of use in an Arabic-to-English machine translator."
P98-1116,Generation that Exploits Corpus-Based Statistical Knowledge,1998,11,349,2,0,54706,irene langkilde,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities."
J98-4003,Machine Transliteration,1998,10,349,1,1,10368,kevin knight,Computational Linguistics,0,"It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly trnasliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as konpyuutaa in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process."
C98-1112,Generation that Exploits Corpus-Based Statistical Knowledge,1998,11,349,2,0,54706,irene langkilde,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities."
knight-al-onaizan-1998-translation,Translation with finite-state devices,1998,12,139,1,1,10368,kevin knight,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Statistical models have recently been applied to machine translation with interesting results. Algorithms for processing these models have not received wide circulation, however. By contrast, general finite-state transduction algorithms have been applied in a variety of tasks. This paper gives a finite-state reconstruction of statistical translation and demonstrates the use of standard tools to compute statistically likely translations. Ours is the first translation algorithm for {``}fertility/permutation{''} statistical models to be described in replicable detail."
P97-1017,Machine Transliteration,1997,7,209,1,1,10368,kevin knight,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as (konpyuutaa) in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process."
1996.amta-1.31,{JAPANGLOSS}: using statistics to fill knowledge gaps,1996,-1,-1,1,1,10368,kevin knight,Conference of the Association for Machine Translation in the Americas,0,None
P95-1034,"Two-Level, Many-Paths Generation",1995,25,109,1,1,10368,kevin knight,33rd Annual Meeting of the Association for Computational Linguistics,1,"Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable."
1994.amta-1.10,Integrating Translations from Multiple Sources within the {PANGLOSS} Mark {III} Machine Translation System,1994,-1,-1,6,0,39652,robert frederking,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.18,Integrating Knowledge Bases and Statistics in {MT},1994,18,48,1,1,10368,kevin knight,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,"We summarize recent machine translation (MT) research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps."
1994.amta-1.41,{PANGLOSS},1994,0,0,6,0,10837,jaime carbonell,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
H93-1036,Building a Large Ontology for Machine Translation,1993,12,35,1,1,10368,kevin knight,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"This paper describes efforts underway to construct a large-scale ontology to support semantic processing in the PAN-GLOSS knowledge-base machine translation system. Because we are aiming at broad semantic coverage, we are focusing on automatic and semi-automatic methods of knowledge acquisition. Here we report on algorithms for merging complementary online resources, in particular the LDOCE and WordNet dictionaries. We discuss empirical results, and how these results have been incorporated into the PANGLOSS ontology."
1991.mtsummit-papers.4,Capturing Language-Specific Semantic Distinctions in Interlingua-based {MT},1991,18,11,5,0,57271,james barnett,Proceedings of Machine Translation Summit III: Papers,0,"We describe an interlingua-based approach to machine translation, in which a DRS representation of the source text is used as the interlingua representation. A target DRS is then created and used to construct the target text. We describe several advantages of this level of representation. We also argue that problems of translation mismatch and divergence should properly be viewed not as translation problems per se but rather as generation problems, although the source text can be used to guide the target generator. The system we have built relics exclusively on monolingual linguistic descriptions that are also, for the most part, bi-directional."
