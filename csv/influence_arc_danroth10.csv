2020.acl-main.406,P11-1056,1,0.741829,". Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “T"
2020.acl-main.406,D13-1184,1,0.871723,"ce and the claim. In terms of creating the corresponding negative examples, we randomly replace either ARG 0 or ARG 1 with other sources or claims. Then we use those created examples to incrementally fine-tune our TE extraction model, which can lead to a better performance. 3.3 Constructing the Graph After extracting the provenance information, the last process is to construct the provenance graph. The first step thereof, is to link the same sources detected in the text with the same statement. Since the sources can be a url or a mention of an entity, we do wikification (Ratinov et al., 2011; Cheng and Roth, 2013) for the extracted sources. Specifically, to wikify a source mention, we first adapt a redirectbased wikification method (RedW) (Shnayderman et al., 2019), which is efficient and context free. Besides Wikipedia redirects, we also include the value of the attribute website as a candidate mention of the entity if it exists, for example nytimes.com for The New York Times. Then we compute the text similarity between the source mention and the other mentions that have already been linked, and eventually map the source mention to the entity in Wikipedia with a similarity score higher than a threshol"
2020.acl-main.406,H05-1045,0,0.150393,"imental Evaluation We evaluate (1) the solutions to infer the provenance graph, and (2) the effectiveness of the claim evidence graph on claim verification, which is adapted from the inferred provenance graph. For each goal, we first elaborate the experimental settings, and then describe the results and analysis 3 . 5.1 Claim Search and Source Extraction To evaluate the methods inferring the provenance, we focus on the performance of claim search and source extraction by looking at if the method can extract the sources accurately and exhaustively. DataSet In this experiment, we use MPQA 2.04 (Choi et al., 2005) as the corpus to train and test our models. The dataset consists of 535 documents that have been manually annotated with opinion related information including sources. For example, given a piece of text “... According to Malan, the dollarization process is irreversible ... ”, “Pedro Malan” is annotated that it has an opinion on “the dollarization process is irreversible”. Note that a single claim can be annotated with multiple sources including the writer of the text, and each source except the writer is a span of text in the given text. MPQA dataset is originally developed for identifying so"
2020.acl-main.406,E17-1044,0,0.0188096,"ng articles as sources is too coarse-grained for claim verification, and thus it is very likely to be biased. The evidence graph provides the models with evidence from more sources (All-Src) and sources that are more likely to be independent (Prov-Src), thus improves the performance. 6 Related Work To the best of our knowledge, our work is the first to formally define and propose a framework to infer the provenance graph of given claims made in natural language. One line of the related work includes identifying sources of opinions in opinion analysis (Choi et al., 2005) and quote attribution (Muzny et al., 2017; Pavllo et al., 2018), which is related to one of the components we use to infer the provenance graph. Earlier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earli"
2020.acl-main.406,Q17-1008,0,0.019617,"written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “The website Hoax Slayer said the message dates back to 2012 and has recently resurfaced ... it also noted Facebook has no plans to start charging users for normal access...” requires a cross-sentence relation extraction (Peng et al., 2017). Rather than tackling the problem as a sequential tagging problem, we model it as a textual entailment (TE) problem (Dagan et al., 2013). Similar to QA-SRL (He et al., 2015), TE-IE task formulation has the advantages of (1) easier annotation (2) being able to capture implicit statements and implicit sources which requires coreference resolution. TE Modeling We use the dataset (Choi et al., 2005) that contains a set of annotated articles. For each article, it annotates “who” has an opinion on “what”. Formally, given a corpus D, for each article d ∈ D, our training data comes in the form of pai"
2020.acl-main.406,N18-1202,0,0.0161118,"h for the given claim, we need to solve the three problems outlined in Section 2. Here, we propose a pipelined solution, and elaborate them one by one. 3.1 Searching for the Context As we described in Section 2, accurately locating the previous statements about the claim is a very challenging problem. Therefore, instead of directly searching for a possible previous statement, we search for related context, where the source are describing a statement related to the claim. Specifically, we rank sentences in the given corpus, by computing the cosine similarity to the given claim with their ELMo (Peters et al., 2018) representations. Then, we choose sentences that are most similar and fetch their context in a window size w, which means we consider w sentences before and after the returned sentence together as the context, from which we will extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tag"
2020.acl-main.406,N13-1092,0,0.0531826,"Missing"
2020.acl-main.406,P18-2058,0,0.0363513,"of positive and negative examples in the annotated data. 4420 For training, we use a loss function L combining both cross-entropy loss for binary prediction and the margin ranking loss to maximize the difference between positive and negative examples to finetune the language model. That is: L = λLcross + (1 − λ)Lpair (5) − where Lpair = L+ pair + Lpair , and λ is the parameter to trade off different objectives. Candidate Generation. The next question is how to generate source candidate list sc(cdi ) for cdi given T (cdi ). Here, we leverage an off-the-shelf semantic role labeling (SRL) tool (He et al., 2018) that can parse the sentences T (cdi ) to tell us “who did what to whom” in the appropriate sentences. We then take all “who”, i.e., the span of the text with tag ARG 0 detected as a candidate source of cdi . Even though only the “who” followed by a verb such as “say” or “claim” can be the source theoretically, we included all of them as candidates, and leave the identification made by our TE model. Note that here we only use SRL to generate candidate sources. Considering (1) the noisy relationship produced by SRL parser, (2) the crosssentence relationship between the source and the claim, and"
2020.acl-main.406,D15-1076,0,0.0427243,"Missing"
2020.acl-main.406,W09-2415,0,0.0269117,"Missing"
2020.acl-main.406,P14-1038,0,0.0121967,"d sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “The website Hoax Sl"
2020.acl-main.406,P11-1138,1,0.505964,"ve example of the source and the claim. In terms of creating the corresponding negative examples, we randomly replace either ARG 0 or ARG 1 with other sources or claims. Then we use those created examples to incrementally fine-tune our TE extraction model, which can lead to a better performance. 3.3 Constructing the Graph After extracting the provenance information, the last process is to construct the provenance graph. The first step thereof, is to link the same sources detected in the text with the same statement. Since the sources can be a url or a mention of an entity, we do wikification (Ratinov et al., 2011; Cheng and Roth, 2013) for the extracted sources. Specifically, to wikify a source mention, we first adapt a redirectbased wikification method (RedW) (Shnayderman et al., 2019), which is efficient and context free. Besides Wikipedia redirects, we also include the value of the attribute website as a candidate mention of the entity if it exists, for example nytimes.com for The New York Times. Then we compute the text similarity between the source mention and the other mentions that have already been linked, and eventually map the source mention to the entity in Wikipedia with a similarity score"
2020.acl-main.406,D12-1048,0,0.0241947,"ll extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be neces"
2020.acl-main.406,D12-1042,0,0.024584,"ntext, from which we will extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference r"
2020.acl-main.406,C18-1283,0,0.0251117,"the provenance graph. Earlier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earlier work. Another line of related work includes factchecking (Thorne et al., 2018; Thorne and Vlachos, 2018; Zhang et al., 2019) and claim verification (Popat et al., 2017, 2018). However, those works focus only on capturing discriminative linguistic features of misinformation, while we argue that determining the provenance of claims is essential for addressing the root of the problem, understanding claims and sources. 7 Conclusion and Future Work We introduce a formal definition and a computational framework for the provenance of a natural language claim given a corpus. We argue that this notion of provenance is essential if we are to understand how claims evolve over time, and what sources contri"
2020.acl-main.406,N18-1074,0,0.0918064,"Missing"
2020.acl-main.406,P19-1040,1,0.560778,"ier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earlier work. Another line of related work includes factchecking (Thorne et al., 2018; Thorne and Vlachos, 2018; Zhang et al., 2019) and claim verification (Popat et al., 2017, 2018). However, those works focus only on capturing discriminative linguistic features of misinformation, while we argue that determining the provenance of claims is essential for addressing the root of the problem, understanding claims and sources. 7 Conclusion and Future Work We introduce a formal definition and a computational framework for the provenance of a natural language claim given a corpus. We argue that this notion of provenance is essential if we are to understand how claims evolve over time, and what sources contributed to earlier vers"
2020.acl-main.506,M92-1003,0,0.198468,"Our surveys of NLP researchers reveals problematic trends (§4), emphasizing the need for increased scrutiny and clarity. We conclude by suggesting guidelines for better testing (§5), as well as providing a toolkit called HyBayes (cf. Footnote 1) tailored towards commonly used NLP metrics. We hope this work will encourage an improved understanding of statistical assessment methods and effective reporting practices with measures of uncertainty. 1.1 Related Work While there is an abundant discussion of significance testing in other fields, only a handful of NLP efforts address it. For instance, Chinchor (1992) defined the principles of using hypothesis testing in the context of NLP problems. Mostnotably, there are works studying various randomized tests (Koehn, 2004; Ojala and Garriga, 2010; Graham et al., 2014), or metric-specific tests (Evert, 2004). More recently, Dror et al. (2018) and Dror and Reichart (2018) provide a thorough review of 5716 3 https://www.aclweb.org/anthology/events/acl-2018/ frequentist tests. While an important step in better informing the community, it covers a subset of statistical tools. Our work complements this effort by pointing out alternative tests. With increasing"
2020.acl-main.506,N19-1423,0,0.0167433,"Missing"
2020.acl-main.506,P18-1128,0,0.575113,"NLP metrics. We hope this work will encourage an improved understanding of statistical assessment methods and effective reporting practices with measures of uncertainty. 1.1 Related Work While there is an abundant discussion of significance testing in other fields, only a handful of NLP efforts address it. For instance, Chinchor (1992) defined the principles of using hypothesis testing in the context of NLP problems. Mostnotably, there are works studying various randomized tests (Koehn, 2004; Ojala and Garriga, 2010; Graham et al., 2014), or metric-specific tests (Evert, 2004). More recently, Dror et al. (2018) and Dror and Reichart (2018) provide a thorough review of 5716 3 https://www.aclweb.org/anthology/events/acl-2018/ frequentist tests. While an important step in better informing the community, it covers a subset of statistical tools. Our work complements this effort by pointing out alternative tests. With increasing over-reliance on certain hypothesis testing techniques, there are growing troubling trends of misuse or misinterpretation of such techniques (Goodman, 2008; Demˇsar, 2008). Some communities, such as statistics and psychology, even have published guidelines and restrictions on the"
2020.acl-main.506,C04-1136,0,0.123774,"ilored towards commonly used NLP metrics. We hope this work will encourage an improved understanding of statistical assessment methods and effective reporting practices with measures of uncertainty. 1.1 Related Work While there is an abundant discussion of significance testing in other fields, only a handful of NLP efforts address it. For instance, Chinchor (1992) defined the principles of using hypothesis testing in the context of NLP problems. Mostnotably, there are works studying various randomized tests (Koehn, 2004; Ojala and Garriga, 2010; Graham et al., 2014), or metric-specific tests (Evert, 2004). More recently, Dror et al. (2018) and Dror and Reichart (2018) provide a thorough review of 5716 3 https://www.aclweb.org/anthology/events/acl-2018/ frequentist tests. While an important step in better informing the community, it covers a subset of statistical tools. Our work complements this effort by pointing out alternative tests. With increasing over-reliance on certain hypothesis testing techniques, there are growing troubling trends of misuse or misinterpretation of such techniques (Goodman, 2008; Demˇsar, 2008). Some communities, such as statistics and psychology, even have published"
2020.acl-main.506,W14-3333,0,0.177602,"ing a toolkit called HyBayes (cf. Footnote 1) tailored towards commonly used NLP metrics. We hope this work will encourage an improved understanding of statistical assessment methods and effective reporting practices with measures of uncertainty. 1.1 Related Work While there is an abundant discussion of significance testing in other fields, only a handful of NLP efforts address it. For instance, Chinchor (1992) defined the principles of using hypothesis testing in the context of NLP problems. Mostnotably, there are works studying various randomized tests (Koehn, 2004; Ojala and Garriga, 2010; Graham et al., 2014), or metric-specific tests (Evert, 2004). More recently, Dror et al. (2018) and Dror and Reichart (2018) provide a thorough review of 5716 3 https://www.aclweb.org/anthology/events/acl-2018/ frequentist tests. While an important step in better informing the community, it covers a subset of statistical tools. Our work complements this effort by pointing out alternative tests. With increasing over-reliance on certain hypothesis testing techniques, there are growing troubling trends of misuse or misinterpretation of such techniques (Goodman, 2008; Demˇsar, 2008). Some communities, such as statist"
2020.acl-main.506,W04-3250,0,0.424652,"better testing (§5), as well as providing a toolkit called HyBayes (cf. Footnote 1) tailored towards commonly used NLP metrics. We hope this work will encourage an improved understanding of statistical assessment methods and effective reporting practices with measures of uncertainty. 1.1 Related Work While there is an abundant discussion of significance testing in other fields, only a handful of NLP efforts address it. For instance, Chinchor (1992) defined the principles of using hypothesis testing in the context of NLP problems. Mostnotably, there are works studying various randomized tests (Koehn, 2004; Ojala and Garriga, 2010; Graham et al., 2014), or metric-specific tests (Evert, 2004). More recently, Dror et al. (2018) and Dror and Reichart (2018) provide a thorough review of 5716 3 https://www.aclweb.org/anthology/events/acl-2018/ frequentist tests. While an important step in better informing the community, it covers a subset of statistical tools. Our work complements this effort by pointing out alternative tests. With increasing over-reliance on certain hypothesis testing techniques, there are growing troubling trends of misuse or misinterpretation of such techniques (Goodman, 2008; De"
2020.acl-main.506,W05-0908,0,0.262833,"sychology, even have published guidelines and restrictions on the use of p-values (Trafimow and Marks, 2015; Wasserstein et al., 2016). In parallel, some authors have advocated for using alternate paradigms such as Bayesian evaluations (Kruschke, 2010). NLP is arguably an equally empirical field, yet with a rare discussion of proper practices of scientific testing, common pitfalls, and various alternatives. In particular, while limitations of p-values are heavily discussed in statistics and psychology, only a few NLP efforts approach them: over-estimation of significance by model-based tests (Riezler and Maxwell, 2005), lack of independence assumption in practice (Berg-Kirkpatrick et al., 2012), and sensitivity to the choice of the significance level (Søgaard et al., 2014). Our goal is to provide a unifying view of the pitfalls and best practices, and equip NLP researchers with Bayesian hypothesis assessment approaches as an important alternative tool in their toolkit. 2 Assessment of Hypotheses We often wish to draw qualitative inferences based on the outcome of experiments (for example, inferring the relative inherent performance of systems). To do so, we usually formulate a hypothesis that can be assesse"
2020.acl-main.506,W14-1601,0,0.143147,"have advocated for using alternate paradigms such as Bayesian evaluations (Kruschke, 2010). NLP is arguably an equally empirical field, yet with a rare discussion of proper practices of scientific testing, common pitfalls, and various alternatives. In particular, while limitations of p-values are heavily discussed in statistics and psychology, only a few NLP efforts approach them: over-estimation of significance by model-based tests (Riezler and Maxwell, 2005), lack of independence assumption in practice (Berg-Kirkpatrick et al., 2012), and sensitivity to the choice of the significance level (Søgaard et al., 2014). Our goal is to provide a unifying view of the pitfalls and best practices, and equip NLP researchers with Bayesian hypothesis assessment approaches as an important alternative tool in their toolkit. 2 Assessment of Hypotheses We often wish to draw qualitative inferences based on the outcome of experiments (for example, inferring the relative inherent performance of systems). To do so, we usually formulate a hypothesis that can be assessed through some analysis. Suppose we want to compare two systems on a dataset of instances x = [x1 , . . . , xn ] with respect to a measure M(S, x) representi"
2020.acl-main.506,N19-1270,0,\N,Missing
2020.acl-main.506,D12-1091,0,\N,Missing
2020.acl-main.678,N12-1049,0,0.0692817,"explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has been no report on the effectiveness of this method on temporal attributes. On the other hand, time has long been an important research area in NLP. Prior works have focused on the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Willi"
2020.acl-main.678,W11-0116,0,0.647422,"2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-day” and “more-than-on"
2020.acl-main.678,D18-1454,0,0.0226009,"anding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Zhang et al., 2017; Bauer et al., 2018; Tandon et al., 2018). This is an even more pronounced phenomenon when it comes to temporal common sense (TCS) (Zhou et al., 2019). In Example 1, human readers know that a typical vacation is likely to last at least a few days, and they would choose “will not” to fill in the blank for the first sentence; instead, with a slight change of context “vacation” → “walk outside,” people typically prefer “will” for the second one. Similarly, any system which correctly answers this example for the right reason would need to incorporate TCS in its reasoning. Example 1: choosing from “will” or “will not"
2020.acl-main.678,D19-1355,0,0.0156157,"y, we construct y as a one-hot vector where only the gold label has a value of 1, and the rest are zeroes. 3.5 Figure 4: The distributions of different temporal dimensions in the collected data. 3.4 with the external knowledge. Consider x as a system’s output logits across labels, and we express our soft loss function as follows: X `=− yi&gt; log(softmax(xi )), (1) Sequential Language Modeling Our goal is to build a model that is able to predict temporal labels (values) given events and dimensions. Instead of building a classification layer on top of a pre-trained model, we follow previous work (Huang et al., 2019) and place the label into the input sequence. We mask the label in the sequence and use the masked token prediction objective as the classification objective. To produce more general representations, we also keep the temporal label and mask the event tokens instead at a certain probability, so that we are 7583 able to maximize both P (Tmp-Label|Event) and P (Event|Tmp-Label) in the same learning process, where Tmp-Label refers to the temporal label associated with the event. Specifically, we use the reserved “unused” tokens in BERT-base model lexicon to construct a 1-to-1 mapping from every va"
2020.acl-main.678,P14-1135,0,0.0501244,"r co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has been no report on the effectiveness of this method on temporal attributes. On the other hand, time has long been an important research area in NLP. Prior works have focused on the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala"
2020.acl-main.678,Q14-1022,0,0.0808909,"end vacation 0.2 0.1 Duration 0 0.3 seconds hour walk 0.2 week weekend year vacation 0.1 Frequency 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but"
2020.acl-main.678,E17-1108,0,0.0699773,"end year vacation 0.1 Frequency 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and"
2020.acl-main.678,W04-3205,0,0.112689,"., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-day” and “more-than-one-day” annotations and provides the first baseline system for this dataset. Vempala et al. (2018) significantly improve"
2020.acl-main.678,D18-1155,0,0.163169,"ntaining more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has been no report on the effectiveness of this method on temporal attributes. On the other hand, time has long been an important research area in NLP. Prior works have focused on the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next a"
2020.acl-main.678,D18-1202,0,0.019037,"in §4. 3 The relationship can be more complex. E.g., “hours” is closer to “minutes” than “centuries” is; days of a week forms a circle: “Mon.” is followed by “Tue.” and preceded by “Sun.” 7580 2 Related Work Common sense has been a popular topic in recent years, and existing NLP works have mainly investigated the acquisition and evaluation of common sense reasoning in the physical world. These works include but are not limited to size, weight, and strength (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Elazar et al., 2019), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). A handful of these works uses cheap supervision. For example, Elazar et al. (2019) recently proposed a general framework that discovers distributions of quantitative attributes (e.g., length, mass, speed, and duration) from explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has been no report on the effectiveness"
2020.acl-main.678,N19-1423,0,0.0311819,"rence: duration (how long an event takes), frequency (how often an event occurs) and typical time (when an event typically happens).2 As a highlight, Fig. 1 shows the distributions (over time units) we predict for the duration and frequency of three events. We can see that “taking a vacation” lasts from days to months while “taking a walk” lasts from minutes to hours. As shown, our model is able to produce different and sensible distributions for the “take” event, depending on the context in which “take” occurs. Our work builds upon pre-trained contextual language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019). However, a standard language modeling objective does not lead to a model that handles the two challenges mentioned above; in addition, other systematic issues limit its ability to handle TCS. In particular, language models do 2 E.g., typical time in a day (the morning), typical day of a week (on Sunday), and typical time of a year (summer). not directly utilize the ordinal relationships among temporal units. For example, “hours” is longer than “minutes,” and “minutes” are longer than “seconds.”3 Fig. 2 shows that BERT does not produce a meaningful duration distribution for"
2020.acl-main.678,P19-1388,1,0.862019,"emporal signals, is in §3 . We conclude by showing intrinsic and extrinsic experiments in §4. 3 The relationship can be more complex. E.g., “hours” is closer to “minutes” than “centuries” is; days of a week forms a circle: “Mon.” is followed by “Tue.” and preceded by “Sun.” 7580 2 Related Work Common sense has been a popular topic in recent years, and existing NLP works have mainly investigated the acquisition and evaluation of common sense reasoning in the physical world. These works include but are not limited to size, weight, and strength (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Elazar et al., 2019), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). A handful of these works uses cheap supervision. For example, Elazar et al. (2019) recently proposed a general framework that discovers distributions of quantitative attributes (e.g., length, mass, speed, and duration) from explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they"
2020.acl-main.678,P17-1025,0,0.0352706,"jective function with temporal signals, is in §3 . We conclude by showing intrinsic and extrinsic experiments in §4. 3 The relationship can be more complex. E.g., “hours” is closer to “minutes” than “centuries” is; days of a week forms a circle: “Mon.” is followed by “Tue.” and preceded by “Sun.” 7580 2 Related Work Common sense has been a popular topic in recent years, and existing NLP works have mainly investigated the acquisition and evaluation of common sense reasoning in the physical world. These works include but are not limited to size, weight, and strength (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Elazar et al., 2019), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). A handful of these works uses cheap supervision. For example, Elazar et al. (2019) recently proposed a general framework that discovers distributions of quantitative attributes (e.g., length, mass, speed, and duration) from explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a"
2020.acl-main.678,glavas-etal-2014-hieve,0,0.418579,"Missing"
2020.acl-main.678,2021.ccl-1.108,0,0.138421,"Missing"
2020.acl-main.678,S15-2134,0,0.0809384,"ration 0 0.3 seconds hour walk 0.2 week weekend year vacation 0.1 Frequency 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight ra"
2020.acl-main.678,D17-1108,1,0.899061,"t al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has been no report on the effectiveness of this method on temporal attributes. On the other hand, time has long been an important research area in NLP. Prior works have focused on the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovs"
2020.acl-main.678,P18-1212,1,0.947371,"cy 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Z"
2020.acl-main.678,N18-1077,1,0.944324,"cy 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Z"
2020.acl-main.678,D18-2013,1,0.947495,"cy 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Z"
2020.acl-main.678,W06-0906,0,0.412906,"¨otgen and Gertz, 2010; Angeli et al., 2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-da"
2020.acl-main.678,K19-1051,1,0.846577,"the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-day” and “more-than-one-day” annotations and provides the first baseline system for this dataset. Vempala et al. (2018) significantly improve earlier work by using additional aspectual features for this task. Vashishtha et al. (2019) annotate the UDS-T dataset with event duration annotations and propos"
2020.acl-main.678,N18-1202,0,0.0507023,"imensions of TCS inference: duration (how long an event takes), frequency (how often an event occurs) and typical time (when an event typically happens).2 As a highlight, Fig. 1 shows the distributions (over time units) we predict for the duration and frequency of three events. We can see that “taking a vacation” lasts from days to months while “taking a walk” lasts from minutes to hours. As shown, our model is able to produce different and sensible distributions for the “take” event, depending on the context in which “take” occurs. Our work builds upon pre-trained contextual language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019). However, a standard language modeling objective does not lead to a model that handles the two challenges mentioned above; in addition, other systematic issues limit its ability to handle TCS. In particular, language models do 2 E.g., typical time in a day (the morning), typical day of a week (on Sunday), and typical time of a year (summer). not directly utilize the ordinal relationships among temporal units. For example, “hours” is longer than “minutes,” and “minutes” are longer than “seconds.”3 Fig. 2 shows that BERT does not produce a meaningful dura"
2020.acl-main.678,D19-1332,1,0.879546,"enging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Zhang et al., 2017; Bauer et al., 2018; Tandon et al., 2018). This is an even more pronounced phenomenon when it comes to temporal common sense (TCS) (Zhou et al., 2019). In Example 1, human readers know that a typical vacation is likely to last at least a few days, and they would choose “will not” to fill in the blank for the first sentence; instead, with a slight change of context “vacation” → “walk outside,” people typically prefer “will” for the second one. Similarly, any system which correctly answers this example for the right reason would need to incorporate TCS in its reasoning. Example 1: choosing from “will” or “will not” Dr. Porter is now (e1:taking) a vacation and to see you soon. Dr. Porter is now (e2:taking) a walk outside and able to see you so"
2020.acl-main.678,S10-1071,0,0.0941875,"Missing"
2020.acl-main.678,D18-1006,0,0.0413621,", understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002; Van Durme, 2009; Gordon and Van Durme, 2013; Zhang et al., 2017; Bauer et al., 2018; Tandon et al., 2018). This is an even more pronounced phenomenon when it comes to temporal common sense (TCS) (Zhou et al., 2019). In Example 1, human readers know that a typical vacation is likely to last at least a few days, and they would choose “will not” to fill in the blank for the first sentence; instead, with a slight change of context “vacation” → “walk outside,” people typically prefer “will” for the second one. Similarly, any system which correctly answers this example for the right reason would need to incorporate TCS in its reasoning. Example 1: choosing from “will” or “will not” Dr. Porter is now (e"
2020.acl-main.678,S13-2001,0,0.111682,"f temporal NLP. 1 weekend vacation 0.2 0.1 Duration 0 0.3 seconds hour walk 0.2 week weekend year vacation 0.1 Frequency 0 seconds hour week year Figure 1: Our model’s predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates. Introduction Time is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Bethard et al., 2016; Leeuwenberg and Moens, 2017; Ning et al., 2018b), rely on understanding time. However, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often 1 https://cogcomp.seas.upenn.edu/page/ publication_view/904 walk 0.3 affected by another type of reporting bias: people rarely say the obvious, in order to communicate"
2020.acl-main.678,P19-1280,0,0.15016,"Missing"
2020.acl-main.678,N18-2026,0,0.171361,"., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-day” and “more-than-one-day” annotations and provides the fi"
2020.acl-main.678,W12-3309,0,0.135028,"2012; Lee et al., 2014; Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017, 2018c; Vashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, M C TACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal.4 M C TACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed. There have been works on temporal common sense, such as event duration (Pan et al., 2006; Gusev et al., 2011; Williams, 2012; Vempala et al., 2018; Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016; Li et al., 2018; Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with 4 They additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely). “less-than-one-day” and “more-than-one-day” annotatio"
2020.acl-main.678,P18-2102,0,0.0795582,"intrinsic and extrinsic experiments in §4. 3 The relationship can be more complex. E.g., “hours” is closer to “minutes” than “centuries” is; days of a week forms a circle: “Mon.” is followed by “Tue.” and preceded by “Sun.” 7580 2 Related Work Common sense has been a popular topic in recent years, and existing NLP works have mainly investigated the acquisition and evaluation of common sense reasoning in the physical world. These works include but are not limited to size, weight, and strength (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Elazar et al., 2019), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). A handful of these works uses cheap supervision. For example, Elazar et al. (2019) recently proposed a general framework that discovers distributions of quantitative attributes (e.g., length, mass, speed, and duration) from explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., “taking a vacation” is very different from “taking a break,” although they share the same verb “take”). Besides, there has"
2020.acl-main.678,S16-1165,0,\N,Missing
2020.acl-main.678,Q17-1027,0,\N,Missing
2020.acl-main.772,C18-1139,0,0.0229501,"Manning (2018) for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in Zhou et al. (2016) is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe (Pennington et al., 2014)) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of Q UASE in B.2, and s-Q UASE combined with other SOTA embeddings (ELMo and Flair (Akbik et al., 2018)) in B.3. 3.1 Necessity of Two Representations We first consider a straightforward method to use QA data for other tasks—to further pre-train BERT on these QA data. We compare BERT further pre-trained on QAMR (denoted by BERTQAM R ) with BERT on two single-sentence tasks (SRL and RE) and two paired-sentence tasks (TE and MRC). We use a feature-based approach for singlesentence tasks and a fine-tuning approach for paired-sentence tasks. The reason is two-fold. On the one hand, current SOTAs of all singlesentence tasks considered in this paper are still 8747 6 For TE, we mean matched examples in"
2020.acl-main.772,P07-1036,1,0.756359,"Missing"
2020.acl-main.772,N19-1423,0,0.0535724,"on T (a smaller) (Talmor and Berant, 2019; Sun et al., 2019). However, it remains unclear how to use these QA data when the target task does not share the same model as the QA task, which is often the case when the target task is not QA. For instance, QA-SRL (He et al., 2015), which uses QA pairs to represent those predicateargument structures in SRL, should be intuitively helpful for SRL parsing, but the significant difference in their surface forms prevents us from using the same model in both tasks. The success of modern language modeling techniques, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and many others, has pointed out an alternative solution to this problem. That is, to further pre-train2 a neural language model (LM) on these QA data in certain ways, obtain a sentence encoder, and use the sentence encoder for the target task, either by fine-tuning or as additional feature vectors. We call this general framework question-answer driven sentence encoding (Q UASE). A straightforward implementation of Q UASE is to first further pre-train BERT (or other LMs) on the QA data in the standard way, as if this QA task is the target, and then fine-tune it on the real target task. This"
2020.acl-main.772,P18-2077,0,0.01932,"and SQuAD 1.0 (Rajpurkar et al., 2016) (MRC). In Table 4, we use CoNLL’12 English subset of OntoNotes 5.0 (Pradhan et al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development set.6 For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for Coref. We also use the biaffine network in Dozat and Manning (2018) for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in Zhou et al. (2016) is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe (Pennington et al., 2014)) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of Q UASE in B.2, and s-Q UASE combined with other SOTA embeddings (ELMo and Flair (Akbik et al., 2"
2020.acl-main.772,P18-1191,0,0.0504668,"can improve downstream tasks in the low-resource setting (i.e. several thousands direct training examples). That is because they help the scalability of machine learning methods, especially for some specific domains or some low-resource languages where direct training data do not exist in large scale. 4 Discussion In this section we discuss a few issues pertaining to improving Q UASE by using additional QA datasets and the comparison of Q UASE with related symbolic representations. 4.1 Further Pre-training Q UASE on Multiple QA Datasets We investigate whether adding the Large QA-SRL dataset (FitzGerald et al., 2018) or the QA-RE9 dataset into QAMR in the further pre-training stage can help SRL and RE. We use s-Q UASE embeddings to replace BERT embeddings instead of concatenating the two embeddings. The effectiveness of adding existing resources (Large QA-SRL or QA-RE) into QAMR in the further pre-training stage of s-Q UASE on SRL and RE are shown in Table 5. We find that adding related QA signals (Large QA-SRL for SRL and QA-RE for RE) into QAMR can help improve specific tasks. Noteworthy is the fact that QA-RE can help SRL (Large QA-SRL can also help RE), though the improvement is minor compared to Larg"
2020.acl-main.772,P17-1147,0,0.072065,"Missing"
2020.acl-main.772,kingsbury-palmer-2002-treebank,0,0.338957,"compared using cosine-similarity, which reduces the computational cost of finding the most similar pairs. In contrast, Q UASE provides a better sentence encoder in the same format as BERT (a sequence of word embeddings) to better support tasks that require complex semantics. 3 Applications of Q UASE In this section, we conduct thorough experiments to show that Q UASE is a good framework to get supervision from QA data for other tasks. We first give an overview of the datasets and models used in these experiments before diving into the details of each experiment. Specifically, we use PropBank (Kingsbury and Palmer, 2002) (SRL), the dataset from the SemEval’15 shared task (Oepen et al., 2015) with DELPH-IN MRS-Derived Semantic Dependencies target representation (SDP), CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) (NER), the dataset in SemEval’10 Task 8 (Hendrickx et al., 2009) (RE), the dataset in the CoNLL’12 shared task (Pradhan et al., 2012) (Coref), MNLI (Williams et al., 2018) (TE), and SQuAD 1.0 (Rajpurkar et al., 2016) (MRC). In Table 4, we use CoNLL’12 English subset of OntoNotes 5.0 (Pradhan et al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development se"
2020.acl-main.772,D17-1018,0,0.0170998,"Pradhan et al., 2012) (Coref), MNLI (Williams et al., 2018) (TE), and SQuAD 1.0 (Rajpurkar et al., 2016) (MRC). In Table 4, we use CoNLL’12 English subset of OntoNotes 5.0 (Pradhan et al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development set.6 For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for Coref. We also use the biaffine network in Dozat and Manning (2018) for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in Zhou et al. (2016) is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe (Pennington et al., 2014)) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of Q UASE in B.2, and s-Q U"
2020.acl-main.772,K17-1034,0,0.0812485,"Missing"
2020.acl-main.772,W15-2205,0,0.0183793,"mantics related to AMR than BERT embeddings. Specifically, we use the same edge probing model as Tenney et al. (2019), and find that the probing accuracy (73.59) of s-Q UASEQAM R embeddings is higher than that (71.58) of BERT. At the same time, we find that p-Q UASEQAM R can achieve 76.91 F1 on the PTB set of QA-SRL, indicating that p-Q UASEQAM R can capture enough information related to SRL to have a good zero-shot SRL performance. More details can be found in Appendix C.1. Another fact worth noting is that AMR can be used to improve downstream tasks, such as MRC (Sachan and Xing, 2016), TE (Lien and Kouylekov, 2015), RE (Garg et al., 2019) and SRL (Song et al., 2018). The benefits of Q UASEQAM R on downstream tasks show that we can take advantage of AMR by learning from much cheaper QA signals dedicated to it. 8750 4.3 Difficulties in Learning Symbolic Representations from QA Signals Q UASE is designed to learn distributed representations from QA signals to help down-stream tasks. We further show the difficulties of learning two types of corresponding symbolic representations from QA signals, which indicates that the two other possible methods are not as tractable as ours. One option of symbolic represen"
2020.acl-main.772,2021.ccl-1.108,0,0.109222,"Missing"
2020.acl-main.772,D17-1159,0,0.0356413,"ults indicate that learning a QAMR parser for down-stream tasks is mainly hindered by question generation, and how to use the full information of QAMR for downstream tasks is still unclear. Another choice of symbolic representation is AMR, since QAMR is proposed to replace AMR. We consider a simpler setting, learning an SRL parser from Large QA-SRL. We propose three models in different perspectives, but the best performance of them is only 54.10 F1, even with fuzzy matching (Intersection/Union ≥ 0.5). More details can be found in Appendix C.2. Although a lot of methods (Khashabi et al., 2018; Marcheggiani and Titov, 2017; Strubell et al., 2018) can be adopted to use SRL/AMR in downstream tasks, the difficulty of learning a good SRL/AMR parser from QA signals hinders this direction. The difficulties of learning the two types of symbolic representations from QA signals indicate that our proposal of learning distributed representations from QA signals is a better way of making use of the latent semantic information in QA pairs for down-stream tasks. 5 Conclusion In this paper, we investigate an important problem in NLP: Can we make use of low-cost signals, such as QA data, to help related tasks? We retrieve sign"
2020.acl-main.772,D14-1162,0,0.0834203,"asks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for Coref. We also use the biaffine network in Dozat and Manning (2018) for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in Zhou et al. (2016) is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe (Pennington et al., 2014)) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of Q UASE in B.2, and s-Q UASE combined with other SOTA embeddings (ELMo and Flair (Akbik et al., 2018)) in B.3. 3.1 Necessity of Two Representations We first consider a straightforward method to use QA data for other tasks—to further pre-train BERT on these QA data. We compare BERT further pre-trained on QAMR (denoted by BERTQAM R ) with BERT on t"
2020.acl-main.772,N18-1202,0,0.697768,"ten larger) before training on T (a smaller) (Talmor and Berant, 2019; Sun et al., 2019). However, it remains unclear how to use these QA data when the target task does not share the same model as the QA task, which is often the case when the target task is not QA. For instance, QA-SRL (He et al., 2015), which uses QA pairs to represent those predicateargument structures in SRL, should be intuitively helpful for SRL parsing, but the significant difference in their surface forms prevents us from using the same model in both tasks. The success of modern language modeling techniques, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and many others, has pointed out an alternative solution to this problem. That is, to further pre-train2 a neural language model (LM) on these QA data in certain ways, obtain a sentence encoder, and use the sentence encoder for the target task, either by fine-tuning or as additional feature vectors. We call this general framework question-answer driven sentence encoding (Q UASE). A straightforward implementation of Q UASE is to first further pre-train BERT (or other LMs) on the QA data in the standard way, as if this QA task is the target, and then fine-tune it on"
2020.acl-main.772,W12-4501,0,0.0126621,"h experiments to show that Q UASE is a good framework to get supervision from QA data for other tasks. We first give an overview of the datasets and models used in these experiments before diving into the details of each experiment. Specifically, we use PropBank (Kingsbury and Palmer, 2002) (SRL), the dataset from the SemEval’15 shared task (Oepen et al., 2015) with DELPH-IN MRS-Derived Semantic Dependencies target representation (SDP), CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) (NER), the dataset in SemEval’10 Task 8 (Hendrickx et al., 2009) (RE), the dataset in the CoNLL’12 shared task (Pradhan et al., 2012) (Coref), MNLI (Williams et al., 2018) (TE), and SQuAD 1.0 (Rajpurkar et al., 2016) (MRC). In Table 4, we use CoNLL’12 English subset of OntoNotes 5.0 (Pradhan et al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development set.6 For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for"
2020.acl-main.772,P19-1439,0,0.0987565,"aining of sentence encoders on unlabeled text; further pre-training refers to continuing training the sentence encoders on an intermediate, non target-task-specific labeled data (e.g. QA data); fine-tuning refers to training on the target task in the fine-tuning approach. 8743 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8743–8758 c July 5 - 10, 2020. 2020 Association for Computational Linguistics STILTS is mainly further pre-trained on textual entailment (TE) data. However, similar to the observations made in STILTS and their follow-up works (Wang et al., 2019), we find that additional QA data does not necessarily help the target task using the implementation above. While it is unclear how to predict this behaviour , we do find that this happens a lot for tasks whose input is a single sentence, e.g., SRL and named entity recognition (NER), instead of a sentence pair, e.g., TE. This might be because QA is itself a paired-sentence task, and the implementation above (i.e., to further pre-train BERT on QA data) may learn certain attention patterns that can transfer to another paired-sentence task more easily than to a single-sentence task. Therefore, we"
2020.acl-main.772,W18-5446,0,0.0522044,"Missing"
2020.acl-main.772,D16-1177,0,0.0521934,"Missing"
2020.acl-main.772,P19-1485,0,0.0184049,"code rich information that ∗ Part of this work was done while the author was at the University of Illinois at Urbana-Champaign. 1 Our code and online demo are publicly available at https://github.com/CogComp/QuASE. is useful for other tasks; (3) it is much easier to answer questions relative to a sentence than to annotate linguistics phenomena in it, making this a plausible supervision signal (Roth, 2017). There has been work showing that QA data for task A can help another QA task T , conceptually by further pre-training the same model on A (an often larger) before training on T (a smaller) (Talmor and Berant, 2019; Sun et al., 2019). However, it remains unclear how to use these QA data when the target task does not share the same model as the QA task, which is often the case when the target task is not QA. For instance, QA-SRL (He et al., 2015), which uses QA pairs to represent those predicateargument structures in SRL, should be intuitively helpful for SRL parsing, but the significant difference in their surface forms prevents us from using the same model in both tasks. The success of modern language modeling techniques, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and many others, ha"
2020.acl-main.772,N18-1101,0,0.0235781,"a good framework to get supervision from QA data for other tasks. We first give an overview of the datasets and models used in these experiments before diving into the details of each experiment. Specifically, we use PropBank (Kingsbury and Palmer, 2002) (SRL), the dataset from the SemEval’15 shared task (Oepen et al., 2015) with DELPH-IN MRS-Derived Semantic Dependencies target representation (SDP), CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) (NER), the dataset in SemEval’10 Task 8 (Hendrickx et al., 2009) (RE), the dataset in the CoNLL’12 shared task (Pradhan et al., 2012) (Coref), MNLI (Williams et al., 2018) (TE), and SQuAD 1.0 (Rajpurkar et al., 2016) (MRC). In Table 4, we use CoNLL’12 English subset of OntoNotes 5.0 (Pradhan et al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development set.6 For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for Coref. We also use the biaffine netwo"
2020.acl-main.772,W03-0419,0,0.127204,"Missing"
2020.acl-main.772,P16-2034,0,0.0334197,"al., 2013), which is larger than PropBank. The performance of TE and MRC is evaluated on the development set.6 For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in He et al. (2017) for SRL, the model in Peters et al. (2018) for NER, and the end-to-end neural model in Lee et al. (2017) for Coref. We also use the biaffine network in Dozat and Manning (2018) for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in Zhou et al. (2016) is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe (Pennington et al., 2014)) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of Q UASE in B.2, and s-Q UASE combined with other SOTA embeddings (ELMo and Flair (Akbik et al., 2018)) in B.3. 3.1 Necessity of Two Representations We first consider a straightforward method to use QA data for"
2020.acl-main.772,S14-2008,0,\N,Missing
2020.acl-main.772,W13-3516,0,\N,Missing
2020.acl-main.772,Q15-1034,0,\N,Missing
2020.acl-main.772,D15-1076,0,\N,Missing
2020.acl-main.772,P16-2079,0,\N,Missing
2020.acl-main.772,W17-2623,0,\N,Missing
2020.acl-main.772,P17-1044,0,\N,Missing
2020.acl-main.772,W18-2501,0,\N,Missing
2020.acl-main.772,N18-2089,0,\N,Missing
2020.acl-main.772,N19-1270,0,\N,Missing
2020.acl-tutorials.7,N19-1112,0,0.133488,"ecent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonsense reasoning using downstream tasks has yielded increased efforts at developing robust benchmarks"
2020.acl-tutorials.7,N18-1202,0,0.0350715,"(e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to"
2020.acl-tutorials.7,D19-1250,0,0.0459438,"Missing"
2020.acl-tutorials.7,N19-1421,0,0.249254,"tanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging,"
2020.acl-tutorials.7,P19-1487,0,0.0181475,"018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman, 2015) and physical commonsense (Hayes, 1978; McRae et al., 2005). We will cover the differences between taxonomic and inferential knowledge (Davis and Marcus, 2015; Pearl and Mackenzie, 2018), and differentiate commonsense knowledge from related concepts (e.g., script learning; Schank and Abelson, 1975; Chambers and Jurafsky, 2008). How to represent commonsense? We will review e"
2020.acl-tutorials.7,D18-1006,1,0.844121,"the 58th Annual Meeting of the Association for Computational Linguistics, pages 27–33 c July 5, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 (Guan et al., 2018), dialogue (Zhou et al., 2018), QA (Mihaylov and Frank, 2018; Bauer et al., 2018; Lin et al., 2019; Weissenborn et al., 2017; Musa et al., 2019), and classification (Chen et al., 2018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman,"
2020.acl-tutorials.7,P18-1213,1,0.900176,"Missing"
2020.acl-tutorials.7,P19-1452,0,0.0164363,"adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense"
2020.acl-tutorials.7,P18-1043,1,0.87151,"Missing"
2020.acl-tutorials.7,S12-1052,0,0.0813807,"Missing"
2020.acl-tutorials.7,D19-1454,1,0.92721,"seamlessly (Apperly, 2010). Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades (Gunning, 2018). Commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it rem"
2020.acl-tutorials.7,K17-1004,1,0.795345,"lection and representation (e.g., automatic extraction; Etzioni et al., 2008; Zhang et al., 2016; Elazar et al., 2019). We will cover recent approaches that use natural language to represent commonsense (Speer et al., 2017; Sap et al., 2019a), and while noting the challenges that come with using datadriven methods (Gordon and Van Durme, 2013; Jastrzebski et al., 2018). How to measure machines’ ability of commonsense reasoning? We will explain that, despite their design, many natural language understanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabil"
2020.acl-tutorials.7,P19-1417,0,0.019746,"d reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonse"
2020.acl-tutorials.7,Q19-1027,1,0.836638,"onsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to integrate such knowledge into NLP tasks. Vario"
2020.acl-tutorials.7,P19-1472,1,0.846992,"ardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreemen"
2020.acl-tutorials.7,2020.emnlp-main.373,1,0.893321,"Missing"
2020.acl-tutorials.7,D19-1332,1,0.837743,"ed efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level sema"
2020.coling-main.10,P98-1013,0,0.359887,". We follow proposals from entity salience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pichotta and Mooney (2014), and others incorporated arguments such as propositional objects. However, most of the 115 (c) CLASSIFICATION (b) EVENT EMBEDDING (a) EVENT EXTRACTION Salience Scores Inter Event Attention 0.6 K1 0.8 Q1 fled Event Embedding Argument Embedding K2 0.3 Q2 K3 Q3 K4 killed shooting Arg0 BiLSTM 0.3 Trigger BiLSTM Arg1 BiLSTM 0.7 K4 Q4 wounded Arg-TMP BiLSTM Q4 confessed Arg-LOC BiLSTM Global Document Features Event Document Token Embedding Arg0 Arg1 Arg-TMP Event Argument Extraction Kuwait’s Interior Ministry says young Kuwa"
2020.coling-main.10,D13-1178,0,0.237465,"ience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pichotta and Mooney (2014), and others incorporated arguments such as propositional objects. However, most of the 115 (c) CLASSIFICATION (b) EVENT EMBEDDING (a) EVENT EXTRACTION Salience Scores Inter Event Attention 0.6 K1 0.8 Q1 fled Event Embedding Argument Embedding K2 0.3 Q2 K3 Q3 K4 killed shooting Arg0 BiLSTM 0.3 Trigger BiLSTM Arg1 BiLSTM 0.7 K4 Q4 wounded Arg-TMP BiLSTM Q4 confessed Arg-LOC BiLSTM Global Document Features Event Document Token Embedding Arg0 Arg1 Arg-TMP Event Argument Extraction Kuwait’s Interior Ministry says young Kuwaiti man who fled to Saudi Arabia after terroris"
2020.coling-main.10,P08-1090,0,0.274484,"tions and those mentions are spread throughout the document. However, we believe that in realistic documents, redundancy is commonly used for various rhetorical or other reasons, it is not necessarily the case that frequently mentioned events convey the main point of the article. We follow proposals from entity salience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pichotta and Mooney (2014), and others incorporated arguments such as propositional objects. However, most of the 115 (c) CLASSIFICATION (b) EVENT EMBEDDING (a) EVENT EXTRACTION Salience Scores Inter Event Attention 0.6 K1 0.8 Q1 fled Event Embedding Argument Embedding K2 0.3 Q2 K"
2020.coling-main.10,N18-2055,0,0.447598,"sses flaws in previous evaluation methodologies. Finally, we discuss the importance of salient event detection for the downstream task of summarization.1 1 Introduction Identifying the salient information in a given piece of text is a ubiquitous and important problem in natural language understanding. While important parts of the text have been identified by attending to entities (Dunietz and Gillick, 2014), elementary discourse units (Xu et al., 2020), or whole sentences (Zhou et al., 2018; Liu and Lapata, 2019), in this work, we choose to model extracting important events (Liu et al., 2018; Choubey et al., 2018). Events are the core parts of most sentences – they center around a predicate and include its key arguments – yet they are compact semantic units, and a salient event in a sentence could carry the sentence’s meaning efficiently. Extracting important events has been shown to be central to many downstream tasks, such as summarization (Marujo, 2015), storyline creation (Martin et al., 2018) and question answering (Kocisk´y et al., 2018). To model the importance of an event, it is critical to understand its context: who is involved, where did it happen, what other events is it related to, and mor"
2020.coling-main.10,P11-1144,0,0.0118549,"tively more salient. We use the model from Wang et al. (2020) to identify the parent-child relationship between every event pair. An event is called a child of another event if it is a subevent of the parent (e.g., “shooting” may be a subevent/child of an “attack” event). The Parent Score of an event is defined as the number of child events it has. 2. Frame Name: Framename provides a more abstract understanding of the event compared to the event trigger. All event triggers (9716 in total) in the Event Salience corpus (Liu et al., 2018) belong to a total of 569 frames (annotated using Semafor (Das and Smith, 2011)). For instance, Figure 3 (right) shows all event triggers under the frame “Killing.” As can be seen from the figure, the frequency of all events within a frame is very different, whereas their salience in the text is usually the same. Therefore, this feature enables events with low frequency to leverage the understanding from more frequent events under the same frame. 3. Sentence Location: One of the most commonly used (Dunietz and Gillick, 2014; Liu et al., 2018) features for salience related tasks. It represents the first location of sentence containing this event. 4. Event Trigger Frequenc"
2020.coling-main.10,P85-1039,0,0.743065,"n the classification module, all events attend to/vote each other and the salience score of an event is calculated by accumulating the votes from all events. work on event salience identification has represented events as verbs/nominal event mentions. To address this, we use a more holistic event representation which includes nominal/verbal event mentions, entities in the subject and object of the predicate as well as the time and location. A similar representation was used by Peng et al. (2016) to build an event detection and co-reference system. From modeling perspective, most earlier work (Decker, 1985; Kay and Aylett, 1996) on event salience has built rule based systems (e.g. presence of the event in the main clause, its voice, etc.). More recent work has focused on capturing coreference relation between events (Choubey et al., 2018) and on automatically capturing salient specific interactions between the discourse units (Liu et al., 2018). However, as mentioned earlier, we believe that events are highly contextual, so we use a more expressive model to obtain contextualized embeddings for events. It additionally helps us in capturing local interevent interactions and, to capture the docume"
2020.coling-main.10,N19-1423,0,0.0204127,"a 5-tuple ei = (mi , si , oi , ti , `i ) where each item in the tuple 116 corresponds to the contiguous span of text for the event mention, subject, object, time, and location, respectively. In practice, the arguments for each event correspond to the A RG 0, A RG 1, A RG M-LOC, and A RG M-TMP arguments output from verbal and nominal semantic role labeling systems (He et al., 2017; Khashabi et al., 2018). After the arguments for each event have been extracted, they are combined together to form a vector representation for the event, denoted ei , as follows. We first obtain the BERT embedding (Devlin et al., 2019) for every token in the input document. Then, since each of the items of the event tuple is a contiguous span of text, we create a fixed-size representation for each argument by encoding the corresponding tokens using a bidirectional LSTM. There is a separate LSTM encoder for each argument type. Finally, the vectors for all of the arguments are concatenated together to form the event encoding, ei = [mi ; si ; oi ; ti ; `i ]. 3.2 Event Augmentation with Global Features Although BERT embeddings have proven to be highly beneficial for a large number of tasks, they may not encode all of the inform"
2020.coling-main.10,D11-1027,1,0.780025,"s rhetorical or other reasons, it is not necessarily the case that frequently mentioned events convey the main point of the article. We follow proposals from entity salience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pichotta and Mooney (2014), and others incorporated arguments such as propositional objects. However, most of the 115 (c) CLASSIFICATION (b) EVENT EMBEDDING (a) EVENT EXTRACTION Salience Scores Inter Event Attention 0.6 K1 0.8 Q1 fled Event Embedding Argument Embedding K2 0.3 Q2 K3 Q3 K4 killed shooting Arg0 BiLSTM 0.3 Trigger BiLSTM Arg1 BiLSTM 0.7 K4 Q4 wounded Arg-TMP BiLSTM Q4 confessed Arg-LOC BiLSTM Global D"
2020.coling-main.10,E14-4040,0,0.182334,"ing some satellites, the text can still be understood. Upadhyay et al. (2016) discusses identifying events that would have triggered the author to write that article. Choubey et al. (2018) proposed the idea that the central events have a large number of coreferential event mentions and those mentions are spread throughout the document. However, we believe that in realistic documents, redundancy is commonly used for various rhetorical or other reasons, it is not necessarily the case that frequently mentioned events convey the main point of the article. We follow proposals from entity salience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pi"
2020.coling-main.10,P17-1044,0,0.0178769,"hether a “snow” event is important by itself, but knowing that the event took place in during the summer increases the rarity of the event, potentially elevating its importance in the document. Subsequently, we define an event to be a 5-tuple ei = (mi , si , oi , ti , `i ) where each item in the tuple 116 corresponds to the contiguous span of text for the event mention, subject, object, time, and location, respectively. In practice, the arguments for each event correspond to the A RG 0, A RG 1, A RG M-LOC, and A RG M-TMP arguments output from verbal and nominal semantic role labeling systems (He et al., 2017; Khashabi et al., 2018). After the arguments for each event have been extracted, they are combined together to form a vector representation for the event, denoted ei , as follows. We first obtain the BERT embedding (Devlin et al., 2019) for every token in the input document. Then, since each of the items of the event tuple is a contiguous span of text, we create a fixed-size representation for each argument by encoding the corresponding tokens using a bidirectional LSTM. There is a separate LSTM encoder for each argument type. Finally, the vectors for all of the arguments are concatenated tog"
2020.coling-main.10,P96-1054,0,0.29052,"cation module, all events attend to/vote each other and the salience score of an event is calculated by accumulating the votes from all events. work on event salience identification has represented events as verbs/nominal event mentions. To address this, we use a more holistic event representation which includes nominal/verbal event mentions, entities in the subject and object of the predicate as well as the time and location. A similar representation was used by Peng et al. (2016) to build an event detection and co-reference system. From modeling perspective, most earlier work (Decker, 1985; Kay and Aylett, 1996) on event salience has built rule based systems (e.g. presence of the event in the main clause, its voice, etc.). More recent work has focused on capturing coreference relation between events (Choubey et al., 2018) and on automatically capturing salient specific interactions between the discourse units (Liu et al., 2018). However, as mentioned earlier, we believe that events are highly contextual, so we use a more expressive model to obtain contextualized embeddings for events. It additionally helps us in capturing local interevent interactions and, to capture the document level interactions,"
2020.coling-main.10,Q18-1023,0,0.069444,"Missing"
2020.coling-main.10,D19-1387,0,0.0475758,"Missing"
2020.coling-main.10,D18-1154,0,0.622134,"ization (Marujo, 2015), storyline creation (Martin et al., 2018) and question answering (Kocisk´y et al., 2018). To model the importance of an event, it is critical to understand its context: who is involved, where did it happen, what other events is it related to, and more. For instance, in Figure 1, the difference in salience of the “fled” events in each document is significantly influenced by their arguments (“2,100 Colombians” versus “shopkeepers”). Previous work that identifies salient events has a limited event representation that is unable to capture these important contextual signals (Liu et al., 2018; Choubey et al., 2018). In contrast, the model which we propose in this work (§3) directly models the context of an event in three different ways as follows. First, instead of representing an event by only its predicate mention, our representation includes the subject, object, time, and location of the event (§3.1). Second, we directly incorporate global features into the model that capture hierarchical relations between events, abstract event frames, position information, and more (§3.2). Third, we use a neural network architecture that includes an inter-event interaction layer, which allows"
2020.coling-main.10,D16-1038,1,0.830043,"embeddings of all constituents from the document encoded using BERT, and compose an event level embedding. Finally, in the classification module, all events attend to/vote each other and the salience score of an event is calculated by accumulating the votes from all events. work on event salience identification has represented events as verbs/nominal event mentions. To address this, we use a more holistic event representation which includes nominal/verbal event mentions, entities in the subject and object of the predicate as well as the time and location. A similar representation was used by Peng et al. (2016) to build an event detection and co-reference system. From modeling perspective, most earlier work (Decker, 1985; Kay and Aylett, 1996) on event salience has built rule based systems (e.g. presence of the event in the main clause, its voice, etc.). More recent work has focused on capturing coreference relation between events (Choubey et al., 2018) and on automatically capturing salient specific interactions between the discourse units (Liu et al., 2018). However, as mentioned earlier, we believe that events are highly contextual, so we use a more expressive model to obtain contextualized embed"
2020.coling-main.10,E14-1024,0,0.0191975,"4) and event salience work (Liu et al., 2018) that suggest that these are difficult to explicitly define, but can be learned from observing human summaries: events that appear in the summary are salient. Event representation has also evolved over time. Chambers and Jurafsky (2008) represented narrative events as pairs of verb and the grammatical dependency relation between the verb and the entity. Do et al. (2011) included nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). Further work by Balasubramanian et al. (2013), Pichotta and Mooney (2014), and others incorporated arguments such as propositional objects. However, most of the 115 (c) CLASSIFICATION (b) EVENT EMBEDDING (a) EVENT EXTRACTION Salience Scores Inter Event Attention 0.6 K1 0.8 Q1 fled Event Embedding Argument Embedding K2 0.3 Q2 K3 Q3 K4 killed shooting Arg0 BiLSTM 0.3 Trigger BiLSTM Arg1 BiLSTM 0.7 K4 Q4 wounded Arg-TMP BiLSTM Q4 confessed Arg-LOC BiLSTM Global Document Features Event Document Token Embedding Arg0 Arg1 Arg-TMP Event Argument Extraction Kuwait’s Interior Ministry says young Kuwaiti man who fled to Saudi Arabia after terrorist shooting in Kuwait that ki"
2020.coling-main.10,W16-1001,1,0.854334,"ovide a sensible and more interpretable metric for evaluating extracted salient events; (3) We demonstrate that extractive summarization systems could potentially gain from modeling important events. 2 Related Work Important Information Identification has been a topic of interest in NLP community since the 1980s and through these years, researchers have defined salience in multiple ways. Mann and Thompson (1988) divides the text into nuclei and satellite with an idea that a satellite is incomprehensible without the nucleus but, after removing some satellites, the text can still be understood. Upadhyay et al. (2016) discusses identifying events that would have triggered the author to write that article. Choubey et al. (2018) proposed the idea that the central events have a large number of coreferential event mentions and those mentions are spread throughout the document. However, we believe that in realistic documents, redundancy is commonly used for various rhetorical or other reasons, it is not necessarily the case that frequently mentioned events convey the main point of the article. We follow proposals from entity salience (Dunietz and Gillick, 2014) and event salience work (Liu et al., 2018) that su"
2020.coling-main.10,2020.emnlp-main.51,1,0.690724,"ong-range dependencies. Therefore, we augment the event representation from the previous section with features that leverage the event structure, document-level statistics, event-event relations, and event abstractions. Refer to Table 1(b) for a summary of the statistics of these features on the New York Times (NYT) Annotated Corpus (Sandhaus, 2008). We extract the following features: 1. Parent Score: This feature leverages the hierarchical relations between events in a document. The intuition is that the higher level and more abstract events are relatively more salient. We use the model from Wang et al. (2020) to identify the parent-child relationship between every event pair. An event is called a child of another event if it is a subevent of the parent (e.g., “shooting” may be a subevent/child of an “attack” event). The Parent Score of an event is defined as the number of child events it has. 2. Frame Name: Framename provides a more abstract understanding of the event compared to the event trigger. All event triggers (9716 in total) in the Event Salience corpus (Liu et al., 2018) belong to a total of 569 frames (annotated using Semafor (Das and Smith, 2011)). For instance, Figure 3 (right) shows a"
2020.coling-main.10,2020.acl-main.451,0,0.0844716,"Missing"
2020.coling-main.10,P18-1061,0,0.0486823,"Missing"
2020.coling-main.274,J12-4003,0,0.0254671,"ing the intuitive nature of QA-SRL, Fitzgerald et al. (2018) crowdsourced a large scale QA-SRL corpus via Amazon Mechanical Turk, and released the first QA-SRL parser. QA-SRL is appealing not just for its scalability, but also for its content. By relying on natural comprehension of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 202"
2020.coling-main.274,N03-1013,0,0.0817758,"ks — detect predicates in text, extract their arguments, and label each with a semantic role. While verb predicates are relatively easy to detect, requiring no more than a POS tagger, this phase is not trivial for nominal SRL. For QANom, the predicate detection task is coupled with finding a corresponding verb whose meaning is aligned to the nominal predicate. In this way, the QA-SRL questions centered by this verb could naturally capture arguments of the noun. Thus, our data collection setting involves leveraging lexical resources to find candidate nouns having a related verb. We use CatVar (Habash and Dorr, 2003), WordNet (Miller, 1995) and an in-house suffix-driven heuristic to identify those noun candidates, along with their corresponding verb. More details about our lexical candidate extraction procedure are in Appendix 8.2. During annotation, workers first determine whether a candidate noun mention carries verbal meaning in the context of the sentence (IS V ERBAL). We instructed the workers to consider the automatically extracted related verb, and to judge whether it will be natural to ask questions about the target noun instance using this verb. For example, given the noun phrase “the organizatio"
2020.coling-main.274,D15-1076,1,0.375208,"uestion-Answer driven SRL for Nominalizations Ayal Klein1 Jonathan Mamou2 Valentina Pyatkin1 Daniela Brook Weiss1 Hangfeng He3 Dan Roth3 Luke Zettlemoyer4 Ido Dagan1 1 Computer Science Department, Bar Ilan University 2 Intel Labs, Israel 3 University of Pennsylvania 4 University of Washington {ayal.s.klein,jonathan.mamou,valpyatkin, daniela.stepanov}@gmail.com {hangfeng,danroth}@seas.upenn.edu lsz@cs.washington.edu dagan@cs.biu.ac.il Abstract We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction S"
2020.coling-main.274,P18-2058,1,0.840711,"the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-d"
2020.coling-main.274,2020.acl-main.772,1,0.904092,"it arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal predicates. We identify verbal nouns that are eventive in nature along with their correspo"
2020.coling-main.274,W06-1617,0,0.0756852,"rhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalization"
2020.coling-main.274,kingsbury-palmer-2002-treebank,0,0.789046,"nnotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument str"
2020.coling-main.274,P13-1008,0,0.0813397,"Missing"
2020.coling-main.274,meyers-etal-2004-annotating,0,0.129033,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,W04-2705,0,0.84605,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,N18-2089,1,0.772819,"ts not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). A related QA-driven semantic annotation dataset is QA-driven Meaning Represen"
2020.coling-main.274,W18-3601,0,0.0213093,"Missing"
2020.coling-main.274,S15-2153,0,0.0987273,"Missing"
2020.coling-main.274,C08-1084,0,0.111033,"Missing"
2020.coling-main.274,D14-1162,0,0.0861543,"Missing"
2020.coling-main.274,W13-3516,0,0.0484846,"Missing"
2020.coling-main.274,P18-2124,0,0.0583201,"in focus, to investigate the interaction between the semantic space of the pre-training task and of the target task. See Appendix 8.5 for further experimental details. Our findings are shown in Table 6. Generally, results re-establish the findings of He et al. (2020), that further pre-training on QA annotations improve downstream performance over the BERT encoder, especially in low resource settings. This effect is more profound for semantically-oriented QA annotation (QAMR, QA-SRL and QANom), tackling semantic relations within a sentence, than for simple questionanswering data, i.e., SQuAD (Rajpurkar et al., 2018). As a trend, results also indicate that the performance gain is larger the more relevant the semantic space of the pre-training task is for the target task. Specifically, QAMR provides the best “generalpurpose” semantic signal, outperforming all the other models on most heterogeneous tasks (i.e. tasks with mixed types of “targets”). This is inline with its “whole-sentence” semantic nature, capturing relations for all word types. On the other hand, QANom’s semantic signal is improving BERT’s performance particularly for noun-targeting tasks, performing comparably to QAMR, and the same is true"
2020.coling-main.274,2020.acl-main.626,1,0.402678,"”. Thomas ARG0 Who has proved something? NomBank ARG1 QANom What has someone proved? the existence of God How did someone prove something? the Ontological argument Table 1: An illustration of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as t"
2020.coling-main.274,D16-1252,1,0.933373,"of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal pre"
2020.coling-main.274,N18-1081,1,0.845623,"sion of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et"
2020.coling-main.274,D18-1548,0,0.0201207,"notating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file"
2020.coling-main.274,W11-2822,0,0.0544657,"Missing"
2020.coling-main.274,P15-2115,0,0.140863,"and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research"
2020.conll-1.43,D14-1159,0,0.132231,"Missing"
2020.conll-1.43,P04-3031,0,0.0985493,"to two alternatives, either about booking online or via phone call. In such cases, each alternative is extracted as a separate process. Moreover, we only preserve processes where every primitive event contains both VERB and ARG1. Any ARG0’s are however omitted, since all events in a process share the same protagonist. To obtain the type information, we ﬁrst run SRL on the clause after “how to” in the article titles, from which the VERB term is seized as the action type label. Then on the ARG1 output of SRL, we fetch only the lemmatized head word based on dependency parsing and lemmatization (Bird and Loper, 2004). This typically gives us the non-plural noun that represents the object type, whereas other dependents including modiﬁers are dropped. Consider the clause in “How to make a birthday cake”, after make is fetched with SRL, the head word cake will be preserved from the ARG1 “a birthday cake”, providing an adequately abstracted label for object typing while being consistent to task deﬁnition. Dataset We construct a large corpus of typed event processes based on wikiHow – an online wiki-style community containing a collection of professionally edited how-to guideline articles. Construction A set o"
2020.conll-1.43,2020.acl-main.95,0,0.011788,"ode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on process-gloss pairs is connected to that approach, whereas we handle a learning-to-rank objective, and make inference in a much larger candidate space than the sense space of a single word. 6 Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 201919051600006 under the BETTER Program, and by Contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reﬂe"
2020.conll-1.43,2020.acl-main.431,0,0.0151837,"RTa improves the original BERT (Devlin et al., 2019) with a modiﬁed training procedure. It is considered one of the SOTA models for semantic representation of lexical sequences. To encode a process P , we concatenate the predicate and object (ai and oi ) of each event (ei ). Then those contents of all primitive events in P are sequentially concatenated, while the separator token of RoBERTa </s> is added between the contents of every consecutive two events. The entire lexical sequence is enclosed between tokens <s> and </s> to denote the beginning and end of the sequence. Following convention (Bommasani et al., 2020), mean-pooling of hidden states produces the encoded representation of the process, denoted P. object create or manufacture a man-made product Axis Projection Process Representation Make RoBERTa 3.1 action Gloss Selection representations of processes. Then, the gloss information of type vocabularies is encoded as intermediate representations for type labels using the same language model, for which WSD is performed to reﬁne the gloss information of polysemous labels during training. Finally, the language model is ﬁnetuned with a ranking task objective to capture the association of process-gloss"
2020.conll-1.43,P08-1090,0,0.774914,"been devoted to inducing how events described in text are procedurally connected (Ning et al., 2017; Radinsky et al., 2012), and how they form event processes3 (Pichotta and Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012; Radinsky and Horvitz, 2013), narrative cloze (Chaturvedi et al., 2017; Lee and Goldwasser, 2019), biological process comprehension (Berant et al., 2014) and diagnostic prediction (Zhang et al., 2020b). Nonetheless, understanding an event process is not just about inducing temporal relations between events or inferring missing steps in an event sequence. As suggested by cognitive studies (Zacks et al., 2001; Zacks and Tversky, 2001; Kurby and Zacks, 2008), a process of events is deﬁned more by the goals, plans, intentions,"
2020.conll-1.43,D17-1168,1,0.889244,"t processes3 (Pichotta and Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012; Radinsky and Horvitz, 2013), narrative cloze (Chaturvedi et al., 2017; Lee and Goldwasser, 2019), biological process comprehension (Berant et al., 2014) and diagnostic prediction (Zhang et al., 2020b). Nonetheless, understanding an event process is not just about inducing temporal relations between events or inferring missing steps in an event sequence. As suggested by cognitive studies (Zacks et al., 2001; Zacks and Tversky, 2001; Kurby and Zacks, 2008), a process of events is deﬁned more by the goals, plans, intentions, or traits of its performer, rather than by physical characteristics. For example, a series of events digging a hole, putting in some seeds, ﬁ"
2020.conll-1.43,K19-1015,1,0.786708,"and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on"
2020.conll-1.43,2020.acl-main.749,0,0.0216959,"Missing"
2020.conll-1.43,P18-1009,0,0.0614963,"ineapple juice Stir with a bar Action: make Object: cocktail Set locations and dates Compare airfares Purchase a ticket Action: book Object: flight Figure 1: Examples of type inference for event processes. information to summarize the goal and intention of the associated events. Speciﬁcally, each event process is typed along two axes: the action type that describes the type of action the process takes, and the object type that semantically types the object(s) that the process seeks to affect. Figure 1 shows several accordingly typed event processes. Motivated by recent works on entity typing (Choi et al., 2018; Zhou et al., 2018), our task employs large type vocabularies supporting diverse free-form semantic labels for both axes. To facilitate related research, we developed a large dataset extracted from wikiHow4 , as the second contribution of this paper. This dataset contains over 60,000 processes of primitive events, and features ﬁne-grained action and object type labels for each process. While the dataset aims at creating rich examples of event process intentions, it is also a challenging dataset from two perspectives. First, vocabularies on both type axes are remarkably diverse, giving over 1,"
2020.conll-1.43,P06-1039,0,0.220272,"Missing"
2020.conll-1.43,N19-1423,0,0.0169926,"relatedness of processes and labels, especially when the labels are often external to the event content. Glosses also adequately provide side information to jump-start few-shot label representations. Given a label L for either type axis, we use the same RoBERTa model (with shared parameters) 534 Cocktail ܁ a short mixed drink process ଵ Fill a highball with ice ଶ Add vodka ଷ Add pineapple juice ସ Stir with a bar ܁ை ۻሺሻ ۾ Joint Learning-to-rank We use the ofﬁcially released RoBERTa-base (Liu et al., 2019) for representations of event processes. RoBERTa improves the original BERT (Devlin et al., 2019) with a modiﬁed training procedure. It is considered one of the SOTA models for semantic representation of lexical sequences. To encode a process P , we concatenate the predicate and object (ai and oi ) of each event (ei ). Then those contents of all primitive events in P are sequentially concatenated, while the separator token of RoBERTa </s> is added between the contents of every consecutive two events. The entire lexical sequence is enclosed between tokens <s> and </s> to denote the beginning and end of the sequence. Following convention (Bommasani et al., 2020), mean-pooling of hidden stat"
2020.conll-1.43,D12-1062,1,0.74254,"ng et al., 2017; Radinsky et al., 2012), and how they form event processes3 (Pichotta and Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012; Radinsky and Horvitz, 2013), narrative cloze (Chaturvedi et al., 2017; Lee and Goldwasser, 2019), biological process comprehension (Berant et al., 2014) and diagnostic prediction (Zhang et al., 2020b). Nonetheless, understanding an event process is not just about inducing temporal relations between events or inferring missing steps in an event sequence. As suggested by cognitive studies (Zacks et al., 2001; Zacks and Tversky, 2001; Kurby and Zacks, 2008), a process of events is deﬁned more by the goals, plans, intentions, or traits of its performer, rather than by physical characteristics. F"
2020.conll-1.43,W18-2501,0,0.0163745,"onstruct a large corpus of typed event processes based on wikiHow – an online wiki-style community containing a collection of professionally edited how-to guideline articles. Construction A set of the articles are crawled from wikiHow, where each included article describes ordered steps of activities to complete a central goal (e.g. the article “How to book a ﬂight” describes necessary steps to complete an airline booking). Each described step of an article forms a standalone section, which provides an easy-toconsume format for obtaining event processes with clear intentions. We use AllenNLP (Gardner et al., 2018) to perform SRL on section titles of a goalstep article, and extract the VERB (predicate ai ) and ARG1 (object oi ) outputs from the section titles to form the corresponding sequence of (primitive) events. Note that some articles may contain multiple step sequences for the same goal, e.g. booking 533 Statistics The above effort obtains 62,277 clean event processes, each of which is labeled with both action and object types. Lengths of the processes are varied, for which the distribution is plotted in Figure 2. While the dataset gives a rich variety of instances for processes and intentions, it"
2020.conll-1.43,D19-1533,0,0.0303594,"Missing"
2020.conll-1.43,W19-0421,0,0.0221008,"Missing"
2020.conll-1.43,Q16-1002,0,0.014645,"ementioned techniques are also employed to supersense typing (Levine et al., 2020; Peters et al., 2019) and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al."
2020.conll-1.43,P16-1025,0,0.029274,"ausible event process by hypothesizing the objectives those co-occurring events aim for, or the ultimate consequence the process seeks to accomplish. Accordingly, we suggest that computational methods for event understanding would beneﬁt from conceptualizing the intentions behind the processes. Moreover, inducing intentions is crucial to rich understanding of text (Rashkin et al., 2018), and could potentially support other applications such as commonsense reasoning (Sap et al., 2019), summarization (Daum´e III and Marcu, 2006), reading comprehension (Berant et al., 2014) and schema induction (Huang et al., 2016). To understand the intentions of event processes, the ﬁrst contribution of this paper is to propose a new semantic typing task. The event process typing task seeks to retrieve ultra ﬁne-grained type 531 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 531–542 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Dig a hole Put seeds in Fill with soil Water soil Action: plant Object: plant Fill a highball with Ice Add vodka Add pineapple juice Stir with a bar Action: make Object: cocktail Set locations"
2020.conll-1.43,D19-1355,0,0.052602,"representations for positive and negative sampled labels. The entire learning process conducts joint learning-to-rank on both type axes. for process representation to encode its gloss sense deﬁnition. The mean-pooling result produces the gloss-based label representation denoted SL . Consider that the verb and noun terms in label vocabularies can be polysemous, we employ either of the following two techniques to select the glosses in the learning phase: • Pre-trained WSD models. One technique is to employ off-the-shelf WSD models that handle both verb and noun senses (Hadiwinoto et al., 2019; Huang et al., 2019). This could more precisely ﬁnd the right deﬁnition for each label given the speciﬁc context of a process, and allows each (polysemous) label to have varied representations when typing different processes. During training P2GT, we run WSD on the concatenation of type labels [A, O] to select the glosses of A and O for each training case. • Most frequent senses (MFS). Suppose a WSD model is not available, then the default way is to match a label only to its most frequent (or predominant) sense in sense-annotated corpora (Langone et al., 2004; Camacho-Collados et al., 2016). The MFS method has be"
2020.conll-1.43,D18-1221,0,0.0137838,"and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on process-gloss pairs is connected to that approach, whereas we handle a learning-to-rank objective, and make inference in a much larger candidate space than the sense space of a single word. 6 Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 201919051600006 under the B"
2020.conll-1.43,P19-1568,0,0.0189755,"aging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on process-gloss pairs is connected to that approach, whereas we handle a learning-to-rank objective, and make inference in a mu"
2020.conll-1.43,W04-2710,0,0.0495505,"Missing"
2020.conll-1.43,P19-1413,0,0.140281,"d Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012; Radinsky and Horvitz, 2013), narrative cloze (Chaturvedi et al., 2017; Lee and Goldwasser, 2019), biological process comprehension (Berant et al., 2014) and diagnostic prediction (Zhang et al., 2020b). Nonetheless, understanding an event process is not just about inducing temporal relations between events or inferring missing steps in an event sequence. As suggested by cognitive studies (Zacks et al., 2001; Zacks and Tversky, 2001; Kurby and Zacks, 2008), a process of events is deﬁned more by the goals, plans, intentions, or traits of its performer, rather than by physical characteristics. For example, a series of events digging a hole, putting in some seeds, ﬁlling with soil and waterin"
2020.conll-1.43,2020.acl-main.423,0,0.0207325,"a highly selected summary for most recent outcomes. For entity typing, recent research has coped with highly challenging problem settings. Those include few-shot or zero-shot typing with contextual distant supervision (Zhou et al., 2018) and description-based label embeddings (Obeidat et al., 2019). Others realize ultra-ﬁne type systems with the help of head-word supervision (Choi et al., 2018), hierarchical learning-to-rank (Chen et al., 2020) and structured label representations (Xiong et al., 2019; Hao et al., 2019). Several aforementioned techniques are also employed to supersense typing (Levine et al., 2020; Peters et al., 2019) and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019)"
2020.conll-1.43,2021.ccl-1.108,0,0.151189,"Missing"
2020.conll-1.43,D17-1086,0,0.025151,"ct supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on process-gloss pairs is connected to that approach, whereas we handle a learning-to-rank objective, and make inference in a much larger candidate space than the sense space of a single word. 6 Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 201919051600006 under the BETTER Program, and by Contract FA8750-1"
2020.conll-1.43,D18-1170,0,0.0211492,"umber of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD problem as classifying context-gloss pairs. Our learning approach on process-gloss pairs is connected to that approach, whereas we handle a learning-to-rank objective, and make inference in a much larger candidate space than the sense space of a single word. 6 Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 201919051600006 under the BETTER Program, and by Contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are thos"
2020.conll-1.43,D19-1359,0,0.0412413,"Missing"
2020.conll-1.43,J07-4005,0,0.0173529,"yping different processes. During training P2GT, we run WSD on the concatenation of type labels [A, O] to select the glosses of A and O for each training case. • Most frequent senses (MFS). Suppose a WSD model is not available, then the default way is to match a label only to its most frequent (or predominant) sense in sense-annotated corpora (Langone et al., 2004; Camacho-Collados et al., 2016). The MFS method has been a very strong baseline for unsupervised WSD (Tripodi and Navigli, 2019), as it is natural in language text that words generally express their predominant senses in most cases (McCarthy et al., 2007). Speciﬁcally for our task, the purpose is not to infer the exact sense, but rather generating a semantically rich (and allowably noisy) repre3.4 Inference sentation for type labels. In practice, we ﬁnd this simple technique to perform reasonably well as we type the event processes (§4.3). Besides these two techniques, we also tried others to represent a label, including concatenating all its gloss sense deﬁnitions, or concatenating most frequent two or more senses. They however do not perform as well as the aforementioned two techniques, hypothetically due to the noise introduced to label rep"
2020.conll-1.43,D17-1108,1,0.843403,"sses the challenging typing problem with indirect supervision from glosses1 and a joint learning-to-rank framework. As our experiments indicate, P2GT supports identifying the intent of processes, as well as the ﬁne semantic type of the affected object. It also demonstrates the capability of handling fewshot cases, and strong generalizability on outof-domain processes.2 1 Introduction Events are the fundamental building blocks of natural languages. To help machines understand events, extensive research effort has been devoted to inducing how events described in text are procedurally connected (Ning et al., 2017; Radinsky et al., 2012), and how they form event processes3 (Pichotta and Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012"
2020.conll-1.43,N19-1087,0,0.0244303,"event based on a single-clause description. This is however essentially different from our focus on processes of multiple events. Semantic typing has been investigated for language components other than events, such as entities and word senses. Due to the large body of work in this line of research, we can only provide a highly selected summary for most recent outcomes. For entity typing, recent research has coped with highly challenging problem settings. Those include few-shot or zero-shot typing with contextual distant supervision (Zhou et al., 2018) and description-based label embeddings (Obeidat et al., 2019). Others realize ultra-ﬁne type systems with the help of head-word supervision (Choi et al., 2018), hierarchical learning-to-rank (Chen et al., 2020) and structured label representations (Xiong et al., 2019; Hao et al., 2019). Several aforementioned techniques are also employed to supersense typing (Levine et al., 2020; Peters et al., 2019) and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguis"
2020.conll-1.43,N13-1039,0,0.0219558,"Missing"
2020.conll-1.43,D17-1008,0,0.0611684,"Missing"
2020.conll-1.43,K19-1051,1,0.787835,"ks on event processes have attracted much attention recently, while many existing works focus on extraction and completion of event processes. For example, Radinsky et al. (2012; 2013) mine sequences of frequently co-ocurring events from multiple temporally connected documents, and use the sequence knowledge to predict the future event(s) of a process. Berant et al. (2014) propose to extract biological processes with SRL, and help machine reading comprehension for biological articles. A series of other works learn for sequential event prediction using language models (Chaturvedi et al., 2017; Peng et al., 2019) or association rules (Letham et al., 2013), and further cope with downstream tasks such as narrative cloze tests. On the contrary, fewer efforts have been made for inferring the intentions or central goals behind a composite of events. A recent work by Rashkin et al. (2018) is particularly relevant to this topic, which learns a sequence-to-label generator to predict the intention of one primitive event based on a single-clause description. This is however essentially different from our focus on processes of multiple events. Semantic typing has been investigated for language components other t"
2020.conll-1.43,D19-1005,0,0.0257016,"Missing"
2020.conll-1.43,E14-1024,0,0.0253931,"a joint learning-to-rank framework. As our experiments indicate, P2GT supports identifying the intent of processes, as well as the ﬁne semantic type of the affected object. It also demonstrates the capability of handling fewshot cases, and strong generalizability on outof-domain processes.2 1 Introduction Events are the fundamental building blocks of natural languages. To help machines understand events, extensive research effort has been devoted to inducing how events described in text are procedurally connected (Ning et al., 2017; Radinsky et al., 2012), and how they form event processes3 (Pichotta and Mooney, 2014; Berant et al., 2014; Jindal and Roth, 2013). Consequently, such prototypical schematic sequences of events have found ∗ This work was done when the author was visiting the University of Pennsylvania. 1 A gloss provides a sense deﬁnition for a lexeme. 2 The contributed learning resources, software and a system demonstration are available at http://cogcomp.org/ page/publication_view/915. 3 A.k.a. event chains (Chambers and Jurafsky, 2008). important use cases including storyline construction (Do et al., 2012; Radinsky and Horvitz, 2013), narrative cloze (Chaturvedi et al., 2017; Lee and Goldwa"
2020.conll-1.43,N19-1222,0,0.0134049,"o supersense typing (Levine et al., 2020; Peters et al., 2019) and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode gloss deﬁnitions for monolingual (Hill et al., 2016; Noraset et al., 2017; Pilehvar, 2019; Hedderich et al., 2019) and cross-lingual 538 (Chen et al., 2019; Zhang et al., 2020a) reverse dictionary prediction, as well as out-of-vocabulary lexical representation (Kumar et al., 2019; Prokhorov et al., 2019; Bahdanau et al., 2017). Deﬁnitions have also been leveraged to generate zero-shot entity representations in knowledge graphs (Kartsaklis et al., 2018; Chen et al., 2018; Long et al., 2017). Some other works inject gloss representations to improve WSD (Huang et al., 2019; Luo et al., 2018; Blevins and Zettlemoyer, 2020). GlossBERT (Huang et al., 2019) thereof formalizes the WSD pro"
2020.conll-1.43,D19-1009,0,0.0521885,"ion for each label given the speciﬁc context of a process, and allows each (polysemous) label to have varied representations when typing different processes. During training P2GT, we run WSD on the concatenation of type labels [A, O] to select the glosses of A and O for each training case. • Most frequent senses (MFS). Suppose a WSD model is not available, then the default way is to match a label only to its most frequent (or predominant) sense in sense-annotated corpora (Langone et al., 2004; Camacho-Collados et al., 2016). The MFS method has been a very strong baseline for unsupervised WSD (Tripodi and Navigli, 2019), as it is natural in language text that words generally express their predominant senses in most cases (McCarthy et al., 2007). Speciﬁcally for our task, the purpose is not to infer the exact sense, but rather generating a semantically rich (and allowably noisy) repre3.4 Inference sentation for type labels. In practice, we ﬁnd this simple technique to perform reasonably well as we type the event processes (§4.3). Besides these two techniques, we also tried others to represent a label, including concatenating all its gloss sense deﬁnitions, or concatenating most frequent two or more senses. Th"
2020.conll-1.43,N19-1084,0,0.024982,"s, such as entities and word senses. Due to the large body of work in this line of research, we can only provide a highly selected summary for most recent outcomes. For entity typing, recent research has coped with highly challenging problem settings. Those include few-shot or zero-shot typing with contextual distant supervision (Zhou et al., 2018) and description-based label embeddings (Obeidat et al., 2019). Others realize ultra-ﬁne type systems with the help of head-word supervision (Choi et al., 2018), hierarchical learning-to-rank (Chen et al., 2020) and structured label representations (Xiong et al., 2019; Hao et al., 2019). Several aforementioned techniques are also employed to supersense typing (Levine et al., 2020; Peters et al., 2019) and POS tagging (Owoputi et al., 2013). In terms of type labeling, our work is inspired by Choi et al. (2018)’s way of leveraging free-form lexemes for ultra-ﬁne entity types. Nevertheless, besides typing on a different modality, our work is also distinguished in the multi-axis typing system, and the way of leveraging gloss-based indirect supervision. Representation learning of gloss knowledge has been incorporated in various tasks. A number of works encode g"
2020.conll-1.43,P18-1043,0,0.353232,"he performer. Similarly, we can tell that making a dough, adding toppings, preheating the oven and baking the dough is likely a chain of actions aimed at cooking pizza. Indeed, aforementioned studies show that humans understand a plausible event process by hypothesizing the objectives those co-occurring events aim for, or the ultimate consequence the process seeks to accomplish. Accordingly, we suggest that computational methods for event understanding would beneﬁt from conceptualizing the intentions behind the processes. Moreover, inducing intentions is crucial to rich understanding of text (Rashkin et al., 2018), and could potentially support other applications such as commonsense reasoning (Sap et al., 2019), summarization (Daum´e III and Marcu, 2006), reading comprehension (Berant et al., 2014) and schema induction (Huang et al., 2016). To understand the intentions of event processes, the ﬁrst contribution of this paper is to propose a new semantic typing task. The event process typing task seeks to retrieve ultra ﬁne-grained type 531 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 531–542 c Online, November 19-20, 2020. 2020 Association for Computational Lingui"
2020.conll-1.43,D18-1231,1,0.908936,"with a bar Action: make Object: cocktail Set locations and dates Compare airfares Purchase a ticket Action: book Object: flight Figure 1: Examples of type inference for event processes. information to summarize the goal and intention of the associated events. Speciﬁcally, each event process is typed along two axes: the action type that describes the type of action the process takes, and the object type that semantically types the object(s) that the process seeks to affect. Figure 1 shows several accordingly typed event processes. Motivated by recent works on entity typing (Choi et al., 2018; Zhou et al., 2018), our task employs large type vocabularies supporting diverse free-form semantic labels for both axes. To facilitate related research, we developed a large dataset extracted from wikiHow4 , as the second contribution of this paper. This dataset contains over 60,000 processes of primitive events, and features ﬁne-grained action and object type labels for each process. While the dataset aims at creating rich examples of event process intentions, it is also a challenging dataset from two perspectives. First, vocabularies on both type axes are remarkably diverse, giving over 1,000 action type labe"
2020.emnlp-main.119,P98-1013,0,0.330579,"12 We randomly select another sub-event sequence that describes the same process from WikiHow, which could be different from the currently tested sequence. As a result, adding such sequence cannot help predict all missing events. 1548 4 Related Works Throughout history, considering the importance of events in understanding human language (e.g., commonsense knowledge (Zhang et al., 2020a)), many efforts have been devoted to define, represent, and understand events. For example, VerbNet (Schuler, 2005) created a verb lexicon to represent the semantic relations among verbs. After that, FrameNet (Baker et al., 1998) proposed to represent the event semantics with schemas, which has one predicate and several arguments. Apart from the structure of events, understanding events by predicting relations among them also becomes a popular research topic (e.g., TimeBank (Pustejovsky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Su"
2020.emnlp-main.119,D14-1159,0,0.0626597,"for unseen processes and can help predict missing events. 1 Figure 1: An illustration of leveraging known processes to predict the sub-event sequence of a new process. Introduction Understanding events has long been a challenging task in NLP, to which many efforts have been devoted by the community. However, most existing works are focusing on procedural (or horizontal) event prediction tasks. Examples include predicting the next event given an observed event sequence (Radinsky et al., 2012) and identifying the effect of a biological process (i.e., a sequence of events) on involved entities (Berant et al., 2014). These tasks mostly focus on predicting related events in a procedure based on their statistical correlations in previously observed text. As a result, understanding the meaning of an event might ∗ This work was done when the first author was visiting the University of Pennsylvania. 1 Code is available at: http://cogcomp.org/ page/publication_view/910. not be crucial for these horizontal tasks. For example, simply selecting the most frequently cooccurring event can offer acceptable performance on the event prediction task (Granroth-Wilding and Clark, 2016). Computational and cognitive studies"
2020.emnlp-main.119,J06-1003,0,0.00623629,"e C) algorithm, which first expands all possible subsets (i.e., it includes all subsets of EC for all C) and then leverages the sort and filter technique to select the final subsets, we reduce the time complexity from O(|C |· |E|2 ) to O(n · |C |· |E|), where n is the number of conceptualized events and is typically much smaller than |E|. 2.2.2 Conceptualization Scoring As mentioned above, for each pair of a sub-event E and its potential conceptualization C, we propose a scoring function F (E, C) to measure how much “semantic information&quot; is preserved after the conceptualization. Motivated by Budanitsky and Hirst (2006) and based on the assumption that the more abstract the conceptualized event is, the more semantic details are lost, we define F (E, C) to be: F (E, C) = m Y Conceptualized Event Ordering After conceptualizing and merging all sub-events, we need to determine their loosely temporal order (e.g., whether they typically appear at the beginning or the end of these sub-event sequences). Let the set of selected conceptualized events be C ∗ . For each C ∈ C ∗ , we define its order score T (C), indicating how likely C is to appear first, as: T (C) = X C 0 ∈C∗ E ,w C ) i wD(wi , (2) i=1 where D(wiE , wi"
2020.emnlp-main.119,P08-1090,0,0.147589,"ening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explicitly. 5 Conclusion In this paper, we try to understand events vertically by viewing them as processes and predicting their sub-event sequences. Our APSI framework is motivated by the notion of analogous processes, and attempts to transfer knowledge from (a very small number of) familiar processes to a"
2020.emnlp-main.119,N19-1423,0,0.0157797,"Missing"
2020.emnlp-main.119,glavas-etal-2014-hieve,0,0.122438,"or temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explicitly. 5 Conclusion In t"
2020.emnlp-main.119,W13-1203,0,0.0619251,"sky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explici"
2020.emnlp-main.119,W04-1013,0,0.0116262,"ar process (Jaccard), (GloVe), and (RoBERTa), respectively. For each process, we also present a randomly generated sequence and a human-generated sequence9 as the lower-bound and upper-bound for sub-event sequence prediction models. 3.3 Intrinsic Evaluation We first present the intrinsic evaluation to show the quality of the predicted sub-event sequences of unseen processes. For each test process, we provide the process name and the sub-event sequence length10 to evaluated systems and ask them to generate a fixed-length sub-event sequence. 3.3.1 Evaluation Metric Motivated by the ROUGE score (Lin, 2004), we propose an event-based ROUGE (E-ROUGE) to evaluate the quality of the predicted sub-event sequence. Specifically, similar to ROUGE, which evaluates the generation quality based on N-gram token occurrence, we evaluate how much percentage of the sub-event and time-ordered sub-event pairs in the induced sequence is covered by the human-provided references. We denote the evaluation over single event and event pairs as EROUGE1 and E-ROUGE2, respectively. We also provide two covering standards to better understand the prediction quality: (1) “String Match”: all words in the predicted event/pair"
2020.emnlp-main.119,2021.ccl-1.108,0,0.0499337,"Missing"
2020.emnlp-main.119,P19-1472,0,0.0462531,"Missing"
2020.emnlp-main.119,D14-1162,0,0.0834515,"e (Seq2seq): One intuitive solution to the sub-event sequence prediction task would be modeling it as a sequence to sequence problem, where the process is treated as the input and the sub-event sequence the output. Here we 7 https://www.wikihow.com. We do not need a development set because the proposed solution APSI is not a learning-based method. 8 adopt the standard GRU-based encoder-decoder framework (Sutskever et al., 2014) as the base framework and change the generation unit from words to events. For each process or sub-event, we leverage pre-trained word embeddings (i.e., GloVe-6b-300d (Pennington et al., 2014)) or language models (i.e., RoBERTa-base (Liu et al., 2019)) as the representation, which are denoted as Seq2seq (GloVe) and Seq2seq (RoBERTa). Top One Similar Process: Another baseline is the “top one similar process”. For each new process, we can always find the most similar observed process. Then we can use the sub-event sequence of the observed process as the prediction. We employ different methods (i.e., token-level Jaccard coefficient or cosine similarity of GloVe/RoBERTa process representations) to measure the process similarity. We denote them as Top one similar process (Jaccard), (Glo"
2020.emnlp-main.119,P18-1043,0,0.0245094,"uage (e.g., commonsense knowledge (Zhang et al., 2020a)), many efforts have been devoted to define, represent, and understand events. For example, VerbNet (Schuler, 2005) created a verb lexicon to represent the semantic relations among verbs. After that, FrameNet (Baker et al., 1998) proposed to represent the event semantics with schemas, which has one predicate and several arguments. Apart from the structure of events, understanding events by predicting relations among them also becomes a popular research topic (e.g., TimeBank (Pustejovsky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and"
2020.emnlp-main.51,P19-1471,0,0.0485076,"ed global inference to facilitate constraints on TempRels. Besides TempRels, a couple of efforts have focused on event hierarchy construction, a.k.a. subevent relation extraction. This task seeks to extract the hierarchy where each parent event contains child events that are described in the same document. To cope with this task, both Araki et al. ˇ (2014) and Glavaˇs and Snajder (2014) introduced a variety of features and employed logistic regression models for classifying event pairs into subevent relations (PARENT-C HILD and C HILD -PARENT, coreference (C OREF), and no relation (N O R EL). Aldawsari and Finlayson (2019) further extended the characterization with more features on the discourse and narrative aspects. Zhou et al. (2020a) presented a data-driven method by fine-tuning a time duration-aware BERT (Devlin et al., 2019) on corpora of time mentions, and used the estimation of time duration to predict subevent relations. Though previous efforts have been devoted to preserving logical consistency through inference or structured learning (Roth and Yih, 2004; Roth and tau Yih, 2007; Chang et al., 2008), this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to c"
2020.emnlp-main.51,araki-etal-2014-detecting,0,0.225389,"Missing"
2020.emnlp-main.51,D08-1073,0,0.129079,"Missing"
2020.emnlp-main.51,D17-1168,1,0.822816,"was visiting the University of Pennsylvania. 1 Our code is publicly available at https://cogcomp. seas.upenn.edu/page/publication_view/914. flights canceled (e3 ) and passengers affected (e4 ). Some of those mentions also follow strict temporal order (e3 , e4 and e5 ). Our goal is to induce such an event complex that recognizes the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering (Khashabi et al., 2018), narrative prediction (Chaturvedi et al., 2017), timeline construction (Do et al., 2012a) and summarization (Daum´e III and Marcu, 2006). Recently, significant research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation (TempRel) extraction (Ning et al., 2018a, 2019) and subevent 696 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 696–706, c November 16–20, 2020. 2020 Association for Computational Linguistics relation extraction (Liu et al., 2018; Aldawsari and Finlayson, 2019). Addressing such challenging tasks requires a model to recogniz"
2020.emnlp-main.51,P06-1039,0,0.340636,"Missing"
2020.emnlp-main.51,N19-1423,0,0.00720752,"here each parent event contains child events that are described in the same document. To cope with this task, both Araki et al. ˇ (2014) and Glavaˇs and Snajder (2014) introduced a variety of features and employed logistic regression models for classifying event pairs into subevent relations (PARENT-C HILD and C HILD -PARENT, coreference (C OREF), and no relation (N O R EL). Aldawsari and Finlayson (2019) further extended the characterization with more features on the discourse and narrative aspects. Zhou et al. (2020a) presented a data-driven method by fine-tuning a time duration-aware BERT (Devlin et al., 2019) on corpora of time mentions, and used the estimation of time duration to predict subevent relations. Though previous efforts have been devoted to preserving logical consistency through inference or structured learning (Roth and Yih, 2004; Roth and tau Yih, 2007; Chang et al., 2008), this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to combine multiple training data in multi-task learning (Lin et al., 2020), our work is distinguished by enhancing the learning process by pushing the model towards a coherent output that satisfies logical constrain"
2020.emnlp-main.51,E17-2118,0,0.043295,"Missing"
2020.emnlp-main.51,D12-1062,1,0.813545,"ur code is publicly available at https://cogcomp. seas.upenn.edu/page/publication_view/914. flights canceled (e3 ) and passengers affected (e4 ). Some of those mentions also follow strict temporal order (e3 , e4 and e5 ). Our goal is to induce such an event complex that recognizes the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering (Khashabi et al., 2018), narrative prediction (Chaturvedi et al., 2017), timeline construction (Do et al., 2012a) and summarization (Daum´e III and Marcu, 2006). Recently, significant research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation (TempRel) extraction (Ning et al., 2018a, 2019) and subevent 696 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 696–706, c November 16–20, 2020. 2020 Association for Computational Linguistics relation extraction (Liu et al., 2018; Aldawsari and Finlayson, 2019). Addressing such challenging tasks requires a model to recognize the inherent connection between event"
2020.emnlp-main.51,W14-3705,0,0.18199,"Missing"
2020.emnlp-main.51,glavas-etal-2014-hieve,0,0.592284,"Missing"
2020.emnlp-main.51,K19-1062,0,0.333667,"tion between event mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents (Glavaˇs et al., 2014; Ning et al., 2017b, 2018c). Such methods often require designing various features to characterize the structural, discourse and narrative aspects of the events, which are costly to produce and are often specific to a certain task or dataset. More recent works attempted to use datadriven methods based on neural relation extraction models (Dligach et al., 2017; Ning et al., 2019; Han et al., 2019a,b) which refrain from feature engineering and offer competent performances. While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available (Glavaˇs et al., 2014; Ning et al., 2018b). For example, the largest temporal relation extraction dataset MATRES (Ning et al., 2018b) only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical pro"
2020.emnlp-main.51,D19-1041,0,0.164466,"Missing"
2020.emnlp-main.51,W13-1203,0,0.0631166,"cularly, we are interested in two subtasks of relation extraction, corresponding to the label set R = RT ∪ RH . RT thereof denotes the set of temporal relations defined in the literature (Ning et al., 2017b, 2018b, 2019; Han et al., 2019b), which contains B EFORE , A FTER , E QUAL, and VAGUE. To be consistent with previous studies (Ning et al., 2018b, 2019), the temporal ordering relations between two events are decided by the order of their starting time, without constraining on their ending time. RH thereof denotes the set of relation labels defined in the subevent relation extraction task (Hovy et al., 2013; Glavaˇs et al., 2014), i.e., PARENT-C HILD , C HILD -PARENT, C OREF and N O R EL. Following the definitions by Hovy et al. (2013), an event e1 is said to have a child event e2 if e1 is a collector event that contains a sequence of activities, where e2 is one of these activities, and e2 is spatially and temporally contained within e1 . Note that each pair of events can be annotated with one relation from each of RH and RT respectively, as the labels within each task-specific relation set are mutually exclusive. Our learning framework first obtains the event pair representation that combines c"
2020.emnlp-main.51,D19-1405,0,0.098222,"vent” and “HasLastSubevent” relations. From ConceptNet we extract around 30k pairs of event concepts labeled with the aforementioned relations, along with 30k randomly corrupted negative samples. We also incorporate com3.3 Joint Constrained Learning Given the characterization of grounded event pairs from the document, we now define the learning objectives for relation prediction. The goal of learning is to let the model capture the data annotation, meanwhile regularizing the model towards consistency on logic constraints. Inspired by the logicdriven framework for consistency of neural models (Li et al., 2019), we specify three types of consistency requirements, i.e. annotation consistency, symmetry consistency and conjunction consistency. We hereby define the requirements with declarative logic rules, and show how we transform them into differentiable loss functions. 699 Annotation Consistency For labeled cases, we expect the model to predict what annotations specify. That is to say, if an event pair is annotated with @β α@ PC CP CR NR BF AF EQ VG PC PC, ¬AF – PC, ¬AF ¬CP, ¬CR BF , ¬CP, ¬CR – BF , ¬CP, ¬CR – CP – CP, ¬BF CP, ¬BF ¬PC, ¬CR – AF, ¬PC, ¬CR AF, ¬PC, ¬CR – CR PC, ¬AF CP, ¬BF CR, EQ NR B"
2020.emnlp-main.51,2020.acl-main.713,0,0.0763496,"res on the discourse and narrative aspects. Zhou et al. (2020a) presented a data-driven method by fine-tuning a time duration-aware BERT (Devlin et al., 2019) on corpora of time mentions, and used the estimation of time duration to predict subevent relations. Though previous efforts have been devoted to preserving logical consistency through inference or structured learning (Roth and Yih, 2004; Roth and tau Yih, 2007; Chang et al., 2008), this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to combine multiple training data in multi-task learning (Lin et al., 2020), our work is distinguished by enhancing the learning process by pushing the model towards a coherent output that satisfies logical constraints across separate tasks. 3 Methods In this section, we present the joint learning framework for event-event relation extraction. We start with the problem formulation (§3.1), followed by the techniques for event pair characterization (§3.2), constrained learning (§3.3) and inference (§3.4). 3.1 Preliminaries A document D is represented as a sequence of tokens D = [t1 , · · · , e1 , · · · , e2 , · · · , tn ]. Some of the tokens belong to the set of annota"
2020.emnlp-main.51,C18-1309,0,0.0415653,"Missing"
2020.emnlp-main.51,P06-1095,0,0.179614,"lar events, as well as the horizontal temporal reasoning within the event complex. As far as we know, this is different from all previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes when evaluated on the RED dataset (O’Gorman et al., 2016). 2 Related Work Various approaches have been proposed to extract event TempRels. Early effort focused on characterizing event pairs based on various types of semantic and linguistic features, and utilizing statistical learning methods, such as logistic regression (Mani et al., 2006; Verhagen and Pustejovsky, 2008) and SVM (Mirza and Tonelli, 2014), to capture the relations. Those methods typically require 697 extensive feature engineering, and do not comprehensively consider the contextual information and global constraints among event-event relations. Recently, data-driven methods have been developed for TempRel extraction, and have offered promising performance. Ning et al. (2019) addressed this problem using a system combining an LSTM document encoder and a Siamese multi-layer perceptron (MLP) encoder for temporal commonsense knowledge from T EM P ROB (Ning et al., 2"
2020.emnlp-main.51,E14-1033,0,0.0200725,"hin the event complex. As far as we know, this is different from all previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes when evaluated on the RED dataset (O’Gorman et al., 2016). 2 Related Work Various approaches have been proposed to extract event TempRels. Early effort focused on characterizing event pairs based on various types of semantic and linguistic features, and utilizing statistical learning methods, such as logistic regression (Mani et al., 2006; Verhagen and Pustejovsky, 2008) and SVM (Mirza and Tonelli, 2014), to capture the relations. Those methods typically require 697 extensive feature engineering, and do not comprehensively consider the contextual information and global constraints among event-event relations. Recently, data-driven methods have been developed for TempRel extraction, and have offered promising performance. Ning et al. (2019) addressed this problem using a system combining an LSTM document encoder and a Siamese multi-layer perceptron (MLP) encoder for temporal commonsense knowledge from T EM P ROB (Ning et al., 2018a). Han et al. (2019a) proposed a bidirectional LSTM (BiLSTM) wi"
2020.emnlp-main.51,D17-1108,1,0.902336,"Missing"
2020.emnlp-main.51,D19-1642,1,0.906961,"Missing"
2020.emnlp-main.51,N18-1077,1,0.0963422,"goal is to induce such an event complex that recognizes the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering (Khashabi et al., 2018), narrative prediction (Chaturvedi et al., 2017), timeline construction (Do et al., 2012a) and summarization (Daum´e III and Marcu, 2006). Recently, significant research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation (TempRel) extraction (Ning et al., 2018a, 2019) and subevent 696 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 696–706, c November 16–20, 2020. 2020 Association for Computational Linguistics relation extraction (Liu et al., 2018; Aldawsari and Finlayson, 2019). Addressing such challenging tasks requires a model to recognize the inherent connection between event mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents (Glavaˇs et al., 2014; Ning et al., 2017b, 2018c)."
2020.emnlp-main.51,P18-1122,1,0.115623,"goal is to induce such an event complex that recognizes the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering (Khashabi et al., 2018), narrative prediction (Chaturvedi et al., 2017), timeline construction (Do et al., 2012a) and summarization (Daum´e III and Marcu, 2006). Recently, significant research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation (TempRel) extraction (Ning et al., 2018a, 2019) and subevent 696 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 696–706, c November 16–20, 2020. 2020 Association for Computational Linguistics relation extraction (Liu et al., 2018; Aldawsari and Finlayson, 2019). Addressing such challenging tasks requires a model to recognize the inherent connection between event mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents (Glavaˇs et al., 2014; Ning et al., 2017b, 2018c)."
2020.emnlp-main.51,D18-2013,1,0.0995771,"goal is to induce such an event complex that recognizes the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering (Khashabi et al., 2018), narrative prediction (Chaturvedi et al., 2017), timeline construction (Do et al., 2012a) and summarization (Daum´e III and Marcu, 2006). Recently, significant research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation (TempRel) extraction (Ning et al., 2018a, 2019) and subevent 696 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 696–706, c November 16–20, 2020. 2020 Association for Computational Linguistics relation extraction (Liu et al., 2018; Aldawsari and Finlayson, 2019). Addressing such challenging tasks requires a model to recognize the inherent connection between event mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents (Glavaˇs et al., 2014; Ning et al., 2017b, 2018c)."
2020.emnlp-main.51,W16-5706,0,0.20545,"Missing"
2020.emnlp-main.51,W04-2401,1,0.790765,"h refrain from feature engineering and offer competent performances. While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available (Glavaˇs et al., 2014; Ning et al., 2018b). For example, the largest temporal relation extraction dataset MATRES (Ning et al., 2018b) only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical properties (Roth and Yih, 2004; Chambers and Jurafsky, 2008), led to employing global inference to comply with transitivity and symmetry consistency, specifically on TempRel (Do et al., 2012b; Ning et al., 2017b; Han et al., 2019a). However, in an event complex, the logical constraints may globally apply to different taskspecific relations, and form more complex conjunctive constraints. Consider the example in Figure 1: given that e2:died is B EFORE e3:canceled and e3:canceled is a PARENT event of e4:affecting, the learning process should enforce e2:died B EFORE e4:affecting by considering the conjunctive constraints on bo"
2020.emnlp-main.51,P11-2061,0,0.0537203,"of MATRES has defined four axes for the actions of events, i.e. main, intention, opinion, and hypothetical axes. The TempRels are considered for all event pairs on the same axis and within a context of two adjacent sentences. The labels are decided by comparing the starting points of the events. The multi-axis annotation helped MATRES to achieve a high IAA of 0.84 in Cohen’s Kappa. The HiEve corpus is a news corpus that contains 100 articles. Within each article, annotations are given for both subevent and coreference relations. The HiEve adopted the IAA measurement proposed for TempRels by (UzZaman and Allen, 2011), resulting in 0.69 F1 . In addition to these two datasets, we also present a case study on an updated version of the RED dataset (O’Gorman et al., 2016). This dataset contains 35 news articles with annotations for event complexes that contain both membership relations and TempRels. Since small dataset is not sufficient for training, we use it only to demonstrate our method’s capability of inducing event complexes on data that are external to training. We briefly summarize the data statistics for HiEve, MATRES, and RED dataset in Table 3. 701 Model CogCompTime (Ning et al., 2018c) Perceptron ("
2020.emnlp-main.51,S13-2001,0,0.0629183,"vertheless, we show in experiments that a well-trained constrained learning model may not additionally require global inference (§4.5). Experiments 4.1 Datasets Since there is not a large-scale dataset that amply annotates for both TempRel and subevent relations, we evaluate the joint training and prediction of both categories of relations on two separate datasets. Specifically, we use MATRES (Ning et al., 2018b) for TempRel extraction and HiEve (Glavaˇs et al., 2014) for subevent relation extraction. MATRES is a new benchmark dataset for TempRel extraction, which is developed from TempEval3 (UzZaman et al., 2013). It annotates on top of 275 documents with TempRels B EFORE , A FTER , E QUAL, and VAGUE. Particularly, the annotation process of MATRES has defined four axes for the actions of events, i.e. main, intention, opinion, and hypothetical axes. The TempRels are considered for all event pairs on the same axis and within a context of two adjacent sentences. The labels are decided by comparing the starting points of the events. The multi-axis annotation helped MATRES to achieve a high IAA of 0.84 in Cohen’s Kappa. The HiEve corpus is a news corpus that contains 100 articles. Within each article, anno"
2020.emnlp-main.51,C08-3012,0,0.0546152,"as the horizontal temporal reasoning within the event complex. As far as we know, this is different from all previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes when evaluated on the RED dataset (O’Gorman et al., 2016). 2 Related Work Various approaches have been proposed to extract event TempRels. Early effort focused on characterizing event pairs based on various types of semantic and linguistic features, and utilizing statistical learning methods, such as logistic regression (Mani et al., 2006; Verhagen and Pustejovsky, 2008) and SVM (Mirza and Tonelli, 2014), to capture the relations. Those methods typically require 697 extensive feature engineering, and do not comprehensively consider the contextual information and global constraints among event-event relations. Recently, data-driven methods have been developed for TempRel extraction, and have offered promising performance. Ning et al. (2019) addressed this problem using a system combining an LSTM document encoder and a Siamese multi-layer perceptron (MLP) encoder for temporal commonsense knowledge from T EM P ROB (Ning et al., 2018a). Han et al. (2019a) propose"
2020.emnlp-main.51,2020.acl-main.678,1,0.838102,"construction, a.k.a. subevent relation extraction. This task seeks to extract the hierarchy where each parent event contains child events that are described in the same document. To cope with this task, both Araki et al. ˇ (2014) and Glavaˇs and Snajder (2014) introduced a variety of features and employed logistic regression models for classifying event pairs into subevent relations (PARENT-C HILD and C HILD -PARENT, coreference (C OREF), and no relation (N O R EL). Aldawsari and Finlayson (2019) further extended the characterization with more features on the discourse and narrative aspects. Zhou et al. (2020a) presented a data-driven method by fine-tuning a time duration-aware BERT (Devlin et al., 2019) on corpora of time mentions, and used the estimation of time duration to predict subevent relations. Though previous efforts have been devoted to preserving logical consistency through inference or structured learning (Roth and Yih, 2004; Roth and tau Yih, 2007; Chang et al., 2008), this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to combine multiple training data in multi-task learning (Lin et al., 2020), our work is distinguished by enhancing the"
2020.emnlp-main.521,P11-1097,0,0.0529628,"Missing"
2020.emnlp-main.521,L16-1521,0,0.168615,"odel on the related HRL, using Wikipedia title mappings and anchor text mappings. Then it applies the model on SL mentions. If SL and related HRL share a script, showing grapheme similarity, it treats SL mentions as related HRL mentions at test time. Otherwise, if SL and related HRL have phoneme similarity it converts both SL and related HRL text into international phonetic alphabet (IPA) symbols. 3.2 Current Methods’ Limitations This section discusses four major limitations that existing methods suffer from, and quantifies these with experimental results. The results use the LORELEI dataset (Strassel and Tracey, 2016), a realistic text corpora that consist of news and social media text, all from outside-Wikipedia (see Section 5). Some tables also include a comparison with the proposed QL based candidate generation method QuEL CG that we describe in Section 4. We use the definition of gold candidate recall in Zhou et al. (2020), which is the proportion of SL mentions that have the gold English entity in the candidate list, as the evaluation metric. 3.2.1 Shortage of Interlanguage Links As illustrated in Figure 3 (specific numbers are in Appendix A.1) with statistics from the 2019-10-20 wikidump6 and in Tabl"
2020.emnlp-main.521,N16-1072,1,0.89062,"Wikipedia articles titles as mappings. It directly links an SL entity to an English Wikipedia entry without ambiguity. Wikipedia anchor text mappings: A clickable text mention in Wikipedia articles is annotated with anchor text linking it to a Wikipedia entry. The following retrieving order: SL anchor text → SL Wikipedia entry → English Wikipedia entry (where the last step is done via the Wikipedia interlanguage links), allows one to build a bilingual title mapping from a SL mentions to Wikipedia English entries, resulting in a probabilistic mapping with scores calculated using total counts (Tsai and Roth, 2016). A LRL Entities B LRL Wiki Entities C English Wiki Entities Figure 2: An illustration of Cross-lingual resources serving current XEL systems. Our observations lead to the conclusion that the LRL setting necessitates the use of outsideWikipedia cross-lingual resources. Specifically, we propose various ways to utilize the abundant query log from online search engines to compensate for the lack of supervision. Similar to Wikipedia, a free online encyclopedia created by Internet users, Query logs (QL) provide a free resource, collaboratively generated by a large number of users, and mildly curate"
2020.emnlp-main.521,D18-1270,1,0.856235,"Name Translation) as introduced in (Pan et al., 2017; Zhang et al., 2018) performs word alignment on Wikipedia title mappings, to induce a fixed word-to-word translation table between SL and English. For instance, to link the Suomi name “Pekingin tekninen instituutti” (Beijing Institute of Technology), it translates each word in the mention: (“Pekingin” – Beijing, “tekninen” – Technology, “instituutti” – Institute). At test time, after mapping each word in the given mention to English, it links the translation to English Wikipedia using an unsupervised collective inference approach. translit (Upadhyay et al., 2018b) trains a seq2seq model on Wikipedia title mappings, to generate the English entity directly. pivoting to a related high-resource language (HRL) (Zhou et al., 2020) attempts to generalize to unseen LRL mentions through grapheme or phoneme similarity between SL and a related HRL. Specifically, it first finds a related HRL for the SL, and learns a XEL model on the related HRL, using Wikipedia title mappings and anchor text mappings. Then it applies the model on SL mentions. If SL and related HRL share a script, showing grapheme similarity, it treats SL mentions as related HRL mentions at test"
2020.emnlp-main.521,D18-1046,1,0.839561,"Name Translation) as introduced in (Pan et al., 2017; Zhang et al., 2018) performs word alignment on Wikipedia title mappings, to induce a fixed word-to-word translation table between SL and English. For instance, to link the Suomi name “Pekingin tekninen instituutti” (Beijing Institute of Technology), it translates each word in the mention: (“Pekingin” – Beijing, “tekninen” – Technology, “instituutti” – Institute). At test time, after mapping each word in the given mention to English, it links the translation to English Wikipedia using an unsupervised collective inference approach. translit (Upadhyay et al., 2018b) trains a seq2seq model on Wikipedia title mappings, to generate the English entity directly. pivoting to a related high-resource language (HRL) (Zhou et al., 2020) attempts to generalize to unseen LRL mentions through grapheme or phoneme similarity between SL and a related HRL. Specifically, it first finds a related HRL for the SL, and learns a XEL model on the related HRL, using Wikipedia title mappings and anchor text mappings. Then it applies the model on SL mentions. If SL and related HRL share a script, showing grapheme similarity, it treats SL mentions as related HRL mentions at test"
2020.emnlp-main.521,N18-5009,0,0.0339857,"Missing"
2020.emnlp-main.521,2020.tacl-1.8,0,0.156521,"learn a ranking model. Word Translation Based Systems including Pan et al. (2017) and ELISA (Zhang et al., 2018), extract SL – English name translation pairs and apply an unsupervised collective inference approach to link the translated mention. Transliteration Based Systems include Tsai and Roth (2018) and translit (Upadhyay et al., 2018b). translit uses a sequence-to-sequence model and bootstrapping to deal with limited data. It is useful when the English and SL word pairs have similar pronunciation. Pivoting Based Systems including (Rijhwani et al., 2019; Zhou et al., 2019) and PBEL PLUS (Zhou et al., 2020), remove the reliance on SL resources and use a pivot language for candidate generation. Specifically, they train the XEL model on a selected high-resource language, and apply it to SL mentions through language conversion. 4 https://en.wikipedia.org/wiki/Help: Interlanguage_links 6419 2.3 Query Logs Query logs have long been used in many tasks such as across-domain generalization (R¨ud et al., 2011) for NER, and ontological knowledge acquisition(Alfonseca et al., 2010; Pantel et al., 2012). In the English entity linking task, Shen et al. (2015) pointed out that google query logs can be an effi"
2020.emnlp-main.521,D19-6127,0,0.0160199,"rvision from multiple languages to learn a ranking model. Word Translation Based Systems including Pan et al. (2017) and ELISA (Zhang et al., 2018), extract SL – English name translation pairs and apply an unsupervised collective inference approach to link the translated mention. Transliteration Based Systems include Tsai and Roth (2018) and translit (Upadhyay et al., 2018b). translit uses a sequence-to-sequence model and bootstrapping to deal with limited data. It is useful when the English and SL word pairs have similar pronunciation. Pivoting Based Systems including (Rijhwani et al., 2019; Zhou et al., 2019) and PBEL PLUS (Zhou et al., 2020), remove the reliance on SL resources and use a pivot language for candidate generation. Specifically, they train the XEL model on a selected high-resource language, and apply it to SL mentions through language conversion. 4 https://en.wikipedia.org/wiki/Help: Interlanguage_links 6419 2.3 Query Logs Query logs have long been used in many tasks such as across-domain generalization (R¨ud et al., 2011) for NER, and ontological knowledge acquisition(Alfonseca et al., 2010; Pantel et al., 2012). In the English entity linking task, Shen et al. (2015) pointed out tha"
2020.emnlp-main.521,C10-1032,0,\N,Missing
2020.emnlp-main.521,K16-1022,1,\N,Missing
2020.emnlp-main.521,L18-1429,0,\N,Missing
2020.emnlp-main.601,D18-1241,0,0.0283105,"s. 1 Table 1: A polar question with 10 indirect responses, taken from our corpus. paper, we present the first large scale corpus and models for interpreting such indirect answers. Previous attempts to interpret indirect yes/no answers have been small scale and without datadriven techniques (Green and Carberry, 1999; de Marneffe et al., 2010). However, recent success on many language understanding problems (Wang et al., 2019), the impressive generation capabilities of modern dialog systems (Zhang et al., 2019; Adiwardana et al., 2020), as well as the huge interest in yes/no question-answering (Choi et al., 2018; Clark et al., 2019) have created a conducive environment for revisiting this hard task. Introduction Humans produce and interpret complex utterances even in simple scenarios. For example, for the polar (yes/no) question ‘Want to get dinner?’, there are many perfectly natural responses in addition to ‘yes’ and ‘no’, as in Table 1. How should a dialog system interpret these INDIRECT answers? Many can be understood based on the answer text alone, e.g. ‘I would like that’ or ‘I’d rather just go to bed’. For others, the question is crucial, e.g. ‘Dinner would be lovely.’ is a positive reply here,"
2020.emnlp-main.601,N19-1300,0,0.0579156,"ar question with 10 indirect responses, taken from our corpus. paper, we present the first large scale corpus and models for interpreting such indirect answers. Previous attempts to interpret indirect yes/no answers have been small scale and without datadriven techniques (Green and Carberry, 1999; de Marneffe et al., 2010). However, recent success on many language understanding problems (Wang et al., 2019), the impressive generation capabilities of modern dialog systems (Zhang et al., 2019; Adiwardana et al., 2020), as well as the huge interest in yes/no question-answering (Choi et al., 2018; Clark et al., 2019) have created a conducive environment for revisiting this hard task. Introduction Humans produce and interpret complex utterances even in simple scenarios. For example, for the polar (yes/no) question ‘Want to get dinner?’, there are many perfectly natural responses in addition to ‘yes’ and ‘no’, as in Table 1. How should a dialog system interpret these INDIRECT answers? Many can be understood based on the answer text alone, e.g. ‘I would like that’ or ‘I’d rather just go to bed’. For others, the question is crucial, e.g. ‘Dinner would be lovely.’ is a positive reply here, but a negative answe"
2020.emnlp-main.601,W11-0609,0,0.0931706,"Missing"
2020.emnlp-main.601,N19-1423,0,0.0876,"Missing"
2020.emnlp-main.601,J99-3004,0,0.827456,"LQ-YN finetunes a BOOLQ model checkpoint (see Section 5.1) on our corpus, with a new output layer. Since BOOLQ is a Yes/No question answering system, even if developed for a different domain, we expect to learn many semantics of yes/no answers from this data. BERT-MNLI-YN is first fine-tuned on the MNLI corpus, followed by our YN data. This configuration tests if the signals we hoped to capture with the out-of-the-box MNLI model (Section 5.1) can be strengthened by training on our target task. BERT-DIS-YN. As discussed, indirect answers also have discourse relations with the speaker’s intent (Green and Carberry, 1999). We implement this idea via a discourse connective prediction task. Consider again the texts from Section 5.1: The likely connective between Q0 and A is ‘because’ as in ‘I like Italian food [because] I love Tuscan food.’. For Q0 and B, ‘but’ would be more reasonable: ‘I like Italian food [but] I prefer Mexican cuisine.’. We hypothesize that these discourse relations will help learn the yes/no meaning via transfer learning. We use 400K examples (matching the MNLI data size) of explicit connectives and their arguments (a subset of Nie et al. (2019)). We aim to predict the 5 connectives (because"
2020.emnlp-main.601,N18-2017,0,0.0450893,"Missing"
2020.emnlp-main.601,W09-3920,0,0.464633,"Missing"
2020.emnlp-main.601,P10-1018,0,0.0738745,"Missing"
2020.emnlp-main.601,P19-1442,0,0.0381978,"Missing"
2020.emnlp-main.601,S18-2023,0,0.0498751,"Missing"
2020.emnlp-main.601,Q19-1016,0,0.0487234,"Missing"
2020.emnlp-main.601,N18-1101,0,0.133957,"Missing"
2020.emnlp-main.88,S16-1165,0,0.0235264,"hat happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event and a time point (e.g., “won the championshi"
2020.emnlp-main.88,S17-2093,0,0.0585138,"Missing"
2020.emnlp-main.88,P14-2082,0,0.317752,"niently incorporate different modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened"
2020.emnlp-main.88,Q14-1022,0,0.13069,"large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula doe"
2020.emnlp-main.88,P17-2001,0,0.0210933,"how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorm"
2020.emnlp-main.88,D19-1606,1,0.782633,"Missing"
2020.emnlp-main.88,N19-1423,0,0.0968295,"Missing"
2020.emnlp-main.88,E17-2118,0,0.0176129,"lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IA"
2020.emnlp-main.88,D12-1062,1,0.857082,"code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific"
2020.emnlp-main.88,N19-1246,1,0.761929,"ns are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” st"
2020.emnlp-main.88,E17-1108,0,0.0440822,"Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018"
2020.emnlp-main.88,D18-1155,0,0.613672,"long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but s"
2020.emnlp-main.88,K17-1034,0,0.137647,"l many relations that cannot be expressed because the assumption that every event has a time interval is inaccurate: The time scope of an event may be fuzzy, an event can have a nonfactual modality, or events can be repetitive and invoke multiple intervals (see Fig. 5). To better handle these phenomena, we move away from the fixed set of relations used in prior work and instead use natural language to annotate the relationships between events, as described in the next section. 3 Natural Language Annotation of Temporal Relations Motivated by recent works (He et al., 2015; Michael et al., 2017; Levy et al., 2017; Gardner et al., 2019b), we propose using natural language question answering as an annotation for2 We could also include relationships between two fixed time points (e.g., compare 2011-03-24 with 2011-04-05), but these are mostly trivial, so we do not discuss them further. 1160 Confusing relations between the following events Questions that query events in different modes Fuzzy time scope: Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the southwest of England. [Negated] What didn’t the lion do after a large meal? “Follow” is negated: Co"
2020.emnlp-main.88,W17-2341,0,0.0161293,"The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy"
2020.emnlp-main.88,D19-5808,1,0.793705,"is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” started before “flooding.” SNOW, RAINFALL"
2020.emnlp-main.88,2021.ccl-1.108,0,0.26394,"Missing"
2020.emnlp-main.88,S15-2134,0,0.0363119,"2015) and QAMR (Michael et al., 2017), where QA pairs were used as representations for predicate-argument structures. In zeroshot relation extraction (RE), they reduced relation slot filling to an MRC problem so as to build very large distant training data and improve zero-shot learning performance (Levy et al., 2017). However, our work differs from zero-shot RE since it centers around entities, while T ORQUE is about events; the way to ask and answer questions, and the way to design a corresponding crowdsourcing pipeline, are thus significantly different between us. The QA-TempEval workshop (Llorens et al., 2015), desipte its name, is actually not studying temporal relations in an RC setting. The differences between T ORQUE and QA-TempEval 1165 are as follows. First, QA TempEval is an evaluation approach for systems that generate TimeML annotations and actually is not a QA task. For instance, QA TempEval is to evaluate whether a system can answer questions like “I S <E NTITY 1> <R ELATION> <E NTITY 2>?”, where one clearly knows which event that <E NTITY> is referring to and where R ELATION is selected from a predefined label set. Second, QA-TempEval’s annotation relies on the existence of a TimeML cor"
2020.emnlp-main.88,P18-1049,0,0.0823981,"n things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at"
2020.emnlp-main.88,W15-0809,0,0.0260832,") ) E2: [?!""#$"" , ?&'( ] [Hypothetical] If the lion has a large meal, it will sleep for 24 hours. [Repetitive] The lion used to sleep for 24 hours after having large meals. E1 starts with E2 E1 is equal to E2 E1 starts E2 E1 includes E2 E1 ends with E2 [Generic] After having a large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb a"
2020.emnlp-main.88,W16-1007,0,0.0771783,"fferent modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened after he ate his breakfast"
2020.emnlp-main.88,D17-1108,1,0.902955,"?”), or modify it to ask about the start/end time (e.g., “what happened after he started eating his breakfast?” or “what would finish after he ate his breakfast?”). We also instructed workers to make sure that the answers to the new question are different from the original one to avoid trivial modifications (e.g., changing “what happened” to “what occurred”). 4 Data Collection We used Amazon Mechanical Turk to build T ORQUE. Following prior work, we focus on passages that consist of two contiguous sentences, as this is sufficient to capture the vast majority of non-trivial temporal relations (Ning et al., 2017). We took all the articles used in the TempEval3 (TE3) workshop (2.8k articles) (UzZaman et al., 2013) and created a pool of 26k two-sentence passages. Given a random passage from this pool, the annotation process for crowd workers was: 1. Label all the events 2. Repeatedly do the following3 (a) Ask a temporal relation question and point out all the answers from the list of events (b) Modify the temporal relation to create one or more new questions and answer them The annotation guidelines4 and interface5 are public. In the following sections, we further discuss issues of quality control and c"
2020.emnlp-main.88,P18-1212,1,0.933706,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,D19-1642,1,0.780018,"equire temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but suffered from low IAA"
2020.emnlp-main.88,P17-2035,0,0.0725231,"is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler I"
2020.emnlp-main.88,S13-2001,0,0.478562,"ile a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event a"
2020.emnlp-main.88,N18-2026,0,0.0208006,"wer than but already comparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior work"
2020.emnlp-main.88,P18-1122,1,0.934335,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,S07-1014,0,0.484675,"andslide Q6: What happened while a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These rela"
2020.emnlp-main.88,W16-5706,0,0.191361,"Missing"
2020.emnlp-main.88,P18-2124,0,0.0526628,"Missing"
2020.emnlp-main.88,D19-1332,1,0.800439,"omparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle th"
2020.emnlp-main.88,2020.acl-main.678,1,0.892419,"Missing"
2020.emnlp-main.88,D16-1264,0,0.0598335,"in color and contrast questions are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfal"
2020.emnlp-main.88,D15-1076,0,\N,Missing
2020.emnlp-main.88,Q14-1012,0,\N,Missing
2020.emnlp-main.88,S18-1011,0,\N,Missing
2020.findings-emnlp.240,E14-1049,0,0.0251389,"ERT and it is superior to M-BERT even for the high-resource languages that are already in it. The key contributions of our work are (i) E XTEND, a simple yet novel approach to add a new language to M-BERT, (ii) experiments that show E XTEND improves M-BERT for languages that are in M-BERT as well as those that are not, (iii) results showing that E XTEND provides performance and efficiency improvements, in most cases, over B-BERT. 2 work even better than multilingual BERT on these low-resource languages. 3 3.1 Cross-lingual learning has seen increased interest in NLP, with such works as BiCCA (Faruqui and Dyer, 2014), LASER (Artetxe and Schwenk, 2019) and XLM (Conneau and Lample, 2019). Although these models have been successful, they need cross-lingual supervision such as bilingual dictionaries or parallel corpora (Upadhyay et al., 2016), which are particularly challenging to obtain for low-resource languages. Our work differs in that we do not require such supervision. While other approaches like MUSE (Lample et al., 2018) and VecMap (Artetxe et al., 2018) can work without any cross-lingual supervision, M-BERT alone often outperforms these approaches (K et al., 2020). Schuster et al. (2019) has a contin"
2020.findings-emnlp.240,W18-2501,0,0.0397238,"mount of monolingual text data in LORELEI. Whether the languages are in or out of M-BERT, E-MBERT performs better. 5.1 Experimental Settings Dataset. Our text corpus and NER dataset are from LORELEI, preprocessed using the tokenization method from BERT. For zero-shot cross-lingual NER, we evaluate the performance on the whole annotated set; for supervised learning, since we just want an understanding of an upper bound, we apply cross validation to estimate the performance: each fold is evaluated by a model trained on the other folds, and the average F1 is reported. NER Model. We use AllenNLP (Gardner et al., 2018) with a standard Bi-LSTM-CRF (Ma and Hovy, 2016; Lample et al., 2016) framework. The score reported in NER is the F1 score averaged across five runs with different random seeds. BERT training. While extending, we use a batch size of 32 and a learning rate of 2e-5, and train for 500K iterations. Whereas for B-BERT we use a batch size of 32 and learning rate of 1e-4 and train for 2M iterations. We follow BERT’s setting for all other hyperparameters. 5.2 Comparing E-MBERT and M-BERT We compare the cross-lingual zero-shot NER performance of M-BERT and E-MBERT. We train with supervised English NER"
2020.findings-emnlp.240,P16-1157,1,0.841949,"show E XTEND improves M-BERT for languages that are in M-BERT as well as those that are not, (iii) results showing that E XTEND provides performance and efficiency improvements, in most cases, over B-BERT. 2 work even better than multilingual BERT on these low-resource languages. 3 3.1 Cross-lingual learning has seen increased interest in NLP, with such works as BiCCA (Faruqui and Dyer, 2014), LASER (Artetxe and Schwenk, 2019) and XLM (Conneau and Lample, 2019). Although these models have been successful, they need cross-lingual supervision such as bilingual dictionaries or parallel corpora (Upadhyay et al., 2016), which are particularly challenging to obtain for low-resource languages. Our work differs in that we do not require such supervision. While other approaches like MUSE (Lample et al., 2018) and VecMap (Artetxe et al., 2018) can work without any cross-lingual supervision, M-BERT alone often outperforms these approaches (K et al., 2020). Schuster et al. (2019) has a continuing training setting that is similar to ours. However, their approach focuses on comparing between whether B-BERT (JointPair) learns cross-lingual features from overlapping word-pieces, while ours aims at improving M-BERT on"
2020.findings-emnlp.240,D19-1077,0,0.0223767,"ame pretraining objectives as BERT – masked language model and next sentence prediction (Devlin et al., 2019) – and is surprisingly cross-lingual despite not being trained with any cross-lingual objective or aligned data. For cross-lingual transfer, M-BERT is fine-tuned on supervised data in high-resource languages and tested on the target language. 3.2 Related works Background Bilingual BERT (B-BERT) B-BERT is trained in the same way as M-BERT except that it contains only two languages – English and the target language. Recent works have shown the effectiveness of M-BERT (Pires et al., 2019; Wu and Dredze, 2019), and B-BERT (K et al., 2020) on NER and other tasks. 4 Our Method: Extend In this section, we discuss our training protocol E XTEND which incorporates the target language by extending the vocabulary, encoders and decoders, and then continues pre-training. Let M-BERT’s vocabulary be Vmbert and let the extended new vocabulary be Vnew . Throughout the paper, we fix the size of |Vnew |= 30, 000. The training goes as following: 1. Extend the vocabulary, encoder, and decoder to accommodate Vnew . That is, let |Vextra |= |Vnew − Vmbert |, and increase the dimension of size |Vmbert |to |Vmbert |+ |Ve"
2020.findings-emnlp.240,N16-1030,0,\N,Missing
2020.findings-emnlp.240,L16-1521,0,\N,Missing
2020.findings-emnlp.240,N19-1380,0,\N,Missing
2020.findings-emnlp.240,N19-1423,0,\N,Missing
2020.findings-emnlp.317,P18-1163,0,0.0269421,"ally, neural models (Kaushik and Lipton, 2018; Jia and Liang, 2017; Khashabi et al., 2016). Our evaluation settings – hiding one of the three inputs to the MCQA models – are similar to Kaushik and Lipton 2018’s partial input settings which were designed to point out the existence of dataset artifacts in reading comprehension datasets. However, we argue that our results additionally point to a need for more robust training methodologies and propose an improved training approach. Our data augmentation approach builds on recent works (Khashabi et al., 2020; Kobayashi, 2018; Kaushik et al., 2020; Cheng et al., 2018; Andreas, 2020) that try to leverage augmenting training data to improve the performance and/or robustness of models. However most of these works are semi-automatic or require human annotation while our augmentation approach requires no additional annotation. 7 Conclusion We formulated three expectation principles that a MCQA model must satisfy, and devised appropriate settings to evaluate a model against these principles. Our evaluations on a RoBERTa-based model showed that the model fails to satisfy any of our expectations, and exposed its brittleness and reliance on dataset artifacts. To i"
2020.findings-emnlp.317,D19-5815,0,0.0343398,"n example from ARC Easy dataset (Clark et al., 2018) showing the three MCQA task inputs. Introduction Question answering (QA) has been a prevalent format for gauging advances in language understanding. Recent advances in contextual language modelling have led to impressive results on multiple NLP tasks, including on several multiple choice question answering (MCQA, depicted in Fig. 1) datasets, a particularly interesting QA task that provides a flexible space of candidate answers along with a simple evaluation. However, recent work (Khashabi et al., 2016; Jia and Liang, 2017; Si et al., 2019; Gardner et al., 2019, inter alia) has questioned the interpretation of these QA successes as progress in natural language understanding. Indeed, they exhibit, in various task 1 Resources for this work are available at: http://cogcomp.org/page/publication_view/913 settings, the brittleness of neural models to various perturbations. They also show (Kaushik and Lipton, 2018; Gururangan et al., 2018) how models could learn to latch on to spurious correlations in the data to achieve high performance on a given dataset. In this paper we continue this line of work with a careful analysis of the extent to which the top p"
2020.findings-emnlp.317,N18-2017,0,0.0638457,"Missing"
2020.findings-emnlp.317,N18-2072,0,0.0213254,", 2020; Si et al., 2019) or, more generally, neural models (Kaushik and Lipton, 2018; Jia and Liang, 2017; Khashabi et al., 2016). Our evaluation settings – hiding one of the three inputs to the MCQA models – are similar to Kaushik and Lipton 2018’s partial input settings which were designed to point out the existence of dataset artifacts in reading comprehension datasets. However, we argue that our results additionally point to a need for more robust training methodologies and propose an improved training approach. Our data augmentation approach builds on recent works (Khashabi et al., 2020; Kobayashi, 2018; Kaushik et al., 2020; Cheng et al., 2018; Andreas, 2020) that try to leverage augmenting training data to improve the performance and/or robustness of models. However most of these works are semi-automatic or require human annotation while our augmentation approach requires no additional annotation. 7 Conclusion We formulated three expectation principles that a MCQA model must satisfy, and devised appropriate settings to evaluate a model against these principles. Our evaluations on a RoBERTa-based model showed that the model fails to satisfy any of our expectations, and exposed its brittlene"
2020.findings-emnlp.317,D17-1082,0,0.0274519,"n, we define the perturbations we design to evaluate a model against our expectation principles (defined in §1). We then analyze how well the baseline model satisfies these expectations. Monotonicity Expectation: The following setting tests whether a model is fooled by an obviously incorrect option, one with high word overlap between its inputs. • Perturbed Incorrect Option (PIO): The option description for an incorrect option is changed to the question itself and its corresponding context is changed to 10 concatenations of the question.4 Datasets We use the following MCQA datasets: 1. R ACE (Lai et al., 2017): A reading comprehension dataset containing questions from the English exams of middle and high school Chinese students. The context for all options is the same input paragraph. 2. QASC (Khot et al., 2020): An MCQA dataset containing science questions of elementary and middle school level, which require composition of facts using common-sense reasoning. 2 All results are reported on the dev split of the datasets. Sanity Expectation: The following settings test how the model’s performance changes when given an unreasonable input, for which it should not be possible to predict the correct answe"
2020.findings-emnlp.317,2021.ccl-1.108,0,0.10562,"Missing"
2020.findings-emnlp.317,D18-1260,0,0.0172511,"el, which require composition of facts using common-sense reasoning. 2 All results are reported on the dev split of the datasets. Sanity Expectation: The following settings test how the model’s performance changes when given an unreasonable input, for which it should not be possible to predict the correct answer. • No option (NO): The option descriptions for all candidate options is changed to empty, “<s&gt;&quot;. • No question (NQ): The question (for all its candidate options O) is changed to empty , “<s&gt;&quot;. 3 Containing questions from the ARC datasets (Clark et al., 2018), NY Regents exams and OBQA(Mihaylov et al., 2018). 4 To approximately simulate a typical context’s length. 3548 Reading Expectation: The following setting tests how crucial the context is for the model to correctly answer the questions. • No context (NC): The contexts for all candidate options is changed to empty, “<s&gt;&quot;. Baseline model performance Table 1 shows that the model achieves impressive accuracy on all three dataset; R ACE (84.8), QASC (85.2), and A RISTO (78.3), which suggests that the model should satisfy the expectations laid out for a good MCQA model. Evaluating expectations When evaluating the model by modifying an incorrect op"
2020.findings-emnlp.317,D19-1250,0,0.0291545,"s surprisingly well. For example, in A RISTO, removal of the question (NQ) only leads to a performance drop from 78.3 → 55.3, and removal of the options (NO), from 78.3 → 46.8. This suggests that the datasets contain unwanted biases that the model relies on to answer correctly. This shows that the baseline model fails to satisfy the Sanity Expectation. The model achieves reasonable performance on the removal of the contexts; thus failing our Reading Expectation, e.g., performance only drops from 78.3 → 63.8 in A RISTO. To achieve this performance the model must rely on its inherent knowledge (Petroni et al., 2019) or, more likely, on dataset artifacts as suggested previously. Eval. Setting A RISTO R ACE QASC Original (↑) 78.3 84.8 85.2 Perturbed Incorrect Option (↑) 25.4 45.8 7.9 No Option (↓) No Question (↓) No Context (↓) 46.8 55.3 63.8 − 62.8 49.1 50.2 34.3 55.8 Table 1: Accuracy of the respective RoBERTa models on R ACE, A RISTO and QASC datasets for the different evaluation settings detailed in Section A. The No Option setting is not applicable for R ACE as all options would have the same inputs. The arrows denote the expected performance where ↑ denotes higher is better and ↓ denotes that lower p"
2020.findings-emnlp.439,N18-1202,0,0.309112,"ge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results. 1 1 Introduction The success of contextualized pretrained Language Models like BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) on tasks like Question Answering and Natural Language Inference, has led to speculation that they are good at Common Sense Reasoning (CSR). On one hand, recent work has approached this question by measuring the ability of LMs to answer questions about physical common sense (Bisk et al., 2020) (”How to separate egg whites from yolks?”), temporal reasoning (Zhou et al., 2020) (”How long does a basketball game take?”), and numerical common sense (Lin et al., 2020). On the other hand, after realizing some high-level reasoning skills like this may be difficult to learn from a language-modeling obj"
2020.findings-emnlp.439,2020.acl-main.420,0,0.0211161,"s heavy. • PRICE: The X is expensive. • LENGTH: The X is big. and use the CLS token emebedding (for BERT) or final state embedding (for ELMo) as the input representation. For LENGTH, We use ”big” instead of ”long”, since LENGTH measurements in DoQ can be widths or heights as well. Variations of these templates with different adjectives and sentence structures (e.g. ”The X is small.” or ”What is the length of X?” for LENGTH) led to very similar performance in our evaluations. Probes We use linear probes in all cases following many previous probing work (Shi et al., 2016; Ettinger et al., 2016; Pimentel et al., 2020) since we want to use a simple probe to find easily accessible information in a representation. Hewitt and Liang (2019) also demonstrates that linear probes achieve relatively high selectivity compared to non-linear ones like MLP. We experiment with two different approaches for predicting scales: Regression (rgr) For the point estimate, we use a standard Linear Regression model trained 2 We use Word2Vec embeddings of dimension size 500 trained on Wikipedia, BERT-Base (L=12, H=768, A=12, Total Parameters=110M) trained on Wikipedia+Books and ELMoSmall (LSTM Hidden Size=1024, Output Size=128, #Hi"
2020.findings-emnlp.439,D16-1159,0,0.0151588,"the following templates: • MASS: The X is heavy. • PRICE: The X is expensive. • LENGTH: The X is big. and use the CLS token emebedding (for BERT) or final state embedding (for ELMo) as the input representation. For LENGTH, We use ”big” instead of ”long”, since LENGTH measurements in DoQ can be widths or heights as well. Variations of these templates with different adjectives and sentence structures (e.g. ”The X is small.” or ”What is the length of X?” for LENGTH) led to very similar performance in our evaluations. Probes We use linear probes in all cases following many previous probing work (Shi et al., 2016; Ettinger et al., 2016; Pimentel et al., 2020) since we want to use a simple probe to find easily accessible information in a representation. Hewitt and Liang (2019) also demonstrates that linear probes achieve relatively high selectivity compared to non-linear ones like MLP. We experiment with two different approaches for predicting scales: Regression (rgr) For the point estimate, we use a standard Linear Regression model trained 2 We use Word2Vec embeddings of dimension size 500 trained on Wikipedia, BERT-Base (L=12, H=768, A=12, Total Parameters=110M) trained on Wikipedia+Books and ELMoSma"
2020.findings-emnlp.439,D19-1534,0,0.180449,"a combination of an exponent and mantissa (for example 314.1 is represented as 3141[EXP]2 where [EXP] is a new token introduced into the vocabulary). This enables the BERT model to more easily associate objects in the sentence directly with the magnitude expressed in the exponent, ignoring the relatively insignificant mantissa. This model converged to a similar loss on the original BERT Masked LM+NSP pre-training task and a standard suite of NLP tasks (See Appendix) as BERT-base, demonstrating that it was not over-specialized for numerical reasoning tasks. Numeracy through Scientific Notation Wallace et al. (2019) showed that BERT and ELMo had a limited amount of numeracy or numerical reasoning ability, when restricted to numbers of small magnitude. Intuitively, it seems that significant model capacity is expended in parsing the natural representation of numbers as Arabic numerals, where higher and lower order digits are given equal prominence. As further evidence of this, it is shown in Appendix B of Wallace et al. (2019) that the simple intervention of left-padding numbers in ELMo instead of the default right-padding used in Char-CNNs greatly improves accuracy on these 5 Evaluation We offer the follo"
2020.findings-emnlp.439,P18-2102,0,0.0203471,"can refer to either a bird or a piece of construction equipment, only the former is relevant in the animal domain, giving the model a simpler distribution of masses to predict. Note that, despite significant differences in the raw numbers for each scale (mass/length/price), the relative behavior of encoders, metrics and probes are the same, indicating that our conclusions are broadly applicable. Transfer experiments On the F&C relative comparison task (Table 2), rgr+NumBERT performed best, approaching the performance of using DoQ as an oracle, though short of specialized models for this task (Yang et al., 2018). Scalar probes trained with mcc perform poorly, possibly because a finer-grained model of predicted distribution is not useful for the 3-class comparative task. On the Amazon price dataset (Table 3) which is a full distribution prediction task, mcc+NumBERT did best on all three metrics. On both zero-shot transfer tasks, NumBERT was the best encoder on all configurations of metric/objective, suggesting that manipulating numeric representations can signifidev mcc rgr word2vec ELMo BERT NumBERT DoQ [Elazar et. al. 2019] Yang et. al. ’18 .40 .47 .48 .51 - .73 .71 .71 .77 .78 .86 test mcc rgr .38"
2020.findings-emnlp.439,2020.acl-main.678,1,0.633331,"e, and show that a simple method of canonicalizing numbers can have a significant effect on the results. 1 1 Introduction The success of contextualized pretrained Language Models like BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) on tasks like Question Answering and Natural Language Inference, has led to speculation that they are good at Common Sense Reasoning (CSR). On one hand, recent work has approached this question by measuring the ability of LMs to answer questions about physical common sense (Bisk et al., 2020) (”How to separate egg whites from yolks?”), temporal reasoning (Zhou et al., 2020) (”How long does a basketball game take?”), and numerical common sense (Lin et al., 2020). On the other hand, after realizing some high-level reasoning skills like this may be difficult to learn from a language-modeling objective only, (Geva et al., 2020) injects numerical reasoning skills into LMs by additional pretraining on automatically generated data. All of these skills are prerequisites for CSR. ∗ Both authors contributed equally. Work done during an internship at Google Research. 1 Code and models are available at https://github. com/google-research-datasets/numbert. † Figure 1: Scalar"
2020.findings-emnlp.64,2020.repl4nlp-1.18,0,0.211905,"nsformerbased pre-trained models (Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019; Clark et al., 2020). However, these models often consume considerable storage, memory bandwidth, and computational resource. To reduce the model size and increase the inference throughput, compression techniques such as knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Sun et al., 2020) and weight pruning (Guo † 80 70 20 Introduction ∗ MNLI-mm Work done as part of the Google AI Residency. Work done at Google Research. et al., 2019; Wang et al., 2019; Gordon et al., 2020; Sanh et al., 2020) have recently been developed. Knowledge distillation methods require the specification of a student network with a smaller architecture, which often has to be identified by a tedious sequence process of trial-and-error based decisions. By comparison, the iterative pruning methods gradually prune the redundant model weights or layers from the full-size model, and provide a full picture of the trade-off between the task performance and the model size with a single training process, as illustrated in Figure 1. This allows the iterative pruning methods to easily determine the"
2020.findings-emnlp.64,P84-1044,0,0.510935,"Missing"
2020.findings-emnlp.64,P19-1356,0,0.0342375,"Missing"
2020.findings-emnlp.64,D19-1445,0,0.0254532,"Missing"
2020.findings-emnlp.64,N19-1112,0,0.354188,"0, pages 719–730 c November 16 - 20, 2020. 2020 Association for Computational Linguistics model tends to maintain the same architecture as the original model despite the reduced parameter count, which does not practically lead to an improvement in inference latency (Wen et al., 2016). This leads to a question: is it possible to perform more structured pruning on a Transformer model to modify its model architecture (e.g., reducing width and depth)? To this end, we notice that many previous works have suggested that the learned Transformer models often have much redundancy (Tenney et al., 2019; Liu et al., 2019a; Jawahar et al., 2019; Kovaleva et al., 2019). For example, Michel et al. (2019); Voita et al. (2019) found that most of the attention heads in a Transformer model can be removed without significantly impacting accuracy, and Tenney et al. (2019) found that while the earlier layers and the later layers in a BERT model play clear roles in extracting either low-level or task-specific linguistic knowledge, the roles of the intermediate layers are less important. These observations have motivated the idea that Transformer models may exhibit considerable structural redundancy - i.e., some layers c"
2020.findings-emnlp.64,2021.ccl-1.108,0,0.179019,"Missing"
2020.findings-emnlp.64,P18-1117,0,0.0941626,"use a structured pruning. 4.3 Analysis In this section, we investigate the contribution of: (1) single attention head pruning, and (2) split pruning for attention heads and FFN sub-layers. Single Head Attention Pruning Multi-head self-attention is a key component of Transformer, where each attention head potentially focuses on different parts of the inputs. The analysis of multi-head attention and its importance is challenging. Previous analysis of multihead attention considered the average of attention weights over all heads at given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), or explicitly takes into account the varying importance of different heads (Voita et al., 2019). Michel et al. (2019) has proved that attention heads can be removed without significantly impacting performance, but they mainly focus on machine translation and NLI. To understand whether and to what extent attention heads play consistent and interpretable roles when trained on different downstream tasks, we 726 Evolution of Accuracy on SST attention pruning (MHA), separate attention/FFN pruning, and whole layer pruning respectively in Figure 5. We find that MHA and separate"
2020.findings-emnlp.64,P19-1580,0,0.265299,"s to maintain the same architecture as the original model despite the reduced parameter count, which does not practically lead to an improvement in inference latency (Wen et al., 2016). This leads to a question: is it possible to perform more structured pruning on a Transformer model to modify its model architecture (e.g., reducing width and depth)? To this end, we notice that many previous works have suggested that the learned Transformer models often have much redundancy (Tenney et al., 2019; Liu et al., 2019a; Jawahar et al., 2019; Kovaleva et al., 2019). For example, Michel et al. (2019); Voita et al. (2019) found that most of the attention heads in a Transformer model can be removed without significantly impacting accuracy, and Tenney et al. (2019) found that while the earlier layers and the later layers in a BERT model play clear roles in extracting either low-level or task-specific linguistic knowledge, the roles of the intermediate layers are less important. These observations have motivated the idea that Transformer models may exhibit considerable structural redundancy - i.e., some layers can be removed during the training without harming the final performance. In this paper, we propose a st"
2020.findings-emnlp.64,D13-1170,0,0.00306016,"ocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it dost not improve the scores (Yang et al., 2019; Liu et al., 2019b). For fine-tuning tasks, we focus on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) in the main text since it is a thoroughly studied setting in many pruning/distillation work, thereby allowing comprehensive comparison. We conduct experiments on the subset of GLUE , classified into three categories: 1. Sentiment analysis: Stanford Sentiment Treebank (SST) (Socher et al., 2013); 2. Paraphrasing: Quora Question Pairs (QQP) (Chen et al., 2018) and Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005); The detailed description of downstream tasks could be found in Appendix. The reason for choosing this subset is that we found the variance in performance for those tasks lower than the other GLUE tasks. We used the hyperparameters from Clark et al. (2020) for the most part. Since we run the training iteratively, we set train epoch as 1 for most of the task but 3 for task MRPC consider that the size of the datasets is much smaller than other tasks. For"
2020.findings-emnlp.64,W18-5446,0,0.05872,"Missing"
2020.findings-emnlp.64,2020.emnlp-main.496,0,0.0462924,"Missing"
2020.findings-emnlp.64,D19-1441,0,0.119673,"Missing"
2020.findings-emnlp.64,2020.acl-main.195,0,0.0511093,"t each single data point, whereas iterative pruning methods can produce continuous curves at once. Natural Language Processing (NLP) has recently achieved great success by using the Transformerbased pre-trained models (Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019; Clark et al., 2020). However, these models often consume considerable storage, memory bandwidth, and computational resource. To reduce the model size and increase the inference throughput, compression techniques such as knowledge distillation (Sanh et al., 2019; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Sun et al., 2020) and weight pruning (Guo † 80 70 20 Introduction ∗ MNLI-mm Work done as part of the Google AI Residency. Work done at Google Research. et al., 2019; Wang et al., 2019; Gordon et al., 2020; Sanh et al., 2020) have recently been developed. Knowledge distillation methods require the specification of a student network with a smaller architecture, which often has to be identified by a tedious sequence process of trial-and-error based decisions. By comparison, the iterative pruning methods gradually prune the redundant model weights or layers from the full-size model, and provide a full picture of t"
2020.findings-emnlp.64,W18-6304,0,0.0521063,"Missing"
2020.findings-emnlp.64,P19-1452,0,0.0668755,"Missing"
2020.lrec-1.288,W18-1406,0,0.0218585,"and facilitates dealing with unseen subjects, objects and relations. Keywords: Knowledge Discovery/Representation Figure 1: woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model. 1. Introduction Humans are able to do common sense reasoning across a variety of modalities – textual, visual and for a variety of tasks – reasoning, locating, navigation. Several such tasks require spatial knowledge understanding and reasoning (Kordjamshidi et al., 2010), (Kordjamshidi et al., 2011), (Johnson et al., 2017), (Wang et al., 2017), (Baldridge et al., 2018). Although, there has been several recent works on common-sense reasoning (Speer and Havasi, 2012), (Emami et al., 2018), (Sap et al., 2018), (Storks et al., 2019), progress on spatial understanding and common-sense is rather limited. Prior work has either not been spatially focused (Lin and Parikh, 2016), (Xu et al., 2015),(Antol et al., 2015) or very restrictive in the class of spatial relations they handle (Xu et al., 2017), (Kordjamshidi et al., 2017). Recently, (Collell et al., 2018) presented the task of predicting an object’s location and size in an image given the subject’s bounding bo"
2020.lrec-1.288,Q18-1010,0,0.0177179,"air (b) cat ON chair Figure 2: Examples of explicit spatial relations from Visual Genome of (cat, ?, chair) h = [σ(Ws [ws ; Ps ] + bs ); σ(Wo [wo ; Po ] + bo )] pˆ = sof tmax(Wh h + bh ) where σ is the activation function ReLU. Feed Forward Network + Image Embeddings (FF+I). In addition to the text information and position information of the subject and object, we use pre-trained visual embeddings to represent the image information Is and Io . For simplicity, we use is and io to represent the visual embeddings for the subject and the object. The visual embeddings of the words are provided by (Collell and Moens, 2018) from a VGG128 network (Chatfield et al., 2014) pre-trained on Imagenet and fine-tuned on Visual Genome (Krishna et al., 2016). The hidden representation in the FF + Image Embeddings are as following: h = [σ(Ws [ws ; Ps ; is ] + bs ); σ(Wo [wo ; Po ; io ] + bo )] 2.3. The Language Model BERT. We use BERT (Devlin et al., 2018) as language model to predict the most likely spatial relation by masking the relation and providing ”subject [MASK] object” as input and running the beam search to obtain top 20 predictions. Fine-tuned BERT (f-BERT). We fine-tune BERT for the Visual Genome datatset by col"
2020.lrec-1.288,D18-1220,0,0.0237476,"woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model. 1. Introduction Humans are able to do common sense reasoning across a variety of modalities – textual, visual and for a variety of tasks – reasoning, locating, navigation. Several such tasks require spatial knowledge understanding and reasoning (Kordjamshidi et al., 2010), (Kordjamshidi et al., 2011), (Johnson et al., 2017), (Wang et al., 2017), (Baldridge et al., 2018). Although, there has been several recent works on common-sense reasoning (Speer and Havasi, 2012), (Emami et al., 2018), (Sap et al., 2018), (Storks et al., 2019), progress on spatial understanding and common-sense is rather limited. Prior work has either not been spatially focused (Lin and Parikh, 2016), (Xu et al., 2015),(Antol et al., 2015) or very restrictive in the class of spatial relations they handle (Xu et al., 2017), (Kordjamshidi et al., 2017). Recently, (Collell et al., 2018) presented the task of predicting an object’s location and size in an image given the subject’s bounding box and the spatial relation between them. In this paper, we address the problem of understanding spatial relations. We sp"
2020.lrec-1.288,kordjamshidi-etal-2010-spatial,0,0.578829,"Missing"
2020.lrec-1.288,W17-4306,0,0.0198428,"l knowledge understanding and reasoning (Kordjamshidi et al., 2010), (Kordjamshidi et al., 2011), (Johnson et al., 2017), (Wang et al., 2017), (Baldridge et al., 2018). Although, there has been several recent works on common-sense reasoning (Speer and Havasi, 2012), (Emami et al., 2018), (Sap et al., 2018), (Storks et al., 2019), progress on spatial understanding and common-sense is rather limited. Prior work has either not been spatially focused (Lin and Parikh, 2016), (Xu et al., 2015),(Antol et al., 2015) or very restrictive in the class of spatial relations they handle (Xu et al., 2017), (Kordjamshidi et al., 2017). Recently, (Collell et al., 2018) presented the task of predicting an object’s location and size in an image given the subject’s bounding box and the spatial relation between them. In this paper, we address the problem of understanding spatial relations. We specifically want to infer the spatial relationship between two entities given an image involving them. Spatial relations can either be – explicit (spatial prepositions such as on, above, under) or implicit (intrinsic spatial concepts associated with actions – sleeping, sitting, flying). We take as input an image and the bounding boxes of"
2020.lrec-1.288,D14-1162,0,0.0913183,"Missing"
2020.lrec-1.288,speer-havasi-2012-representing,0,0.0425986,"/Representation Figure 1: woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model. 1. Introduction Humans are able to do common sense reasoning across a variety of modalities – textual, visual and for a variety of tasks – reasoning, locating, navigation. Several such tasks require spatial knowledge understanding and reasoning (Kordjamshidi et al., 2010), (Kordjamshidi et al., 2011), (Johnson et al., 2017), (Wang et al., 2017), (Baldridge et al., 2018). Although, there has been several recent works on common-sense reasoning (Speer and Havasi, 2012), (Emami et al., 2018), (Sap et al., 2018), (Storks et al., 2019), progress on spatial understanding and common-sense is rather limited. Prior work has either not been spatially focused (Lin and Parikh, 2016), (Xu et al., 2015),(Antol et al., 2015) or very restrictive in the class of spatial relations they handle (Xu et al., 2017), (Kordjamshidi et al., 2017). Recently, (Collell et al., 2018) presented the task of predicting an object’s location and size in an image given the subject’s bounding box and the spatial relation between them. In this paper, we address the problem of understanding sp"
2020.lrec-1.288,P17-1086,0,0.0215062,"ccuracy and coverage and facilitates dealing with unseen subjects, objects and relations. Keywords: Knowledge Discovery/Representation Figure 1: woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model. 1. Introduction Humans are able to do common sense reasoning across a variety of modalities – textual, visual and for a variety of tasks – reasoning, locating, navigation. Several such tasks require spatial knowledge understanding and reasoning (Kordjamshidi et al., 2010), (Kordjamshidi et al., 2011), (Johnson et al., 2017), (Wang et al., 2017), (Baldridge et al., 2018). Although, there has been several recent works on common-sense reasoning (Speer and Havasi, 2012), (Emami et al., 2018), (Sap et al., 2018), (Storks et al., 2019), progress on spatial understanding and common-sense is rather limited. Prior work has either not been spatially focused (Lin and Parikh, 2016), (Xu et al., 2015),(Antol et al., 2015) or very restrictive in the class of spatial relations they handle (Xu et al., 2017), (Kordjamshidi et al., 2017). Recently, (Collell et al., 2018) presented the task of predicting an object’s location and size in an image given"
2020.lrec-1.717,W13-2322,1,0.75012,"retability and generalizability (Hu et al., 2017). Symbolic representations that can support reasoning capabilities are shown to have a critical role in improving both of the above-mentioned aspects (Goldman et al., 2018; Krishnaswamy et al., 2019; Suhr et al., 2017b). A robust intermediate representation can help an off-the-shelf planner to ground a symbolic representation of the spatial concepts to a final executable output. This representation should be as general and domain independent as possible to eliminate the need to retrain an automatic annotation system for a new domain. Using AMR (Banarescu et al., 2013) as a stepping stone to our spatial configurations helps to ensure generalizability and domain independence. Our goal is to identify 5855 Figure 1: An instance of the collaborative building task. The last instruction was : Now add 3 red bricks on the diagonal adjacent to the yellow bricks. Figure 2: An example from the NLVR corpus that demonstrates spatial focus shift from the black item to the yellow item. various spatial semantic aspects that are required by a concrete meaning representation for downstream tasks. 2. Expanded AMRs In this section, we briefly describe the spatial AMR extension"
2020.lrec-1.717,J81-4005,0,0.621277,"Missing"
2020.lrec-1.717,P02-1031,1,0.287852,"ships and spatial object properties from the AMR and organize them into an easily interpretable format that maintains the complex relationships and nesting. The corpus that we describe here 1 includes annotation on over 5000 dialogue sentences (170 full dialogues) that discuss collaborative construction events in the Minecraft dataset (Narayan-Chen et al., 2019). We have an additional 7600 annotated automatically-generated sentences representing builder actions, giving a total of 12,600 spatial AMRs. The AMR expansion involves the addition of 150+ new or updated PropBank (Palmer et al., 2005; Gildea and Palmer, 2002) rolesets as well as a dozen general semantic frames, roles, and concepts that signal spatial relation1 https://github.com/jbonn/CwC-Minecraft-AMR ships not previously covered by AMR. Using rolesets to annotate spatial relations allows us to disambiguate senses and to group together etymologically-related relations from different parts of speech within those senses. For example, the roleset align-02 includes aliases align-v, alignedj, line-up-v, and in-line-p, and down-03 includes aliases down-p, down-r, downward-r, etc.. Prepositional and adverbial aliases are new for PropBank and AMR. The ro"
2020.lrec-1.717,P18-1168,0,0.0144952,"d spatial AMR framework (Bonn et al., 2019). Our spatial configuration scheme takes the spatial roles and relations identified in spatial AMR graphs and converts them into an easy to read general framework, resulting in a maximally expressive spatial meaning representation language. Recent research shows the limitations of training monolithic deep learning models in two important aspects of interpretability and generalizability (Hu et al., 2017). Symbolic representations that can support reasoning capabilities are shown to have a critical role in improving both of the above-mentioned aspects (Goldman et al., 2018; Krishnaswamy et al., 2019; Suhr et al., 2017b). A robust intermediate representation can help an off-the-shelf planner to ground a symbolic representation of the spatial concepts to a final executable output. This representation should be as general and domain independent as possible to eliminate the need to retrain an automatic annotation system for a new domain. Using AMR (Banarescu et al., 2013) as a stepping stone to our spatial configurations helps to ensure generalizability and domain independence. Our goal is to identify 5855 Figure 1: An instance of the collaborative building task. T"
2020.lrec-1.717,S13-2044,1,0.824048,"ration, we present an example from the NVLR dataset (Figure 10), which is a TRUE statement: There is a blue square closely touching the bottom of a box. We also present another example from BLOCKS (Figure 12): Move the Mercedes block to the right of the Nvidia block. The annotation is shown in Table 7 and the graphical representation with aligned AMR in Figure 13. 6. Related Representations The presented scheme is related to previous spatial annotation schemes such as ISO-space (Pustejovsky et al., 2011; Pustejovsky et al., 2015) and SpRL (Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013; Kordjamshidi et al., 2017). SpRL is based on holistic spatial semantics (Zlatev, 2003) and takes both cognitivelinguistic elements and spatial calculi models into account to bridge natural language and formal spatial representations that facilitate reasoning. The result is a light-weight spatial ontology compared to more extensive, linguistically motivated spatial ontologies such as GUM (Bateman et al., 2010; Hois and Kutz, 2008) or type theory motivated (Dobnik and Cooper, 2017) ones. ISO-Space scheme consid5861 id e1 e2 Spatial Entities properties head {name = span} {brand = M ercedes} blo"
2020.lrec-1.717,kordjamshidi-etal-2010-spatial,1,0.89171,"Missing"
2020.lrec-1.717,S12-1048,1,0.86488,"Missing"
2020.lrec-1.717,W18-4704,0,0.205805,"ion or orientation of the current object’s spatial description, e.g., my left and your right. Absolute FoR (geocentered) is a fixed FoR, e.g., North. As an example of a configuration having multiple FoRs with different values, consider : I walked from the center of the park to the left of the statue. In the path for this configuration, the first landmark has an intrinsic FoR while the second landmark has a relative FoR. In contrast to the FoR, the viewer is unique in a single configuration and is represented as a value v ∈ {f irst−person, second−person, third−person} (Tenbrink and Kuhn, 2011; Lee et al., 2018). A configuration 5857 Dataset BLOCKS GQA NLVR SpaceEval SpRL Minecraft Example Sentence Texaco should line up on the top right corner of BMW in the middle of Adidas and HP (1) Are there any cups to the left of the tray that is on top of the table? (2) There is a black square touching the wall with a blue square right on top of it. (3) While driving along the village’s main road the GPS showed us the direction right ahead, and after two minutes, just few hundred meters from the end of the village, we reached the spot. (4) a white bungalow with big windows, stairs to the left and the right, a n"
2020.lrec-1.717,mani-etal-2008-spatialml,0,0.0601948,"2 >, &lt; l3, e1 > &lt;s2,in between > NULL NULL &lt; l2, relative > &lt; l3, relative > first-person &lt;topological, EC> Table 4: Relational representation of spatial entities and configurations in the sentence: Place an orange block to the right of the base red block with two spaces in between. 3.3. Qualitative Types Fine-grained spatial relation semantics can be represented using specialized linguistically motivated ontologies such as General Upper Model (Bateman et al., 2010). Another approach has been mapping to formal spatial knowledge representation models that are independent from natural language (Mani et al., 2008; Kordjamshidi et al., 2010; Pustejovsky et al., 2011). The latter provides a more tractable formalism for spatial reasoning. The spatial formal models are divided into three categories: topological, directional and distal, and for each category specific formalisms have been invented (Wallgr¨un et al., 2007; Liu et al., 2009). We use these qualitative types. Directional systems can be relative or absolute, distal systems can be qualitative or quantitative and topological models can follow 5858 Figure 3: Graphical Representation of Configuration 1 of Table 3 with aligned AMR : Move the large re"
2020.lrec-1.717,P19-1537,0,0.0942124,"ct Meaning Representation (AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole. Keywords: Semantics, Knowledge Discovery/Representation 1. Introduction Spatial reasoning is necessary for many tasks. For example, consider the collaborative building task (Narayan-Chen et al., 2019) where a human architect (knows the target structure but cannot manipulate blocks) issues commands to a builder (who does not know the target structure but can place and manipulate blocks) with the aim of creating a complex target structure in a grounded environment (Minecraft). This task is challenging from a spatial reasoning perspective because the architect does not issue commands in terms of the unit blocks that the builder uses, but complex shapes and their spatial aspects. A single instruction can convey information about complex spatial configurations of several objects and describe ne"
2020.lrec-1.717,J05-1004,1,0.277483,"the spatial relationships and spatial object properties from the AMR and organize them into an easily interpretable format that maintains the complex relationships and nesting. The corpus that we describe here 1 includes annotation on over 5000 dialogue sentences (170 full dialogues) that discuss collaborative construction events in the Minecraft dataset (Narayan-Chen et al., 2019). We have an additional 7600 annotated automatically-generated sentences representing builder actions, giving a total of 12,600 spatial AMRs. The AMR expansion involves the addition of 150+ new or updated PropBank (Palmer et al., 2005; Gildea and Palmer, 2002) rolesets as well as a dozen general semantic frames, roles, and concepts that signal spatial relation1 https://github.com/jbonn/CwC-Minecraft-AMR ships not previously covered by AMR. Using rolesets to annotate spatial relations allows us to disambiguate senses and to group together etymologically-related relations from different parts of speech within those senses. For example, the roleset align-02 includes aliases align-v, alignedj, line-up-v, and in-line-p, and down-03 includes aliases down-p, down-r, downward-r, etc.. Prepositional and adverbial aliases are new fo"
2020.lrec-1.717,P17-2034,0,0.338907,"sented by existing spatial representations( for a more detailed comparison with previous schemes refer to Section 6). For instance, the spatial representation in (Kordjamshidi et al., 2010) does not capture the fine-grained semantics of the object properties and does not distinguish between the frame of reference and the perspective. The scheme of (Pustejovsky et al., 2011) does not handle complex landmarks in the first example with the sequence of spatial properties. We observe the ubiquity of these hard spatial descriptions across different datasets. Here is another such instance from NLVR (Suhr et al., 2017a) depicting spatial focus shift: There is a box with a black item between 2 items of the same color and no item on top of that. We propose an integrated view called SPATIAL CONFIGURATION that captures extensive spatial semantics that is necessary for reasoning. To develop an automatic builder, in the case of the collaborative construction task, we need an explicit representation of all involved spatial components and their spatial relations in the instruction. One of our key contributions is to represent this information compactly as a part of the configuration. Further, we can represent spat"
2020.lrec-1.717,P19-1009,0,0.0468791,"Missing"
2020.nlposs-1.17,D19-3009,0,0.0212728,"on machine translation datasets. Whereas SacreBLEU is mainly for evaluating machine translation models with BLEU, our library focuses on summarization A JSONL file contains one serialized JSON per line. 123 3 https://duc.nist.gov/ https://tac.nist.gov/ 5 http://multiling.iit.demokritos.gr/ 4 and includes a large number of evaluation metrics. Further, SacreROUGE also provides a framework for developing and evaluating new metrics. One goal of SacreROUGE is to standardize the implementation and meta-evaluation of metrics. The Perl-based Asiya (Gim´enez and M`arquez, 2010) and Python-based EASSE (Alva-Manchego et al., 2019) have been developed with similar goals for machine translation and text simplification, respectively. Much of the design of SacreROUGE was inspired by AllenNLP (Gardner et al., 2018), a library built on PyTorch (Paszke et al., 2017) for developing deep learning models. AllenNLP provides useful abstractions over different models and neural network modules that allows for the sharing of boilerplate code so developers can quickly create and train new machine learning models. SacreROUGE provides similar abstractions for evaluation metrics. Two concurrent works set out to achieve similar goals to"
2020.nlposs-1.17,P18-1060,0,0.0171633,"arguments. Then, SacreROUGE will load and preprocess the respective dataset. For datasets which are publicly available, the scripts will download the data automatically. However, many summarization datasets are licensed, so the corresponding preprocessing scripts require paths to the original data supplied to the command. The datasets which are currently supported by SacreROUGE are the Document Understanding Conference from 2001 to 2007,3 Text Analysis Conference from 2008 to 2011,4 the MultiLing 2011, 2013, 2015, 2017, and 2019 Workshops,5 and the CNN/DailyMail dataset judgments provided by Chaganty et al. (2018) and Fabbri et al. (2020). We intend to add more datasets as they become available, and other researchers can easily incorporate their own datasets to our library by serializing the data into the shared format. 5 Related Work The idea for SacreROUGE came from the SacreBLEU (Post, 2018) library. SacreBLEU was developed to standardize and simplify calculating BLEU (Papineni et al., 2002) for machine translation. Like SacreROUGE, it provides a simple command-line interface to download and evaluate on common machine translation datasets. Whereas SacreBLEU is mainly for evaluating machine translati"
2020.nlposs-1.17,W14-3348,0,0.031507,"(Tratz and Hovy, 2008) The two most common use cases of an evaluation metric are to evaluate a summarization system and to evaluate a metric itself by calculating its correlation to human judgments. Since all of the metrics in SacreROUGE implement a common interface, the code for these procedures is shared, so developers of new metrics do not need to rewrite the code to implement these procedures. This logic is exposed through evaluate, score, and correlate, which are subcommands of sacrerouge, the entry point for the library’s command-line interface. • BLEURT (Sellam et al., 2020) • METEOR (Denkowski and Lavie, 2014) • MeMoG (Giannakopoulos and Karkaletsis, 2010) • MoverScore (Zhao et al., 2019) • NPowER (Giannakopoulos and Karkaletsis, 2013) • Pyramid Score (Nenkova and Passonneau, 2004) • PyrEval (Gao et al., 2019) 121 Evaluating Systems and Metrics The evaluate Subcommand The purpose of the evaluate subcommand is to calculate a metric’s score for one summarization system on The correlate Subcommand After all of the summaries have been scored using the score subcommand, the second step of the meta-evaluation of a metric is to calculate the correlation of those scores to human judgments. This step is don"
2020.nlposs-1.17,2020.emnlp-main.751,0,0.0615103,"Missing"
2020.nlposs-1.17,K19-1038,0,0.0169674,"the metrics in SacreROUGE implement a common interface, the code for these procedures is shared, so developers of new metrics do not need to rewrite the code to implement these procedures. This logic is exposed through evaluate, score, and correlate, which are subcommands of sacrerouge, the entry point for the library’s command-line interface. • BLEURT (Sellam et al., 2020) • METEOR (Denkowski and Lavie, 2014) • MeMoG (Giannakopoulos and Karkaletsis, 2010) • MoverScore (Zhao et al., 2019) • NPowER (Giannakopoulos and Karkaletsis, 2013) • Pyramid Score (Nenkova and Passonneau, 2004) • PyrEval (Gao et al., 2019) 121 Evaluating Systems and Metrics The evaluate Subcommand The purpose of the evaluate subcommand is to calculate a metric’s score for one summarization system on The correlate Subcommand After all of the summaries have been scored using the score subcommand, the second step of the meta-evaluation of a metric is to calculate the correlation of those scores to human judgments. This step is done via the correlate subcommand. one dataset, which most typically occurs when researchers compare their system’s performance to others’. The evaluate subcommand accepts a specific metric and an input file"
2020.nlposs-1.17,W18-2501,0,0.159055,"is ongoing and open to contributions from the community. 1 Introduction Evaluating models is a critical step of the machine learning workflow. However, unlike classificationbased tasks, evaluating models which generate text is difficult and is a research area on its own. The basic workflow for developing a new automatic evaluation metric is to design/implement the metric, calculate its correlation to human judgments, then use that metric to evaluate text generation systems. While there have been significant efforts to build libraries for developing machine learning models (Klein et al., 2017; Gardner et al., 2018; Ott et al., 2019), no equivalent library exists for developing evaluation metrics. In this work, we present SacreROUGE, an open-source, Python-based li1 https://github.com/danieldeutsch/ sacrerouge brary for using and developing text generation metrics, with an emphasis on summarization. SacreROUGE removes many obstacles that researchers face when they use or develop evaluation metrics. First, the official implementations of various metrics do not share a common interface or programming language, so using many metrics to evaluate a model can be frustrating and time consuming. SacreROUGE prov"
2020.nlposs-1.17,P17-4012,0,0.0257273,"pment of SacreROUGE is ongoing and open to contributions from the community. 1 Introduction Evaluating models is a critical step of the machine learning workflow. However, unlike classificationbased tasks, evaluating models which generate text is difficult and is a research area on its own. The basic workflow for developing a new automatic evaluation metric is to design/implement the metric, calculate its correlation to human judgments, then use that metric to evaluate text generation systems. While there have been significant efforts to build libraries for developing machine learning models (Klein et al., 2017; Gardner et al., 2018; Ott et al., 2019), no equivalent library exists for developing evaluation metrics. In this work, we present SacreROUGE, an open-source, Python-based li1 https://github.com/danieldeutsch/ sacrerouge brary for using and developing text generation metrics, with an emphasis on summarization. SacreROUGE removes many obstacles that researchers face when they use or develop evaluation metrics. First, the official implementations of various metrics do not share a common interface or programming language, so using many metrics to evaluate a model can be frustrating and time cons"
2020.nlposs-1.17,W04-1013,0,0.129424,"support older evaluation metrics written in languages such as Perl or Java, we have written Python wrappers around the original code that still implement the Metric interface. Internally, the wrappers serialize the input summaries to the format required by the underlying metric, a subprocess is created to run the original metric’s code, and the output is then loaded from disk again in Python. This way, we do not have to port the original metric’s code to Python and end-users can still use the metrics with the Python API. SacreROUGE currently supports the following evaluation metrics: • ROUGE (Lin, 2004), including a Python-port that we wrote, which is significantly faster than the original Perl version • SIMetrix (Louis and Nenkova, 2009) • SumQE (Xenouleas et al., 2019) Among these metrics, 6 have original implementations in Java, 6 in Python, 1 in Perl, and 1 with no known official implementation (Pyramid Score). Handling Dependencies Many of the evaluation metrics rely on external resources in the form of code, models, or data files. Setting up these dependencies in the right format to use the metrics can be difficult. The SacreROUGE library addresses this problem by providing setup scrip"
2020.nlposs-1.17,D09-1032,0,0.0403875,"nal code that still implement the Metric interface. Internally, the wrappers serialize the input summaries to the format required by the underlying metric, a subprocess is created to run the original metric’s code, and the output is then loaded from disk again in Python. This way, we do not have to port the original metric’s code to Python and end-users can still use the metrics with the Python API. SacreROUGE currently supports the following evaluation metrics: • ROUGE (Lin, 2004), including a Python-port that we wrote, which is significantly faster than the original Perl version • SIMetrix (Louis and Nenkova, 2009) • SumQE (Xenouleas et al., 2019) Among these metrics, 6 have original implementations in Java, 6 in Python, 1 in Perl, and 1 with no known official implementation (Pyramid Score). Handling Dependencies Many of the evaluation metrics rely on external resources in the form of code, models, or data files. Setting up these dependencies in the right format to use the metrics can be difficult. The SacreROUGE library addresses this problem by providing setup scripts for each metric which download or compile any required resources. To make this process as easy as possible for the end-user, these scri"
2020.nlposs-1.17,N04-1019,0,0.221153,"elation to human judgments. Since all of the metrics in SacreROUGE implement a common interface, the code for these procedures is shared, so developers of new metrics do not need to rewrite the code to implement these procedures. This logic is exposed through evaluate, score, and correlate, which are subcommands of sacrerouge, the entry point for the library’s command-line interface. • BLEURT (Sellam et al., 2020) • METEOR (Denkowski and Lavie, 2014) • MeMoG (Giannakopoulos and Karkaletsis, 2010) • MoverScore (Zhao et al., 2019) • NPowER (Giannakopoulos and Karkaletsis, 2013) • Pyramid Score (Nenkova and Passonneau, 2004) • PyrEval (Gao et al., 2019) 121 Evaluating Systems and Metrics The evaluate Subcommand The purpose of the evaluate subcommand is to calculate a metric’s score for one summarization system on The correlate Subcommand After all of the summaries have been scored using the score subcommand, the second step of the meta-evaluation of a metric is to calculate the correlation of those scores to human judgments. This step is done via the correlate subcommand. one dataset, which most typically occurs when researchers compare their system’s performance to others’. The evaluate subcommand accepts a spec"
2020.nlposs-1.17,N19-4009,0,0.0173182,"contributions from the community. 1 Introduction Evaluating models is a critical step of the machine learning workflow. However, unlike classificationbased tasks, evaluating models which generate text is difficult and is a research area on its own. The basic workflow for developing a new automatic evaluation metric is to design/implement the metric, calculate its correlation to human judgments, then use that metric to evaluate text generation systems. While there have been significant efforts to build libraries for developing machine learning models (Klein et al., 2017; Gardner et al., 2018; Ott et al., 2019), no equivalent library exists for developing evaluation metrics. In this work, we present SacreROUGE, an open-source, Python-based li1 https://github.com/danieldeutsch/ sacrerouge brary for using and developing text generation metrics, with an emphasis on summarization. SacreROUGE removes many obstacles that researchers face when they use or develop evaluation metrics. First, the official implementations of various metrics do not share a common interface or programming language, so using many metrics to evaluate a model can be frustrating and time consuming. SacreROUGE provides Python-based w"
2020.nlposs-1.17,P02-1040,0,0.1084,"e Document Understanding Conference from 2001 to 2007,3 Text Analysis Conference from 2008 to 2011,4 the MultiLing 2011, 2013, 2015, 2017, and 2019 Workshops,5 and the CNN/DailyMail dataset judgments provided by Chaganty et al. (2018) and Fabbri et al. (2020). We intend to add more datasets as they become available, and other researchers can easily incorporate their own datasets to our library by serializing the data into the shared format. 5 Related Work The idea for SacreROUGE came from the SacreBLEU (Post, 2018) library. SacreBLEU was developed to standardize and simplify calculating BLEU (Papineni et al., 2002) for machine translation. Like SacreROUGE, it provides a simple command-line interface to download and evaluate on common machine translation datasets. Whereas SacreBLEU is mainly for evaluating machine translation models with BLEU, our library focuses on summarization A JSONL file contains one serialized JSON per line. 123 3 https://duc.nist.gov/ https://tac.nist.gov/ 5 http://multiling.iit.demokritos.gr/ 4 and includes a large number of evaluation metrics. Further, SacreROUGE also provides a framework for developing and evaluating new metrics. One goal of SacreROUGE is to standardize the imp"
2020.nlposs-1.17,2020.acl-main.704,0,0.0151972,"(Zhang et al., 2019) 3 • BEwT-E (Tratz and Hovy, 2008) The two most common use cases of an evaluation metric are to evaluate a summarization system and to evaluate a metric itself by calculating its correlation to human judgments. Since all of the metrics in SacreROUGE implement a common interface, the code for these procedures is shared, so developers of new metrics do not need to rewrite the code to implement these procedures. This logic is exposed through evaluate, score, and correlate, which are subcommands of sacrerouge, the entry point for the library’s command-line interface. • BLEURT (Sellam et al., 2020) • METEOR (Denkowski and Lavie, 2014) • MeMoG (Giannakopoulos and Karkaletsis, 2010) • MoverScore (Zhao et al., 2019) • NPowER (Giannakopoulos and Karkaletsis, 2013) • Pyramid Score (Nenkova and Passonneau, 2004) • PyrEval (Gao et al., 2019) 121 Evaluating Systems and Metrics The evaluate Subcommand The purpose of the evaluate subcommand is to calculate a metric’s score for one summarization system on The correlate Subcommand After all of the summaries have been scored using the score subcommand, the second step of the meta-evaluation of a metric is to calculate the correlation of those scores"
2020.nlposs-1.17,D19-1053,0,0.0157971,"luate a summarization system and to evaluate a metric itself by calculating its correlation to human judgments. Since all of the metrics in SacreROUGE implement a common interface, the code for these procedures is shared, so developers of new metrics do not need to rewrite the code to implement these procedures. This logic is exposed through evaluate, score, and correlate, which are subcommands of sacrerouge, the entry point for the library’s command-line interface. • BLEURT (Sellam et al., 2020) • METEOR (Denkowski and Lavie, 2014) • MeMoG (Giannakopoulos and Karkaletsis, 2010) • MoverScore (Zhao et al., 2019) • NPowER (Giannakopoulos and Karkaletsis, 2013) • Pyramid Score (Nenkova and Passonneau, 2004) • PyrEval (Gao et al., 2019) 121 Evaluating Systems and Metrics The evaluate Subcommand The purpose of the evaluate subcommand is to calculate a metric’s score for one summarization system on The correlate Subcommand After all of the summaries have been scored using the score subcommand, the second step of the meta-evaluation of a metric is to calculate the correlation of those scores to human judgments. This step is done via the correlate subcommand. one dataset, which most typically occurs when re"
2020.tacl-1.36,P96-1009,0,0.132698,"uces the same result, or an alternative interpretation that is also contextually appropriate. Count 8 Related work 3 The view of dialogue as an interactive process of shared plan synthesis dates back to Grosz and Sidner’s earliest work on discourse structure (1986; 1988). That work represents the state of a dialogue as a predicate recognizing whether a desired piece of information has been communicated or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2"
2020.tacl-1.36,D18-1547,0,0.165132,"65 1,052 3,315 Dataflow inline .729 .696 .665 .606 .642 .533 .574 .465 .697 .631 .565 .474 Dataflow inline refer inline both TRADE Table 2: SMCalFlow results. Agent action accuracy is significantly higher than a baseline without metacomputation, especially on turns that involve a reference (Ref. Turns) or revision (Rev. Turns) to earlier turns in the dialogue (p &lt; 10−6 , McNemar’s test). Joint Goal Dialogue Prefix .467 .447 .467 .454 .220 .202 .205 .168 3.07 2.97 2.90 2.73 Table 3: MultiWOZ 2.1 test set results. TRADE (Wu et al., 2019) results are from the public implementation. “Joint Goal” (Budzianowski et al., 2018) is average dialogue state exact-match, “Dialogue” is average dialogue-level exact-match, and “Prefix” is the average number of turns before an incorrect prediction. Within each column, the best result is boldfaced, along with all results that are not significantly worse (p &lt; 0.05, paired permutation test). Moreover, all of “Dataflow,” “inline refer,” and “inline both” have higher dialogue accuracy than TRADE (p &lt; 0.005). els that train on inlined metacomputation. These experiments make it possible to evaluate the importance of explicit dataflow manipulation compared to a standard contextual s"
2020.tacl-1.36,D19-1459,0,0.0502204,"bling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it p"
2020.tacl-1.36,P17-1167,0,0.0181575,"bout underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible inter"
2020.tacl-1.36,P17-4012,0,0.0122901,"propriate type constraint, provided that the reference resolution heuristic would retrieve the correct string from earlier in the dataflow. This covers references like the same day. Otherwise, our re-annotation retains the literal string value. Data statistics are shown in Table 1. To the best of our knowledge, SMCalFlow is the largest annotated task-oriented dialogue dataset to date. Compared to MultiWOZ, it features a larger user vocabulary, a more complex space of statemanipulation primitives, and a long tail of agent programs built from numerous function calls and deep composition. 7 NMT (Klein et al., 2017) pointer-generator network (See et al., 2017), a sequence-to-sequence model that can copy tokens from the source sequence while decoding. Our goal is to demonstrate that dataflow-based representations benefit standard neural model architectures. Dataflowspecific modeling might improve on this baseline, and we leave this as a challenge for future work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi"
2020.tacl-1.36,J94-4002,0,0.175555,"tate representations. While a complete description of dataflow-based language generation is beyond the scope of this paper, we briefly describe the components of the generation system relevant to the understanding system presented here. 3 Reference resolution In a dialogue, entities that have been introduced once may be referred to again. In dataflow dialogues, the entities available for reference are given by the nodes in the dataflow graph. Entities are salient to conversation participants to different degrees, and their relative salience determines the ways in which they may be referenced (Lappin and Leass, 1994). For example, it generally refers to the most salient non-human entity, while more specific expressions like the Friday meeting are needed to refer to accessible but less salient entities. Not all references to entities are overt: if the agent says “You have a meeting tomorrow” and the user responds “What time?”, the agent must predict the implicit reference to a salient event. 559 Dataflow pointers We have seen that refer is used to find referents for referring expressions. In general, these referents may be existing dataflow nodes or new subgraphs for newly mentioned entities. We now give m"
2020.tacl-1.36,J86-3001,0,0.780827,"Missing"
2020.tacl-1.36,W18-6322,0,0.0167485,"ser utterances with multiple possible interpretations). See §7 for discussion. Error analysis Beyond the quantitative results shown in Tables 2–3, we manually analyzed 100 SMCalFlow turns where our model mispredicted. Table 4 breaks down the errors by type. Three categories involve straightforward parsing errors. In underprediction errors, the model fails to predict some computation (e.g., a search constraint or property extractor) specified in the user request. This behavior is not specific to our system: under-length predictions are also welldocumented in neural machine translation systems (Murray and Chiang, 2018). In entity linking errors, the model correctly identifies the presence of an entity mention in the input utterance, but uses it incorrectly in the input plan. Sometimes the entity that appears in the plan is hallucinated, appearing nowhere in the utterance; sometimes the entity is cast to a wrong type (e.g., locations interpreted as event names) used in the wrong field or extracted with wrong boundaries. In fencing errors, the model interprets an out-of-scope user utterance as an interpretable command, or vice-versa versions of the full dataset, and inlined and non-inlined versions of our mod"
2020.tacl-1.36,D18-1300,0,0.0282209,"It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it possible to represent and learn from complex, natural dialogues. Future work might focus on improving prediction by introducing learned implementations of r"
2020.tacl-1.36,H90-1021,0,0.582952,"ted or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach"
2020.tacl-1.36,D14-1162,0,0.0839852,"with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window size c, hidden layer size d, number of hidden layers l, and dropout rates r are selected based on the agent action accuracy (for SMCalFlow) or dialogue-level exact match (for MultiWoZ) on the development set from {2, 4, 10}, {256, 300, 320, 384}, {1, 2, 3}, {0.3, 0.5, 0.7} respectively. Approximate 1-best decoding uses a beam of size 5. Quantitative evaluation Table 2 shows results for the SMCalFlow dataset. We report program accuracy: specifically, exact-match accuracy of the"
2020.tacl-1.36,P17-1062,0,0.0717515,"Missing"
2020.tacl-1.36,P19-1078,0,0.0158945,"work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi−1 xi (for MultiWOZ 2.1). Here c is a context window size, xj is the user utterance at user turn j, yj is the agent’s naturallanguage response, and zj is the linearized agent program. Each sequence xj , yj , or zj begins with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window s"
2020.tacl-1.36,P17-1099,0,0.0599554,"Missing"
2020.tacl-1.36,J00-3003,0,0.766051,"Missing"
2020.tacl-1.36,N18-1203,0,0.0172756,"goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible interpretations). See §7"
2020.tacl-1.36,P19-1443,0,0.056047,"state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse wi"
2020.tacl-1.36,Q14-1042,0,0.0241672,", Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors o"
2020.tacl-1.36,E17-1042,0,0.105146,"Missing"
2020.tacl-1.36,P09-1110,0,0.038465,"derstanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with mul"
2020.tacl-1.36,W16-3601,0,0.0677409,"Missing"
2020.tacl-1.36,H94-1037,0,0.573739,"odel’s test set predictions, enabling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state repres"
2021.acl-long.448,D19-6115,0,0.024124,"tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b). While there is an emerging tr"
2021.acl-long.448,D15-1162,0,0.0203263,"e answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer. 5770 4 For wh-word, empty question, and short distance reasoning, we use the TASE model (Segal et al., 2020) to learn the bias. Biased examples are then those that can be correctly solved by these models. We only change the training data for biased example detection, if necessary, and the development set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question sem"
2021.acl-long.448,2020.acl-main.132,0,0.024726,"as a span-prediction task by generating a query for each mention using the surrounding context, thus converting coreference resolution to a reading comprehension problem. They leverage the plethora of existing MRC datasets for data augmentation and improve the generalization of the coreference model. In parallel to Wu et al. (2020b), Aralikatte et al. (2019) also cast ellipsis and coreference resolution as reading comprehension tasks. They leverage the existing neural archi5769 tectures designed for MRC for ellipsis resolution and outperform the previous best results. In a similar direction, Hou (2020) propose to cast bridging anaphora resolution as question answering and present a question answering framework for this task. However, none of the above works investigate the impact of using coreference data on QA. Dua et al. (2020) use Amazon Mechanical Turkers to annotate the corresponding coreference chains of the answers in the passages of Quoref for 2,000 QA pairs. They then use this additional coreference annotation for training a model on Quoref. They show that including these additional coreference annotations improves the overall performance on Quoref. The proposed method by Dua et al"
2021.acl-long.448,D17-1215,0,0.0165097,"ggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related cor"
2021.acl-long.448,N18-1023,1,0.927993,"Comprehension Mingzhu Wu1 , Nafise Sadat Moosavi1 , Dan Roth2 , Iryna Gurevych1 1 2 UKP Lab, Technische Universitat Darmstadt Department of Computer and Information Science, UPenn 1 2 https://www.ukp.tu-darmstadt.de https://www.seas.upenn.edu/˜danroth/ Abstract improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019). There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al., 2018), etc. Quoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models. Figure 1 shows a QA sample from Quoref in which the model needs to resolve the coreference relation between “his” and “John Motteux” to answer the question. Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to e"
2021.acl-long.448,J13-4004,0,0.0555938,"Missing"
2021.acl-long.448,2020.acl-main.703,0,0.148622,"tation artifacts, and its distribution of biases is closer to a coreference resolution dataset. The performance of state-of-the-art models on Quoref considerably drops on our evaluation set suggesting that (1) coreference reasoning is still an open problem for MRC models, and (2) our methodology opens a promising direction to create future challenging MRC datasets. Second, we propose to directly use coreference resolution datasets for training MRC models to improve their coreference reasoning. We automatically create a question whose answer is a coreferring expression m1 using the BART model (Lewis et al., 2020). We then consider this question, m1 ’s antecedent, and the corresponding document as a new (question, answer, context) tuple. This data helps the model learning to resolve the coreference relation between m1 and its antecedent to answer the question. We show that incorporating these additional data improves the performance of the state-of-the-art models on our new evaluation set. Our main contributions are as follows: • We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge. •"
2021.acl-long.448,2020.acl-main.441,0,0.228439,"are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it"
2021.acl-long.448,W12-4501,0,0.572109,"velopment set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question semantic overlap short-distance reasoning Quoref CoNLLbart 9.39 22.99 21.51 28.66 50.70 1.52 13.12 11.60 21.38 9.86 Table 1: The proportion of examples in the Quoref development set and CoNLL-2012 coreference resolution dataset that contain each of the examined biases. We also investigate whether these biases have similar ratios in a coreference resolution dataset. We use the CoNLL-2012 coreference resolution dataset (Pradhan et al., 2012a) and convert it to a reading comprehension format, i.e., CoNLLbart in Section 5.5 This data contains question-answer pairs in which the question is created based on a coreferring expression in CoNLL-2012, and the answer is its closest antecedent. We split this data into training and test sets and train bias models on the training split. The CoNLLbart column in Table 1 shows the bias proportions on this data. 4 Creating an MRC Dataset that Better Reflects Coreference Reasoning There is a growing trend in using adversarial models for data creation to make the dataset more challenging or discar"
2021.acl-long.448,2020.acl-main.770,1,0.869579,"MRC models. We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models. 2 Related Work 2.1 Artifacts in NLP datasets One of the known drawbacks of many NLP datasets is that they contain artifacts.2 Models tend to ex2 I.e., the conditional distribution of the target label based on specific attributes of the training domain diverges while testing on other domains. ploit these easy-to-learn patterns in the early stages of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo e"
2021.acl-long.448,2020.emnlp-main.613,1,0.893602,"MRC models. We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models. 2 Related Work 2.1 Artifacts in NLP datasets One of the known drawbacks of many NLP datasets is that they contain artifacts.2 Models tend to ex2 I.e., the conditional distribution of the target label based on specific attributes of the training domain diverges while testing on other domains. ploit these easy-to-learn patterns in the early stages of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo e"
2021.acl-long.448,D16-1264,0,0.192798,"on simple lexical overlap without requiring coreference reasoning. We investigate five artifacts (biases) as follows: • Random named entity: the majority of answers in Quoref are person names. To evaluate this artifact, we randomly select a PERSON named entity from the context as the answer.3 How Well Quoref Presents Coreference Reasoning? For creating the Quoref dataset, annotators first identify coreferring expressions and then ask questions that connect the two coreferring expressions. Dasigi et al. (2019) use a BERT-base model (Devlin et al., 2019) that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) as an adversarial model to exclude QA samples that the adversarial model can already answer. The goal of using this adversarial model is to avoid including questionanswer pairs that can be solved using surface cues. They claim that most examples in Quoref cannot be answered without coreference reasoning. If we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indi"
2021.acl-long.448,D19-1410,1,0.830573,"that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer"
2021.acl-long.448,P18-1156,0,0.0175811,"Reasoning in Machine Reading Comprehension Mingzhu Wu1 , Nafise Sadat Moosavi1 , Dan Roth2 , Iryna Gurevych1 1 2 UKP Lab, Technische Universitat Darmstadt Department of Computer and Information Science, UPenn 1 2 https://www.ukp.tu-darmstadt.de https://www.seas.upenn.edu/˜danroth/ Abstract improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019). There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al., 2018), etc. Quoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models. Figure 1 shows a QA sample from Quoref in which the model needs to resolve the coreference relation between “his” and “John Motteux” to answer the question. Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi"
2021.acl-long.448,2020.emnlp-main.248,0,0.401736,"odel only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer. 5770 4 For wh-word, empty question, and short distance reasoning, we use the TASE model (Segal et al., 2020) to learn the bias. Biased examples are then those that can be correctly solved by these models. We only change the training data for biased example detection, if necessary, and the development set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question semantic overlap short-distance reasoning Quoref CoNLLbart 9.39 22.99 21.51 28.66 50.70 1.52 13.12 11.60 21.38 9.86 Table 1: The proportion of examples in the Quoref development set and CoNLL-2012 coreference resolution dataset that conta"
2021.acl-long.448,K17-1028,0,0.0202608,"d 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the ques"
2021.acl-long.448,2020.emnlp-demos.6,0,0.056864,"Missing"
2021.acl-long.448,2020.findings-emnlp.74,1,0.855048,"Missing"
2021.acl-long.448,2020.acl-main.622,0,0.214665,"jective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b). While there is an emerging trend of including adversarial models in data collection, their effectiveness is not yet compared with using debiasing methods, e.g., whether they are still beneficial when we use debiasing methods or vice versa. 2.2 Joint QA and Coreference Reasoning There are a few studies on the joint understanding of coreference relations and reading comprehension. Wu et al. (2020b) propose to formulate coreference resolution as a span-prediction task by generating a query for each mention using the surrounding context, thus converting coreference resolut"
2021.acl-long.448,D18-1259,0,0.111811,"et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al.,"
2021.acl-long.448,P19-1452,0,0.028621,"coreference reasoning. If we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this"
2021.acl-long.448,D18-1009,0,0.171086,"ges of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2"
2021.acl-long.458,N18-1022,0,0.026421,"icle that should be recommended. However, the meaning of the “reference” is different in these two problems. When recommending citations for a paper, the system is to look for previous works that are related to the arguments in the given paper. The argument was created by the author, and the criteria of the recommendation is the relatedness. While inferring provenance is to do reverse engineering to the given article, so that we can find the articles whose information or claims were actually used when the author was writing. Technically, there are two types of citation recommendation systems (Bhagavatula et al., 2018). One is called local (Huang et al., 2012, 2015), that is, a system takes a few sentences (and an optional placeholder for the candidate citation) as input and recommends citations based on the context of the input sentences. Another one is called global (Kataria et al., 2010; Ren et al., 2014; Bhagavatula et al., 2018), that is, a system takes the entire article (and its meta-data which is optional) as input and recommends cita5901 tions for the paper. Our solution is more related to local recommendation systems, while we do not assume we can access all of the articles that can be cited and h"
2021.acl-long.458,D13-1184,1,0.803314,"s the sources, the titles of the articles are also very likely to be related. In the same example, some of them are all talking about the interviews done by Anthony Fauci at different time, and some of them are talking about the white house’s Coronavirus Task Force in Press Briefing. Therefore, we propose an algorithmic inference framework that can take advantage of those relations between the source articles to determine the correct source articles of identified sentences jointly. 4.3.2 ILP-based Inference We formulate the inference as an Integer Linear Program (ILP) (Roth and tau Yih, 2004; Cheng and Roth, 2013), that allows us to jointly determine the best candidate for each identified sentence. Formally, we introduce two types of Boolean variables: xki , which represents if the k th candidate kl , is the source article of the ith sentence, and zij which represents if the source article of the ith sentence and the source article of the j th sentence are related, which means either they come from related source websites or provide related content. To infer the value of the Boolean variables, our 5898 objective is to assign the best candidate to each identified sentence that can (1) maximize the overa"
2021.acl-long.458,N19-1423,0,0.00572243,"oal to find the best assignments Γd of candidates for the identified sentences is as follows: Γd = argmaxΓ XX i s.t. k ωik xki + XX i,j kl kl  kl τij + γij zij (2) k,l kl xki ∈ {0, 1}, zij ∈ {0, 1} X ∀i, xki = 1 (3) k kl 2zij ≤ xki + xlj P Here, k xki = 1 means only one candidate will finally be chosen as the source article of the ith kl ≤ xk + xl means only if the k th sentence, and 2zij i j candidate of the ith sentence and the lth candidate of the j th sentence have been chosen, we need to consider the relations between them. In our experiments, we use the last hidden layer of BERT-large (Devlin et al., 2019) as the representation for titles and source domains, and use cosine similarity to compute the similarity score. The ILP problem is solved using an off-the-shelf high-performance package 3 . 5 RQ4 Given the identified sentences, can we use the query we generated to find candidates, and successfully use them to improve the inference of source articles? Among those questions, RQ1-RQ3 are to evaluate a specific component of our solution, and RQ4 is to evaluate the joint performance of candidate generation and source article inference. In the following part, we will elaborate the answers to those"
2021.acl-long.458,2020.acl-main.761,0,0.0211339,"ing articles from www.politifa ct.com; our experimental results show that our solution leads to a significant improvement over baselines. 1 Figure 1: An example of a claim (in the red box) with its article. Sentence 1 and sentence 2 (blue boxes) show examples from the article. Each sentence refers to external information: source article 1 and 2, respectively, with accompanying urls. Introduction Misinformation is on the rise, and people are fighting it with fact checking. However, most of the work in the current literature (Thorne et al., 2018; Zhang et al., 2019; Barr´on-Cedeno et al., 2020; Hidey et al., 2020) focuses on automating factchecking for a single claim. In reality, a claim can be complex, and proposed as a conclusion of an article. Therefore, understanding what information supports the article, especially information 1 The data and the code will be available at http://co gcomp.org/page/publication view/944 that was not originated within the same article, and where it originates from, are very important for readers who want to determine whether they can believe the claim. Figure 1 shows an example of such a claim, “Marco Rubio says Anthony Fauci lies about masks. Fauci didn’t.”2 with its"
2021.acl-long.458,2021.ccl-1.108,0,0.0568591,"Missing"
2021.acl-long.458,N19-4014,0,0.0176045,"d by our query generator are helpful. When k = 5, the mean recall can achieve around 0.21, which is much better than 0.15, the best performance achieved by searching the identified sentence directly. However, as what we can observe in the figure, there is still a gap to the performance of MS-UB in Figure 6. This may result from the insufficiency of the query generation, which implies Fact-checking Fact-checking is related to our problem, since there is usually a document retrieval step to find articles that may provide evidence in most of the solutions (Wang et al., 2018; Thorne et al., 2018; Nadeem et al., 2019). Typically, the input of fact-checking is a single claim instead of an article, therefore it is hard to directly extend their solutions to our problem. Even though fact-checking may find various evidentiary articles for the claim, the source articles we are looking for are those that have been used by the author, which is actually a specific subset of the articles that fact-checking targets to, and the size is also much smaller. Furthermore, we try to extract the metadata of the source articles from the text to support a better search, which is not considered in the document retrieval step of"
2021.acl-long.458,W04-2401,1,0.436364,"Missing"
2021.acl-long.458,N18-1074,0,0.0571488,"Missing"
2021.acl-long.458,P19-1040,1,0.751774,"tion dataset 1 , Politi-Prov, based on fact-checking articles from www.politifa ct.com; our experimental results show that our solution leads to a significant improvement over baselines. 1 Figure 1: An example of a claim (in the red box) with its article. Sentence 1 and sentence 2 (blue boxes) show examples from the article. Each sentence refers to external information: source article 1 and 2, respectively, with accompanying urls. Introduction Misinformation is on the rise, and people are fighting it with fact checking. However, most of the work in the current literature (Thorne et al., 2018; Zhang et al., 2019; Barr´on-Cedeno et al., 2020; Hidey et al., 2020) focuses on automating factchecking for a single claim. In reality, a claim can be complex, and proposed as a conclusion of an article. Therefore, understanding what information supports the article, especially information 1 The data and the code will be available at http://co gcomp.org/page/publication view/944 that was not originated within the same article, and where it originates from, are very important for readers who want to determine whether they can believe the claim. Figure 1 shows an example of such a claim, “Marco Rubio says Anthony"
2021.acl-long.458,2020.acl-main.406,1,0.90662,"utational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5894–5903 August 1–6, 2021. ©2021 Association for Computational Linguistics used when they were writing. Furthermore, the problem we address is critical also to authors who want to give credit to those who have contributed to their article, and it enables a recursive analysis that can trace back to the starting points of an article. This motivates the study of provenance for natural language claims, which describes where a specific claim may have come from and how it has spread. Early work (Zhang et al., 2020) proposed a formulation to model, and a solution to infer, the provenance graph for the given claim. However, that model is insufficient to capture the provenance of an article, because (1) an article consists of multiple claims, and it leverages information from other sources, therefore the provenance of all claims should be included in the article’s provenance; (2) the inference solution they proposed can only extract domain-level provenance information, e.g., cbsnews.com, while it can not directly link the claim to its source article, e.g., https://www.cbsnews.com/news/preventingcoronavirus"
2021.acl-long.458,2020.acl-main.703,0,0.0149295,"the figure) is “... On March 29, President ... ”. The source domain of the article it refers to (source article 2 in the figure) is white house, the title of the article is coronavirus task force press briefing, and the published date is March 29, 2020. It is obvious that most of those information has been somehow mentioned in the context or at least can be very easily associated with. Therefore, we treat this problem as a text generation problem, where we feed the identified sentence with its context, and try to generate its metadata. As a baseline, we train this model via fine-tuning BART (Lewis et al., 2020), a pretrained text generation model. 4.2.2 Integrating Search Engine Signals Besides the metadata to generate, the content of the identified sentence itself should be useful for searching, when there is an overlap between the sentence and the content of the target article. In this case, if we search for the identified sentence on a search engine, the results returned can be related articles, and their metadata may provide additional useful information that can tell the model what should be included in the target output. In our running example mentioned in the last section, if we search that s"
2021.acl-short.42,N18-1101,0,0.0515893,"h is both expensive and inflexible when moving to new event ontologies. Recent works (Chen et al., 2020; Du and Cardie, 2020) have pointed out the connection between Question Answering (QA) and EE in developing supervised systems. Meanwhile, several efforts have explored unsupervised methods. Peng et al. (2016) first attempted to extract event triggers with minimal supervision using similarity-based ∗ This work was done when the author was visiting the University of Pennsylvania. 1 Our code and models will be available at http:// cogcomp.org/page/publication_view/943. heuristics. Huang et al. (2018) and Lai et al. (2020) explored both trigger and argument extraction under a slightly different setting: training on some event types and testing on unseen ones. Recently, Liu et al. (2020) proposed a QA-based zero-shot argument extraction method, which did not handle triggers. So far, no method has been proposed to extract both event triggers and arguments without any EE training data2 . Moreover, the performance of existing zero-shot attempts, especially on arguments, is still far from satisfactory, yet little is known about possible underlying reasons. In this work, we investigate the possi"
2021.acl-tutorials.2,P08-1090,0,0.0818516,"tical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic sali"
2021.acl-tutorials.2,D17-1168,1,0.911618,"alization, coreference resolution and prediction of events and their relations, (iii) induction of event processes and properties, and (iv) a wide range of NLP and commonsense understanding tasks that benefit from aforementioned techniques. We will conclude the tutorial by outlining emerging research problems in this area. 1 Introduction Human languages always involve the description of real-world events. Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predica"
2021.acl-tutorials.2,2021.acl-long.133,1,0.81067,"Missing"
2021.acl-tutorials.2,glavas-etal-2014-hieve,0,0.0604228,"Missing"
2021.acl-tutorials.2,2020.conll-1.43,1,0.898616,"many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which links references to the same event together, also plays a critical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk abou"
2021.acl-tutorials.2,P19-1433,0,0.0195631,"ng the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend informa"
2021.acl-tutorials.2,D15-1076,0,0.0239549,"min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based enc"
2021.acl-tutorials.2,P18-1201,1,0.822982,"lined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress"
2021.acl-tutorials.2,P06-1039,0,0.0890017,"Missing"
2021.acl-tutorials.2,D18-1208,1,0.845181,"Missing"
2021.acl-tutorials.2,2020.acl-main.713,1,0.724423,"presentation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large sca"
2021.acl-tutorials.2,P15-1155,1,0.800367,"movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present differen"
2021.acl-tutorials.2,Y18-1046,0,0.0145132,"ls (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and ex"
2021.acl-tutorials.2,W17-2701,0,0.0175787,"also interested in large-scale events that unfold over time. Over the past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g.,"
2021.acl-tutorials.2,P19-1413,0,0.0244441,"to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which"
2021.acl-tutorials.2,P19-1500,0,0.0234852,"Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Linguistics and philosophy. 9(1):5-16, 1986. • Nathanael Chambers. Event Schema Induction with a Probabilistic Entity-Driven Model. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process"
2021.acl-tutorials.2,2021.emnlp-main.422,1,0.760995,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,N18-2089,0,0.0177113,"t the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for"
2021.acl-tutorials.2,2020.acl-main.230,1,0.883102,"Missing"
2021.acl-tutorials.2,D18-1483,0,0.0226446,"Missing"
2021.acl-tutorials.2,2020.emnlp-main.50,1,0.742388,"es, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted"
2021.acl-tutorials.2,C14-1114,0,0.0297635,"ents was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Li"
2021.acl-tutorials.2,P18-1122,1,0.835058,"conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motiv"
2021.acl-tutorials.2,2021.naacl-main.69,1,0.705823,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,D19-1405,0,0.0220875,"e will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of prese"
2021.acl-tutorials.2,P18-1043,0,0.0258892,"Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predicates, frontier research 2 Outline of Tutorial Content This half-day tutorial presents a systematic overview of recent advances in event-centric NLP technologies. We will begin with motivating this topic with several real-world applications, and introduce the main research problems. Then, we will introduce methods for automated extraction of events as well as their participants, properties and relations fr"
2021.acl-tutorials.2,2021.eacl-main.198,1,0.68319,"past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many"
2021.acl-tutorials.2,2021.naacl-main.4,1,0.738456,"gs (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-or"
2021.acl-tutorials.2,D19-1030,1,0.89034,"Missing"
2021.acl-tutorials.2,2020.acl-main.678,1,0.834145,"d expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-order selectional preference over event knowledge and demonstrates that how to convert relatively cheap event knowledge, which can be easily acquired from raw documents with linguistic patterns, to precious commonsense knowledge defined in ConceptNet (Zhang et al., 2020b). Beyond that, we will also introduce how to automatically acquire other event-centric commonsense knowledge including but not limited to temporal properties (Zhou et al., 2020), intentions (Chen et al., 2020), effects (Sap et al., 2019) and graph schemas (Li et al., 2020c) of events. This part is estimated to be 35 minutes. 2.6 Emerging Research Problems [20min] Event-centric NLP impacts on a wide spectrum of knowledge-driven AI tasks, and is particularly knotted with commonsense understanding. We will conclude the tutorial using 20 minutes by presenting some challenges and potential research topics in applying eventuality knowledge in downstream tasks (e.g., reading comprehension, dialogue generation, and event timeline generation), and grounding eventuality knowle"
2021.acl-tutorials.2,2020.emnlp-main.51,1,0.784661,"er investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequence"
2021.acl-tutorials.2,N15-1112,0,0.062705,"Missing"
2021.conll-1.24,D18-1450,0,0.0128365,"iments offer an explanation for why this is the case: The metrics do not compare summaries based on their information, therefore they cannot determine if a summary is factually consistent with its input. Metrics which do attempt to directly measure information overlap between summaries are based on the gold-standard comparison technique, the Pyramid Method (Nenkova and Passonneau, 2004). Although it relies heavily on annotations by experts, there have been attempts to crowdsource (Shapira et al., 2019) or automate all or parts of the Pyramid Method (Passonneau et al., 2013; Yang et al., 2016; Hirao et al., 2018) including PyrEval (Gao et al., 2019), which we analyzed in §6. These metrics have been met with less success than the text overlap-based ones covered by this work, potentially because measuring information overlap is more difficult than comparing summaries by their topics, and topic-based evaluations strongly correlate to responsiveness judgments (see §8). Limitations There are some limitations to our analysis. First, the results are specific to the datasets and summarization models that were used. However, TAC’08 and ’09 are the benchmark datasets for evaluating content quality and have been"
2021.conll-1.24,W14-3348,0,0.0453407,"Missing"
2021.conll-1.24,K19-1038,0,0.0142386,"s is the case: The metrics do not compare summaries based on their information, therefore they cannot determine if a summary is factually consistent with its input. Metrics which do attempt to directly measure information overlap between summaries are based on the gold-standard comparison technique, the Pyramid Method (Nenkova and Passonneau, 2004). Although it relies heavily on annotations by experts, there have been attempts to crowdsource (Shapira et al., 2019) or automate all or parts of the Pyramid Method (Passonneau et al., 2013; Yang et al., 2016; Hirao et al., 2018) including PyrEval (Gao et al., 2019), which we analyzed in §6. These metrics have been met with less success than the text overlap-based ones covered by this work, potentially because measuring information overlap is more difficult than comparing summaries by their topics, and topic-based evaluations strongly correlate to responsiveness judgments (see §8). Limitations There are some limitations to our analysis. First, the results are specific to the datasets and summarization models that were used. However, TAC’08 and ’09 are the benchmark datasets for evaluating content quality and have been widely used to measure the performan"
2021.conll-1.24,hovy-etal-2006-automated,0,0.1153,"ty of the information in the summary. However, it is not clear whether metrics such as ROUGE and BERTScore evaluate summaries based on how much informa1 Introduction tion they have in common with the reference or The development of a reliable metric that can au- some less desirable dimension of similarity, such as whether the two summaries discuss the same tomatically evaluate the content of a summary has topics (see Fig. 1). been an active area of research for nearly two decades. Over the years, many different metrics In this work, we demonstrate that ROUGE and have been proposed (Lin, 2004; Hovy et al., 2006; BERTScore largely do not measure how much inGiannakopoulos et al., 2008; Louis and Nenkova, formation two summaries have in common. Fur2013; Zhao et al., 2019; Zhang et al., 2020; Deutsch ther, we provide evidence that suggests this result et al., 2021). holds true for many other evaluation metrics as The majority of these of approaches score a well. Together, this allows us to conclude that the candidate summary by measuring its similarity to most frequently used metrics in summarization fail a reference summary. The most popular metric, to evaluate the quality of information in a summary."
2021.conll-1.24,W04-1013,0,0.0842932,"e the quality of the information in the summary. However, it is not clear whether metrics such as ROUGE and BERTScore evaluate summaries based on how much informa1 Introduction tion they have in common with the reference or The development of a reliable metric that can au- some less desirable dimension of similarity, such as whether the two summaries discuss the same tomatically evaluate the content of a summary has topics (see Fig. 1). been an active area of research for nearly two decades. Over the years, many different metrics In this work, we demonstrate that ROUGE and have been proposed (Lin, 2004; Hovy et al., 2006; BERTScore largely do not measure how much inGiannakopoulos et al., 2008; Louis and Nenkova, formation two summaries have in common. Fur2013; Zhao et al., 2019; Zhang et al., 2020; Deutsch ther, we provide evidence that suggests this result et al., 2021). holds true for many other evaluation metrics as The majority of these of approaches score a well. Together, this allows us to conclude that the candidate summary by measuring its similarity to most frequently used metrics in summarization fail a reference summary. The most popular metric, to evaluate the quality of informa"
2021.conll-1.24,D19-1387,0,0.0172458,"VB + NSUBJ, VB + DOBJ, and VB + NSUBJ + DOBJ. They are representative of information expressed as predicateargument relations (e.g., {subject, verb, object} tuples). We consider 2 through 5 to be keywords that represent the topics discussed in the summaries, whereas 6 describes tuples which express the summaries’ information as predicate-argument relations. The contributions of each category on the TAC 2008 summaries as well as the summaries produced 4. NP Chunks: One category to select matches by baseline (See et al., 2017) and state-of-the-art between tokens that are part of noun phrases, (Liu and Lapata, 2019) abstractive models on the denoted NP - CHUNKS. CNN/DailyMail dataset (Nallapati et al., 2016) is presented in Table 1. The POS/NER tagging and 5. Dependency: Three categories that select matches between tokens with the same de- parsing are all done with spaCy (Honnibal et al., 2020). pendency tree arc label for ROOT, NSUBJ, and DOBJ labels. The results across datasets and evaluation met304 TAC’08 CNN/DM Content Type R BS R BS Topic Information Stopwords 70.6 2.2 54.6 57.9 0.9 32.4 75.0 6.7 48.4 59.2 3.2 28.7 Table 2: The contributions of different categories of token matches when grouped by w"
2021.conll-1.24,J13-2002,0,0.0754259,"Missing"
2021.conll-1.24,K16-1028,0,0.0770674,"Missing"
2021.conll-1.24,N04-1019,0,0.46552,"omain experts (§4). The second reveals that token alignments which represent common information are vastly outnumbered by those which represent the summaries discussing the same topic (§5). Overall, both analyses support the conclusion that ROUGE and BERTScore largely do not measure information overlap. Then, we expand our analysis to consider if 10 other evaluation metrics successfully measure information quality or not (§6). By demonstrating that nearly all of the metrics correlate much more strongly to ROUGE than to the gold-standard method of manually comparing two summaries’ information (Nenkova and Passonneau, 2004), we argue the metrics are likely to measure information overlap no better than ROUGE does. However, one recently proposed metric that evaluates summaries using question-answering (QA), QAEval (Deutsch et al., 2021), correlates equally well to ROUGE and gold-standard annotations of information overlap. By viewing QAEval as inducing an alignment between two summaries and reasoning about its behavior, we demonstrate evidence that it measures information overlap much more strongly than either ROUGE or BERTScore does, supporting that QA-based metrics are a promising direction for future research ("
2021.conll-1.24,P13-2026,0,0.021697,"dtruth judgments. The results from our experiments offer an explanation for why this is the case: The metrics do not compare summaries based on their information, therefore they cannot determine if a summary is factually consistent with its input. Metrics which do attempt to directly measure information overlap between summaries are based on the gold-standard comparison technique, the Pyramid Method (Nenkova and Passonneau, 2004). Although it relies heavily on annotations by experts, there have been attempts to crowdsource (Shapira et al., 2019) or automate all or parts of the Pyramid Method (Passonneau et al., 2013; Yang et al., 2016; Hirao et al., 2018) including PyrEval (Gao et al., 2019), which we analyzed in §6. These metrics have been met with less success than the text overlap-based ones covered by this work, potentially because measuring information overlap is more difficult than comparing summaries by their topics, and topic-based evaluations strongly correlate to responsiveness judgments (see §8). Limitations There are some limitations to our analysis. First, the results are specific to the datasets and summarization models that were used. However, TAC’08 and ’09 are the benchmark datasets for"
2021.conll-1.24,W17-4510,0,0.0497114,"Missing"
2021.conll-1.24,D16-1264,0,0.116968,"Missing"
2021.conll-1.24,D19-1053,0,0.0115256,"ction tion they have in common with the reference or The development of a reliable metric that can au- some less desirable dimension of similarity, such as whether the two summaries discuss the same tomatically evaluate the content of a summary has topics (see Fig. 1). been an active area of research for nearly two decades. Over the years, many different metrics In this work, we demonstrate that ROUGE and have been proposed (Lin, 2004; Hovy et al., 2006; BERTScore largely do not measure how much inGiannakopoulos et al., 2008; Louis and Nenkova, formation two summaries have in common. Fur2013; Zhao et al., 2019; Zhang et al., 2020; Deutsch ther, we provide evidence that suggests this result et al., 2021). holds true for many other evaluation metrics as The majority of these of approaches score a well. Together, this allows us to conclude that the candidate summary by measuring its similarity to most frequently used metrics in summarization fail a reference summary. The most popular metric, to evaluate the quality of information in a summary. ROUGE (Lin, 2004), compares summaries based Our analysis casts ROUGE and BERTScore into 1 a unified framework in which the similarity of two Our code is availab"
2021.conll-1.24,P17-1099,0,0.0433788,"its subject child (NSUBJ), object child (DOBJ), or both. These categories are denoted VB + NSUBJ, VB + DOBJ, and VB + NSUBJ + DOBJ. They are representative of information expressed as predicateargument relations (e.g., {subject, verb, object} tuples). We consider 2 through 5 to be keywords that represent the topics discussed in the summaries, whereas 6 describes tuples which express the summaries’ information as predicate-argument relations. The contributions of each category on the TAC 2008 summaries as well as the summaries produced 4. NP Chunks: One category to select matches by baseline (See et al., 2017) and state-of-the-art between tokens that are part of noun phrases, (Liu and Lapata, 2019) abstractive models on the denoted NP - CHUNKS. CNN/DailyMail dataset (Nallapati et al., 2016) is presented in Table 1. The POS/NER tagging and 5. Dependency: Three categories that select matches between tokens with the same de- parsing are all done with spaCy (Honnibal et al., 2020). pendency tree arc label for ROOT, NSUBJ, and DOBJ labels. The results across datasets and evaluation met304 TAC’08 CNN/DM Content Type R BS R BS Topic Information Stopwords 70.6 2.2 54.6 57.9 0.9 32.4 75.0 6.7 48.4 59.2 3.2"
2021.conll-1.24,N19-1072,0,0.0167411,"asure the faithfulness of a summary based on low correlations to groundtruth judgments. The results from our experiments offer an explanation for why this is the case: The metrics do not compare summaries based on their information, therefore they cannot determine if a summary is factually consistent with its input. Metrics which do attempt to directly measure information overlap between summaries are based on the gold-standard comparison technique, the Pyramid Method (Nenkova and Passonneau, 2004). Although it relies heavily on annotations by experts, there have been attempts to crowdsource (Shapira et al., 2019) or automate all or parts of the Pyramid Method (Passonneau et al., 2013; Yang et al., 2016; Hirao et al., 2018) including PyrEval (Gao et al., 2019), which we analyzed in §6. These metrics have been met with less success than the text overlap-based ones covered by this work, potentially because measuring information overlap is more difficult than comparing summaries by their topics, and topic-based evaluations strongly correlate to responsiveness judgments (see §8). Limitations There are some limitations to our analysis. First, the results are specific to the datasets and summarization models"
2021.conll-1.24,2020.acl-main.450,0,0.0191069,"for this task is quite high. 9 Related Work Most of the work that reasons about how to interpret the scores of evaluation metrics does so indirectly through correlations to human judgments (Dang and Owczarzak, 2009; Owczarzak and Dang, 2011). However, a high correlation is not conclusive evidence about what a metric measures since it is possible for the metric to directly measure some other aspect of a summary, which is in turn correlated with the ground-truth judgments (see §8). Our work can be viewed as more direct evidence about what ROUGE and BERTScore measure. Recent work by Wang et al. (2020) argues that many of the same evaluation metrics covered in this work do not successfully measure the faithfulness of a summary based on low correlations to groundtruth judgments. The results from our experiments offer an explanation for why this is the case: The metrics do not compare summaries based on their information, therefore they cannot determine if a summary is factually consistent with its input. Metrics which do attempt to directly measure information overlap between summaries are based on the gold-standard comparison technique, the Pyramid Method (Nenkova and Passonneau, 2004). Alt"
2021.conll-1.49,2020.acl-main.240,0,0.286868,"ounterbalancing, templates used for generating test sentences, and additional phenomena can be found in Appendix B. 2.5 Evaluation Method For each minimal pair, we computed a model’s preference for the grammatical as opposed to ungrammatical sentence. The preference score was calculated by summing the cross-entropy errors at each position in the sentence (Zaczynska et al., 2020). This has the advantage of considering the test sentence as a whole, rather than just the left context of a specific position where surprisal is expected to be high for ungrammatical sentences (Warstadt et al., 2020a; Salazar et al., 2020a). We computed the accuracy by dividing the number of correct choices by the total number of pairs. To enable fair comparisons, we use this method to evaluate all models considered in this paper. 3 Results 3.1 RoBERTa is data-hungry Figure 1: Average accuracy on our grammar test suite. Blue: RoBERTa-base pre-trained by Liu et al. (2019) on 30B words. Orange: RoBERTa-base pre-trained by Warstadt et al. (2020b) on 10M words. The following models were all trained on AO-CHILDES, 5M words of child-directed input: Green: RoBERTa-base pretrained by us. Red: BabyBERTa trained with the original maskin"
2021.conll-1.49,2020.acl-main.465,0,0.105442,"6,000X fewer words than the 30B used 1.2 Related Work to pre-train RoBERTa-base - scored within half a In recent years, there has been a surge of interest in point of RoBERTa-base. See Table 1 for a comlinguistic evaluations of language models (Glock- parison. Furthermore, BabyBERTa performs well ner et al., 2018; Ettinger, 2020; Linzen et al., 2016; above chance in a majority of 13 grammatical pheGulordava et al., 2018; Goldberg, 2019; Tran et al., nomena evaluated. This demonstrates that masked 2018; Bacon and Regier, 2019; McCoy et al., 2020; language modeling can yield strong grammatical Linzen, 2020). In a review of this literature, Linzen knowledge even when the input consists of a small and Baroni (2021) concluded that language models corpus of child-directed language. 625 2 2.1 Methods BabyBERTa We introduce a scaled-down masked language model based on RoBERTa (Liu et al., 2019), with 8M parameters, 8912 vocabulary items, and trained on no more than 30M words. Additionally, and more importantly, during training, we modified the probability of unmasking from 0.1 to 0.0 - effectively removing unmasking. We will refer to this model as BabyBERTa to highlight both its origin in RoBERTa and"
2021.conll-1.49,Q15-1021,0,0.0178336,"ely 500K sentences each. This method of splitting within articles resulted in three corpora nearly identical in vocabulary and content, which we will refer to as Wikipedia-1, Wikipedia-2, and Wikipedia-3. Our Wikipedia corpora differ from AOCHILDES in a number of ways: First, Wikipedia is a corpus of written, not spoken language. Second, many articles were written by professionals with topical expertise and attention to grammatical correctness. Thus, in order to further isolate the effect of domain, we included a fifth corpus, which we will refer to as AO-Newsela, based on the Newsela corpus (Xu et al., 2015). It includes 1,911 English news articles, and 4 or 5 simplified versions of each rewritten by professional annotators for children with different reading proficiency. Each simplification level 1-5 (targeted to gradelevels 2 through 12), contains close to 1M words; AO-Newsela is therefore roughly equivalent in size to AO-CHILDES. Because this corpus contains written language but is directed towards children instead of adults, it is an ideal middle ground between the spoken child-directed language in AOCHILDES and the written adult-directed language in our Wikipedia corpora. 2.3 Vocabulary Baby"
2021.dash-1.11,P18-4003,0,0.0294438,"Missing"
2021.dash-1.11,P18-4011,0,0.0699023,", self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation (Hermjakob et al., 2018b) and speech recognition (Chen et al., 2016). In the former case (assuming the translation direction is Foreign-to-English), it is often sufficient to translate several of the most important content words, then reconstruct the most likely sentence that uses these. In speech recognition, it is possible to listen to a language one does not speak, and produce a phonetic transcriptions that can be aggregated with others into a reasonable transcription, a process referred to as mismatched crowdsourcing. Language Selection We chose three target languages: Indonesian, Russian, and Hindi. These langu"
2021.dash-1.11,D19-1520,0,0.0289331,"Missing"
2021.dash-1.11,N16-1030,0,0.0561538,"Missing"
2021.dash-1.11,P18-4001,0,0.0337637,"Missing"
2021.dash-1.11,2020.lrec-1.862,0,0.0537892,"g (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation (Hermjakob et al., 2018b) and speech recognition (Chen et al., 2016). In the former case (assuming the translation direction is Foreign-to-English), it is often sufficient to translate several of the most important content words, then reconstruct the most likely sentence that uses these. In speech recognition, it is possible to listen to a language one does not speak, and produce a phonetic transcriptions that can be aggregated with others into a reasonable transc"
2021.dash-1.11,N19-1423,0,0.0106348,"used English exercise scores to choose between the resulting conflicting annotations for the same document sets. A summary of the data selection process is shown in Figure 2. In order to ensure that documents lacking annotations were considered to be NS annotator mistakes rather than negative training examples, we removed all empty documents from the NS data before training. No other pre-processing was done. Machine Learning Models For all experiments, we used a standard BiLSTM-CRF model (Ma and Hovy, 2016) implemented in AllenNLP (Gardner et al., 2018), and used multilingual BERT embeddings (Devlin et al., 2019), which have 65 Indonesian Russian 80 Gold: 77.2 Gold: 75.2 Model Performance F1 Hindi Gold: 72.8 70 66.3 64.1 63.7 64.3 60.3 58.2 58.2 59.2 61.0 60 53.4 54.8 54.0 Eng Eng+NS 50 FS (CBL) NS (CBL) Eng Eng+NS FS (CBL) NS (CBL) Eng Eng+NS FS (CBL) NS (CBL) Figure 3: Comparison of models trained on fluent speaker (FS) and non-speaker (NS) annotations to English crosslingual models, showing comparable or improved performance across all languages. Error bars show one standard deviation calculated over five trials. CBL refers to Constrained Binary Learning. The Eng+NS model is trained on the concaten"
2021.dash-1.11,P16-1101,0,0.185494,"anagari Yes Yes No Amerika Америка अमे रका Table 1: Factors contributing to language difficulty, with examples of the English word “America.” 3 Experimental Setup Our experiment consisted of a series of trials, typically attended by 1–5 participants. Each trial ran for four hours and consisted of three tasks: (1) one-hour instructional training, (2) 20-minute English annotation exercise, and (3) series of five 30-minute sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an"
2021.dash-1.11,P18-4014,1,0.930319,"e NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation (Hermjakob et al., 2018b) and speech recognition (Chen et al., 2016). In the former case (assuming the translation direction is Foreign-to-English), it is often sufficient to translate several of the most important content words, then reconstruct the most likely sentence that uses these. In speech recognition, it is possible to listen to a language one does not speak, and produce a p"
2021.dash-1.11,W18-2501,0,0.0216724,"least two participants to annotate each document set. We then used English exercise scores to choose between the resulting conflicting annotations for the same document sets. A summary of the data selection process is shown in Figure 2. In order to ensure that documents lacking annotations were considered to be NS annotator mistakes rather than negative training examples, we removed all empty documents from the NS data before training. No other pre-processing was done. Machine Learning Models For all experiments, we used a standard BiLSTM-CRF model (Ma and Hovy, 2016) implemented in AllenNLP (Gardner et al., 2018), and used multilingual BERT embeddings (Devlin et al., 2019), which have 65 Indonesian Russian 80 Gold: 77.2 Gold: 75.2 Model Performance F1 Hindi Gold: 72.8 70 66.3 64.1 63.7 64.3 60.3 58.2 58.2 59.2 61.0 60 53.4 54.8 54.0 Eng Eng+NS 50 FS (CBL) NS (CBL) Eng Eng+NS FS (CBL) NS (CBL) Eng Eng+NS FS (CBL) NS (CBL) Figure 3: Comparison of models trained on fluent speaker (FS) and non-speaker (NS) annotations to English crosslingual models, showing comparable or improved performance across all languages. Error bars show one standard deviation calculated over five trials. CBL refers to Constrained"
2021.dash-1.11,K19-1060,1,0.90189,"Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation (Hermjakob et al., 2018b) and speech recognition (Chen et al., 2016). In the former case (assuming the translation direction is Foreign-to-English), it is often sufficient to translate several"
2021.dash-1.11,W19-6808,0,0.0435579,"Missing"
2021.dash-1.11,K16-1022,1,0.831575,"se, and (3) series of five 30-minute sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation ("
2021.dash-1.11,D19-1077,0,0.200602,"methods built on modern contextual representations, and have the potential to outperform with additional effort. We conclude with observations of common annotation patterns and recommended implementation practices, and motivate how NS annotations can be used in addition to prior methods for improved performance.1 1 ROMANIZATION GPE LOC nyuyorka shahar ke aham hisse semttral paarka par uddatii sii najar ddaalate hue gujarataa huum to... New York? Central Park? Figure 1: An example of how romanized Hindi text can be annotated without prior language knowledge. shown to be surprisingly effective (Wu and Dredze, 2019; Lample and Conneau, 2019). However, in common circumstances, such as when working with languages with insufficient training corpora or those far from the available source languages, cross-lingual methods suffer (Wu and Dredze, 2020; K et al., 2020). Absent sufficient crosslingual methods, conventional wisdom suggests that only native (or fluent) speakers of a language can provide useful data to train NLP models. But in low-resource scenarios, fluent speakers may not be readily available. To address this limitation, we hypothesize that the search for annotators can be extended beyond fluent s"
2021.dash-1.11,D18-1061,0,0.0650307,"Missing"
2021.dash-1.11,2020.repl4nlp-1.16,0,0.108746,"S annotations can be used in addition to prior methods for improved performance.1 1 ROMANIZATION GPE LOC nyuyorka shahar ke aham hisse semttral paarka par uddatii sii najar ddaalate hue gujarataa huum to... New York? Central Park? Figure 1: An example of how romanized Hindi text can be annotated without prior language knowledge. shown to be surprisingly effective (Wu and Dredze, 2019; Lample and Conneau, 2019). However, in common circumstances, such as when working with languages with insufficient training corpora or those far from the available source languages, cross-lingual methods suffer (Wu and Dredze, 2020; K et al., 2020). Absent sufficient crosslingual methods, conventional wisdom suggests that only native (or fluent) speakers of a language can provide useful data to train NLP models. But in low-resource scenarios, fluent speakers may not be readily available. To address this limitation, we hypothesize that the search for annotators can be extended beyond fluent speakers. In this work, we propose an unconventional approach for low-resource named entity recognition (NER) by getting annotations from annotators with no familiarity in the target language, referred to as “non-speaker” (NS) annotat"
2021.dash-1.11,Q17-1020,0,0.0133038,"ch trial ran for four hours and consisted of three tasks: (1) one-hour instructional training, (2) 20-minute English annotation exercise, and (3) series of five 30-minute sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which"
2021.dash-1.11,P19-1579,0,0.0137386,"e tasks: (1) one-hour instructional training, (2) 20-minute English annotation exercise, and (3) series of five 30-minute sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with"
2021.dash-1.11,W09-1119,1,0.446445,"ipt Capitalization Example Latin Cyrillic Devanagari Yes Yes No Amerika Америка अमे रका Table 1: Factors contributing to language difficulty, with examples of the English word “America.” 3 Experimental Setup Our experiment consisted of a series of trials, typically attended by 1–5 participants. Each trial ran for four hours and consisted of three tasks: (1) one-hour instructional training, (2) 20-minute English annotation exercise, and (3) series of five 30-minute sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations"
2021.dash-1.11,D17-1035,0,0.0128372,"di 55.5 42.5 24.5 54.7 47.5 32.3 57.8 47.5 41.1 57.3 49.7 40.3 57.5 50.6 41.8 Table 4: Annotation quality of annotations collected from fluent speaker (FS) and non-speaker (NS) annotators against the gold data. Table 5: Changes in mean annotation quality of nonspeaker (NS) annotations over time show an upwards trajectory that steepens with language difficulty. 4 been shown to exhibit surprising cross-lingual properties (Wu and Dredze, 2019). For the sake of speed and simplicity, we use BERT embeddings as features, and do not fine-tune the model. For each dataset, we train with 5 random seeds (Reimers and Gurevych, 2017) and report the average. We recognize that these annotations are missing many entities. Following recent work on partial annotations, we use an iterative method from (Mayhew et al., 2019a) called Constrained Binary Learning (CBL) that detects unmarked tokens likely to be entities and down-weights them in training. Subsequent results reported use this method on all FS and NS annotations. Experiments & Analysis This section describes the analysis done on the gathered FS and NS annotations, through the setup of our models and metrics used and key experimental takeaways. 4.1 Models & Metrics Two P"
2021.dash-1.11,D18-1034,0,0.0155112,"sessions annotating documents in the target language. Related Work Named Entity Recognition (NER) has been studied for many years (Ratinov and Roth, 2009; Lample et al., 2016; Ma and Hovy, 2016), with most focus on English and a few other European languages (Tjong Kim Sang and De Meulder, 2003). Recently, there has been growing interest in low-resource NLP, with work in part-of-speech tagging (Plank and Agić, 2018), parsing (Rasooli and Collins, 2017), machine translation (Xia et al., 2019), and other fields. Low-resource NER has seen work using Wikipedia (Tsai et al., 2016), self attention (Xie et al., 2018), and multilingual contextual representations (Wu and Dredze, 2019). There has been a small amount of work using non-speaker annotations (Mayhew et al., 2019a), but mainly as an application of a technique, falling short of the exhaustive study in this paper. Several interfaces exist for non-speaker annotations in NER, including TALEN (Mayhew, 2018), which we use, ELISA IE (Lin et al., 2018), and Dragonfly (Costello et al., 2020), which performed small-scale experiments with non-speaker annotators. A similar approach has been proposed for machine translation (Hermjakob et al., 2018b) and speech"
2021.dash-1.11,L16-1521,0,0.0298591,"Missing"
2021.dash-1.11,W03-0419,0,0.126096,"Missing"
2021.eacl-main.231,P15-2097,0,0.0184758,"of the system outputs, whereas we are creating golds by directly correcting the hypotheses output by the system. We note that there have been other proposals that argue that this principle still does not make the output fluent and propose generating references based on fluency (Sakaguchi et al., 2016). As suggested by Choshen and Abend (2018b), correcting for fluency further increases the space of valid corrections for a sentence, and we do not attempt to do this in this work. Reference-based evaluations include several measures, such as the MaxMatch scorer M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015), ERRANT (Bryant et al., 2017), and Imeasure (Felice and Briscoe, 2015). These met2687 rics have some commonalities, e.g. both MaxMatch and ERRANT measure precision, recall, and F-score. M 2 has been used with different beta parameter values, the default is beta = 0.5, weighting precision twice as high as recall, which is more common than assigning equal weights and has been shown to have stronger correlation with human ratings (Grundkiewicz et al., 2015). GLEU focuses on the fluency aspect – it is an extension of the BLEU metric in Machine Translation (Papineni et al., 2002). I-measure emphas"
2021.eacl-main.231,E17-2037,0,0.0131704,"upy. A system is evaluated using reference-based metrics where for each source sentence there is at least one corresponding corrected version that was generated by a human expert. We refer to this corrected version as Reference Gold (RG). The set of possible correct versions for a given source sentence is very large – possibly infinite – and any single reference gold is just a single point in that space. Most of the GEC evaluation sets contain one RG for each source sentence, although some (English) datasets contain more (CoNLL-test has 2 and an additional set of 8 generated later, and JFLEG (Napoles et al., 2017) has 4 fluency-based references). System performance is computed by scoring the topranked hypothesis H1 for each sentence against the corresponding RG. In addition to RGs, we create for each pair of (source, Hi ), where Hi is the system hypothesis, • • • • • S – original text Hi – hypothesis at rank i RG – reference gold CGi – closest gold to hypothesis Hi Gold edit – an edit between a source sentence and an RG or CG Proposed edit – an edit between a source sentence and a system hypothesis Correct edit – a proposed edit that is also a gold edit relative to a system hypothesis and specific refe"
2021.eacl-main.231,W13-3601,0,0.0740022,"Missing"
2021.eacl-main.231,W14-1701,0,0.0245585,"(hypotheses at ranks 1 and 10, H1 and H10 ); the reference gold (RG), and two additional golds generated on top of each of the hypotheses (CG1 and CG10 ). edit), while Dist (H10 , CG10 ) is 0. The three golds – two CGs and the RG – illustrate the notion of semantic equivalence (multiple ways of correcting the same source sentence, while preserving its meaning), not reflected in the standard evaluation. 3.2 Experimental Setup We perform experiments on 2 English and 2 Russian datasets, using diverse NMT GEC model frameworks. The English datasets include the commonly used benchmarks – CoNLL-14 (Ng et al., 2014; Dahlmeier et al., 2013), and the BEA corpus (Bryant et al., 2019). The Russian datasets include the RULEC-GEC corpus (Rozovskaya and Roth, 2019) (henceforth RULEC) and another dataset of Russian learner writing that has been recently collected from the online language learning platform Lang-8 (Mizumoto et al., 2011) and annotated by native speakers.2 We refer to this dataset as Lang8. CoNLL-14 contains two primary RGs against which the systems are standardly evaluated, while the other datasets include one RG for each sentence. We report results using one RG for each dataset for uniformity, a"
2021.eacl-main.53,P18-1073,0,0.0486409,"Missing"
2021.eacl-main.53,Q17-1010,0,0.152253,"Missing"
2021.eacl-main.53,2021.eacl-main.53,1,0.0530913,"Missing"
2021.eacl-main.53,P13-1133,0,0.149627,"Missing"
2021.eacl-main.53,2020.acl-main.747,0,0.0922775,"Missing"
2021.eacl-main.53,N19-1423,0,0.0287513,"Missing"
2021.eacl-main.53,P17-1149,0,0.0768407,"Missing"
2021.eacl-main.53,P19-1140,0,0.101127,"Missing"
2021.eacl-main.53,N15-1184,0,0.153421,"Missing"
2021.eacl-main.53,D17-1284,1,0.915768,"Missing"
2021.eacl-main.53,2020.acl-main.772,1,0.809827,"Missing"
2021.eacl-main.53,L18-1086,1,0.569185,"Missing"
2021.eacl-main.53,D19-1282,0,0.0998123,"Missing"
2021.eacl-main.53,2020.emnlp-main.515,0,0.284641,"Missing"
2021.eacl-main.53,P14-5010,0,0.0233178,"Missing"
2021.eacl-main.53,D14-1162,0,0.0882575,"Missing"
2021.eacl-main.53,N18-1202,0,0.0272654,"Missing"
2021.eacl-main.53,P19-1493,0,0.0601826,"Missing"
2021.eacl-main.53,D17-1184,0,0.141876,"Missing"
2021.eacl-main.53,W18-6246,0,0.262137,"Missing"
2021.eacl-main.53,D19-1113,1,0.865929,"Missing"
2021.eacl-main.53,D19-1075,0,0.0771458,"Missing"
2021.eacl-main.53,P18-5008,1,0.702317,"Missing"
2021.eacl-main.53,N15-1138,1,0.92995,"Missing"
2021.eacl-main.53,Q19-1014,0,0.0411516,"Missing"
2021.eacl-main.53,D15-1174,0,0.124315,"Missing"
2021.eacl-main.53,D18-1270,1,0.922146,"Missing"
2021.eacl-main.53,D14-1167,0,0.0269876,"Missing"
2021.eacl-main.53,D18-1032,0,0.0682124,"Missing"
2021.eacl-main.53,D19-1023,0,0.138875,"Missing"
2021.eacl-main.53,P19-1304,0,0.107631,"Missing"
2021.eacl-main.53,Q17-1028,0,0.106597,"Missing"
2021.eacl-main.53,D19-1451,0,0.101674,"Missing"
2021.eacl-main.53,D15-1031,0,0.058013,"Missing"
2021.eacl-main.53,D13-1141,0,0.241604,"Missing"
2021.emnlp-main.134,P17-1147,0,0.0607312,"Missing"
2021.emnlp-main.134,2020.findings-emnlp.171,0,0.0916629,"ps://github. com/CogComp/PABI. (Pan and Yang, 2009; Vapnik and Vashist, 2009; Roth, 2017; Kolesnikov et al., 2019). However, it remains unclear how to predict the benefits of these incidental signals on our target task beforehand. For example, given two incidental signals that are relevant to the target task, it is still difficult for us to predict which one is more beneficial. Therefore, the common practice is often trial-and-error: perform experiments with different combinations of datasets and learning protocols, often exhaustively, to measure the impact on a target task (Liu et al., 2019; Khashabi et al., 2020). Not only is this very costly, but this trial-and-error approach can also be hard to interpret: if we don’t see improvements, is it because the incidental signals themselves are not useful for our target task, or is it because the learning protocols we have tried are inappropriate? The difficulties of foreseeing the benefits of various incidental supervision signals, including partial labels, noisy labels, constraints2 , and cross-domain signals, are two-fold. First, it is hard to provide a unified measure because of the intrinsic differences among different signals (e.g., how do we 2 Constra"
2021.emnlp-main.134,K17-1034,0,0.0429606,"Missing"
2021.emnlp-main.134,N19-1112,0,0.143518,"y available at https://github. com/CogComp/PABI. (Pan and Yang, 2009; Vapnik and Vashist, 2009; Roth, 2017; Kolesnikov et al., 2019). However, it remains unclear how to predict the benefits of these incidental signals on our target task beforehand. For example, given two incidental signals that are relevant to the target task, it is still difficult for us to predict which one is more beneficial. Therefore, the common practice is often trial-and-error: perform experiments with different combinations of datasets and learning protocols, often exhaustively, to measure the impact on a target task (Liu et al., 2019; Khashabi et al., 2020). Not only is this very costly, but this trial-and-error approach can also be hard to interpret: if we don’t see improvements, is it because the incidental signals themselves are not useful for our target task, or is it because the learning protocols we have tried are inappropriate? The difficulties of foreseeing the benefits of various incidental supervision signals, including partial labels, noisy labels, constraints2 , and cross-domain signals, are two-fold. First, it is hard to provide a unified measure because of the intrinsic differences among different signals (e"
2021.emnlp-main.134,K19-1060,1,0.89533,"Missing"
2021.emnlp-main.134,N19-1227,1,0.912195,"measure the benefit of mixed incidental signals (e.g., a dataset that is both noisy and partially annotated). (3) PABI is derived from PAC-Bayesian theory but also easy to compute in practice; PABI is shown to have similar or better predicting capability of signals’ benefit (see Figs. 2 and 3 and Sec. 4.1). Papers are denoted as: CST’11 (Cour et al., 2011), HH’12 (Hovy and Hovy, 2012), LD’14 (Liu and Dietterich, 2014), AL’88 (Angluin and Laird, 1988), NDRT’13 (Natarajan et al., 2013), RVBS’17 (Rolnick et al., 2017), VW’17 (Van Rooyen and Williamson, 2017), WNR’20 (Wang et al., 2020), NHFR’19 (Ning et al., 2019), B’17 (Bjerva, 2017), GMSLBDS’20 (Gururangan et al., 2020). predict and compare the benefit of learning from partial data and the benefit of knowing some constraints on the target task?). Second, it is hard to provide a practical measure supported by theory. Previous attempts are either not practical (Baxter, 1998; Ben-David et al., 2010) or too heuristic (Gururangan et al., 2020), or apply to only one type of signals, e.g., noisy labels (Natarajan et al., 2013; Zhang et al., 2021). In this paper, we propose a unified PAC-Bayesian motivated informativeness measure (PABI) to quantify the value"
2021.emnlp-main.134,D14-1162,0,0.0853444,"Missing"
2021.emnlp-main.134,D16-1264,0,0.076842,"Missing"
2021.emnlp-main.134,W17-2623,0,0.0331824,"Missing"
2021.emnlp-main.423,P19-1471,0,0.0162404,"nsion to the average of the input and output neurons, following Chen et al. (2018). The parameters of the model are optimized using AMSGrad (Reddi et al., 2018), with the learning rate set to 10−6 . The training process is limited to 40 epochs since it is sufficient for convergence. 5.4 Results We report the results for subevent detection on two benchmark datasets, HiEve and IC, in Tab. 2. Among the baseline methods, Wang et al. (2020) has the best results in terms of F1 on both datasets. They integrate event temporal relation extraction, 3 Despite carefully following the details described in Aldawsari and Finlayson (2019) and communicating with the authors, we were not able to reproduce their results. Therefore, we choose to compare with other methods. 4 To make predictions on event complexes, we keep all negative N O R EL instances in our experiments instead of strictly following Zhou et al. (2020) and Wang et al. (2020) where negative instances are down-sampled with a probability of 0.4. 5221 Corpus IC HiEve Model Araki et al. (2014) Wang et al. (2020) Our model Zhou et al. (2020) Wang et al. (2020) Our model PC 0.421 0.446 0.485 0.472 0.534 F1 score CP 0.495 0.516 0.494 0.524 0.510 5.5 Avg. 0.262 0.458 0.48"
2021.emnlp-main.423,2020.tacl-1.36,1,0.730362,"d arrows denote cross-segment PARENT-C HILD relations. more fine-grained events such as “writing the paper,” “passing the peer review,” and “presenting at the conference.” Naturally, understanding events requires to resolve the granularity of events and infer their memberships, which corresponds to the task of subevent detection (a.k.a. event hierarchy extraction). Practically, subevent detection is a key component of event-centric NLU (Chen et al., 2021), and is beneficial to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, p"
2021.emnlp-main.423,araki-etal-2014-detecting,0,0.0712949,"Missing"
2021.emnlp-main.423,A00-2004,0,0.233754,"et al., 2016). articles have been studied in Choubey et al. (2020). To capture the logical dependency between Hence, we attempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regard"
2021.emnlp-main.423,2020.acl-main.478,0,0.0323221,"segmentation signals are not task seeks to provide effective incidental superviincorporated into the task of subevent detection. sion signals (Roth, 2017) to the subevent detection Actually, research on event-centric NLU (Chen task. This is especially important in the current et al., 2021) has witnessed the usage of documentscenario where annotated learning resources for level discourse relations: different functional dissubevents are typically limited (Hovy et al., 2013; course structures around the main event in news Glavaš et al., 2014; O’Gorman et al., 2016). articles have been studied in Choubey et al. (2020). To capture the logical dependency between Hence, we attempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been s"
2021.emnlp-main.423,N19-1423,0,0.00574743,"019) collect a variety of features to segment a document into descriptive contexts before feeding into classifiers for pairwise decision. of different event complexes. Evidently, with Nevertheless the features often require costly huE VENT S EG information, it would be relatively easy man effort to obtain, and are often dataset-specific. to infer the memberships of events in the same deData-driven methods, on the other hand, automatiscriptive context. Using annotations for subevent cally characterize events with neural language moddetection and E VENT S EG prediction, we aim to els like BERT (Devlin et al., 2019), and can simuladopt a neural model to jointly learn these two tasks tanously incorporate various signals such as event along with the (soft) logical constraints that bridge time duration (Zhou et al., 2020), joint constraints their labels together. In this way, we incorporate with event temporal relations (Wang et al., 2020) linear discourse structure of segments into memberand subevent knowledge (Yao et al., 2020). Among ship relation extraction, avoiding complicated fearecent methods, only Aldawsari and Finlayson ture engineering in the previous work (Aldawsari (2019) utilize discourse feat"
2021.emnlp-main.423,N09-1040,0,0.0190305,"ttempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent Detection is to i"
2021.emnlp-main.423,D08-1035,0,0.0481045,"dependency between Hence, we attempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent"
2021.emnlp-main.423,W14-3705,0,0.395069,"ow that describes the event complex. As can be seen in the paragraph, though we cannot deny the existence of cross-segment subevent relations (dotted arrows), events belonging to the same membership are much more often to co-occur in a text segment. This correlation has been overlooked by existing data-driven methods (Zhou et al., 2020; Yao et al., 2020), which formulate subevent detection as pairwise relation extraction. On the other hand, while prior studies have demonstrated the benefits of incorporating logical constraints among event memberships and other relations (such as coreference) (Glavaš and Šnajder, 2014; Wang et al., 2020), the constraints between the memberships and event co-occurences in text segments remain uncertain. Hence, how to effectively learn and enforce hard-to-articulate constraints as in the case of subevent detection and segmentation of text is another challenge. subevent structure and E VENT S EG, our second contribution is an approach to automatically learning and enforcing logical constraints. Motivated by Pan et al. (2020), we use Rectifier Networks to learn constraints in the form of linear inequalities, and then convert the constraints to a regularization term that can be"
2021.emnlp-main.423,glavas-etal-2014-hieve,0,0.402325,"t tive, adding E VENT S EG prediction as an auxiliary still document-level segmentation signals are not task seeks to provide effective incidental superviincorporated into the task of subevent detection. sion signals (Roth, 2017) to the subevent detection Actually, research on event-centric NLU (Chen task. This is especially important in the current et al., 2021) has witnessed the usage of documentscenario where annotated learning resources for level discourse relations: different functional dissubevents are typically limited (Hovy et al., 2013; course structures around the main event in news Glavaš et al., 2014; O’Gorman et al., 2016). articles have been studied in Choubey et al. (2020). To capture the logical dependency between Hence, we attempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text"
2021.emnlp-main.423,2021.emnlp-main.597,1,0.703397,"ions, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5216–5226 c November 7–11, 2021. 2021 Association for Computational Linguistics ral to perceive that in documents, there might be several different event complexes and they often span in different descriptive contexts that form relatively independent text segments. Consider the example in Fig. 1, where the two membership relations in the event complex (graph consisting of “scandal (e7),” “charges (e6),” “ousting (e8),” and relations) are both within the segment marked in yellow that d"
2021.emnlp-main.423,W13-1203,0,0.32124,"the learning perspeclations between elementary discourse units, but tive, adding E VENT S EG prediction as an auxiliary still document-level segmentation signals are not task seeks to provide effective incidental superviincorporated into the task of subevent detection. sion signals (Roth, 2017) to the subevent detection Actually, research on event-centric NLU (Chen task. This is especially important in the current et al., 2021) has witnessed the usage of documentscenario where annotated learning resources for level discourse relations: different functional dissubevents are typically limited (Hovy et al., 2013; course structures around the main event in news Glavaš et al., 2014; O’Gorman et al., 2016). articles have been studied in Choubey et al. (2020). To capture the logical dependency between Hence, we attempt to capture the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervi"
2021.emnlp-main.423,N18-2075,0,0.098984,"nt hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent Detection is to identify membership relations between events, given event mentions in documents. Particularly, R denotes the set of relation labels as defined in Hovy et"
2021.emnlp-main.423,2020.emnlp-main.50,0,0.0396386,"relations within a text segment, whereas the dotted arrows denote cross-segment PARENT-C HILD relations. more fine-grained events such as “writing the paper,” “passing the peer review,” and “presenting at the conference.” Naturally, understanding events requires to resolve the granularity of events and infer their memberships, which corresponds to the task of subevent detection (a.k.a. event hierarchy extraction). Practically, subevent detection is a key component of event-centric NLU (Chen et al., 2021), and is beneficial to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conferenc"
2021.emnlp-main.423,D19-1405,0,0.129224,"04; Glavaš and Šnajder, 2014) formulate the inference process as Integer Linear Programming (ILP) problems. Pan et al. (2020) also employ ILP to enforce constraints learned automatically from Rectifier Networks with strong expressiveness (Pan and Srikumar, 2016). Yet the main drawback of solving an ILP problem is its inefficiency in a large feasible solution space. Recent work on integrating neural networks with structured outputs has emphasized the importance of the interaction between constraints and representations (Rocktäschel and Riedel, 2017; Niculae et al., 2018; Li and Srikumar, 2019; Li et al., 2019, 2020b). However there has been no automatic and efficient ways to learn and enforce constraints that are not limited to first-order logic, e.g., linear inequalities learned via Rectifier Networks. And this is the research focus of our paper. 3 EventSeg prediction aims at finding an optimal segmentation of text that breaks the document into several groups of consecutive sentences, and each sequence is a descriptive context of an event complex (Wang et al., 2020). Being different from the traditional definition of text segmentation, E VENT S EG focuses on the change of event complex (which is"
2021.emnlp-main.423,2020.acl-main.744,0,0.0156131,"relations within a text segment, whereas the dotted arrows denote cross-segment PARENT-C HILD relations. more fine-grained events such as “writing the paper,” “passing the peer review,” and “presenting at the conference.” Naturally, understanding events requires to resolve the granularity of events and infer their memberships, which corresponds to the task of subevent detection (a.k.a. event hierarchy extraction). Practically, subevent detection is a key component of event-centric NLU (Chen et al., 2021), and is beneficial to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conferenc"
2021.emnlp-main.423,P19-1028,0,0.0283545,"forts (Roth and Yih, 2004; Glavaš and Šnajder, 2014) formulate the inference process as Integer Linear Programming (ILP) problems. Pan et al. (2020) also employ ILP to enforce constraints learned automatically from Rectifier Networks with strong expressiveness (Pan and Srikumar, 2016). Yet the main drawback of solving an ILP problem is its inefficiency in a large feasible solution space. Recent work on integrating neural networks with structured outputs has emphasized the importance of the interaction between constraints and representations (Rocktäschel and Riedel, 2017; Niculae et al., 2018; Li and Srikumar, 2019; Li et al., 2019, 2020b). However there has been no automatic and efficient ways to learn and enforce constraints that are not limited to first-order logic, e.g., linear inequalities learned via Rectifier Networks. And this is the research focus of our paper. 3 EventSeg prediction aims at finding an optimal segmentation of text that breaks the document into several groups of consecutive sentences, and each sequence is a descriptive context of an event complex (Wang et al., 2020). Being different from the traditional definition of text segmentation, E VENT S EG focuses on the change of event c"
2021.emnlp-main.423,2020.emnlp-main.380,0,0.0146719,"es: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent Detection is to identify membership relations between events, given event mentions in documents. Particularly, R denotes the set of relation labels as defined in Hovy et al. (2013) and Glavaš et al. (2014) (i.e., PARENT-C HILD , C HILD PARENT, C"
2021.emnlp-main.423,K19-1054,0,0.0164778,"5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent Detection is to identify membership relations between even"
2021.emnlp-main.423,C12-1127,0,0.0300394,"the interdependencies 5217 between subevent detection and segmentation of text, in order to enhance the model performance for event hierarchy extraction. ument D is represented as a sequence of binary values: QD = {q1 , q2 , · · · , qm−1 }, where qi indicates whether sentence si is the end of a segment. Text Segmentation. Early studies in this line have concentrated on unsupervised text segmentation, quantifying lexical cohesion within small text segments (Choi, 2000), and unsupervised Bayesian approaches have also been successful in this task (Eisenstein and Barzilay, 2008; Eisenstein, 2009; Newman et al., 2012; Mota et al., 2019). Given that unsupervised algorithms are difficult to specialize for a particular domain, Koshorek et al. (2018) formulate the problem as a supervised learning task. Lukasik et al. (2020) follow this idea by using transformer-based architectures with cross segment attention to achieve state-of-the-art performance. Focusing on creating logically coherent sub-document units, these prior work do not cover segmentation of text regarding descriptive contexts of event complexes, which is the focus of the auxiliary task in this work. Subevent Detection is to identify membership re"
2021.emnlp-main.423,W16-5706,0,0.0526514,"Missing"
2021.emnlp-main.423,2020.acl-main.438,0,0.379534,"ior studies have demonstrated the benefits of incorporating logical constraints among event memberships and other relations (such as coreference) (Glavaš and Šnajder, 2014; Wang et al., 2020), the constraints between the memberships and event co-occurences in text segments remain uncertain. Hence, how to effectively learn and enforce hard-to-articulate constraints as in the case of subevent detection and segmentation of text is another challenge. subevent structure and E VENT S EG, our second contribution is an approach to automatically learning and enforcing logical constraints. Motivated by Pan et al. (2020), we use Rectifier Networks to learn constraints in the form of linear inequalities, and then convert the constraints to a regularization term that can be incorporated into the loss function of the neural model. This allows any hard-to-articulate constraints to be automatically captured for interrelated tasks, and efficiently guides the model to make globally consistent inference. By learning and enforcing task-specific constraints for subevent relations, the proposed method achieves comparable results with SOTA subevent detection methods on the HiEve and IC dataset. Moreover, by jointly learn"
2021.emnlp-main.423,W04-2401,1,0.410484,"ubevent Detection is to identify membership relations between events, given event mentions in documents. Particularly, R denotes the set of relation labels as defined in Hovy et al. (2013) and Glavaš et al. (2014) (i.e., PARENT-C HILD , C HILD PARENT, C OREF, and N O R EL). For a relation r to denote r ∈ R, we use a binary indicator Yi,j whether an event pair (ei , ej ) has relation r, and r to denote the model-predicted possibility use yi,j of an event pair (ei , ej ) to have relation r. Learning with Constraints. In terms of enforcing declarative constraints in neural models, early efforts (Roth and Yih, 2004; Glavaš and Šnajder, 2014) formulate the inference process as Integer Linear Programming (ILP) problems. Pan et al. (2020) also employ ILP to enforce constraints learned automatically from Rectifier Networks with strong expressiveness (Pan and Srikumar, 2016). Yet the main drawback of solving an ILP problem is its inefficiency in a large feasible solution space. Recent work on integrating neural networks with structured outputs has emphasized the importance of the interaction between constraints and representations (Rocktäschel and Riedel, 2017; Niculae et al., 2018; Li and Srikumar, 2019; Li"
2021.emnlp-main.423,P11-2061,0,0.0113331,"ablation study for illustrating the importance of each model component (§5.2-§5.4). We also provide a case study on E VENT S EG prediction (§5.5) and an analysis of the constraints learned in the model (§5.6). 5.1 Datasets HiEve The HiEve corpus (Glavaš et al., 2014) contains 100 news articles. Within each article, annotations are given for both subevent membership 4.3 Inference and coreference relations. Using the same measureAt inference time, to extract relations in the ment of inter-annotator agreement (IAA) as event subevent detection task, we input a pair of events temporal relations in UzZaman and Allen (2011), 5220 Relations Parent-Child Child-Parent Coref NoRel HiEve Within Across 1,123 679 1,067 779 322 436 32,029 31,726 5.2 IC Within 1,698 1,475 1,476 40,072 Across 550 863 877 41,815 Table 1: Statistics of the HiEve and IC dataset. Numbers in column “Within” denote the number of relations appearing within the same descriptive context of event complex, whereas numbers under “Across” denote those across different segments. the HiEve dataset has an IAA of 0.69 F1. Intelligence Community (IC) The IC corpus (Hovy et al., 2013) also contains 100 news articles annotated with membership relations. The"
2021.emnlp-main.423,2020.emnlp-main.51,1,0.353206,"to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5216–5226 c November 7–11, 2021. 2021 Association for Computational Linguistics ral to perceive that in documents, there might be several different event complexes and they often span in different descriptive contexts that form relatively independent text segments. Consider the example in Fig. 1, where the two membership relations in the event complex (graph consisting of “scandal (e7),” “charges (e6),” “ousting (e8),” and relations) are both within the segment mark"
2021.emnlp-main.423,2020.emnlp-main.430,0,0.142186,"hat form relatively independent text segments. Consider the example in Fig. 1, where the two membership relations in the event complex (graph consisting of “scandal (e7),” “charges (e6),” “ousting (e8),” and relations) are both within the segment marked in yellow that describes the event complex. As can be seen in the paragraph, though we cannot deny the existence of cross-segment subevent relations (dotted arrows), events belonging to the same membership are much more often to co-occur in a text segment. This correlation has been overlooked by existing data-driven methods (Zhou et al., 2020; Yao et al., 2020), which formulate subevent detection as pairwise relation extraction. On the other hand, while prior studies have demonstrated the benefits of incorporating logical constraints among event memberships and other relations (such as coreference) (Glavaš and Šnajder, 2014; Wang et al., 2020), the constraints between the memberships and event co-occurences in text segments remain uncertain. Hence, how to effectively learn and enforce hard-to-articulate constraints as in the case of subevent detection and segmentation of text is another challenge. subevent structure and E VENT S EG, our second contr"
2021.emnlp-main.423,2020.emnlp-main.119,1,0.779026,"enote PARENT-C HILD relations within a text segment, whereas the dotted arrows denote cross-segment PARENT-C HILD relations. more fine-grained events such as “writing the paper,” “passing the peer review,” and “presenting at the conference.” Naturally, understanding events requires to resolve the granularity of events and infer their memberships, which corresponds to the task of subevent detection (a.k.a. event hierarchy extraction). Practically, subevent detection is a key component of event-centric NLU (Chen et al., 2021), and is beneficial to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of t"
2021.emnlp-main.423,2020.coling-main.39,0,0.0378944,"re fine-grained events such as “writing the paper,” “passing the peer review,” and “presenting at the conference.” Naturally, understanding events requires to resolve the granularity of events and infer their memberships, which corresponds to the task of subevent detection (a.k.a. event hierarchy extraction). Practically, subevent detection is a key component of event-centric NLU (Chen et al., 2021), and is beneficial to various applications, such as schema induction (Zhang et al., 2020; Li et al., 2020a), task-oriented dialogue agents (Andreas et al., 2020), summarization (Chen et al., 2019; Zhao et al., 2020), and risk detection (Pohl et al., 2012). As a significant step towards inducing event complexes (graphs that recognize the relationship ∗ This work was done when the author was visiting the of multi-granular events) in documents, subevent University of Pennsylvania. 1 Our code is publicly available at http://cogcomp. detection has started to receive attention recently org/page/publication_view/950. (Wang et al., 2020; Han et al., 2021). It is natu5216 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5216–5226 c November 7–11, 2021. 2021 Association"
2021.emnlp-main.423,2020.acl-main.678,1,0.915714,"criptive contexts that form relatively independent text segments. Consider the example in Fig. 1, where the two membership relations in the event complex (graph consisting of “scandal (e7),” “charges (e6),” “ousting (e8),” and relations) are both within the segment marked in yellow that describes the event complex. As can be seen in the paragraph, though we cannot deny the existence of cross-segment subevent relations (dotted arrows), events belonging to the same membership are much more often to co-occur in a text segment. This correlation has been overlooked by existing data-driven methods (Zhou et al., 2020; Yao et al., 2020), which formulate subevent detection as pairwise relation extraction. On the other hand, while prior studies have demonstrated the benefits of incorporating logical constraints among event memberships and other relations (such as coreference) (Glavaš and Šnajder, 2014; Wang et al., 2020), the constraints between the memberships and event co-occurences in text segments remain uncertain. Hence, how to effectively learn and enforce hard-to-articulate constraints as in the case of subevent detection and segmentation of text is another challenge. subevent structure and E VENT S E"
2021.emnlp-main.466,W18-2501,1,0.728493,"Missing"
2021.emnlp-main.466,P07-1036,1,0.753615,"Missing"
2021.emnlp-main.466,D19-1418,0,0.0257944,"To counter the authors and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yo"
2021.emnlp-main.466,P17-1123,0,0.0285745,"years after the Battle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and lab"
2021.emnlp-main.466,N19-1246,1,0.943113,"ciation for Computational Linguistics puts using related examples and provides the model with a richer training signal than what is provided by a single example. For example, What was the shortest field goal? shares the substructure of finding all field goals with How many field goals were scored?. For this paired example, our proposed objective would enforce that the output of this latent decision for the two questions is the same. We demonstrate the benefits of our paired training objective using a textual-NMN (Gupta et al., 2020a) designed to answer complex compositional questions on DROP (Dua et al., 2019), a dataset requiring natural language and symbolic reasoning against a paragraph of text. While there can be many ways of acquiring paired examples, we explore three directions. First, we show how naturally occurring paired questions can be automatically found from within the dataset. Further, since our method does not require end-task supervision for the paired example, one can also use data augmentation techniques to acquire paired questions without requiring additional annotation. We show how paired questions can be constructed using simple templates, and how a question generation model ca"
2021.emnlp-main.466,N16-1024,0,0.0321259,") where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect l"
2021.emnlp-main.466,P18-1033,0,0.157998,"substructures provides the model with a dense enough training signal to learn correct module execution. That is, not only does performance improve by using the paired objective, this result shows that the model’s performance is improving for the right reasons. In §5.4 we explore how this faithfulness is actually achieved. 5.3 Evaluating Compositional Generalization A natural expectation from structured models is that the explicit structure should help the model learn reusable operations that generalize to novel contexts. We test this capability using the compositional generalization setup of Finegan-Dollak et al. (2018), where the model is tested on questions whose program templates are unseen during training. In our case, this tests whether module executions generalize to new contexts in a program. We create two test sets to measure our model’s capability to generalize to such out-of-distribution examples. In both settings, we identify certain program templates to keep in a held-out test set, and use the remaining questions for training and validation purposes. Complex Arithmetic This set contains questions that require addition and subtraction operations in complex contexts: questions whose program contain"
2021.emnlp-main.466,2021.acl-short.22,1,0.78435,"Missing"
2021.emnlp-main.466,N18-2017,0,0.0217897,"d training examLearning such models using just the end-task ples share internal substructure, we add an adsupervision is difficult, since the decision boundary ditional training objective to encourage conthat the model is trying to learn is complex, and sistency between their latent decisions. Such the lack of any supervision for the latent decisions an objective does not require external superprovides only a weak training signal. Moreover, the vision for the values of the latent output, or presence of dataset artifacts (Lai and Hockenmaier, even the end task, yet provides an additional 2014; Gururangan et al., 2018, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Su"
2021.emnlp-main.466,D19-1170,0,0.0449838,"Missing"
2021.emnlp-main.466,D17-1195,0,0.0252874,"(xi )) (2) zj = f (k(g(xj ))) (3) where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learn"
2021.emnlp-main.466,D07-1031,0,0.0821882,"ard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these latent tasks correctly and usually end up modeling irrelevant correlations in the data (Johnson, 2007; Subramanian et al., 2020). In this work, we propose a method to leverage paired examples—examples whose one or more latent decisions are related to each other—to provide an indirect supervision to these latent decisions. Consider paired training examples xi and xj with the following computation trees: We focus on structured compositional models for reasoning that perform an explicit problem decomposition and predict interpretable latent decisions that are composed to predict the final output. These intermediate outputs are often grounded in real5775 Figure 1: Proposed paired objective: For t"
2021.emnlp-main.466,D16-1032,0,0.0280223,"The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these"
2021.emnlp-main.466,W19-4801,0,0.0253044,"uding the one in our model) often fail to general- better count performance. Using paired examples ize compositionally (Finegan-Dollak et al., 2018; only for max and count questions (L max+count ) does Lake and Baroni, 2018; Bahdanau et al., 2019). Re- not constrain the find operation sufficiently—the cent advancements in semantic parsing models that model has freedom to optimize the paired objective aim at compositional generalization should help by learning to incorrectly ground to the max-event improve overall model performance (Lake, 2019; mention for both the original and constructed quesKorrel et al., 2019; Herzig and Berant, 2020). tion’s find operation. This analysis reveals that augmented paired examples are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max"
2021.emnlp-main.466,P19-1224,0,0.0216156,"attle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program f"
2021.emnlp-main.466,S14-2055,0,0.02464,"Missing"
2021.emnlp-main.466,2020.acl-main.703,0,0.0194548,"min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modules in a DROP question’s program, we see if a gen"
2021.emnlp-main.466,D19-1405,0,0.0240789,"n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a suitable S for the"
2021.emnlp-main.466,P19-1598,1,0.850477,"b-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct pr"
2021.emnlp-main.466,K18-1007,0,0.171852,": n]) = g(BERT(xi , p)[m : n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a s"
2021.emnlp-main.466,2020.findings-emnlp.225,1,0.843383,"Missing"
2021.emnlp-main.466,P19-1621,1,0.734336,"ork in complex compositional question answering. We also show that using our paired objective leads to improved prediction of latent decisions. The challenge in learning models for complex problems can be viewed as the emergence of artificially simple decision boundaries due to data sparsity and the presence of spurious dataset biases (Gardner et al., 2020). To counter data sparsity, data augmentation techniques have been proposed to provide a compositional inductive bias to the model (Chen et al., 2020; Andreas, 2020) or induce consistent outputs (Xie et al., 2020; Asai and Hajishirzi, 2020; Ribeiro et al., 2019). In order to induce correct internal learning, Teney et al. (2019) use auxiliary Acknowledgements relations between questions in VQA to enforce constraints between related questions’ embeddings, We would like to thank Dan Deutsch and the anonyand Teney et al. (2020) propose an auxiliary objec- mous reviewers for their helpful comments. This tive for the gradient update of an example based work was supported by Contract FA8750-19-2on existing counterfactual data. However, appli- 0201 with the US Defense Advanced Research cability of these approaches is limited to prob- Projects Agency (DARPA),"
2021.emnlp-main.466,P05-1044,0,0.564274,"are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max Count score (↓) 57.4 60.9 60.8 71.1 82.1 85.5 81.4 85.4 36.2 39.7 43.0 58.8 110.4 56.5 99.2 25.9 tures (Smith and Eisner, 2005; Chang et al., 2010). These approaches use auxiliary objectives on a single training instance or global conditions on posterior distributions, whereas our training objective uses paired examples. 7 Conclusion Table 4: Using constructed paired examples for all three types of questions—min, max, and count—leads to dramatically better count performance. Without all three, the model finds shortcuts to satisfy the consistency constraint and does not learn correct module execution. 6 Related Work We propose a method to leverage paired examples— instances that share internal substructure—to provide"
2021.emnlp-main.466,2020.emnlp-main.665,0,0.023609,"rs and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yoshua rect supervision"
2021.emnlp-main.466,2020.acl-main.495,1,0.914301,"18, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Subramanian et al., 2020). within the dataset, (b) constructing paired exWe propose a method to leverage related trainamples using templates, and (c) generating paired examples using a question generation ing examples to provide an indirect supervision to model. We empirically demonstrate that our these intermediate decisions. Our method is based proposed approach improves both in- and outon the intuition that related examples involve simiof-distribution generalization and leads to corlar sub-tasks; hence, we can use an objective on the rect latent decision predictions. outputs of these sub-tasks to provide an additio"
2021.emnlp-main.466,2020.acl-main.450,0,0.0176762,"stions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modul"
2021.emnlp-main.466,2020.tacl-1.13,1,0.836504,"und, it is used as a paired example for the DROP question to enforce consistency between the find modules. For example, How many percentage points did the population of non-Hispanic Whites drop from 1990 to 2010? is paired with the generated question What percentage of the population was nonHispanic Whites in 2010?. 5 Experiments Dataset and Setup We perform experiments on the subset of the DROP dataset (Dua et al., 2019) that is covered by the modules in Text-NMN. This subset is a union of the data used by Gupta et al. (2020a) and the question decomposition annotations in the B REAK dataset (Wolfson et al., 2020). All questions in our dataset contain program annotations (heuristically annotated by Gupta et al. (2020a); crowd-sourced in B REAK). The program annotations only supervise the layouts of the modules, and not the intermediate outputs. We only use these programs for training; all test results are based on predicted programs. Our complete subset of DROP contains 23215 question-answer pairs. For an i.i.d. split, since the DROP test set is hidden, we split the training set into train/validation and use the provided validation set as the test set. Our train/validation/test sets contain 18299/2460/"
2021.emnlp-main.597,glavas-etal-2014-hieve,0,0.513779,"n Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https://github.com/PlusLabNLP/ESTER. knowledge, which could suffer from relatively low 7543 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7543–7559 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: Examples of event annotations and 5 types of QAs in our dataset. Not all events are annotated for clarity purpose. Different colors are used for better visualization. inter-annotator agreements (Glavaš et al., 2014; A few noticeable event-centric MRC datasets O’Gorman et al., 2016) and may not be the most have been proposed recently. TORQUE (Ning natural way to exploit the semantic connections et al., 2020b) and MCTACO (Zhou et al., 2019) are between relations and events in their context. two recent MRC datasets that study event tempoWe instead propose to reason about event seman- ral relations. However, knowing only the temporal aspect of events could not solve many important tic relations as a reading comprehension / question event semantic relations. For example, in Figure 1, answering task. Natural"
2021.emnlp-main.597,D19-1243,0,0.0609774,"Missing"
2021.emnlp-main.597,2020.acl-main.703,0,0.0207559,"quals to 1 if the top predicted answer, i.e. a0i,1 or a00i,1 contains a correct event trigger; otherwise it is 0. This metrics is well defined as all questions in our data contain at least an answer and all (well trained) models return at least one answers. For both generative and extractive QAs, we use the leftmost answer as the top answer. • EM or exact-match equals to 1 if ∀a0i ∈ A0i , a0i ∈ Ai and ∀ai ∈ Ai , ai ∈ A0i ; otherwise, EM = 0. 6.4 Baselines Model Baselines. For our primary generation QA task, we fine-tuned several sequence-to-sequence pre-trained language models on ESTER: BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and UnifiedQA. As mentioned, UnifiedQA (based on BART and T5) is pre-trained on various QA tasks. It also demonstrates powerful zero-shot learning capabilities on unknown QA tasks, which we tested on ESTER too. Due to computation constraints, the largest model we are able to finetune is UnifiedQA (T5-large). We leave further investigation to future modeling studies. Since extractive QA can be considered as a token prediction task, we build our model based on RoBERTa-large with token mask prediction pretraining objectives. Models and fine-tuning details can be found i"
2021.emnlp-main.597,2021.naacl-main.69,0,0.0280889,"UB - EVENT C O - REFERENCE 1.3 1.3 1.2 3.0 1.2 1.5 1.9 1.3 3.6 1.2 1.9 2.0 1.7 3.1 1.6 Table 2: Average number of answers by semantic types. 6 Experimental setup We design experiments to provide benchmark performances and understand learning challenges to facilitate future research on ESTER. We formulate our QA task as a conditional answer generation problem. This choice is inspired by recent works such as UnifiedQA (Khashabi et al., 2020) that achieve impressive outcomes by integrating various QA tasks (extractive, abstractive and multiplechoice) as a single generative QA pre-training task. Li et al. (2021) and Paolini et al. (2021) also show that by reformulating original extractive tasks as generation tasks, it enables models to better exploit semantic relations between context and labels as well as the dependencies between different outputs. To better demonstrate the benefits of the proposed generative QA task, we compare it with a traditional extractive QA task. We introduce our experimental design and evaluation metrics subsequently. 6.1 Generative QA Given a question qi and a passage Pi = {x1 , x2 , ...xj , ...xn } where xj represents a token in the passage, the answer generation task requ"
2021.emnlp-main.597,2020.emnlp-main.128,0,0.0392974,"mantic relations in ROCStories (Mostafazadeh et al., 2016a) and Event StoryLine Corpus (Caselli and Vossen, 2017) respectively. ESTER differs from these works by disentangling temporal from other semantic relations and focusing on MRC to capture five proposed event semantic relations. 3.2 Event-centric MRC Datasets leveraging natural language queries for event-centric machine reading comprehension have been proposed recently (Zhou et al., 2019; Ning et al., 2020b). However, they focus on event temporal commonsense, whereas ESTER studies other event semantic relations. Du and Cardie (2020) and Liu et al. (2020) reformulate event extraction data as QA tasks to detect event triggers and arguments in a short passage. However, they did not propose new data, and knowing event triggers and arguments are merely a sub-task in ESTER, which require both event detection and relation understanding. exams and steps to validate and train workers. 4.1 Passage Preparation Passages are selected from news articles in TempEval3 (TE3) workshop (UzZaman et al., 2013) with initial event triggers provided. We extracted 3-4 continuous sentences that contain at least 7 event triggers. Our choice of the number of sentences i"
2021.emnlp-main.597,2021.ccl-1.108,0,0.0537314,"Missing"
2021.emnlp-main.597,W14-0702,0,0.345106,"ose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https:"
2021.emnlp-main.597,C14-1198,0,0.0810508,"anguage questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https://github.com/PlusLabNLP/E"
2021.emnlp-main.597,2020.findings-emnlp.171,0,0.0764485,"an event, ESTER captures 10.1K event pairs, which are larger than previous RE datasets such as RED and HiEve. Semantic Types Train Dev Test C AUSAL C ONDITIONAL C OUNTERFACTUAL S UB - EVENT C O - REFERENCE 1.3 1.3 1.2 3.0 1.2 1.5 1.9 1.3 3.6 1.2 1.9 2.0 1.7 3.1 1.6 Table 2: Average number of answers by semantic types. 6 Experimental setup We design experiments to provide benchmark performances and understand learning challenges to facilitate future research on ESTER. We formulate our QA task as a conditional answer generation problem. This choice is inspired by recent works such as UnifiedQA (Khashabi et al., 2020) that achieve impressive outcomes by integrating various QA tasks (extractive, abstractive and multiplechoice) as a single generative QA pre-training task. Li et al. (2021) and Paolini et al. (2021) also show that by reformulating original extractive tasks as generation tasks, it enables models to better exploit semantic relations between context and labels as well as the dependencies between different outputs. To better demonstrate the benefits of the proposed generative QA task, we compare it with a traditional extractive QA task. We introduce our experimental design and evaluation metrics s"
2021.emnlp-main.597,N16-1098,0,0.205267,"C O REFERENCE , C ONDITIONAL and C OUNTERFAC TUAL , and propose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, m"
2021.emnlp-main.597,2020.emnlp-main.430,0,0.298435,"lations: C AUSAL, S UB - EVENT, C O REFERENCE , C ONDITIONAL and C OUNTERFAC TUAL , and propose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined"
2021.emnlp-main.597,D19-1332,1,0.926103,"ngs of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7543–7559 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: Examples of event annotations and 5 types of QAs in our dataset. Not all events are annotated for clarity purpose. Different colors are used for better visualization. inter-annotator agreements (Glavaš et al., 2014; A few noticeable event-centric MRC datasets O’Gorman et al., 2016) and may not be the most have been proposed recently. TORQUE (Ning natural way to exploit the semantic connections et al., 2020b) and MCTACO (Zhou et al., 2019) are between relations and events in their context. two recent MRC datasets that study event tempoWe instead propose to reason about event seman- ral relations. However, knowing only the temporal aspect of events could not solve many important tic relations as a reading comprehension / question event semantic relations. For example, in Figure 1, answering task. Natural language queries ease the annotation efforts in the RE formulation by sup- to understand that “assumed debt,” “gives access” and “takes over projects” are sub-events of “the plementing expert-defined relations with textual deal,"
2021.emnlp-main.597,W16-5706,0,0.31388,"Missing"
2021.emnlp-main.597,S13-2001,0,0.260197,"ntify different 2.1 Events valid answers simultaneously as in the S UB - EVENT QA of Figure 2. These challenges make our task Adopting the general guideline of ACE (2005), we more difficult than the classification tasks in RE. define an event as a trigger word and its arguments 7544 (subject, object, time and location). An event trigger is a word that most clearly describes the event’s occurrence, and it is often a verb or noun that evokes the action or the status of the target event (Pustejovsky et al., 2003). Later event-centric reasoning work mostly uses this trigger definition, e.g., TE3 (UzZaman et al., 2013), HiEve (Glavaš et al., 2014), RED (O’Gorman et al., 2016) and TORQUE (Ning et al., 2020b). While event triggers must exist in the context, some event arguments need to be inferred by annotators. In Figure 2, for example, “getting” is an event trigger and its subject, object and location are “Europe,” “Albania” and “Europe” respectively. The event’s time can be inferred to be approximately the document writing time. To ensure event-centric reasoning, we require all questions and answers to include a trigger. Annotators are allowed to use any event arguments including those inferred to make que"
2021.emnlp-main.819,2020.acl-main.679,0,0.263749,"ion that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2019). Typically, these works report the results separately on the new test set, with no reference to the original test set. We extend over previous work that proposes to evaluate pairs (Abdou et al., 2020) or groups (Elazar et al., 2021) of related instances and assign a point only if they are all correctly predicted by a model. Our evaluation framework exploits groups of minimal-distance instances and results in a more robust evaluation. Specifically, for an arbitrary scoring function f , and a group of minimal-distance instances xi , score each of the examples xij in the group and assign the group its worse-performing score:6 groupScore(xi ) = min f (xij ) j The motivation behind this new evaluation is three-fold: (1) Predicting correctly all examples in a group provides a more robust measure"
2021.emnlp-main.819,P19-1388,1,0.830343,"xamples for WS, the proposed control baselines, and zero-shot instances can be found in Figure 1. Our main premise in this work is that, from a commonsense perspective, the generalization capabilities models can get from large training data are limited. Due to the vast number of commonsense facts (e.g. steel is hard, planets are big), it is infeasible to learn them all from a limited-scale training set. However, this knowledge can still be acquired in different ways, such as self-supervision (Mitchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabiliti"
2021.emnlp-main.819,D18-1220,0,0.140158,"d a new dataset, WinoWhy, which requires models to distinguish between plausible and erroneous reasons for the correct answer. 3 A Robust Group Score Evaluation Since WSC was proposed as a benchmark for commonsense (Levesque et al., 2012), there were many Many works in recent years have shown that large attempts to improve performance on this bench- neural networks can achieve high performance on mark, that involved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the"
2021.emnlp-main.819,2020.coling-main.515,0,0.0623393,"Missing"
2021.emnlp-main.819,2020.tacl-1.3,0,0.0584407,"Missing"
2021.emnlp-main.819,P17-1025,0,0.0245019,"in Figure 1. Our main premise in this work is that, from a commonsense perspective, the generalization capabilities models can get from large training data are limited. Due to the vast number of commonsense facts (e.g. steel is hard, planets are big), it is infeasible to learn them all from a limited-scale training set. However, this knowledge can still be acquired in different ways, such as self-supervision (Mitchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabilities, like WS. tains much less of these.5 (iii) Finally, to bypass the supervised trainin"
2021.emnlp-main.819,D19-1107,1,0.845982,"n achieve high performance on mark, that involved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that account"
2021.emnlp-main.819,P18-2103,1,0.791043,"ly. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2019). Typically, these works report the results separately on the new test set, with no reference to the original test set. We extend over previous work that proposes to evaluate pairs (Abdou et al., 2020) or groups (Elazar et al., 2021) of related instances and assign a point only if they are all correctly predicted by a model. Our evaluation framework exploits groups of minimal-distance instances and results in a more robust evaluation. Specifically, for an arbitrary scoring function f , and a group of minimal-distance instances xi , score each of the examples xij in the grou"
2021.emnlp-main.819,P19-1477,0,0.0136398,"ved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more"
2021.emnlp-main.819,2020.acl-main.671,0,0.172958,"tchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabilities, like WS. tains much less of these.5 (iii) Finally, to bypass the supervised training step, we propose to directly evaluate PLMs on WS in a zero-shot setup; this allows for assessing how many commonsense reasoning capabilities were acquired in the pretraining step. Specifically, this evaluation disentangles the commonsense capabilities of PLMs from the knowledge they acquire from the training set. Combining our new evaluation method and taking into account the data artifacts with the zero-shot"
2021.emnlp-main.819,P19-1478,0,0.415998,"8; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2"
2021.emnlp-main.819,2020.acl-main.508,1,0.832876,"luding WSC, by computing the probability the LM assigns each alternative and choosing the more probable one. The advantage of this method is its unsupervised approach; it does not teach the model any new knowledge. Notably, their evaluation protocol, which computes the average log probability of each masked word is problematic, since special words that get tokenized into more than one wordpiece are still masked independently, thus priming the model towards a certain answer (§6.1). In this work, we propose a new evaluation methodology and show that these models’ performance is random. Finally, Zhang et al. (2020b) provided an analysis of different types of commonsense knowledge needed to solve the different WSC questions, including properties, eventualities, and quantities. They also created a new dataset, WinoWhy, which requires models to distinguish between plausible and erroneous reasons for the correct answer. 3 A Robust Group Score Evaluation Since WSC was proposed as a benchmark for commonsense (Levesque et al., 2012), there were many Many works in recent years have shown that large attempts to improve performance on this bench- neural networks can achieve high performance on mark, that involve"
2021.emnlp-main.819,D19-1332,1,0.843656,"datasets: Winograd Schema Challenge (WSC) (Levesque et al., 2012) contains 273 manually curated examples. We also report results on the non-associative examples that were filtered by Trichelair et al. (2019), named WSC-na. Winogrande (Sakaguchi et al., 2019) is a recent crowdsourced dataset that contains WS questions. Winogrande contains 40,938, 1,267, 1,767 exam6 The minimum in cases where higher scores indicate better ples for train, development, and test respectively. performance, and maximum otherwise. 7 Since the test labels were not published, we report A similar evaluation was used by Zhou et al. (2019), with the “Exact Match” metric for a multi-label classification task. our results on the development set. We provide 10489 Dataset Setup Single Group WSC original no-cands part-sent 89.71 60.72 64.88 79.41 40.35 33.88 WSC-na original no-cands part-sent 89.45 58.06 59.90 79.09 34.41 25.00 Winogrande original no-cands part-sent 71.49 53.07 53.11 58.45 31.05 22.34 Table 1: Results of RoBERTa-large trained on Winogrande, evaluated on the different datasets in the regular condition (original) and the two bias-exposing baselines. Reporting results both on the original accuracy (Single), and the gro"
2021.findings-acl.114,P98-1013,0,0.500077,"nts from the raw documents, we still need the support from other NLP modules to identify the event triggers and arguments first. In this section, we present the performance of a zero-shot event extraction pipeline, which combines our classification model with other NLP tools. 8.1 Identification The first step is identifying event triggers and arguments from the raw sentences. In our pipeline, we use a BERT-based SRL model (Shi and Lin, 2019) and a nominal SRL model5 to detect verbal and nominal events. A limitation of these models is that they adopt a different event ontology (i.e., FrameNet (Baker et al., 1998)) from ACE2005. As a result, they could miss some events or detect irrelevant events, which are not annotated as events under the ACE definition. To include more ACE-specified events, we include all nomi5 As no previous work has applied BERT to the nominal SRL task, we trained one BERT-based nominal SRL model by ourselves. nal anchor words6 in the sentences as triggers. To filter out events that are not covered by the ACE ontology, we introduce an additional filtering step, whose details are introduced in the next sub-section. Last but not least, considering that the definition of arguments in"
2021.findings-acl.114,P15-1017,0,0.0249443,"mention detection module to detect mentions inside the arguments given by SRL systems. Consequently, we cover more gold arguments but also introduce noise. How to automatically identify arguments that fit the ACE definition is a problem worth exploring in the future. 9 Related Works In this section, we introduce related works about the event extraction and NLP without annotation. Event Extraction Previous event extraction works often aim at learning supervised models, employing either symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2010; Liu et al., 2016) or distributed features (Chen et al., 2015b; Lin et al., 2020). To address the problem that supervised models cannot be easily applied to new types, (Huang et al., 2018) separates the event extraction task into two parts (i.e., identification and classification) and proposes a zero-shot transfer-learning classification framework to apply the model trained with seen event types to unseen ones. However, the prerequisite of their high performance is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation and only leverage the label semantics to classify event triggers and arguments. By comb"
2021.findings-acl.114,N19-1423,0,0.533306,"s choose to use “attack” to describe the whole event type and the labels “attacker”, “target”, and “place” for its roles, since the semantics of these words reflect ones’ understanding of this event type. To fully utilize the semantics of these labels, we propose to represent the labels with a cluster of contextualized embeddings rather than just words. In the aforementioned example, we first select several sentences that contain the word “attack” from an external corpus (e.g., New York Times (NYT) (Sandhaus, 2008)). For each selected sentence, we use a pre-trained language model (e.g., BERT (Devlin et al., 2019)) to encode it and then use the resulting embedding of “attack” as a data point in the “attack&quot; cluster. At the inference time, we can then acquire the contextualized representation of identified triggers and arguments, and map them to their corresponding types based on their similarities to those clusters in the embedding space. Beyond the labels, event definitions also provide constraints between types and roles. For example, the role “Attacker” can only appear as an argument of “Conflict:Attack” rather than “Life:Marry,” and only a person or a nation can take the role of an “Attacker&quot;. In o"
2021.findings-acl.114,P18-1201,0,0.172902,"unts of eventspecific annotated text. Take ACE-2005 (Grishman et al., 2005) as an example; the training set of ACE consists of 4,419 events, annotated and typed into 33 event types. Such large-scale and high-quality annotation requires significant expertise, and it facilitates the success of supervised learning models. However, scaling these efforts to new domains and more event types is very costly and unrealistic. Indeed, there has already been some effort to address the limitation of supervised models on new event types via a transfer-learning based zero-shot event classification approach (Huang et al., 2018). By jointly encoding the event structures (i.e., the relations between event triggers and their arguments) of event mentions and of pre-defined event types, their model learns to map event mentions to the most similar event types. As a result, at inference time, the model can be extended to new types as long as the structures of new types are provided. Nonetheless, the success of this transfer learning approach also heavily relies on the similarity between observed event types and new ones. When 1331 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1331–1340 A"
2021.findings-acl.114,P08-1030,0,0.018557,"guments in ACE are often just the key entities. To solve this problem, we propose to use a mention detection module to detect mentions inside the arguments given by SRL systems. Consequently, we cover more gold arguments but also introduce noise. How to automatically identify arguments that fit the ACE definition is a problem worth exploring in the future. 9 Related Works In this section, we introduce related works about the event extraction and NLP without annotation. Event Extraction Previous event extraction works often aim at learning supervised models, employing either symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2010; Liu et al., 2016) or distributed features (Chen et al., 2015b; Lin et al., 2020). To address the problem that supervised models cannot be easily applied to new types, (Huang et al., 2018) separates the event extraction task into two parts (i.e., identification and classification) and proposes a zero-shot transfer-learning classification framework to apply the model trained with seen event types to unseen ones. However, the prerequisite of their high performance is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation"
2021.findings-acl.114,C16-1252,0,0.0644022,"Missing"
2021.findings-acl.114,P10-1081,0,0.0215124,"n just the key entities. To solve this problem, we propose to use a mention detection module to detect mentions inside the arguments given by SRL systems. Consequently, we cover more gold arguments but also introduce noise. How to automatically identify arguments that fit the ACE definition is a problem worth exploring in the future. 9 Related Works In this section, we introduce related works about the event extraction and NLP without annotation. Event Extraction Previous event extraction works often aim at learning supervised models, employing either symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2010; Liu et al., 2016) or distributed features (Chen et al., 2015b; Lin et al., 2020). To address the problem that supervised models cannot be easily applied to new types, (Huang et al., 2018) separates the event extraction task into two parts (i.e., identification and classification) and proposes a zero-shot transfer-learning classification framework to apply the model trained with seen event types to unseen ones. However, the prerequisite of their high performance is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation and only leverage the lab"
2021.findings-acl.114,2020.acl-main.713,0,0.0445824,"Missing"
2021.findings-acl.114,P16-1201,0,0.015398,"To solve this problem, we propose to use a mention detection module to detect mentions inside the arguments given by SRL systems. Consequently, we cover more gold arguments but also introduce noise. How to automatically identify arguments that fit the ACE definition is a problem worth exploring in the future. 9 Related Works In this section, we introduce related works about the event extraction and NLP without annotation. Event Extraction Previous event extraction works often aim at learning supervised models, employing either symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2010; Liu et al., 2016) or distributed features (Chen et al., 2015b; Lin et al., 2020). To address the problem that supervised models cannot be easily applied to new types, (Huang et al., 2018) separates the event extraction task into two parts (i.e., identification and classification) and proposes a zero-shot transfer-learning classification framework to apply the model trained with seen event types to unseen ones. However, the prerequisite of their high performance is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation and only leverage the label semantics to cla"
2021.findings-acl.114,N18-1027,0,0.0165813,"event types. Unlike previous works, we do not use any annotation and only leverage the label semantics to classify event triggers and arguments. By combining our classification model and other NLP modules (i.e., SRL and mention detection), we achieve a decent zeroshot event extraction pipeline that can be easily applied to any new documents and event types. 9.2 NLP without Annotation Solving NLP problems without using annotations has been explored in many NLP tasks including text classification (Chang et al., 2008; Yin et al., 2019), entity typing (Zhou et al., 2018), sequence classification (Rei and Sogaard, 2018), and intent detection (Xia et al., 2018). The idea of leveraging the label semantics was first proposed in the dataless classification framework (Chang et al., 2008), a predecessor name to what is now called zero-shot classification. The idea was to first map the text and labels into a common space using Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) and then pick the label with the highest matching score. This direction was later extended in (Song and Roth, 2014; Chen et al., 2015a; Li et al., 2016a,b; Song et al., 2016). The most significant difference between our work"
2021.findings-acl.114,D18-1231,1,0.837488,"ce is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation and only leverage the label semantics to classify event triggers and arguments. By combining our classification model and other NLP modules (i.e., SRL and mention detection), we achieve a decent zeroshot event extraction pipeline that can be easily applied to any new documents and event types. 9.2 NLP without Annotation Solving NLP problems without using annotations has been explored in many NLP tasks including text classification (Chang et al., 2008; Yin et al., 2019), entity typing (Zhou et al., 2018), sequence classification (Rei and Sogaard, 2018), and intent detection (Xia et al., 2018). The idea of leveraging the label semantics was first proposed in the dataless classification framework (Chang et al., 2008), a predecessor name to what is now called zero-shot classification. The idea was to first map the text and labels into a common space using Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) and then pick the label with the highest matching score. This direction was later extended in (Song and Roth, 2014; Chen et al., 2015a; Li et al., 2016a,b; Song et al., 2016)."
2021.findings-acl.114,W04-2401,1,0.670538,"Missing"
2021.findings-acl.114,D19-1585,0,0.0138987,"ding, and a commonly studied NLP task (Grishman et al., 2005). Consider the example shown in Figure 1, where two events (i.e., “war” and “protesting”) are identified. ∗ This work was done when the first author was visiting the University of Pennsylvania. 1 Our code and models will be available at http:// cogcomp.org/page/publication_view/942. By mapping them to [Conflict:Attack] and [Conflict:Demonstrate] and using the knowledge of “Attack” might result in “Demonstrate”, we can infer that the war in Iraq is probably the cause of the protesting in Pakistan. Most existing event extraction work (Wadden et al., 2019; Lin et al., 2020) treats event identification as a supervised sequence labeling task and event classification as a supervised classification problem, and relies on large amounts of eventspecific annotated text. Take ACE-2005 (Grishman et al., 2005) as an example; the training set of ACE consists of 4,419 events, annotated and typed into 33 event types. Such large-scale and high-quality annotation requires significant expertise, and it facilitates the success of supervised learning models. However, scaling these efforts to new domains and more event types is very costly and unrealistic. Indee"
2021.findings-acl.114,N15-1040,0,0.0158265,"s methods were used to solve event typing in a zeroshot manner, thus we compare with both of them: 1. WSD-embedding (Huang et al., 2018): The WSD baseline first obtains the sense of event mentions with a word sense disambiguation module (Zhong and Ng, 2010) and then acquires the sense embedding with the skip-gram model (Mikolov et al., 2013). During the inference, it can then map triggers and arguments to the candidate types with the pre-trained word sense embeddings. 2. Transfer Learning (Huang et al., 2018): The transfer-learning based zero-shot approach first learns to map the AMR parsing (Wang et al., 2015) result of events to a few observed event types and then apply the learned model to unseen event types. In the original paper, four experiment settings are provided, which are distinguished by the number of seen event types, and we consider all of them to be the baselines. Besides them, we also present the performance of the “Frequency” baseline, which predicts all triggers and arguments with the most frequent types. 6.2 Implementation Details We implement our model with Huggingface (Wolf et al., 2019) and use BERT-large (Devlin et al., 2019) as the pre-trained language model. For each anchor"
2021.findings-acl.114,D18-1348,0,0.0273649,"use any annotation and only leverage the label semantics to classify event triggers and arguments. By combining our classification model and other NLP modules (i.e., SRL and mention detection), we achieve a decent zeroshot event extraction pipeline that can be easily applied to any new documents and event types. 9.2 NLP without Annotation Solving NLP problems without using annotations has been explored in many NLP tasks including text classification (Chang et al., 2008; Yin et al., 2019), entity typing (Zhou et al., 2018), sequence classification (Rei and Sogaard, 2018), and intent detection (Xia et al., 2018). The idea of leveraging the label semantics was first proposed in the dataless classification framework (Chang et al., 2008), a predecessor name to what is now called zero-shot classification. The idea was to first map the text and labels into a common space using Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) and then pick the label with the highest matching score. This direction was later extended in (Song and Roth, 2014; Chen et al., 2015a; Li et al., 2016a,b; Song et al., 2016). The most significant difference between our work and pre1338 vious approaches is that, rat"
2021.findings-acl.114,D19-1404,1,0.817489,"erequisite of their high performance is the similarity between seen and unseen event types. Unlike previous works, we do not use any annotation and only leverage the label semantics to classify event triggers and arguments. By combining our classification model and other NLP modules (i.e., SRL and mention detection), we achieve a decent zeroshot event extraction pipeline that can be easily applied to any new documents and event types. 9.2 NLP without Annotation Solving NLP problems without using annotations has been explored in many NLP tasks including text classification (Chang et al., 2008; Yin et al., 2019), entity typing (Zhou et al., 2018), sequence classification (Rei and Sogaard, 2018), and intent detection (Xia et al., 2018). The idea of leveraging the label semantics was first proposed in the dataless classification framework (Chang et al., 2008), a predecessor name to what is now called zero-shot classification. The idea was to first map the text and labels into a common space using Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) and then pick the label with the highest matching score. This direction was later extended in (Song and Roth, 2014; Chen et al., 2015a; Li et"
2021.findings-acl.114,P10-4014,0,0.0297407,") 82.9 (0.5) 92.6 (0.4) 93.1 (0.1) 95.7 (0.2) 96.2 (0.1) 17.1 (0.7) 53.6 (1.3) 38.0 (0.4) 87.9 (0.4) 49.5 (0.9) 92.4 (0.5) Table 3: Event trigger and argument classification results on ACE-2005. Best performing models are annotated with the bold font. Standard deviations are shown in brackets. 6.1 Baseline Methods To the best of our knowledge, only two previous methods were used to solve event typing in a zeroshot manner, thus we compare with both of them: 1. WSD-embedding (Huang et al., 2018): The WSD baseline first obtains the sense of event mentions with a word sense disambiguation module (Zhong and Ng, 2010) and then acquires the sense embedding with the skip-gram model (Mikolov et al., 2013). During the inference, it can then map triggers and arguments to the candidate types with the pre-trained word sense embeddings. 2. Transfer Learning (Huang et al., 2018): The transfer-learning based zero-shot approach first learns to map the AMR parsing (Wang et al., 2015) result of events to a few observed event types and then apply the learned model to unseen event types. In the original paper, four experiment settings are provided, which are distinguished by the number of seen event types, and we conside"
2021.findings-acl.396,P11-1061,0,0.0475297,"tric. In our experiments, we use this method to select input to the language model. Unlike ProGeT which generates sequences in multiple stages, we complete the text at one time. Cross-lingual NLP There are two main approaches to cross-lingual learning: parallel projection, and developing language-independent features. The first approach obtains pseudo labeled target-language data by projecting annotations from the source to the target using a parallel corpus. A model is then trained in the target language. It has been applied to many tasks, such as part-of-speech tagging (Fang and Cohn, 2016; Das and Petrov, 2011), NER (Wang and Manning, 2014; Mayhew et al., 2017) and parsing (McDonald et al., 2011). The second method attempts to learn language-independent features with which a model trained in the source can transfer directly to the target language. For example, 2.3 Constrained Text Generation Constrained text generation aims to decode sentences with expected attributes such as topics (Feng et al., 2018), style (Luo et al., 2019), etc. In this work, we focus on hard constraints. MaskGAN (Fedus et al., 2018) fills in missing text conditioned on context. It can be used for hardconstrained generation by"
2021.findings-acl.396,P17-1141,0,0.0677532,"Missing"
2021.findings-acl.396,P17-1178,0,0.0221656,"elop a good NER system with little to no annotated data has become a challenging problem. 1 The code and data of the paper are available at: http: //cogcomp.org/page/publication_view/945 To address this challenge in low-resource NER, recent works study the benefits of weakly- or partially-annotated data (Dehghani et al., 2018; Mayhew et al., 2019), and that of transferring knowledge from the high-resource languages to the low-resource languages. Common corpora for developing cross-linguality include parallel text (Wang and Manning, 2014; Ni and Florian, 2016), Wikipedia (Nothman et al., 2013; Pan et al., 2017), and multilingual dictionaries or gazetteers (Tsai et al., 2016). However, the effectiveness of these approaches depends on the quality and quantity of data. For example, parallel text in some lowresource languages is unavailable and the dictionary size is usually smaller; there are 295 languages in Wikipedia, but most of them are too sparse to be useful. Mayhew et al. (2017) and Xie et al. (2018) employed phrase-level and word-level translation respectively to produce target-language training 4519 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4519–4533 Aug"
2021.findings-acl.396,2020.acl-main.703,0,0.0230902,"al representations for many tasks. 2.2 Transformers for Text Generation Self-supervised learning has achieved remarkable success in a wide range of NLP tasks (Vaswani et al., 2017; Peters et al., 2018; Devlin et al., 2019b). Pourdamghani et al. (2019) apply transformers to unsupervised machine translation, but it is hard to align named entities in the translated sentences. In terms of text generation, transformer-based models like GPT (Radford et al., 2019; Brown et al., 2020) have shown great potential. These models are pretrained on the large unsupervised corpora crawled from the web. BART (Lewis et al., 2020) proposes to learn a model by reconstructing the input corrupted by an arbitrary operation (e.g. token masking, token deletion, text infilling, etc.). It is particularly effective in text generation. T5 (Raffel et al., 2020) improves transfer learning by reformulating all tasks into a unified “text-to-text” format. It achieves state-of-the-art results on benchmarks, such as summarization. To overcome the challenge of generating coherent long text, ProGeT (Tan et al., 2020) first produces a sequence of informative words and then progressively adds tokens until completing a full passage. It eval"
2021.findings-acl.396,N18-1202,0,0.00968074,"ow-resource languages. Since our approach generates pseudo data from the labeled source-language tokens, it can potentially generalize to other cross-lingual NLP tasks. 2 2.1 Related Work Tsai et al. (2016) developed cross-lingual features from inter-language links in Wikipedia. Multilingual BERT (Devlin et al., 2019a; Pires et al., 2019) is trained on 104 languages and it can provide powerful cross-lingual contextual representations for many tasks. 2.2 Transformers for Text Generation Self-supervised learning has achieved remarkable success in a wide range of NLP tasks (Vaswani et al., 2017; Peters et al., 2018; Devlin et al., 2019b). Pourdamghani et al. (2019) apply transformers to unsupervised machine translation, but it is hard to align named entities in the translated sentences. In terms of text generation, transformer-based models like GPT (Radford et al., 2019; Brown et al., 2020) have shown great potential. These models are pretrained on the large unsupervised corpora crawled from the web. BART (Lewis et al., 2020) proposes to learn a model by reconstructing the input corrupted by an arbitrary operation (e.g. token masking, token deletion, text infilling, etc.). It is particularly effective i"
2021.findings-acl.396,P19-1493,0,0.0131334,"an be generated with our method from a given annotated sentence in English. To the best of our knowledge, this work is the first to artificially generate labeled data via constrained text generation. Our method improves the current state-ofthe-art results on NER across several low-resource languages. Since our approach generates pseudo data from the labeled source-language tokens, it can potentially generalize to other cross-lingual NLP tasks. 2 2.1 Related Work Tsai et al. (2016) developed cross-lingual features from inter-language links in Wikipedia. Multilingual BERT (Devlin et al., 2019a; Pires et al., 2019) is trained on 104 languages and it can provide powerful cross-lingual contextual representations for many tasks. 2.2 Transformers for Text Generation Self-supervised learning has achieved remarkable success in a wide range of NLP tasks (Vaswani et al., 2017; Peters et al., 2018; Devlin et al., 2019b). Pourdamghani et al. (2019) apply transformers to unsupervised machine translation, but it is hard to align named entities in the translated sentences. In terms of text generation, transformer-based models like GPT (Radford et al., 2019; Brown et al., 2020) have shown great potential. These model"
2021.findings-acl.396,P16-1101,0,0.0329047,"o alleviate the word order issue by translating data between similar languages. However, there is no such limitation for CLDG. We can always start from English as there is much more labeled data in English. Generating More Data. We aim to generate multiple labeled sentences in the target language for each source sentence. Our experiments show that by training on a combination of generated data, the model performs better. 4 Experiments We generate target-language annotated data via the pipeline in §3. Then we train an NER model on the generated data. We use the standard BiLSTMCRF architecture (Ma and Hovy, 2016) with an AllenNLP implementation (Gardner et al., 2018). 4.1 Datasets We evaluate our method on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) which contain 4 languages: English, Spanish, German, Dutch. Previous study shows that English is closely related to the above European languages in terms of word order (Mayhew et al., 2017). Hence, in order to demonstrate the advantages of our method, we add several languages that are dissimilar to English: Akan, Arabic, Turkish, Uzbek, Wolof, Yoruba. We evaluate their performances on the LOREL"
2021.findings-acl.396,K19-1060,1,0.837475,"sifying named entities in text, has been a mature topic in natural language processing (NLP). However, its success is highly dependent on the amount and quality of annotated data. For most of the world’s languages, the amount of supervised resource is limited. How to develop a good NER system with little to no annotated data has become a challenging problem. 1 The code and data of the paper are available at: http: //cogcomp.org/page/publication_view/945 To address this challenge in low-resource NER, recent works study the benefits of weakly- or partially-annotated data (Dehghani et al., 2018; Mayhew et al., 2019), and that of transferring knowledge from the high-resource languages to the low-resource languages. Common corpora for developing cross-linguality include parallel text (Wang and Manning, 2014; Ni and Florian, 2016), Wikipedia (Nothman et al., 2013; Pan et al., 2017), and multilingual dictionaries or gazetteers (Tsai et al., 2016). However, the effectiveness of these approaches depends on the quality and quantity of data. For example, parallel text in some lowresource languages is unavailable and the dictionary size is usually smaller; there are 295 languages in Wikipedia, but most of them ar"
2021.findings-acl.396,D17-1269,1,0.891636,"Missing"
2021.findings-acl.396,D11-1006,0,0.115664,"Missing"
2021.findings-acl.396,D16-1135,0,0.0143653,"the amount of supervised resource is limited. How to develop a good NER system with little to no annotated data has become a challenging problem. 1 The code and data of the paper are available at: http: //cogcomp.org/page/publication_view/945 To address this challenge in low-resource NER, recent works study the benefits of weakly- or partially-annotated data (Dehghani et al., 2018; Mayhew et al., 2019), and that of transferring knowledge from the high-resource languages to the low-resource languages. Common corpora for developing cross-linguality include parallel text (Wang and Manning, 2014; Ni and Florian, 2016), Wikipedia (Nothman et al., 2013; Pan et al., 2017), and multilingual dictionaries or gazetteers (Tsai et al., 2016). However, the effectiveness of these approaches depends on the quality and quantity of data. For example, parallel text in some lowresource languages is unavailable and the dictionary size is usually smaller; there are 295 languages in Wikipedia, but most of them are too sparse to be useful. Mayhew et al. (2017) and Xie et al. (2018) employed phrase-level and word-level translation respectively to produce target-language training 4519 Findings of the Association for Computation"
2021.findings-acl.396,P19-1293,0,0.0208092,"nerates pseudo data from the labeled source-language tokens, it can potentially generalize to other cross-lingual NLP tasks. 2 2.1 Related Work Tsai et al. (2016) developed cross-lingual features from inter-language links in Wikipedia. Multilingual BERT (Devlin et al., 2019a; Pires et al., 2019) is trained on 104 languages and it can provide powerful cross-lingual contextual representations for many tasks. 2.2 Transformers for Text Generation Self-supervised learning has achieved remarkable success in a wide range of NLP tasks (Vaswani et al., 2017; Peters et al., 2018; Devlin et al., 2019b). Pourdamghani et al. (2019) apply transformers to unsupervised machine translation, but it is hard to align named entities in the translated sentences. In terms of text generation, transformer-based models like GPT (Radford et al., 2019; Brown et al., 2020) have shown great potential. These models are pretrained on the large unsupervised corpora crawled from the web. BART (Lewis et al., 2020) proposes to learn a model by reconstructing the input corrupted by an arbitrary operation (e.g. token masking, token deletion, text infilling, etc.). It is particularly effective in text generation. T5 (Raffel et al., 2020) improve"
2021.findings-acl.396,D17-1035,0,0.0157019,"et into the target language sentence by sentence. We project labels across translations using fast align (Dyer et al., 2013). For languages supported by Google Translate, this serves as an upper bound for the translation quality. Compared Methods We experiment with different methods as described below. Resources used for each approach are reported in Table 4. All the methods are evaluated on the same NER model with multilingual BERT (Devlin et al., 2019a), hereafter mBERT, as word embeddings. For each experiment, we run 5 times using different seeds and report the mean and standard deviation (Reimers and Gurevych, 2017). 1. Zero-shot Learning We train an NER model on English CoNLL data, and directly evaluate it on the target language. 2. Cheap Translation (CT) Cheap translation (Mayhew et al., 2017) translates labeled data with MASTERLEXES dictionaries (Rolston and Kirchhoff, 2016). Prior study shows that a larger dictionary has a better chance of covering valuable entries for NER, such as context words of named entities (Mayhew et al., 2017). Since dictionaries for LORELEI languages are much smaller, we augment them with the lexicons provided in the LORELEI project. Dictionary sizes are presented in Table 3"
2021.findings-acl.396,L16-1521,0,0.0277416,"mplementation (Gardner et al., 2018). 4.1 Datasets We evaluate our method on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) which contain 4 languages: English, Spanish, German, Dutch. Previous study shows that English is closely related to the above European languages in terms of word order (Mayhew et al., 2017). Hence, in order to demonstrate the advantages of our method, we add several languages that are dissimilar to English: Akan, Arabic, Turkish, Uzbek, Wolof, Yoruba. We evaluate their performances on the LORELEI project’s data (Strassel and Tracey, 2016). Among the 9 languages we evaluate, Wolof 4523 Language Dict Size Dutch 961K German 1.36M Spanish 1.25M Akan 15K Arabic 410K Turkish 630K Wolof 5K Yoruba 465K Uzbek 65K Table 3: Dictionary size for each language in CT method. Method CT BWET Extra Resources MASTERLEXES dictionary fastText embeddings, 1.5K word pairs in MASTERLEXES dictionary word-level mapping with the fastText embeddings trained on Wikipedia and the MASTERLEXES dictionaries. Table 4: Resources used in each method. 4. Our Method (CLDG) We follow the procedure described in §3 to produce training data. Table 2 presents statistic"
2021.findings-acl.396,W02-2024,0,0.308443,"much more labeled data in English. Generating More Data. We aim to generate multiple labeled sentences in the target language for each source sentence. Our experiments show that by training on a combination of generated data, the model performs better. 4 Experiments We generate target-language annotated data via the pipeline in §3. Then we train an NER model on the generated data. We use the standard BiLSTMCRF architecture (Ma and Hovy, 2016) with an AllenNLP implementation (Gardner et al., 2018). 4.1 Datasets We evaluate our method on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) which contain 4 languages: English, Spanish, German, Dutch. Previous study shows that English is closely related to the above European languages in terms of word order (Mayhew et al., 2017). Hence, in order to demonstrate the advantages of our method, we add several languages that are dissimilar to English: Akan, Arabic, Turkish, Uzbek, Wolof, Yoruba. We evaluate their performances on the LORELEI project’s data (Strassel and Tracey, 2016). Among the 9 languages we evaluate, Wolof 4523 Language Dict Size Dutch 961K German 1.36M Spanish 1.25M Akan 15K Arabi"
2021.findings-acl.396,2020.emnlp-demos.6,0,0.0550379,"Missing"
2021.findings-acl.396,D19-1077,0,0.0218479,"Missing"
2021.findings-acl.396,D18-1034,0,0.13871,"esource languages to the low-resource languages. Common corpora for developing cross-linguality include parallel text (Wang and Manning, 2014; Ni and Florian, 2016), Wikipedia (Nothman et al., 2013; Pan et al., 2017), and multilingual dictionaries or gazetteers (Tsai et al., 2016). However, the effectiveness of these approaches depends on the quality and quantity of data. For example, parallel text in some lowresource languages is unavailable and the dictionary size is usually smaller; there are 295 languages in Wikipedia, but most of them are too sparse to be useful. Mayhew et al. (2017) and Xie et al. (2018) employed phrase-level and word-level translation respectively to produce target-language training 4519 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4519–4533 August 1–6, 2021. ©2021 Association for Computational Linguistics data by projecting annotations. Xie et al. (2018) also tried to alleviate word order divergence across languages by adding self-attention layers. However, this only makes the NER classifier insensitive to word order and the benefits of order information are still ignored. In this study, we propose Constrained Labeled Data Generation (CL"
2021.findings-acl.396,K16-1022,1,0.919366,"me a challenging problem. 1 The code and data of the paper are available at: http: //cogcomp.org/page/publication_view/945 To address this challenge in low-resource NER, recent works study the benefits of weakly- or partially-annotated data (Dehghani et al., 2018; Mayhew et al., 2019), and that of transferring knowledge from the high-resource languages to the low-resource languages. Common corpora for developing cross-linguality include parallel text (Wang and Manning, 2014; Ni and Florian, 2016), Wikipedia (Nothman et al., 2013; Pan et al., 2017), and multilingual dictionaries or gazetteers (Tsai et al., 2016). However, the effectiveness of these approaches depends on the quality and quantity of data. For example, parallel text in some lowresource languages is unavailable and the dictionary size is usually smaller; there are 295 languages in Wikipedia, but most of them are too sparse to be useful. Mayhew et al. (2017) and Xie et al. (2018) employed phrase-level and word-level translation respectively to produce target-language training 4519 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4519–4533 August 1–6, 2021. ©2021 Association for Computational Linguistics da"
2021.findings-emnlp.178,N16-1089,0,0.0737413,"Missing"
2021.findings-emnlp.178,P19-1537,0,0.0346883,"Missing"
2021.findings-emnlp.180,D15-1075,0,0.0866658,"Missing"
2021.findings-emnlp.180,2020.emnlp-main.21,0,0.0719259,"ns even when they make errors, revolutionized natural language processing, achiev- especially on out-of-distribution data (Gupta et al.), ing state-of-the-art results in several tasks. The pro- it is crucial to carefully study model calibration. cess of applying these models on a downstream task Recently, there has been some progress on studyconsists of two components: (1) Self-supervised ing the calibration of deep neural networks and pre-training on a large amount of text corpora and specifically, pre-trained transformers (Guo et al., (2) Supervised fine-tuning on the downstream task. 2017; Desai and Durrett, 2020; Kong et al., 2020; Due to the very large number of parameters of such Jagannatha et al., 2020). However, a careful study transformer based architectures, the high down- of how the size of the pre-trained model influences stream accuracies comes at a large computational calibration is lacking. With the computational concost (Sharir et al., 2020; Bender et al., 2021) during straints of training large transformers like BERT http://cogcomp.org/page/publication_view/953 and the increasingly wide adoption of smaller mod2096 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.findings-emnlp.180,N19-1423,0,0.0374341,"portantly, in the seminal domain, larger models tend to be better caliwork of (Guo et al., 2017), the authors demonstrate brated, and label-smoothing instead is an effecthat for deep neural architectures increasing model tive strategy to calibrate models in this setting. size negatively affects its calibration, even though . classification accuracy increases. In this paper, we extend this to investigate the dependence of cali1 Introduction bration on model size for pre-trained transformer Large pre-trained transformer language models like models. Since miscalibrated models can make very BERT (Devlin et al., 2019; Liu et al., 2019) have confident predictions even when they make errors, revolutionized natural language processing, achiev- especially on out-of-distribution data (Gupta et al.), ing state-of-the-art results in several tasks. The pro- it is crucial to carefully study model calibration. cess of applying these models on a downstream task Recently, there has been some progress on studyconsists of two components: (1) Self-supervised ing the calibration of deep neural networks and pre-training on a large amount of text corpora and specifically, pre-trained transformers (Guo et al., (2) Supervise"
2021.findings-emnlp.180,2020.acl-main.188,0,0.040288,"Missing"
2021.findings-emnlp.180,2020.emnlp-main.102,0,0.0429694,"rors, revolutionized natural language processing, achiev- especially on out-of-distribution data (Gupta et al.), ing state-of-the-art results in several tasks. The pro- it is crucial to carefully study model calibration. cess of applying these models on a downstream task Recently, there has been some progress on studyconsists of two components: (1) Self-supervised ing the calibration of deep neural networks and pre-training on a large amount of text corpora and specifically, pre-trained transformers (Guo et al., (2) Supervised fine-tuning on the downstream task. 2017; Desai and Durrett, 2020; Kong et al., 2020; Due to the very large number of parameters of such Jagannatha et al., 2020). However, a careful study transformer based architectures, the high down- of how the size of the pre-trained model influences stream accuracies comes at a large computational calibration is lacking. With the computational concost (Sharir et al., 2020; Bender et al., 2021) during straints of training large transformers like BERT http://cogcomp.org/page/publication_view/953 and the increasingly wide adoption of smaller mod2096 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2096–2101 Novemb"
2021.findings-emnlp.180,D17-1126,0,0.035258,"Missing"
2021.findings-emnlp.180,D13-1170,0,0.00805735,"Missing"
2021.findings-emnlp.180,2020.acl-main.195,0,0.0353162,"Missing"
2021.findings-emnlp.180,W18-5446,0,0.0608803,"Missing"
2021.findings-emnlp.385,2020.spnlp-1.9,0,0.0244087,"Missing"
2021.findings-emnlp.385,P18-1078,0,0.021167,"urkar et al. (2018) enriched the SQuAD 1.1 corpus by including unanswerable questions for the same paragraphs via crowdsourcing, resulting in SQuAD 2.0, that we are using in this paper for the Extractive QA task. We show that training on SQuAD 2.0 is not sufficient to address IDK in out-of-domain settings (focusing on simple, eventbased questions) and that the RTE data can be useful to address a particular type of IDK questions. Rajpurkar et al. (2018) experimented on SQuAD 2.0 using the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017) and the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2018). These models learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. This also holds in the BERT implementation we use here. 2 Related Work An alternative way for training and prediction in the case of unanswerable questions has been adUnanswerable questions have been first addressed vanced by Tan et al. (2018) who proposed to first in the context of the annual TREC competition for open-domain QA (Voorhees, 2002), where the sub- predict whether there is an answer in the context. Tan et al. (2018) also used a predict+validate aptask"
2021.findings-emnlp.385,N19-1423,0,0.0315467,"begin addressing this important phenomenon, 1 Introduction Rajpurkar et al. (2018) added unanswerable quesExtractive Question Answering (Extractive QA) tions to SQuAD 1.1 (Rajpurkar et al., 2016), prohas attracted a lot of interest in recent years with viding a useful resource for identifying IDK cases the creation of large-scale datasets (Rajpurkar et al., in the Extactive QA case (SQuAD 2.0). How2016, 2018) and has seen large improvements with ever, as we show, the performance of a top systhe use of contextualized language models such tem trained on SQuAD 2.0 considerably drops on as BERT (Devlin et al., 2019) and RoBERTa (Liu out-of-domain simple questions. et al., 2019). However, the ability to extract inforIn this paper, we show that SQuAD 2.0 alone mation from text only addresses one aspect of the is not sufficient to address IDK questions in pracexpectations we have from a comprehension sys- tical situations. For this purpose, we introduce a tem. Another main aspect concerns the ability to new evaluation dataset of very simple questions on single-sentence contexts that we compile based 1 The new datasets along with all the other artifacts generon an event extraction corpus (ACE, Walker et al.,"
2021.findings-emnlp.385,W18-5446,0,0.0674692,"Missing"
2021.findings-emnlp.385,2020.acl-main.503,0,0.0320998,"not modify the uments. In Extractive QA, a system being able to training and prediction used in the BERT paper answer &quot;I don’t know&quot; has been proposed by Levy et al. (2017) in the framework of the relation extrac- approach but rather explore the performance in outof-domain scenarios as well as the use of RTE to tion task which is formulated in QA terms. Another example is the use of QA systems for event extrac- improve the performance. The selective question answering task in out-oftion, as recently proposed by Chen et al. (2020) who modified a BERT-based QA system to predict domain settings (Kamath et al., 2020) is related to an argument role in a clozed test format. In this the identification of unanswerable questions. Howwork we evaluate our Extract QA systems on event- ever, it targets the ability of a system to refrain from based questions questions derived from the ACE answering in some of the cases in order to avoid ercorpus (Walker et al., 2006), focusing on the loca- rors in out-of-domain settings, independently from tion and time argument types. We differ from Chen the presence of the answer in the context. The au4544 Figure 2: Examples of RTE hypotheses (left) and whquestions (right) given"
2021.findings-emnlp.385,K17-1034,0,0.0220502,"tity of the same type appears in the sentence (see Section 3). Rajpurkar et al. (2018) enriched the SQuAD 1.1 corpus by including unanswerable questions for the same paragraphs via crowdsourcing, resulting in SQuAD 2.0, that we are using in this paper for the Extractive QA task. We show that training on SQuAD 2.0 is not sufficient to address IDK in out-of-domain settings (focusing on simple, eventbased questions) and that the RTE data can be useful to address a particular type of IDK questions. Rajpurkar et al. (2018) experimented on SQuAD 2.0 using the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017) and the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2018). These models learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. This also holds in the BERT implementation we use here. 2 Related Work An alternative way for training and prediction in the case of unanswerable questions has been adUnanswerable questions have been first addressed vanced by Tan et al. (2018) who proposed to first in the context of the annual TREC competition for open-domain QA (Voorhees, 2002), where the sub- predict whether there is an ans"
2021.findings-emnlp.385,2021.acl-short.42,1,0.722729,"n be used to improve the performance.1 Context: John was born in New York. Q1: Where did John marry? Answer: IDK - Competitive Q2: When was John born? Answer: IDK - Non-competitive Figure 1: Examples of a competitive (Q1) and a noncompetitive (Q2) IDK questions. identify that a given information is not in the text, a witness of understanding in human comprehension. The ability to answer &quot;IDK&quot; allows one to address more realistic situations in reading comprehension, both as an end task and as an intermediary step for other NLP applications, such as QA-based event extraction (Chen et al., 2020; Lyu et al., 2021) or QA-based summarization evaluation (Deutsch et al., 2021). To begin addressing this important phenomenon, 1 Introduction Rajpurkar et al. (2018) added unanswerable quesExtractive Question Answering (Extractive QA) tions to SQuAD 1.1 (Rajpurkar et al., 2016), prohas attracted a lot of interest in recent years with viding a useful resource for identifying IDK cases the creation of large-scale datasets (Rajpurkar et al., in the Extactive QA case (SQuAD 2.0). How2016, 2018) and has seen large improvements with ever, as we show, the performance of a top systhe use of contextualized language mode"
2021.findings-emnlp.385,P18-2124,0,0.0834792,"ohn born? Answer: IDK - Non-competitive Figure 1: Examples of a competitive (Q1) and a noncompetitive (Q2) IDK questions. identify that a given information is not in the text, a witness of understanding in human comprehension. The ability to answer &quot;IDK&quot; allows one to address more realistic situations in reading comprehension, both as an end task and as an intermediary step for other NLP applications, such as QA-based event extraction (Chen et al., 2020; Lyu et al., 2021) or QA-based summarization evaluation (Deutsch et al., 2021). To begin addressing this important phenomenon, 1 Introduction Rajpurkar et al. (2018) added unanswerable quesExtractive Question Answering (Extractive QA) tions to SQuAD 1.1 (Rajpurkar et al., 2016), prohas attracted a lot of interest in recent years with viding a useful resource for identifying IDK cases the creation of large-scale datasets (Rajpurkar et al., in the Extactive QA case (SQuAD 2.0). How2016, 2018) and has seen large improvements with ever, as we show, the performance of a top systhe use of contextualized language models such tem trained on SQuAD 2.0 considerably drops on as BERT (Devlin et al., 2019) and RoBERTa (Liu out-of-domain simple questions. et al., 2019)"
2021.findings-emnlp.385,D16-1264,0,0.0239143,"estions. identify that a given information is not in the text, a witness of understanding in human comprehension. The ability to answer &quot;IDK&quot; allows one to address more realistic situations in reading comprehension, both as an end task and as an intermediary step for other NLP applications, such as QA-based event extraction (Chen et al., 2020; Lyu et al., 2021) or QA-based summarization evaluation (Deutsch et al., 2021). To begin addressing this important phenomenon, 1 Introduction Rajpurkar et al. (2018) added unanswerable quesExtractive Question Answering (Extractive QA) tions to SQuAD 1.1 (Rajpurkar et al., 2016), prohas attracted a lot of interest in recent years with viding a useful resource for identifying IDK cases the creation of large-scale datasets (Rajpurkar et al., in the Extactive QA case (SQuAD 2.0). How2016, 2018) and has seen large improvements with ever, as we show, the performance of a top systhe use of contextualized language models such tem trained on SQuAD 2.0 considerably drops on as BERT (Devlin et al., 2019) and RoBERTa (Liu out-of-domain simple questions. et al., 2019). However, the ability to extract inforIn this paper, we show that SQuAD 2.0 alone mation from text only addresse"
2021.naacl-demos.16,D14-1148,0,0.0290568,". We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very fe"
2021.naacl-demos.16,C16-1201,0,0.0607333,"Missing"
2021.naacl-demos.16,N13-1073,0,0.043681,"luster, we apply these patterns as highprecision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results. Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we apply Google Cloud neural machine translation 6 to translate Spanish documents into English and apply the FastAlign algorithm (Dyer et al., 2013) to obtain word alignment. 2.6 Cross-media Information Grounding and Fusion Visual event and argument role extraction: Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from visual data, we have collected a new dataset called Video M2E2 which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively annotated the the videos and sampled key frames for annotating bounding boxes of argument rol"
2021.naacl-demos.16,2020.acl-main.718,0,0.068856,"Missing"
2021.naacl-demos.16,D19-5102,0,0.0208286,"arch purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Pr"
2021.naacl-demos.16,glavas-etal-2014-hieve,0,0.0312373,"l cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: ht"
2021.naacl-demos.16,R13-2011,0,0.025761,"ross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeli"
2021.naacl-demos.16,D19-1041,0,0.0480812,"Missing"
2021.naacl-demos.16,N19-1085,0,0.0191037,"oend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE sy"
2021.naacl-demos.16,W18-3101,0,0.0195422,"ly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who di"
2021.naacl-demos.16,N16-1056,0,0.030098,"nologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of even"
2021.naacl-demos.16,W18-5620,0,0.0245812,"ts described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generati"
2021.naacl-demos.16,L16-1545,0,0.0202604,"rstand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but th"
2021.naacl-demos.16,2021.naacl-main.274,1,0.542222,"e postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents, we create N (N2−1) pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all megadocuments to extract the coreference clusters. Finally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence. Our event coreference resolution method (Lai et al., 2021) is similar to entity coreference resolution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific complex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type bombing and they have several arguments in common, these two mentions will be considered as coreferential. 2.5 Cross-document"
2021.naacl-demos.16,D17-1018,0,0.0120578,"5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that app"
2021.naacl-demos.16,2020.acl-main.703,0,0.0204692,"racted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments. Our model is based on BART (Lewis et al., 2020), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argument extraction, we construct an input sequence of hsi template hsih/sidocument h/si. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token hargi. This model is trained in an end-to-end fashion by directly optimizing the generation probability. To align the extracted arguments back to the document, we adopt a simple postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents"
2021.naacl-demos.16,N19-4019,1,0.796479,"ence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide ra"
2021.naacl-demos.16,2020.acl-demos.11,1,0.922974,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2020.emnlp-main.50,1,0.880674,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2021.naacl-main.69,1,0.764758,"chemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments."
2021.naacl-demos.16,2020.acl-main.713,1,0.86722,"und the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section 2.6). Finally, our system selects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section 2.7). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS3 ontology. times for each detected words, as well as potential alternative transcriptions. Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (Lin et al., 2020), a stateof-the-art joint neural model for sentence-level information extraction. Given a sentence, the goal of this module is to extract an information graph G = (V, E), where V is the node set containing entity mentions and event triggers and E is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (Devlin et al., 2018) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based taggers to identify entity mention and event trigger spans from the sentence. We represent each span,"
2021.naacl-demos.16,2021.ccl-1.108,0,0.0312676,"Missing"
2021.naacl-demos.16,L16-1631,0,0.0207744,"ences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolutio"
2021.naacl-demos.16,P17-1009,0,0.0621125,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.253,0,0.0765544,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.344,0,0.0324985,"match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columb"
2021.naacl-demos.16,D17-1108,1,0.83481,"on. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolution by grounding the extracted visual poral attributes through shared arguments. Furtherentities to text. We are the first to perform cross- more, we take advantage of the schema repository media joint event extraction and coreference reso- knowledge by using the frequent temporal"
2021.naacl-demos.16,P18-1212,1,0.895436,"Missing"
2021.naacl-demos.16,D19-1642,1,0.844444,"Missing"
2021.naacl-demos.16,P18-1122,1,0.846572,"oss-document Temporal Event Ordering Based on the event coreference resolution component described above, we group all mentions into clusters. Next we aim to order events along a timeline. We follow Zhou et al. (2020) to design a component for temporal event ordering. Specifically, we further pre-train a T5 model (Raffel et al., 2020) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time"
2021.naacl-demos.16,P17-1178,1,0.849941,"he audio signal. It cific global feature. We compute the global feature returns the transcription with starting and ending score as uf , where u is a learnable weight vec3 https://www.darpa.mil/program/knowledge-di tor. Finally, we use a beam search-based decoder rected-artificial-intelligence-reasoning-overto generate the information graph with the highest schemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple argument"
2021.naacl-demos.16,W15-0812,0,0.0200332,"11 139 1,213 Videos 31 Table 4: Data statistics for schema matching corpus (LDC2020E39). Schema-guided Information Extraction. The Category Extracted Schema Instantiated Events Steps Steps performance of each component is shown in Table 3. We evaluate the end-to-end perfor# 3,180 1,738 958 mance of our system on a complex event corTable 5: Results of schema matching. pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table 4. We train our mention ex3.3 Qualitative Analysis traction component on ACE 2005 (Walker et al., 2006) and ERE (Song et al., 2015); document- Figure 2 illustrates a subset of examples for the best matched results from our end-to-end system. We level argument exraction on ACE 2005 (Walker 7 et al., 2006) and RAMS (Ebner et al., 2020); corefLDC2017E03 8 erence component on ACE 2005 (Walker et al., LDC2017E52 137 Extracted Graph Old Bailey A court in British legal history Max Hill Manchester Communicator Place JudgeCourt JudgeCourt Place JudgeCourt Broadcast ReleaseParole Resident ... ... ChargeIndict TrialHearing Defendant Sentence ArrestJailDetain Defendant Defendant Detainee Defendant ReleaseParole Defendant Salman Abedi"
2021.naacl-demos.16,W02-2024,0,0.567788,"Missing"
2021.naacl-demos.16,D19-1585,0,0.0252013,".. ... ChargeIndict TrialHearing Defendant Defendant Sentence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et"
2021.naacl-demos.16,W12-4501,0,0.0999034,"Missing"
2021.naacl-demos.16,2021.naacl-main.6,1,0.690677,"Missing"
2021.naacl-demos.16,P18-4009,0,0.0278418,"lized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a compl"
2021.naacl-demos.16,N18-5009,1,0.831121,"Missing"
2021.naacl-main.107,P14-2082,0,0.132033,"ACIE focusing on implicit events (§3); (2) a distant supervision process for temporal understanding of implicit events (§4); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural langu"
2021.naacl-main.107,D12-1062,1,0.78784,"ability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captu"
2021.naacl-main.107,2020.findings-emnlp.117,1,0.841232,"Missing"
2021.naacl-main.107,W18-2501,0,0.0115858,"at are not explicitly mentioned by the given story, but are inferable and relevant. The annotator additionally rewrites two explicit events closest to the implicit event’s start and end time, respectively. With these 2 We release T RACIE and its leaderboard at https:// two events, we can build two T RACIE instances leaderboard.allenai.org/tracie 3 (minus the temporal-relation) per implicit event, All event phrases are shortened to triggers here for simplicity. See Fig. 2 for actual phrases. which accounts for 10 instances in total per story. 1363 Automatic Instance Generation We use AllenNLP (Gardner et al., 2018) to extract all verbs and relevant arguments with its semantic role labeling (SRL) model. With all the verbs and their arguments, we construct a pool of explicit events in the form of short phrases. For each implicit event, we randomly select two {explicit-event, comparator} pairs from the pool and build 10 additional instances (without temporal-relation). Label Collection For each of the 20 instances per story, we annotate the temporal-relation with four different annotators. Annotators follow the label definition in §3.1 to produce four temporalrelations for each instance. We use the majorit"
2021.naacl-main.107,2021.naacl-main.99,1,0.811477,"Missing"
2021.naacl-main.107,E14-3011,0,0.0230325,"Missing"
2021.naacl-main.107,I17-1011,0,0.020461,"ponding explicit event from the story. This work is broadly related to works on causal dynamics (Pearl, 2009). The nature of combined temporal and causal focuses is also related to procedural text modeling (Tandon et al., 2018, 2020). 3 The TRACIE Dataset In this section, we introduce the T RACIE dataset.2 3.1 Task Overview and Dataset Construction The goal of T RACIE is to test a system’s ability to compare start and end times of non-extractive implicit event phrases instead of extractive triggers from the context. Such tests in T RACIE take the form of multi-premise textual entailment (TE) (Lai et al., 2017). Each T RACIE instance contains 1) a context story (or premise) consisting of a sequence of explicit narrative events; 2) an implicit event in the form of a natural language phrase that is unmentioned but has some role in the story; 3) a comparator of either {starts,ends}; 4) an explicit event also in the form of a phrase, and 5) a temporal relation of either {before,after} that marks the relationship in the dimension defined by the comparator between the implicit-event and the explicit-event. With these 4 components, we are able to generate TE-style instances, using the context story as the"
2021.naacl-main.107,D18-1155,0,0.0825754,"l benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for relat"
2021.naacl-main.107,D19-1405,0,0.060922,"Missing"
2021.naacl-main.107,2021.ccl-1.108,0,0.100399,"Missing"
2021.naacl-main.107,P18-1049,0,0.0209346,"ork Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu"
2021.naacl-main.107,E17-1108,0,0.0174948,"proach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies"
2021.naacl-main.107,N16-1098,0,0.023991,"t’s start or end time with them (depending on the comparator), according to the label definitions shown in Fig. 3. In rare cases where two time points are the same (e.g., hit and get hit start at the same time in Fig.1), we use the causal relation to decide the order, so that hit starts before get hit. Such instances are created through a multi-stage annotation process as detailed (in respective order) below. All steps are implemented with the CrowdAQ platform (Ning et al., 2020a) with qualification exams. Implicit Event Generation We randomly sample short stories from the ROCStories dataset (Mostafazadeh et al., 2016). For each story, one annotator writes 5 implicit event phrases that are not explicitly mentioned by the given story, but are inferable and relevant. The annotator additionally rewrites two explicit events closest to the implicit event’s start and end time, respectively. With these 2 We release T RACIE and its leaderboard at https:// two events, we can build two T RACIE instances leaderboard.allenai.org/tracie 3 (minus the temporal-relation) per implicit event, All event phrases are shortened to triggers here for simplicity. See Fig. 2 for actual phrases. which accounts for 10 instances in tot"
2021.naacl-main.107,2020.acl-main.441,0,0.0422398,"., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu et al. (2021)). We are inspired by structural predictions and constraints that combat the sparsity of temporal knowledge (Ning et al., 2017; Do et al., 2012), as well as neural module networks (Andreas et al., 2016; Gupta et al., 2019) and o"
2021.naacl-main.107,D17-1108,1,0.836239,"Missing"
2021.naacl-main.107,N18-1077,1,0.849271,"ocess for temporal understanding of implicit events (§4); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavat"
2021.naacl-main.107,W11-0116,0,0.0243172,"le systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu et al. (2021)). We are inspired by structural predictions and constraints that combat the sparsity of temporal knowledge (Ning et al., 2017; Do et al., 2012), as well as neural module networks (Andreas et al., 2016; Gupta et al., 2019) and other decomposition-based approaches (Talmor and Berant, 2018; Khashabi et al., 2018; Li et al., 2019; Wolfson et al., 2020; Khot et al., 2021). In particular, we build neuralsymbolic transformer models th"
2021.naacl-main.107,2020.emnlp-demos.17,1,0.735991,"the start times of explicit-events are more obvious to human annotators, we use them as reference points and compare the implicit-event’s start or end time with them (depending on the comparator), according to the label definitions shown in Fig. 3. In rare cases where two time points are the same (e.g., hit and get hit start at the same time in Fig.1), we use the causal relation to decide the order, so that hit starts before get hit. Such instances are created through a multi-stage annotation process as detailed (in respective order) below. All steps are implemented with the CrowdAQ platform (Ning et al., 2020a) with qualification exams. Implicit Event Generation We randomly sample short stories from the ROCStories dataset (Mostafazadeh et al., 2016). For each story, one annotator writes 5 implicit event phrases that are not explicitly mentioned by the given story, but are inferable and relevant. The annotator additionally rewrites two explicit events closest to the implicit event’s start and end time, respectively. With these 2 We release T RACIE and its leaderboard at https:// two events, we can build two T RACIE instances leaderboard.allenai.org/tracie 3 (minus the temporal-relation) per implici"
2021.naacl-main.107,D19-1041,1,0.883484,"tion in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu et al. (2021)). We are inspired by stru"
2021.naacl-main.107,2020.emnlp-main.88,1,0.862529,"the start times of explicit-events are more obvious to human annotators, we use them as reference points and compare the implicit-event’s start or end time with them (depending on the comparator), according to the label definitions shown in Fig. 3. In rare cases where two time points are the same (e.g., hit and get hit start at the same time in Fig.1), we use the causal relation to decide the order, so that hit starts before get hit. Such instances are created through a multi-stage annotation process as detailed (in respective order) below. All steps are implemented with the CrowdAQ platform (Ning et al., 2020a) with qualification exams. Implicit Event Generation We randomly sample short stories from the ROCStories dataset (Mostafazadeh et al., 2016). For each story, one annotator writes 5 implicit event phrases that are not explicitly mentioned by the given story, but are inferable and relevant. The annotator additionally rewrites two explicit events closest to the implicit event’s start and end time, respectively. With these 2 We release T RACIE and its leaderboard at https:// two events, we can build two T RACIE instances leaderboard.allenai.org/tracie 3 (minus the temporal-relation) per implici"
2021.naacl-main.107,P18-1122,1,0.849338,"ocess for temporal understanding of implicit events (§4); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavat"
2021.naacl-main.107,2020.emnlp-main.51,1,0.883325,"majority classes to produce a uniform-prior training set such that a model can no longer rely on such prior distributions. We believe this setting better evaluates a system’s true understanding of the task. 7 To ensure that tanh returns a value close to 1 or -1, we multiply the distance by a big number denoted as INTmax . 8 We release the systems for reproduction at http:// cogcomp.org/page/publication_view/937 dur(·) = duration1 = cT v (2) 1367 System Start End All Story System Majority BiLSTM Roberta-Large T5-3B 57.3 53.7 78.5 79.4 69.8 63.5 78.3 77.4 64.1 59.1 78.4 78.3 18.1 10.9 26.1 26.9 Wang et al. (2020) BaseLM S YM T IME BaseLM (T5-large) BaseLM-MATRES P TN T IME (ours) S YM T IME (ours) 75.5 76.7 81.4 82.1 75.4 76.3 77.5 79.4 75.4 76.5 79.3 80.6 22.6 25.3 31.0 32.0 S YM T IME -Z ERO S HOT 77.0 73.1 74.9 21.6 Table 1: Performance on T RACIE, best numbers in bold. BaseLM is T5-large; Story is the percentage of story-wide exact match; Majority is based on the comparator and temporal-relation distribution; Zeroshot uses no T RACIE instance as supervision. System Start End All ∆All Random BiLSTM Roberta-Large T5-3B 50.0 50.5 75.1 72.8 50.0 51.2 68.1 68.6 50.0 50.9 71.3 70.5 -14.1 -8.2 -7.1 -7.8"
2021.naacl-main.107,D18-2013,1,0.848455,"ocess for temporal understanding of implicit events (§4); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavat"
2021.naacl-main.107,W16-5706,0,0.0420696,"Missing"
2021.naacl-main.107,W06-0906,0,0.0520626,"and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for"
2021.naacl-main.107,2020.tacl-1.13,0,0.0528672,"Missing"
2021.naacl-main.107,2020.acl-main.678,1,0.938603,"research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017; Nie et al., 2020; Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011; Ning et al., 2018a; Zhou et al., 2020). Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu et al. (2021)). We are inspired by structural predictions and constraints that combat the sparsity of temporal knowledge (Ning et al., 2017; Do et al., 2012), as well as neural module networks (Andreas et al., 2016; Gupta et al., 2019) and other decomposition-based approaches (Talmor and Berant, 2018; Khashabi et al., 2018; Li et al., 2019; Wolfson et al., 2020; Khot et al., 2021). In particular, we build neuralsymbolic transformer models that operationalize some of the classical"
2021.naacl-main.107,P16-1207,0,0.021335,"cit events (§3); (2) a distant supervision process for temporal understanding of implicit events (§4); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations (§5). Finally, we demonstrate the effectiveness of our models on T RACIE, as well as the applicability of our approach to an existing temporal benchmark (§6). 2 Related Work Temporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003; Bethard et al., 2007; Cassidy et al., 2014; Reimers et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b, 2020b), and other temporal knowledge (Pan et al., 2006; Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012; Moens and Leeuwenberg, 2017; Leeuwenberg and Moens, 2018; Meng and Rumshisky, 2018; Ning et al., 2018c; Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (William"
2021.naacl-main.107,N18-1059,0,0.0471205,"Missing"
2021.naacl-main.107,D18-1006,0,0.0239287,"rts before the braces are removed. The adults laughed at the jokes ends before we watch Spongebob as a family Inference Label entailment The ball was in the boys hand starts after he reached for the ball contradiction contradiction Figure 2: Example T RACIE instances. The comparator l ∈{starts,ends} and relation r ∈{before,after} in each hypothesis are highlighted, in addition to the corresponding explicit event from the story. This work is broadly related to works on causal dynamics (Pearl, 2009). The nature of combined temporal and causal focuses is also related to procedural text modeling (Tandon et al., 2018, 2020). 3 The TRACIE Dataset In this section, we introduce the T RACIE dataset.2 3.1 Task Overview and Dataset Construction The goal of T RACIE is to test a system’s ability to compare start and end times of non-extractive implicit event phrases instead of extractive triggers from the context. Such tests in T RACIE take the form of multi-premise textual entailment (TE) (Lai et al., 2017). Each T RACIE instance contains 1) a context story (or premise) consisting of a sequence of explicit narrative events; 2) an implicit event in the form of a natural language phrase that is unmentioned but has"
2021.naacl-main.107,2020.emnlp-main.520,1,0.827901,"Missing"
2021.naacl-main.107,2020.findings-emnlp.363,0,0.0561035,"Missing"
2021.naacl-main.107,P19-1280,0,0.148691,"Missing"
2021.naacl-main.217,D12-1062,1,0.682723,"has impact beyond intelligent task management. For example, learning to decompose complex natural language expressions could have impact on complex question answering (Chali et al., 2009; Luo et al., 2018), where question decomposition, multi-hop reasoning, information synthesis, and implicit knowledge all play an important role. More generally, the ability to model mappings between short text fragments and elements in multiple documents could benefit research in areas such as topic-focused multidocument summarization (Wan et al., 2007) and event timeline extraction of evolving news stories (Do et al., 2012). as the temporal dependencies between them; and (iii) extending a neural text generator by injecting signals for relevance, abstraction and consensus, thereby making it more capable at tackling task decomposition. 2 Problem Definition We begin by defining some key concepts. We refer to a task as a text fragment that represents a goal people want to track, remind themselves of, or learn how to do; for example, “buy a Christmas present”, “eat healthier” or “change a tire”. In order to disambiguate the intent of tasks (consider the fragment “Harry Potter”, which could equally refer to “read [the"
2021.naacl-main.217,2020.acl-main.703,0,0.0266042,"Missing"
2021.naacl-main.217,D18-1242,0,0.0150561,"ur augmented neural text generator, as well as predict dependency edges between the sub-tasks it generates. In experiments, we demonstrate that our optimal solution – which encodes relevance, abstraction and consensus – yields significant improvements over a state-of-the-art text generator on both subtask generation and dependency prediction. The focus of this paper is on Complex Tasks; however, our research has impact beyond intelligent task management. For example, learning to decompose complex natural language expressions could have impact on complex question answering (Chali et al., 2009; Luo et al., 2018), where question decomposition, multi-hop reasoning, information synthesis, and implicit knowledge all play an important role. More generally, the ability to model mappings between short text fragments and elements in multiple documents could benefit research in areas such as topic-focused multidocument summarization (Wan et al., 2007) and event timeline extraction of evolving news stories (Do et al., 2012). as the temporal dependencies between them; and (iii) extending a neural text generator by injecting signals for relevance, abstraction and consensus, thereby making it more capable at tack"
2021.naacl-main.217,2020.acl-main.767,1,0.773501,"Missing"
2021.naacl-main.344,2020.acl-main.703,0,0.0378458,"Missing"
2021.naacl-main.344,P18-1064,0,0.0209843,"stract instead of ground truth as input. the setting where the target topic, or the query, is given as input to the generation model. Due to the implicit nature of the perspectives (Habernal et al., 2018), one key challenge to the task is keep the semantics of the perspective generated truthful to the abstract and editorial article. We approach this by measuring the compatibility of the perspective to the context along the dimensions of content salience (Bar-Haim et al., 2020) and stance correctness (Bar-Haim et al., 2017). Our multi-task generation approach conceptually resembles the work by Guo et al. (2018), where multiple auxiliary tasks is employed to improve the quality of the generated summary. 7 6 6.1 Related Work Argumentation in News Editorials News editorials have been studied as a resource for studying many argumentation-related tasks. Wilson and Wiebe (2003); Yu and Hatzivassiloglou (2003) use editorials for the study on sentiments and opinions. Later works (Reed et al., 2008; Bal and Saint-Dizier, 2009; Chow, 2016) shift focus on the argumentation structure within editorials, and their persuasiveness effect (Al Khatib et al., 2016; El Baff et al., 2020). A few other recent studies hav"
2021.naacl-main.344,W04-1013,0,0.0141224,"between generated and target permeasured against our test set, the relevance and spective. The auxiliary losses LR EL and LS TANCE 4350 are weighted by tunable hyperparameters α1 and α2 respectively. Query L = LS UM + α1 · LR EL + α2 · LS TANCE BART Results + R EL 5.2 5.2.1 Automatic Evaluations Table 3 shows our evaluation results of our multitask model with different combinations of auxiliary tasks. The reported results are averaged over three trained models with different random initialization. We first evaluate the generated perspective summaries against the target perspective with ROUGE (Lin, 2004) and B ERT S CORE (Zhang et al., 2020) metrics. We observe that relevance and stance auxiliary tasks both increase the ROUGE and B ERT S CORE, and combining the two objectives yields the best performance under the ROUGE metrics. To empirically verify whether the perspectives generated by our multi-task model are improved in terms of the relevance and stance correctness, we again use the two pretrained BERT classifiers to measure the percentage of generated summary with correct relevance and stance label. The results potentially suggest that by “mimicing” the predictions made by the two pretrai"
2021.naacl-main.344,reed-etal-2008-language,0,0.297341,"ay cover content not directly related to the query, the generated perspective must present a relevant argument in the query’s context. Such structural constraints can be studied in the format of classification problems. And being able to study such problems along side the perspective summa2 https://www.theperspective.com/ rization task on one high-quality corpus is imporperspectives/ 3 Our code and data is available at http://cogcomp. tant in our case, as it opens up the probability of org/page/publication_view/935 modeling the tasks jointly. We show the benefit of 4346 Dataset A RAUCARIA DB (Reed et al., 2008) (Stab and Gurevych, 2014a) (Eckle-Kohler et al., 2015) (Hua and Wang, 2018) P ERSPECTRUM (Chen et al., 2019) M ULTI O P E D Source News Ed. Essay News Reddit/Wiki. Debate News Ed. Open Domain X X X Politics only X X Cross Article × × × × × X Abstractive × × × X X X Table 1: A comparison across datasets with similar purpose to M ULTI O P E D. We compare the datasets along three dimensions. Open Domain: whether the dataset features a wide variety of topics. Cross Article: whether the argumentation structure between documents are annotated. Abstractive: whether the elements in argumentation stru"
2021.naacl-main.344,N18-1175,0,0.0284369,"many other candidate tasks that might be helpful in the setting. For instance, measuring the quality (Toledo et al., 2019), or more specifically persuasiveness (Carlile et al., 2018) of the perspective might be two, amongst other, viable options. As our study assumes that the abstract is provided for each editorial, the overall performance of perspective summarization will likely drop, if we use model-generated abstract instead of ground truth as input. the setting where the target topic, or the query, is given as input to the generation model. Due to the implicit nature of the perspectives (Habernal et al., 2018), one key challenge to the task is keep the semantics of the perspective generated truthful to the abstract and editorial article. We approach this by measuring the compatibility of the perspective to the context along the dimensions of content salience (Bar-Haim et al., 2020) and stance correctness (Bar-Haim et al., 2017). Our multi-task generation approach conceptually resembles the work by Guo et al. (2018), where multiple auxiliary tasks is employed to improve the quality of the generated summary. 7 6 6.1 Related Work Argumentation in News Editorials News editorials have been studied as a"
2021.naacl-main.344,C14-1142,0,0.187093,"that induces multiple argumentation-related tasks. Third, we demonstrate the utility of our multi-purpose dataset and induced tasks, by using the perspective summarization task as a case study. We include the induced tasks as auxiliary objectives in multi-task learning setting, and demonstrate their effectiveness to perspective summarization. 2 Design Principles Our goal of perspective discovery follows similar definition proposed by Chen et al. (2019), and is closely related to a widely studied area of argumentation mining, i.e. identifying the argumentation structure within persuasive text (Stab and Gurevych, 2014b; Kiesel et al., 2015). However, most studies in this domain focus on extractive methods, which becomes less applicable to our study. As the arguments are usually presented in an subtle and implicit way in news editorials, we instead focus on the generation methods for the perspectives. This closely resembles the argument conclusion generation task (Alshomary et al., 2020). One key distinction here is the presense of query to provide topic guidance during the perspective generation. Compared to other conditional text generation tasks, perspective generation subjects to a few more constraints"
2021.naacl-main.344,I13-1191,0,0.154283,"ic in our dataset, it features two (rather long) news editorials. Each editorial features a single-sentence perspective, which is abstractively summarized from the editorial by human experts. A short abstract then highlights the details in the editorial that support the perspective. The perspectives of the two editorials represents responses of opposite stances towards the query. Naturally, the structure of the dataset induces a range of important argumentation-related natural language understanding tasks. For instance, the presence of the summary perspective allows for stance classification (Hasan and Ng, 2013) with respect to the query, which arguably is more tangible than inferring the stance from the entire editorial. Another example task is the conditional generation of the perspective from the abstract/editorial, which relates to the widely studied task of argument generation (Hua and Wang, 2018; Alshomary et al., 2020). We defer the more detailed description of the induced tasks to Section 3. One key advantage of M ULTI O P E D that is absent from earlier datasets is that a large number of argumentation-related tasks can be studied jointly using a single high quality corpus. To demonstrate thi"
2021.naacl-main.344,D14-1006,0,0.177321,"that induces multiple argumentation-related tasks. Third, we demonstrate the utility of our multi-purpose dataset and induced tasks, by using the perspective summarization task as a case study. We include the induced tasks as auxiliary objectives in multi-task learning setting, and demonstrate their effectiveness to perspective summarization. 2 Design Principles Our goal of perspective discovery follows similar definition proposed by Chen et al. (2019), and is closely related to a widely studied area of argumentation mining, i.e. identifying the argumentation structure within persuasive text (Stab and Gurevych, 2014b; Kiesel et al., 2015). However, most studies in this domain focus on extractive methods, which becomes less applicable to our study. As the arguments are usually presented in an subtle and implicit way in news editorials, we instead focus on the generation methods for the perspectives. This closely resembles the argument conclusion generation task (Alshomary et al., 2020). One key distinction here is the presense of query to provide topic guidance during the perspective generation. Compared to other conditional text generation tasks, perspective generation subjects to a few more constraints"
2021.naacl-main.344,P19-1255,0,0.0573086,"es, or relevant details to support the perspective, forms the abstract a. Naturally, the relation between these elements induces several tasks, most of which encompass similar definitions to existing argumentation-related tasks. We define and describe the tasks and their connection to our end goal of perspective discovery below. 1. Generating an Abstract: Given an editorial e, a system is expected to identify and summarize the relevant arguments into an abstract paragraph a to the context provided by the query q. This is closely related to the task of argument synthesis (El Baff et al., 2019; Hua et al., 2019). We set aside this problem in our case study in section 5, and use the abstract provided by the dataset. 2. Perspective Summarization: Given the generated abstract a and the query q, a system is expected to generate the perspective p, a concise summary of the arguments presented in a. Conceptually, this problem resembles the task of argument conclusion generation (Alshomary et al., 2020). We adopt a slightly different setting where the target topic is expressed in the form of a natural language query. Following the design principles outlined in the previous section, we propose a topic-aligned"
2021.naacl-main.344,D19-1564,0,0.0233547,"text provided. For the query query (2) constitutes the correct stance as the target shown in Table 6, the baseline BART model instance label. The kappa agreement between the two correctly produces a supporting perspective to the 4351 query, while the editorial or abstract presents the opposite stance. The model with the stance objective generates a perspective with a matching stance. While we choose relevance and stance classification as the two auxiliary tasks in this case study, there exist many other candidate tasks that might be helpful in the setting. For instance, measuring the quality (Toledo et al., 2019), or more specifically persuasiveness (Carlile et al., 2018) of the perspective might be two, amongst other, viable options. As our study assumes that the abstract is provided for each editorial, the overall performance of perspective summarization will likely drop, if we use model-generated abstract instead of ground truth as input. the setting where the target topic, or the query, is given as input to the generation model. Due to the implicit nature of the perspectives (Habernal et al., 2018), one key challenge to the task is keep the semantics of the perspective generated truthful to the ab"
2021.naacl-main.344,P18-1021,0,0.306689,"ives of the two editorials represents responses of opposite stances towards the query. Naturally, the structure of the dataset induces a range of important argumentation-related natural language understanding tasks. For instance, the presence of the summary perspective allows for stance classification (Hasan and Ng, 2013) with respect to the query, which arguably is more tangible than inferring the stance from the entire editorial. Another example task is the conditional generation of the perspective from the abstract/editorial, which relates to the widely studied task of argument generation (Hua and Wang, 2018; Alshomary et al., 2020). We defer the more detailed description of the induced tasks to Section 3. One key advantage of M ULTI O P E D that is absent from earlier datasets is that a large number of argumentation-related tasks can be studied jointly using a single high quality corpus. To demonstrate this benefit and the utility of the M ULTI O P E D dataset 3 along with its induced tasks, we study the problem of perspective summarization in a multitask learning setting. We employ perspective relevance and stance classifications as two auxilliary tasks to the summarization objective. Our empir"
2021.naacl-main.344,W15-0505,0,0.0215138,"mentation-related tasks. Third, we demonstrate the utility of our multi-purpose dataset and induced tasks, by using the perspective summarization task as a case study. We include the induced tasks as auxiliary objectives in multi-task learning setting, and demonstrate their effectiveness to perspective summarization. 2 Design Principles Our goal of perspective discovery follows similar definition proposed by Chen et al. (2019), and is closely related to a widely studied area of argumentation mining, i.e. identifying the argumentation structure within persuasive text (Stab and Gurevych, 2014b; Kiesel et al., 2015). However, most studies in this domain focus on extractive methods, which becomes less applicable to our study. As the arguments are usually presented in an subtle and implicit way in news editorials, we instead focus on the generation methods for the perspectives. This closely resembles the argument conclusion generation task (Alshomary et al., 2020). One key distinction here is the presense of query to provide topic guidance during the perspective generation. Compared to other conditional text generation tasks, perspective generation subjects to a few more constraints with respect to the arg"
2021.naacl-main.344,N16-1007,0,0.0289386,"lso like to identify the challenges and potential solution to the problem. We hope that M ULTI O P E D presents opportunities and challenges to future research in argumentation. Most early efforts in argument generation, i.e. generating components in an argumentation structure, study rule-based synthesis methods based on argumentation theories (Reed et al., 1996; Zukerman et al., 2000). With the recent progress in neural, sequence to sequence text generation methods (Sutskever et al., 2014), a few studies have adapted Ethical Considerations such techniques for end-to-end argument generation. (Wang and Ling, 2016; Hua and Wang, 2018; We collected data for M ULTI O P E D by automatiHua et al., 2019). cally extracting data from www.theperspective. The task of perspective generation in this work com/perspectives. The CEO of the website, closely relates to argument conclusion generation Daniel Ravner, granted us permission to extract (Alshomary et al., 2020). Our study focuses on and use their data for academic research. We fur4352 ther annotated the data using crowd-workers. All crowd-workers were compensated by a fair wage determined by estimating the average completing time of each annotation task. Ple"
2021.naacl-main.344,W03-2102,0,0.39543,"P E D and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that M ULTI O P E D will be a useful resource for future studies on argumentation in the news editorial domain. As news editorials function as professionally produced written discourse for conveying media attitude and guidance, they have traditionally been studied by the community as a rich resource for many argumantation-related tasks. (Wilson and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Bal and Saint-Dizier, 2009). This work targets the problem of developing computational methods to identify and compara1 Introduction tively analyze the authors’ perspectives and supNews editorial is a form of persuasive text that con- porting arguments behind news editorials. One challenge to studying the argumentation structure veys consensus opinion on a controversial topic from the editors of a newspaper. Much like an ar- in news editorials is that its elements are rarely expressed explicitly (El Baff et al., 2018). For exgumentative essay, a news editorial"
2021.naacl-main.344,W03-1017,0,0.823118,"sks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that M ULTI O P E D will be a useful resource for future studies on argumentation in the news editorial domain. As news editorials function as professionally produced written discourse for conveying media attitude and guidance, they have traditionally been studied by the community as a rich resource for many argumantation-related tasks. (Wilson and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Bal and Saint-Dizier, 2009). This work targets the problem of developing computational methods to identify and compara1 Introduction tively analyze the authors’ perspectives and supNews editorial is a form of persuasive text that con- porting arguments behind news editorials. One challenge to studying the argumentation structure veys consensus opinion on a controversial topic from the editors of a newspaper. Much like an ar- in news editorials is that its elements are rarely expressed explicitly (El Baff et al., 2018). For exgumentative essay, a news editorial centers around a thesis, which"
2021.naacl-main.344,W00-1408,0,0.404888,"ll quality of the perspective generated. In future work, we hope to utilize the corpus to improve the multi-task framework for perspective summarization. As we set aside the problem of abstract generation in our case study, we would also like to identify the challenges and potential solution to the problem. We hope that M ULTI O P E D presents opportunities and challenges to future research in argumentation. Most early efforts in argument generation, i.e. generating components in an argumentation structure, study rule-based synthesis methods based on argumentation theories (Reed et al., 1996; Zukerman et al., 2000). With the recent progress in neural, sequence to sequence text generation methods (Sutskever et al., 2014), a few studies have adapted Ethical Considerations such techniques for end-to-end argument generation. (Wang and Ling, 2016; Hua and Wang, 2018; We collected data for M ULTI O P E D by automatiHua et al., 2019). cally extracting data from www.theperspective. The task of perspective generation in this work com/perspectives. The CEO of the website, closely relates to argument conclusion generation Daniel Ravner, granted us permission to extract (Alshomary et al., 2020). Our study focuses o"
2021.naacl-main.475,2020.emnlp-main.506,0,0.244379,"o study pre- or post- processing methods to improving faithfulness of generated summaries. Falke et al. (2019) attempt to use textual entailment models to re-rank the summary candidates generated from beam search or different neural systems. As Maynez et al. (2020) highlight the existence of hallucinations in training data, truncating potentially unfaithful gold summaries during training is an effective strategy (Kang and Hashimoto, 2020; Filippova, 2020). Kryscinski et al. (2020) take similar apporach as in this work to identify the hallucinations in summary. A concurrent study to this work (Cao et al., 2020) uses similar strategies as in this paper on a dataset with a very small fraction of hallucinations present. Our study instead focuses on the more challenging setting (Goyal and Durrett, 2021) where a large part of training data suffers from extrinsic and intrinsic hallucinations, and provides cross-system analysis on the both hallucinations categories. 6 nations. We conduct our experiments on the XSum dataset, and show that our method is able to correct extrinsic hullucinations, but incurs a small fraction of intrinsic hallucinations on mistakes. We also provide detailed analysis and discussi"
2021.naacl-main.475,2020.findings-emnlp.76,0,0.0689638,"Missing"
2021.naacl-main.475,2020.findings-emnlp.322,0,0.0228746,"e solution we propose here that focuses only on entites and quantities would likely be insufficient to solve the entire problem. 5 Related Work There have been growing interests in quantitatively measuring the faithfulness of text generation models. Most widely-adopted evaluation metrics for text generation, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020), correlate poorly with the human perceived faithfulness of the generated text (Kryscinski et al., 2019; Durmus et al., 2020). Recent studies explore categorical, contentbased analysis for measuring the faithfulness of summaries (Goyal and Durrett, 2020; Deutsch and Roth, 2020). Narayan et al. (2018b); Deutsch et al. (2020); Durmus et al. (2020) propose to use question answering to test the consistency of summary content to the information presented in the source text. There have been efforts to study pre- or post- processing methods to improving faithfulness of generated summaries. Falke et al. (2019) attempt to use textual entailment models to re-rank the summary candidates generated from beam search or different neural systems. As Maynez et al. (2020) highlight the existence of hallucinations in training data, truncating potentially unfai"
2021.naacl-main.475,2021.naacl-main.114,0,0.0282966,"generated from beam search or different neural systems. As Maynez et al. (2020) highlight the existence of hallucinations in training data, truncating potentially unfaithful gold summaries during training is an effective strategy (Kang and Hashimoto, 2020; Filippova, 2020). Kryscinski et al. (2020) take similar apporach as in this work to identify the hallucinations in summary. A concurrent study to this work (Cao et al., 2020) uses similar strategies as in this paper on a dataset with a very small fraction of hallucinations present. Our study instead focuses on the more challenging setting (Goyal and Durrett, 2021) where a large part of training data suffers from extrinsic and intrinsic hallucinations, and provides cross-system analysis on the both hallucinations categories. 6 nations. We conduct our experiments on the XSum dataset, and show that our method is able to correct extrinsic hullucinations, but incurs a small fraction of intrinsic hallucinations on mistakes. We also provide detailed analysis and discussions on the capabilities and limitations of our method. We hope our findings in the paper will provide insights to future work in this direction. Conclusion Acknowledgments We thank Sunita Verm"
2021.naacl-main.475,2020.acl-main.66,0,0.0170835,"l. (2020); Durmus et al. (2020) propose to use question answering to test the consistency of summary content to the information presented in the source text. There have been efforts to study pre- or post- processing methods to improving faithfulness of generated summaries. Falke et al. (2019) attempt to use textual entailment models to re-rank the summary candidates generated from beam search or different neural systems. As Maynez et al. (2020) highlight the existence of hallucinations in training data, truncating potentially unfaithful gold summaries during training is an effective strategy (Kang and Hashimoto, 2020; Filippova, 2020). Kryscinski et al. (2020) take similar apporach as in this work to identify the hallucinations in summary. A concurrent study to this work (Cao et al., 2020) uses similar strategies as in this paper on a dataset with a very small fraction of hallucinations present. Our study instead focuses on the more challenging setting (Goyal and Durrett, 2021) where a large part of training data suffers from extrinsic and intrinsic hallucinations, and provides cross-system analysis on the both hallucinations categories. 6 nations. We conduct our experiments on the XSum dataset, and show"
2021.naacl-main.475,D19-1051,0,0.0328376,"Missing"
2021.naacl-main.475,2020.emnlp-main.750,0,0.143688,"se question answering to test the consistency of summary content to the information presented in the source text. There have been efforts to study pre- or post- processing methods to improving faithfulness of generated summaries. Falke et al. (2019) attempt to use textual entailment models to re-rank the summary candidates generated from beam search or different neural systems. As Maynez et al. (2020) highlight the existence of hallucinations in training data, truncating potentially unfaithful gold summaries during training is an effective strategy (Kang and Hashimoto, 2020; Filippova, 2020). Kryscinski et al. (2020) take similar apporach as in this work to identify the hallucinations in summary. A concurrent study to this work (Cao et al., 2020) uses similar strategies as in this paper on a dataset with a very small fraction of hallucinations present. Our study instead focuses on the more challenging setting (Goyal and Durrett, 2021) where a large part of training data suffers from extrinsic and intrinsic hallucinations, and provides cross-system analysis on the both hallucinations categories. 6 nations. We conduct our experiments on the XSum dataset, and show that our method is able to correct extrinsic"
2021.naacl-main.475,2020.acl-main.703,0,0.225251,"er language models (Vaswani et al., 2017; Devlin et al., 2019; Liu and Lapata, 2019), have shown improvements in the fluency and salience of generated summaries. However, less progress has been made on improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ERT S CORE (Zhang et al., 2020), current state of the art models (Liu and Lapata, 2019; Lewis et al., 2020) produce summaries that suffer from intrinsic and extrinsic hallucinations – the fabrication of untruthful text spans containing information either present or absent from the source (Maynez et al., 2020). Table 1 shows an example of such summary, generated by BART (Lewis et al., 2020), an auto-regressive, transformer-based sequence-tosequence model. The article describes an event where the former UN-Secretary-General Ban KiMoon was re-elected for a second term. The model hallucinates ""2007"", which never appears in the source document, leading to inconsistency with the correct date of the event"
2021.naacl-main.475,W04-1013,0,0.137567,"(Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), and the more recent, pretrained transformer language models (Vaswani et al., 2017; Devlin et al., 2019; Liu and Lapata, 2019), have shown improvements in the fluency and salience of generated summaries. However, less progress has been made on improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ERT S CORE (Zhang et al., 2020), current state of the art models (Liu and Lapata, 2019; Lewis et al., 2020) produce summaries that suffer from intrinsic and extrinsic hallucinations – the fabrication of untruthful text spans containing information either present or absent from the source (Maynez et al., 2020). Table 1 shows an example of such summary, generated by BART (Lewis et al., 2020), an auto-regressive, transformer-based sequence-tosequence model. The article describes an event where the former UN-Secretary-General Ban KiMoon was re-elected for a second term. The model hallucinate"
2021.naacl-main.475,D19-1387,0,0.025403,"It suffers from extrinsic hallucination, where information not present in the source document was generated. Our method attempts to correct the unfaithful summary by replacing ""2007"" with an entity from the source with compatible semantic type (i.e. DATE). Introduction Abstractive Summarization is the task of producing a concise and fluent summary that is salient and faithful to the source document(s). Datadriven, neural methods (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), and the more recent, pretrained transformer language models (Vaswani et al., 2017; Devlin et al., 2019; Liu and Lapata, 2019), have shown improvements in the fluency and salience of generated summaries. However, less progress has been made on improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ERT S CORE (Zhang et al., 2020), current state of the art models (Liu and Lapata, 2019; Lewis et al., 2020) produce summaries that suffer from intrinsic and extrinsic hallu"
2021.naacl-main.475,2020.acl-main.173,0,0.414311,"improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ERT S CORE (Zhang et al., 2020), current state of the art models (Liu and Lapata, 2019; Lewis et al., 2020) produce summaries that suffer from intrinsic and extrinsic hallucinations – the fabrication of untruthful text spans containing information either present or absent from the source (Maynez et al., 2020). Table 1 shows an example of such summary, generated by BART (Lewis et al., 2020), an auto-regressive, transformer-based sequence-tosequence model. The article describes an event where the former UN-Secretary-General Ban KiMoon was re-elected for a second term. The model hallucinates ""2007"", which never appears in the source document, leading to inconsistency with the correct date of the event presented. In this work, we focus on the problem of correcting such hallucinations as a post processing step1 . A post processing correction step allows us to rely on the fluency of the text generated b"
2021.naacl-main.475,K16-1028,0,0.111211,"Missing"
2021.naacl-main.475,D18-1206,0,0.205752,"study the method of contrast candidate generation and selection. In the generation step, we replace named entities in a potentially hallucinated summary with ones with compatible semantic types that are present in the source, and create variants of candidate summaries. In the selection step, we rank the generated candidates with a discriminative model trained to distinguish between faithful summaries and synthetic negative candidates generated given the source. We experiment on a range of RNN- and transformer-based abstractive summarization models. Our preliminary results on the XSum corpus (Narayan et al., 2018a), which contains substantial presence of hallucinated ground truth examples, show the effectiveness of our method in correcting unfaithful summaries with extrinsic hallucinations. Our main contributions are as follows. First, our work is the first to study the effectiveness of contrast candidate generation and selection as a model-agnostic method for correcting hallucinations, under the setting where a large fraction of ground truth summarization data suffers from hallucinations. Second, we validate our method on various neural summarization systems trained on XSum, and provide detailed anal"
2021.naacl-main.475,N18-1158,0,0.118236,"study the method of contrast candidate generation and selection. In the generation step, we replace named entities in a potentially hallucinated summary with ones with compatible semantic types that are present in the source, and create variants of candidate summaries. In the selection step, we rank the generated candidates with a discriminative model trained to distinguish between faithful summaries and synthetic negative candidates generated given the source. We experiment on a range of RNN- and transformer-based abstractive summarization models. Our preliminary results on the XSum corpus (Narayan et al., 2018a), which contains substantial presence of hallucinated ground truth examples, show the effectiveness of our method in correcting unfaithful summaries with extrinsic hallucinations. Our main contributions are as follows. First, our work is the first to study the effectiveness of contrast candidate generation and selection as a model-agnostic method for correcting hallucinations, under the setting where a large fraction of ground truth summarization data suffers from hallucinations. Second, we validate our method on various neural summarization systems trained on XSum, and provide detailed anal"
2021.naacl-main.475,2020.acl-demos.14,0,0.0667514,"Missing"
2021.naacl-main.475,2020.tacl-1.18,0,0.0337688,"4.2 Intrinsic vs. Extrinsic Hallucinations Trade-off As our method detects and corrects extrinsichallucinated entities, naturally any entities replaced wrong would introduce intrinsic hallucinations in the changed summary, as indicated by the results in Table 4. To speculate why the mistakes happen, we analyzed the typical mistakes by the model, and listed a few representative examples in Table 5. For example, our method could not find the correct replacement for a hallucinated entity when no such one exists in the source text. We observe that the models with pretraining, such as B ERT S2S, (Rothe et al., 2020) and BART, suffer from the issue by most, as they tend to be affected by artifacts/priors from the pretraining process. Table 6 shows our selection model’s performance when measuring P, R, F1 w.r.t all the hallucinated instances. We use the test set from Maynez et al. 4.3 Entity Faithfulness $ Summary Faithfulness (2020), who have annotated hallucination categories of generated summaries from four neural From the observation that models often hallucinate summariazaiton models: P T G EN (See et al., 2017) entities with no correct replacement in the source, TC ONV S2S (Narayan et al., 2018a), B"
2021.naacl-main.475,D15-1044,0,0.0572977,"henomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction. 1 Table 1: An example unfaithful summary. It suffers from extrinsic hallucination, where information not present in the source document was generated. Our method attempts to correct the unfaithful summary by replacing ""2007"" with an entity from the source with compatible semantic type (i.e. DATE). Introduction Abstractive Summarization is the task of producing a concise and fluent summary that is salient and faithful to the source document(s). Datadriven, neural methods (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), and the more recent, pretrained transformer language models (Vaswani et al., 2017; Devlin et al., 2019; Liu and Lapata, 2019), have shown improvements in the fluency and salience of generated summaries. However, less progress has been made on improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ER"
2021.naacl-main.475,P17-1099,0,0.0674096,"arization systems, in hope to provide insights for future work on the direction. 1 Table 1: An example unfaithful summary. It suffers from extrinsic hallucination, where information not present in the source document was generated. Our method attempts to correct the unfaithful summary by replacing ""2007"" with an entity from the source with compatible semantic type (i.e. DATE). Introduction Abstractive Summarization is the task of producing a concise and fluent summary that is salient and faithful to the source document(s). Datadriven, neural methods (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), and the more recent, pretrained transformer language models (Vaswani et al., 2017; Devlin et al., 2019; Liu and Lapata, 2019), have shown improvements in the fluency and salience of generated summaries. However, less progress has been made on improving the faithfulness of the generated summaries, that is, producing a summary that is entailed by the information presented in the source document. Despite the increased level of performance under automatic metrics such as ROUGE ∗ Most of the work done while the authors were at Google. (Lin, 2004) or B ERT S CORE (Zhang et al., 2020), current stat"
2021.naacl-main.475,N18-1101,0,0.0440553,"he best candidate among the variants generated the human analysis by Maynez et al. (2020) on in the previous step as the final output summary. the hallucinations of 500 randomly sampled gold As the contrast candidates vary in no more than a summaries from the XSum corpus . We break few tokens from the original summary, it requires a down each category and annotate the proportion model with more delicate local decision boundaries of hallucinations that happen on entity and num- (Gardner et al., 2020) to select the correct candiber/quantity spans. date. For example, we observe that MNLI models (Williams et al., 2018) fail to produce satisfactory As Maynez et al. (2020) further show that the results. hallucinations in training data translate to similar issues for the generated outputs across different To create training data for that purpose, we samsummarization models, we want to study a model- ple examples from the XSum training set where agnostic, post-processing method that can correct all entities in the ground truth summary appear in such entity and quantity hallucinations. We frame the source document. We then follow the same the problem as a correction task and make it concep- procedure in the gene"
2021.naacl-main.76,2020.findings-emnlp.117,0,0.0245284,"g the use of involved referential expressions in the instructions. Fig.1 shows that the instruction and block configuration are semantically dependent, jointly determining the outcome. (3) Length Invariance Expectation: The performance of a model should not depend on the length of the input, as long as the semantics is unchanged. Our expectations complement existing work in three dimensions: (1) is related to adversarial perturbations (Goodfellow et al., 2014) and (2) is related to equivariance of CNNs explored in computer vision (Cohen and Welling, 2016). It is also related to contrast sets (Gardner et al., 2020; Li et al., 2020a) and counterfactual data augmentation (Kaushik et al., 2019). Here, we extend the investigation to this new task of instruction following involving both natural language and an environment, discrete and continuous perturbations and both regression and classification tasks. Contrast (3) is related to Lake and Baroni (2018) where vulnerability to length in a toy sequence-sequence task was demonstrated. Here we show that length-based vulnerability exists in another modality—the number of blocks present on the board, for this much more complicated task. While these form only a s"
2021.naacl-main.76,D18-1316,0,0.0218573,"ks (Abiodun et al., 2018) an investigation into the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which these models truly understand the spatial relations in the environment, nor their robustness to variability in the environment or in the instructions. This understanding is also important from the viewpoint of safety critical applications , where robustness to variability is essential. While robustness to input perturbations at test-time has been studied in computer vision (Goodfellow et al., 2014) and in certain natural language tasks (Alzantot et al., 2018; Wallace et al., 2019; Shah et al., 2020), it remains relatively elusive in the instruction following task in a grounded environment. This can be attributed to the difficulty in characterizing the different expectations of robustness in this setting, due to the multiple channels of input involved, which semantically constrain each other. The Blocks World domain is an ideal platform to study the abilities of a system to understand instructions (Winograd, 1972; Bisk et al., 2016; Narayan-Chen et al., 2019; Misra et al., 2017; Bisk et al., 2018; Mehta and Goldwasser, 2019; Tan and Bansal, 2018)."
2021.naacl-main.76,2020.blackboxnlp-1.12,0,0.168769,"yields models that are competitive on the original test set while satisfying our expectations much better.1 . 1 Figure 1: Task: Given a configuration of blocks and an instruction, predict the source and target location. Introduction Building agents that can understand and execute natural language instructions in a grounded environment is a hallmark of artificial intelligence (Winograd, 1972). There is wide applicability of this technology in navigation (Chen et al., 2019; Tellex et al., 2011; Chen and Mooney, 2011), collaborative building (Narayan-Chen et al., 2019), and several others areas (Li et al., 2020b; Branavan et al., 2009). The key challenge underlying these and many other applications is the need to understand the natural language instruction (to the extent that it is possible) and ground relevant parts of it in the environment. While the use of deep networks has led to significant progress on several 1 Our code is publicly available http://cogcomp.org/page/publication_view/936 benchmarks (Abiodun et al., 2018) an investigation into the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which these models truly understand the spatial re"
2021.naacl-main.76,2020.acl-main.729,0,0.10514,"yields models that are competitive on the original test set while satisfying our expectations much better.1 . 1 Figure 1: Task: Given a configuration of blocks and an instruction, predict the source and target location. Introduction Building agents that can understand and execute natural language instructions in a grounded environment is a hallmark of artificial intelligence (Winograd, 1972). There is wide applicability of this technology in navigation (Chen et al., 2019; Tellex et al., 2011; Chen and Mooney, 2011), collaborative building (Narayan-Chen et al., 2019), and several others areas (Li et al., 2020b; Branavan et al., 2009). The key challenge underlying these and many other applications is the need to understand the natural language instruction (to the extent that it is possible) and ground relevant parts of it in the environment. While the use of deep networks has led to significant progress on several 1 Our code is publicly available http://cogcomp.org/page/publication_view/936 benchmarks (Abiodun et al., 2018) an investigation into the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which these models truly understand the spatial re"
2021.naacl-main.76,N19-1195,0,0.0163219,"tain natural language tasks (Alzantot et al., 2018; Wallace et al., 2019; Shah et al., 2020), it remains relatively elusive in the instruction following task in a grounded environment. This can be attributed to the difficulty in characterizing the different expectations of robustness in this setting, due to the multiple channels of input involved, which semantically constrain each other. The Blocks World domain is an ideal platform to study the abilities of a system to understand instructions (Winograd, 1972; Bisk et al., 2016; Narayan-Chen et al., 2019; Misra et al., 2017; Bisk et al., 2018; Mehta and Goldwasser, 2019; Tan and Bansal, 2018). Despite being seemingly simple, it presents key reasoning challenges, including compositional language understanding and spatial understanding, that need to be addressed in any at: instructional domain. In Bisk et al. (2016), the en976 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 976–981 June 6–11, 2021. ©2021 Association for Computational Linguistics (a) (b) Figure 2: (a) A symmetric example to Fig. 1. The model should respect this symmetry equivariance (SA). (b) A"
2021.naacl-main.76,D17-1106,0,0.0542859,"Missing"
2021.naacl-main.76,P19-1537,0,0.149349,"nsive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.1 . 1 Figure 1: Task: Given a configuration of blocks and an instruction, predict the source and target location. Introduction Building agents that can understand and execute natural language instructions in a grounded environment is a hallmark of artificial intelligence (Winograd, 1972). There is wide applicability of this technology in navigation (Chen et al., 2019; Tellex et al., 2011; Chen and Mooney, 2011), collaborative building (Narayan-Chen et al., 2019), and several others areas (Li et al., 2020b; Branavan et al., 2009). The key challenge underlying these and many other applications is the need to understand the natural language instruction (to the extent that it is possible) and ground relevant parts of it in the environment. While the use of deep networks has led to significant progress on several 1 Our code is publicly available http://cogcomp.org/page/publication_view/936 benchmarks (Abiodun et al., 2018) an investigation into the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which t"
2021.naacl-main.76,2020.findings-emnlp.317,1,0.589135,"to the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which these models truly understand the spatial relations in the environment, nor their robustness to variability in the environment or in the instructions. This understanding is also important from the viewpoint of safety critical applications , where robustness to variability is essential. While robustness to input perturbations at test-time has been studied in computer vision (Goodfellow et al., 2014) and in certain natural language tasks (Alzantot et al., 2018; Wallace et al., 2019; Shah et al., 2020), it remains relatively elusive in the instruction following task in a grounded environment. This can be attributed to the difficulty in characterizing the different expectations of robustness in this setting, due to the multiple channels of input involved, which semantically constrain each other. The Blocks World domain is an ideal platform to study the abilities of a system to understand instructions (Winograd, 1972; Bisk et al., 2016; Narayan-Chen et al., 2019; Misra et al., 2017; Bisk et al., 2018; Mehta and Goldwasser, 2019; Tan and Bansal, 2018). Despite being seemingly simple, it presen"
2021.naacl-main.76,D19-1221,0,0.0275811,"8) an investigation into the instruction understanding capabilities of such systems remains lacking. We do not know the extent to which these models truly understand the spatial relations in the environment, nor their robustness to variability in the environment or in the instructions. This understanding is also important from the viewpoint of safety critical applications , where robustness to variability is essential. While robustness to input perturbations at test-time has been studied in computer vision (Goodfellow et al., 2014) and in certain natural language tasks (Alzantot et al., 2018; Wallace et al., 2019; Shah et al., 2020), it remains relatively elusive in the instruction following task in a grounded environment. This can be attributed to the difficulty in characterizing the different expectations of robustness in this setting, due to the multiple channels of input involved, which semantically constrain each other. The Blocks World domain is an ideal platform to study the abilities of a system to understand instructions (Winograd, 1972; Bisk et al., 2016; Narayan-Chen et al., 2019; Misra et al., 2017; Bisk et al., 2018; Mehta and Goldwasser, 2019; Tan and Bansal, 2018). Despite being seeming"
2021.naacl-main.76,H89-1033,0,0.685945,"of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.1 . 1 Figure 1: Task: Given a configuration of blocks and an instruction, predict the source and target location. Introduction Building agents that can understand and execute natural language instructions in a grounded environment is a hallmark of artificial intelligence (Winograd, 1972). There is wide applicability of this technology in navigation (Chen et al., 2019; Tellex et al., 2011; Chen and Mooney, 2011), collaborative building (Narayan-Chen et al., 2019), and several others areas (Li et al., 2020b; Branavan et al., 2009). The key challenge underlying these and many other applications is the need to understand the natural language instruction (to the extent that it is possible) and ground relevant parts of it in the environment. While the use of deep networks has led to significant progress on several 1 Our code is publicly available http://cogcomp.org/page/publication"
A00-2017,J95-4004,0,0.0794762,"y is derived which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form P r ( w l h ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994). It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn 124 better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (du"
A00-2017,P98-1035,0,0.0514272,"Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994). It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn 124 better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999). Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996). We believe that the main reason for that is that incorporating information sources in NLP needs to be coupled with a learning approach that is su"
A00-2017,P99-1005,0,0.0141371,"hms with naive Bayes, trigram with backoff and a simple maximum likelihood estimation (MLE) baseline. 2. To create a set of experiments which is comparable with similar experiments that were previously conducted by other researchers. 3. To build a baseline for two types of extensions of the simple use of linear features: (i) Non-Linear features (ii) Automatic focus of attention. 4. To evaluate word prediction as a simple language model. We chose the verb prediction task which is similar to other word prediction tasks (e.g.,(Golding and Roth, 1999)) and, in particular, follows the paradigm in (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999). There, a list of the confusion sets is constructed first, each consists of two different verbs. The verb vl is coupled with v2 provided that they occur equally likely in the corpus. In the test set, every occurrence of vl or v2 was replaced by a set {vl, v2} and the classification task was to predict the correct verb. For example, if a confusion set is created for the verbs ""make"" and ""sell"", then the data is altered as follows: make the paper sell sensitive data Once target subnetworks have been learned and the network is being evaluated, a decision support m"
A00-2017,W99-0621,1,0.816171,"s information can be produced by a functional dependency grammar (FDG), which assigns each word a specific function, a n d t h e n s t r u c t u r e s t h e s e n t e n c e h i e r a r c h i c a l l y b a s e d on it, as we do here ( T a p a n a i n e n a n d J r v i n e n , 1997), b u t can also be g e n e r a t e d by a n e x t e r n a l r u l e - b a s e d p a r s e r or a l e a r n e d one. 127 3 The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998). Earlier versions of S N o W (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction. 3.1 T h e S N O W A r c h i t e c t u r e The SNo W architecture is a sparse network of linear units over a common pre-defined or incrementally learned feature space. It is specifically tailored for learning in domains in which the potential number of features might be very large but only a small subset of them is actually relevant to the decision mad"
A00-2017,H94-1048,0,0.017722,"is computed by estimating the probability of each word given its local context or history. Estimating terms of the form P r ( w l h ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994). It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn 124 better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information"
A00-2017,P98-2186,1,0.842914,". A sub j - v e r b 3This information can be produced by a functional dependency grammar (FDG), which assigns each word a specific function, a n d t h e n s t r u c t u r e s t h e s e n t e n c e h i e r a r c h i c a l l y b a s e d on it, as we do here ( T a p a n a i n e n a n d J r v i n e n , 1997), b u t can also be g e n e r a t e d by a n e x t e r n a l r u l e - b a s e d p a r s e r or a l e a r n e d one. 127 3 The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998). Earlier versions of S N o W (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction. 3.1 T h e S N O W A r c h i t e c t u r e The SNo W architecture is a sparse network of linear units over a common pre-defined or incrementally learned feature space. It is specifically tailored for learning in domains in which the potential number of features might be very large but only a small subset of them is actually relevan"
A00-2017,A97-1011,0,0.022005,"Missing"
A00-2017,P94-1013,0,0.0440733,"which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form P r ( w l h ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994). It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn 124 better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsi"
A00-2017,C98-1035,0,\N,Missing
A00-2017,C98-2181,1,\N,Missing
A00-2017,P99-1004,0,\N,Missing
C00-2124,J96-1002,0,0.00566433,"tion process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active (i.e. a feature is active when the context meets the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi er built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software (Dehaspe, 1997). MBSL (Argamon et al., 1999) uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in th"
C00-2124,P98-1029,0,0.184833,"Missing"
C00-2124,J93-2004,0,0.0295021,"Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1 . The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ (POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi cations). In the baseNP identi cation task, performance is measured with three rates. First, with the percentage of detected noun phrases that are correct (precision). Second, with the percentage of noun phrases in t"
C00-2124,W99-0621,1,0.89852,"Missing"
C00-2124,W95-0107,0,0.246942,"ing algorithms that we will apply to the task. We will conclude with an overview of the combination methods that we will test. 2.1 Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1 . The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ (POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi cations). In the baseNP identi cation task, performance is measured with three rates. First"
C00-2124,A00-2007,1,0.844897,"ection 2.1, noun phrases are represented by bracket structures. It has been shown by Mu~noz et al. (1999) that for baseNP recognition, the representation with brackets outperforms other data representations. One classi er can be trained to recognize open brackets (O) and another can handle close brackets (C). Their results can be combined by making pairs of open and close brackets with large probability scores. We have used this bracket representation (O+C) as well. However, we have not used the combination strategy from Mu~noz et al. (1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases. An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995). They have de ned baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (O). In the case that one baseNP immediately follows another baseNP, the rst word in the second baseNP receives tag B. Example: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O This set of three tags is sucient for encoding baseNP structures si"
C00-2124,P98-1081,1,0.877991,"Missing"
C00-2124,C00-1034,1,\N,Missing
C00-2124,C98-1078,1,\N,Missing
C00-2124,C98-1029,0,\N,Missing
C02-1150,W01-0502,1,0.171414,"l Classifier Question classification is a multi-class classification. A question can be mapped to one of 50 possible classes (We call the set of all possible class labels for a given question a confusion set (Golding and Roth, 1999)). Our learned classifier is based on the SNoW learning architecture (Carlson et al., 1999; Roth, 1998)2 where, in order to allow the classifier to output more than one class label, we map the classifier’s output activation into a conditional probability of the class labels and threshold it. The question classifier makes use of a sequence of two simple classifiers (Even-Zohar and Roth, 2001), each utilizing the Winnow algorithm within SNoW. The first classifies questions into coarse classes (Coarse Classifier) and the second into fine classes (Fine Classifier). A feature extractor automatically extracts the same features for each classifier. The second classifier depends on the first in 2  Freely available at http://L2R.cs.uiuc.edu/ cogcomp/ccsoftware.html C0 ABBR, ENTITY,DESC,HUMAN,LOC,NUM all possible subsets Coarse Classifier of C wih size &lt;= 5 0 C1 Map coarse classes to fine classes ABBR, ENTITY abb, animal, food, plant… ENTITY, HUMAN ENTITY, LOC,NUM food,plant, ind,group …"
C02-1150,W01-1203,0,0.0588184,"Missing"
C02-1150,P99-1042,0,0.0105661,"Missing"
C02-1150,H01-1069,0,0.0517019,"eded. One of the important stages in this process is analyzing the question to a degree that allows determining the “type” of the sought after answer. In the TREC competition (Voorhees, 2000), participants are requested to build a system which, given a set of English questions, can automatically extract answers (a short phrase) of no more than 50 bytes from a 5-gigabyte document library. Participants have re Research supported by NSF grants IIS-9801638 and ITR IIS0085836 and an ONR MURI Award. alized that locating an answer accurately hinges on first filtering out a wide range of candidates (Hovy et al., 2001; Ittycheriah et al., 2001) based on some categorization of answer types. This work develops a machine learning approach to question classification (QC) (Harabagiu et al., 2001; Hermjakob, 2001). Our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process. For example, when considering the question Q: What Canadian city has the largest population?, the hope is to classify this question as having answer type city, implying that only candidate answers that are citi"
C02-1150,W01-0706,1,0.0934084,"wo consecutive words and their pos tags) is written and the features themselves are extracted in a data driven way. Only “active” features are listed in our representation so that despite the large number of potential features, the size of each example is small. Among the 6 primitive feature types, pos tags, chunks and head chunks are syntactic features while named entities and semantically related words are semantic features. Pos tags are extracted using a SNoW-based pos tagger (Even-Zohar and Roth, 2001). Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001). The named entity classifier is also learned and makes use of the same technology developed for the chunker (Roth et al., 2002). The ‘related word’ sensors were constructed semiautomatically. Most question classes have a semantically related word list. Features will be extracted for this class if a word in a question belongs to the list. For example, when “away”, which belongs to a list of words semantically related to the class distance, occurs in the sentence, the sensor Rel(distance) will be active. We note that the features from these sensors are different from those achieved using named"
C02-1150,voorhees-tice-2000-trec,0,0.0280996,"Missing"
C02-1151,W99-0613,0,0.00584324,"ethod and learning algorithm we used. Section 3.2 introduces the idea of using a belief network in search of the best global class labeling and the applied inference algorithm. 3.1 Learning Basic Classifiers Although the labels of entities and relations from a sentence mutually depend on each other, two basic classifiers for entities and relations are first learned, in which a multi-class classifier for E(or R) is learned as a function of all other “known” properties of the observation. The classifier for entities is a named entity classifier, in which the boundary of an entity is predefined (Collins and Singer, 1999). On the other hand, the relation classifier is given a pair of entities, which denote the two arguments of the target relation. Accurate predictions of these two classifiers seem to rely on complicated syntax analysis and semantics related information of the whole sentence. However, we derive weak classifiers by treating these two learning tasks as shallow text processing problems. This strategy has been successfully applied on several NLP tasks, such as information extraction (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001) and chunking (i.e. shallow paring) (Munoz et al., 1999)."
C02-1151,P99-1042,0,0.00873966,"tion, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities. Our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately. 1 Introduction Recognizing and classifying entities and relations in text data is a key task in many NLP problems such as information extraction (IE) (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001), question answering (QA) (Voorhees, 2000) and story comprehension (Hirschman et al., 1999). In a typical IE application of constructing a jobs database from unstructured text, the system has to extract meaningful entities like title and salary and, ideally, to determine whether the entities are associated with the same position. In a QA system, many questions ask for specific entities involved in some relations. For example, the question “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. The question “Who killed Lee Harvey Oswald?” seeks a person entity that has the relation kill with the person Lee Harvey Oswald. In all earlier works we know of, th"
C02-1151,W99-0621,1,0.758572,"ns and Singer, 1999). On the other hand, the relation classifier is given a pair of entities, which denote the two arguments of the target relation. Accurate predictions of these two classifiers seem to rely on complicated syntax analysis and semantics related information of the whole sentence. However, we derive weak classifiers by treating these two learning tasks as shallow text processing problems. This strategy has been successfully applied on several NLP tasks, such as information extraction (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001) and chunking (i.e. shallow paring) (Munoz et al., 1999). It assumes that the class labels can be decided by local properties, such as the information provided by the words around or inside the target. Examples include the spelling of a word, part-of-speech, and semantic related attributes acquired from external resources such as WordNet. The propositional learner we use is SNoW (Roth, 1998; Carleson et al., 1999) 1 SNoW is a multi-class classifier that is specifically tailored for large scale learning tasks. The learning architecture makes use of a network of linear functions, in which the targets (entity classes or relation classes, in this case)"
C02-1151,voorhees-tice-2000-trec,0,0.0250636,"information in the sentence; this information, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities. Our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately. 1 Introduction Recognizing and classifying entities and relations in text data is a key task in many NLP problems such as information extraction (IE) (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001), question answering (QA) (Voorhees, 2000) and story comprehension (Hirschman et al., 1999). In a typical IE application of constructing a jobs database from unstructured text, the system has to extract meaningful entities like title and salary and, ideally, to determine whether the entities are associated with the same position. In a QA system, many questions ask for specific entities involved in some relations. For example, the question “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. The question “Who killed Lee Harvey Oswald?” seeks a person entity that has the relation kill with the person Lee H"
C04-1197,W03-1006,0,0.00848763,"Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assu"
C04-1197,W03-1008,0,0.0133448,"Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL"
C04-1197,P02-1031,0,0.0218397,"d their adjuncts, such as Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder"
C04-1197,W04-2416,0,0.0803492,"Missing"
C04-1197,kingsbury-palmer-2002-treebank,0,0.148443,"data provided in CoNLL2004 shared task on semantic role labeling and achieves very competitive results. 1 Introduction Semantic parsing of sentences is believed to be an important task toward natural language understanding, and has immediate applications in tasks such information extraction and question answering. We study semantic role labeling(SRL). For each verb in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles, such as Agent, Patient or Instrument, and their adjuncts, such as Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradh"
C04-1197,J93-2004,0,0.0263009,"mercial numerical packages, this problem can usually be solved very fast in practice. For instance, it only takes about 10 minutes to solve the inference problem for 4305 sentences on a Pentium-III 800 MHz machine in our experiments. Note that ordinary search methods (e.g., beam search) are not necessarily faster than solving an ILP problem and do not guarantee the optimal solution. 5 Experimental Results The system is evaluated on the data provided in the CoNLL-2004 semantic-role labeling shared task which consists of a portion of PropBank corpus. The training set is extracted from TreeBank (Marcus et al., 1993) section 15–18, the development set, used in tuning parameters of the system, from section 20, and the test set from section 21. We first compare this system with the basic tagger that we have, the CSCL shallow parser from (Punyakanok and Roth, 2001), which is equivalent to using the scoring function from the first phase with only the non-overlapping/embedding constraints. In Fβ=1 65.71 65.46 67.13 68.26 set. All results are for overall performance. Without Inference With Inference i=1 M X Rec. 61.50 60.74 64.75 64.93 Table 1: Summary of experiments on the development zji A0 ≥ zmR-A0 Constrain"
C04-1197,N04-1030,0,0.0808165,"2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect h"
C04-1197,C02-1151,1,0.61986,"r (2002) and Carreras and M`arquez (2003). 3 System Architecture Our semantic role labeling system consists of two phases. The first phase finds a subset of arguments from all possible candidates. The goal here is to filter out as many as possible false argument candidates, while still maintaining high recall. The second phase focuses on identifying the types of those argument candidates. Since the number of candidates is much fewer, the second phase is able to use SNoW Learning Architecture The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It incorporates several improvements over the basic Winnow multiplicative update rule. In particular, a regularization term is added, which has the effect of trying to separate the data with a thick separator (Grove and Roth, 2001; Hang et al., 2002). In the work presented here we use this regularizat"
C04-1197,W04-2401,1,0.784199,"ez, 2003)). Here, We reformulate the constraints as linear (in)equalities by introducing indicator variables. The optimization problem (Eq. 2) is solved using ILP. 4.2 Using Integer Linear Programming As discussed previously, a collection of potential arguments is not necessarily a valid semantic labeling since it must satisfy all of the constraints. In this context, inference is the process of finding the best (according to Equation 1) valid semantic labels that satisfy all of the specified constraints. We take a similar approach that has been previously used for entity/relation recognition (Roth and Yih, 2004), and model this inference procedure as solving an ILP. An integer linear program(ILP) is basically the same as a linear program. The cost function and the (in)equality constraints are all linear in terms of the variables. The only difference in an ILP is the variables can only take integers as their values. In our inference problem, the variables are in fact binary. A general binary integer programming problem can be stated as follows. Given a cost vector p ∈ <d , a set of variables, z = (z1 , . . . , zd ) and cost matrices C1 ∈ <t1 × <d , C2 ∈ <t2 ×<d , where t1 and t2 are the numbers of ine"
C04-1197,P03-1002,0,0.119163,"human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect high levels of performanc"
C04-1197,W00-0726,0,0.00801379,"c parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect high levels of performance from either purely manual classifiers or purely learned classifiers. Rather, supplemental linguistic information must be used to support and correct a learning system. So far, machine learning approaches to SRL have incorporated linguistic information only implicitly, via the classifiers’ features. The key innovation in our approach is the development of a principled method to"
C04-1197,W01-0708,0,0.0170573,"Missing"
C04-1197,W05-0620,0,\N,Missing
C04-1197,W03-0419,0,\N,Missing
C10-1018,J92-4003,0,0.407076,"defined constraints. In this work, we use its prediction as follows:  1, if parent-child(mi , mj ) w2 (mi , mj ) = 0, otherwise 156 Figure 1: An example of Brown word cluster hierarchy from (Koo et al., 2008). where we combine w2 (mi , mj ) with mi , mj Emaintype, introducing this as a new feature into our RE system. 3.5 Using Word Clusters An inherent problem faced by supervised systems is that of data sparseness. To mitigate such issues in the lexical features, we use word clusters which are automatically generated from unlabeled texts. In this work, we use the Brown clustering algorithm (Brown et al., 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extraction (Boschee et al., 2005). The algorithm performs a hierarchical clustering of the words and represents them as a binary tree. Each word is uniquely identified by its path from the root and every path is represented with a bit string. Figure 1 shows an example clustering where the maximum path length is 3. By using path prefixes of different lengths, one can obtain clusterings at different granularity. F"
C10-1018,H05-1091,0,0.834581,"h of the mention pairs in the sentence. In building these systems, researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al., 2008) and databases (Mintz et al., 2009), or attempted open information extraction (IE) (Banko et al., 2007) to extract all possible relations. In this work, we focus on supervised RE. In prior work, the feature and kernel functions employed are usually restricted to being defined on the various representations (e.g. lexical or structural) of the target sentences. However, in recognizing rel"
C10-1018,H05-1003,0,0.0242089,"ric way of using the relation hierarchy which at the same time, gives globally coherent predictions and allows for easy injection of knowledge as constraints. Recently, Jiang (2009) proposed using features which are common across all relations. Her method is complementary to our approach, as she does not consider information such as the relatedness between different relations. On using semantic resources, Zhou et al. (2005) gathered two gazettes, one containing country names and another containing words indicating personal relationships. In relating the tasks of RE and coreference resolution, Ji et al. (2005) used the output of a RE system to rescore coreference hypotheses. In our work, we reverse the setting and explore using coreference to improve RE. 7 Conclusion In this paper, we proposed a broad range of methods to inject background knowledge into a relation extraction system. Some of these methods, such as exploiting the relation hierarchy, are general in nature and could be easily applied to other NLP tasks. To combine the various relation predictions and knowledge, we perform global inference within an ILP framework. Besides allowing for easy injection of knowledge as constraints, this ens"
C10-1018,N07-1015,0,0.849116,"plications such as building an ontology of entities, biomedical information extraction, and question answering. Prior work have employed diverse approaches towards resolving the task. One approach is to build supervised RE systems using sentences annotated with entity mentions and predefined target relations. When given a new sentence, the RE system has to detect and disambiguate the presence of any predefined relations that might exist between each of the mention pairs in the sentence. In building these systems, researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extract examples for superv"
C10-1018,P09-1114,0,0.0632388,"esident). Most of these intuitively seemed to be information one would find being mentioned in an encyclopedia. 6 Related Work Few prior work has explored using background knowledge to improve relation extraction performance. Zhou et al. (2008) took advantage of the hierarchical ontology of relations by proposing methods customized for the perceptron learning algorithm and support vector machines. In contrast, we propose a generic way of using the relation hierarchy which at the same time, gives globally coherent predictions and allows for easy injection of knowledge as constraints. Recently, Jiang (2009) proposed using features which are common across all relations. Her method is complementary to our approach, as she does not consider information such as the relatedness between different relations. On using semantic resources, Zhou et al. (2005) gathered two gazettes, one containing country names and another containing words indicating personal relationships. In relating the tasks of RE and coreference resolution, Ji et al. (2005) used the output of a RE system to rescore coreference hypotheses. In our work, we reverse the setting and explore using coreference to improve RE. 7 Conclusion In t"
C10-1018,P04-3022,0,0.407046,"ls”. RE is important for many NLP applications such as building an ontology of entities, biomedical information extraction, and question answering. Prior work have employed diverse approaches towards resolving the task. One approach is to build supervised RE systems using sentences annotated with entity mentions and predefined target relations. When given a new sentence, the RE system has to detect and disambiguate the presence of any predefined relations that might exist between each of the mention pairs in the sentence. In building these systems, researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to"
C10-1018,P09-1113,0,0.138829,"s appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al., 2008) and databases (Mintz et al., 2009), or attempted open information extraction (IE) (Banko et al., 2007) to extract all possible relations. In this work, we focus on supervised RE. In prior work, the feature and kernel functions employed are usually restricted to being defined on the various representations (e.g. lexical or structural) of the target sentences. However, in recognizing relations, humans are not thus constrained and rely on an abundance of implicit world knowledge or background information. What quantifies as world or background knowledge is rarely explored in the RE literature and we do not attempt to provide comp"
C10-1018,W09-1119,1,0.459899,"rd cluster hierarchy from (Koo et al., 2008). where we combine w2 (mi , mj ) with mi , mj Emaintype, introducing this as a new feature into our RE system. 3.5 Using Word Clusters An inherent problem faced by supervised systems is that of data sparseness. To mitigate such issues in the lexical features, we use word clusters which are automatically generated from unlabeled texts. In this work, we use the Brown clustering algorithm (Brown et al., 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extraction (Boschee et al., 2005). The algorithm performs a hierarchical clustering of the words and represents them as a binary tree. Each word is uniquely identified by its path from the root and every path is represented with a bit string. Figure 1 shows an example clustering where the maximum path length is 3. By using path prefixes of different lengths, one can obtain clusterings at different granularity. For instance, using prefixes of length 2 will put apple and pear into the same cluster, Apple and IBM into the same cluster, etc. In our work, we use clusters generated fr"
C10-1018,W04-2401,1,0.671645,"tures (especially lexical features) is a common problem for supervised systems. In this work, we show that one can make fruitful use of unlabeled data, by using word clusters automatically gathered from unlabeled texts as a way of generalizing the lexical features. • We combine the various relational predictions and background knowledge through a global inference procedure, which we formalize via an Integer Linear Programming (ILP) framework as a constraint optimization problem (Roth and Yih, 2007). This allows us to easily incorporate various constraints that encode the background knowledge. Roth and Yih (2004) develop a relation extraction approach that exploits constraints among entity types and the relations allowed among them. We extend this view significantly, within a similar computational framework, to exploit relations among target relations, background information and world knowledge, as a way to improve relation extraction and make globally coherent predictions. In the rest of this paper, we first describe the features used in our basic RE system in Section 2. We then describe how we make use of background knowledge in Section 3. In Section 4, we show our experimental results and perform a"
C10-1018,P06-1104,0,0.299903,"researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al., 2008) and databases (Mintz et al., 2009), or attempted open information extraction (IE) (Banko et al., 2007) to extract all possible relations. In this work, we focus on supervised RE. In prior work, the feature and kernel functions employed are usually restricted to being defined on the various representations (e.g. lexical or structural) of the target sentences. However, in recognizing relations, humans are not thus constrained and rely on an abundan"
C10-1018,P05-1053,0,0.845482,"ant for many NLP applications such as building an ontology of entities, biomedical information extraction, and question answering. Prior work have employed diverse approaches towards resolving the task. One approach is to build supervised RE systems using sentences annotated with entity mentions and predefined target relations. When given a new sentence, the RE system has to detect and disambiguate the presence of any predefined relations that might exist between each of the mention pairs in the sentence. In building these systems, researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extr"
C10-1018,D07-1076,0,0.605229,"the sentence. In building these systems, researchers used a wide variety of features (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007). Some of the common features used to analyze the target sentence include the words appearing in the sentence, their part-ofspeech (POS) tags, the syntactic parse of the sentence, and the dependency path between the pair of mentions. In a related line of work, researchers have also proposed various kernel functions based on different structured representations (e.g. dependency or syntactic tree parses) of the target sentences (Bunescu and Mooney, 2005; Zhou et al., 2007; Zelenko et al., 2003; Zhang et al., 2006). Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al., 2008) and databases (Mintz et al., 2009), or attempted open information extraction (IE) (Banko et al., 2007) to extract all possible relations. In this work, we focus on supervised RE. In prior work, the feature and kernel functions employed are usually restricted to being defined on the various representations (e.g. lexical or structural) of the target sentences. However, in recognizing relations, humans are"
C10-1018,P08-1068,0,0.0225398,"a citizen of a country), etc. Given a pair of mentions mi and mj , we use a Parent-Child system (Do and Roth, 2010) to predict whether they have a parent-child relation. To achieve this, the system first gathers all Wikipedia articles that are related to mi and mj . It then uses the words in these pages and the category ontology of Wikipedia to make its parent-child predictions, while respecting certain defined constraints. In this work, we use its prediction as follows:  1, if parent-child(mi , mj ) w2 (mi , mj ) = 0, otherwise 156 Figure 1: An example of Brown word cluster hierarchy from (Koo et al., 2008). where we combine w2 (mi , mj ) with mi , mj Emaintype, introducing this as a new feature into our RE system. 3.5 Using Word Clusters An inherent problem faced by supervised systems is that of data sparseness. To mitigate such issues in the lexical features, we use word clusters which are automatically generated from unlabeled texts. In this work, we use the Brown clustering algorithm (Brown et al., 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extrac"
C10-1099,W04-2401,1,0.533973,"John, 25), Age(John, 35). Our propositional clauses (after removing redundancies) are then Age(T om, 30) ⇒ Age(John, 25) ∧ (Age(T om, 30) ⊕ Age(T om, 40)) ∧ (Age(John, 25) ⊕ Age(John, 35)). Each claim c will be represented by a proposition, and ultimately a [0, 1] variable in the linear program corresponding, informally, to P (c).2 Propositionalized constraints have previously been used with integer linear programming (ILP) using binary {0, 1} values corresponding to {f alse, true}, to find an (exact) consistent truth assignment minimizing some cost and solve a global inference problem, e.g. (Roth and Yih, 2004; Roth and Yih, 2007). However, propositional linear programming has two significant advantages: 1. ILP is “winner take all”, shifting all belief to one claim in each mutual exclusion set (even when other claims are nearly as plausible) and finding the single most believable consistent binary assignment; we instead wish to find a distribution of belief over the claims that is consistent with our prior knowledge and as close as possible to the distribution produced by the fact-finder. 2. Linear programs can be solved in polynomial time (e.g. by interior point methods (Karmarkar, 1984)), but ILP"
C10-2145,bird-etal-2008-acl,0,0.0487662,"aper proposed a novel author topic model, CAT, which extends the existing author topic model with additional cited author information. We applied it to the domain of expert retrieval and demonstrated the effectiveness of our model in improving coherence in topic clustering and author topic association. The proposed model also provides an effective solution to the problem of community mining as shown by the promising retrieval results derived in our expert search system. One immediate improvement would result from extending our corpus. For example, we can apply our model to the ACL ARC corpus (Bird et al., 2008) to check the model’s robustness and enhance the ranking by learning from more data. We can also apply our model to data sets with rich linkage structure, such as the TREC benchmark data set or ACL Anthology Network (Radev et al., 2009) and try to enhance our model with the appropriate network analysis. Acknowledgments Table 6: Recall comparison between our proposed model and the model without cited author information. Since we do not have a gold standard experts pool for our queries, to evaluate recall, we collected a pool of authors returned from an academic search engine, ArnetMiner (Tang e"
C10-2145,W10-1202,1,0.444365,"rch at the TREC enterprise track from 2005 to 2007, which focus on enterprise scale search and discovering relationships between entities. In that setting, the task is to find the experts, given a web domain, a list of candidate experts and a set of topics 1 . The task defined in our paper is different in the sense that our topics are hidden and our document repositories are more homogeneous since our documents are all research papers authored by the experts. Within this setting, we can explore in depth the influence of the hidden topics and contents to the ranking of our experts. Similar to (Johri et al., 2010), in this paper we apply CAT in a semantic retrieval scenario, where searching people is associated with a set of hidden semantically meaningful topics instead of their personal names. In recent literature, there are three main lines of work that extend author topic analyses. One line of work is to relax the model’s “bag-of-words” assumption by automatically discovering multiword phrases and adding them into the original model (Johri et al., 2010). Similar work has also been proposed for other topic models such as Ngram topic models (Wallach, 2006; Wang and McCallum, 2005; Wang et al., 2007; G"
C10-2145,W09-1119,1,0.356308,"icitly lists each paper together with its title and author information. Therefore, the author information of each paper can be obtained accurately without extracting it from the original paper. However, many author names are not represented consistently. For example, the same author may have his/her middle name listed in some papers, but not in others. We therefore normalized all author names by eliminating middle names from all authors. Cited authors of each paper are extracted from the reference section and automatically identified by a named entity recognizer tuned for citation extraction (Ratinov and Roth, 2009). Similar to regular authors, all cited authors are also normalized 3 http://www.arnetminer.org Conf. ACL EMNLP CONLL Total Year 03-09 93-09 97-09 93-09 Paper 1,326 912 495 2,733 Author 2,084 1,453 833 2,911 uni. 34,012 40,785 27,312 62,958 Vocab. 205,260 219,496 123,176 366,565 Table 1: Statistics about our data set. Uni. denotes unigram words and Vocab. denotes all unigrams and multiword phrases discovered in the data set. with their first name initial and their full last name. We extracted about 20,000 cited authors from our corpus. However, for the sake of efficiency, we only keep those ci"
C12-1081,N04-1038,0,0.0327768,"h, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating other factors like distance between mentions, several constraints etc. during the inference step. There has been an increasing interest in knowledge-rich coreference resolution (Uryupina et al., 2011; Rahman and Ng, 2011; Bryl et al., 2010; Ng, 2010; Ponzetto and Strube, 2006; Bean and Riloff, 2004). Wikipedia is one of the most common knowledge resources that have been used by researchers. However, Wikipedia is not very good for clinical text because it doesn’t have sufficient coverage of medical terms and also lacks precision. In this paper, we used domain-specific knowledge sources like UMLS, MeSH and SNOMED CT to improve coreference resolution in clinical 1339 domain. One of the earliest works in coreference resolution in clinical domain is that of Zheng et al. (Zheng et al., 2011). In this work, authors review recent advances in general purpose coreference resolution to lay the foun"
C12-1081,D08-1031,1,0.920971,"ions can corefer with the mentions of other types, there are no separate pronoun (PRON) chains. 3 Coreference Model In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, generate a set of edges such that any two mentions that belong in the same equivalence class are connected by some path in the graph. We construct this entity-mention graph by finding out the best antecedent of each given mention (anaphor) such that the antecedent belongs to the same equivalence class as the anaphor. The “Best-Link” strategy (Ng and Cardie, 2002; Bengtson and Roth, 2008; Chang et al., 2011) for selecting the antecedent of a mention chooses as the antecedent that candidate which gets the maximum score according to a pairwise coreference function pc . We extend the “Best-Link” strategy by including several constraints in its objective function as shown below. 3.1 Decision Model: Constrained Best-Link Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential, we generate a coreference graph Gd according to the following decision model: For each mentio"
C12-1081,W11-1904,1,0.918776,"eference resolution. Recently, entity-based models for coreference resolution have been proposed. Such approaches try to directly model the entities in the text and usually involve some kind of global inference and tend to be quite complex. However, most of the best results on coreference resolution were achieved with simpler architectures which use a pairwise classifier between mentions and a decoding strategy like “closest-first” or “best-link” to first find the best antecedent for every mention. This step is then followed up by an inference procedure in which coreference chains are formed (Chang et al., 2011; Pradhan et al., 2011). In this paper, we extend the “best-link” model to include several constraints derived from surfaceform of the mentions and the context in which they appear. Another contribution of this paper is to show the use of domain-specific knowledge sources (like UMLS1 , MetaMap), mention parsing and clinical descriptors (obtained from medical ontologies) in deriving the features which are helpful for coreference resolution. In clinical Information Extraction (IE), researchers often map clinical text to UMLS concepts (Zheng et al., 2012; Rink et al., 2012). But such mapping alon"
C12-1081,N07-1011,0,0.0223166,"time (if mentioned) at which a particular test was conducted. It would be also beneficial to know whether a particular problem is mentioned in relation to the patient or one of his/her family members. On inspection, we found that our system made recall errors only on very difficult mention pairs. Predicting coreference relation among such mention pairs requires a lot of reasoning. 10 Related Work For news text, several different architectures have been proposed for coreference resolution. Systems have been developed which allow for entity-level features or features over sets of noun phrases (Culotta et al., 2007). Such methods generally involve some kind of global inference which is difficult to implement and may also be intractable. Research (Finkel and Manning, 2008; Haghighi and Klein, 2007; Poon and Domingos, 2008) has also been carried out to explore how to reconcile pairwise decisions to form coherent clusters. However, pairwise models with rich knowledge base have been shown to be very successful in both supervised and unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent"
C12-1081,P08-2012,0,0.0175018,"the patient or one of his/her family members. On inspection, we found that our system made recall errors only on very difficult mention pairs. Predicting coreference relation among such mention pairs requires a lot of reasoning. 10 Related Work For news text, several different architectures have been proposed for coreference resolution. Systems have been developed which allow for entity-level features or features over sets of noun phrases (Culotta et al., 2007). Such methods generally involve some kind of global inference which is difficult to implement and may also be intractable. Research (Finkel and Manning, 2008; Haghighi and Klein, 2007; Poon and Domingos, 2008) has also been carried out to explore how to reconcile pairwise decisions to form coherent clusters. However, pairwise models with rich knowledge base have been shown to be very successful in both supervised and unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by resear"
C12-1081,P07-1107,0,0.0179316,"/her family members. On inspection, we found that our system made recall errors only on very difficult mention pairs. Predicting coreference relation among such mention pairs requires a lot of reasoning. 10 Related Work For news text, several different architectures have been proposed for coreference resolution. Systems have been developed which allow for entity-level features or features over sets of noun phrases (Culotta et al., 2007). Such methods generally involve some kind of global inference which is difficult to implement and may also be intractable. Research (Finkel and Manning, 2008; Haghighi and Klein, 2007; Poon and Domingos, 2008) has also been carried out to explore how to reconcile pairwise decisions to form coherent clusters. However, pairwise models with rich knowledge base have been shown to be very successful in both supervised and unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating oth"
C12-1081,D09-1120,0,0.0988995,"t from m’s equivalence class the closest preceding mention a and present the pair (a, m) as a positive training example to the classifier which corresponds to the type of mention m. For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class. We learn the pairwise classifiers using LIBSVM package (Chang and Lin, 2011). 4 Baseline In this section, we describe the baseline system used by us. We designed the baseline system based on the existing state-of-the-art coreference systems which use pairwise models (Bengtson and Roth 2008; Haghighi and Klein 2009). Baseline system uses the coreference model as described in the previous section. However, there are no constraints in the baseline system. The features used for training the pairwise classifier have been described below. All the features used by us take only two values: 1 (if the feature is active) or 0 (if the feature is not active). 4.1 Lexical Features Lexical features indicate whether two strings share some property. These features are listed below: 1330 • Both the mentions have identical surface forms (i.e. e x t ent mi == e x t ent m j ). • Surface form of one of the mentions is a prop"
C12-1081,W04-3250,0,0.013784,"behave differently in evaluating the performance of the systems. B-cubed metric gives higher F1 scores that CEAF metric which in turn gives higher F1 scores than MUC metric. This is because of the presence of large number of singletons in the corpora. B-cubed metric highly awards the correct prediction of singletons. MUC, on the other hand, is totally insensitive to singletons. CEAF is intermediate between B-cubed and MUC as far as singletons are concerned. Next, we note the following major points about each category of mentions. For statistical significance tests, Bootstrap Resampling Test (Koehn, 2004) was used at p = 0.05. 1. Test: For Test mentions, the best configuration is Baseline+Constraints (BC). For MUC metric, both BKC and BC performed the best for 2 corpora each. However, for B-cubed and CEAF evaluation metrics, BC performed the best for all the corpora. Hence, overall, we can say that BC is the best configuration for Test mentions. This is because of the fact that coreference for Test mentions (like “his ct scan”, “a mammogram” etc.) can generally be easily predicted simply by looking at the surface forms. Also, many of the Test coreference chains are quite short with only 2-3 me"
C12-1081,H05-1004,0,0.0353489,"records. All records have been fully de-identified and manually annotated for coreference. This gave us a total of 4 datasets. We would refer to these datasets as Part, Beth, PitD and PitP in the following discussion. The total number of documents in the training set of Part, Beth, PitD and PitP are 136, 115, 119 and 122 respectively. Test set of Part, Beth, PitD and PitP contains 94, 79, 77 and 72 documents respectively. For more information about the datasets, please refer to Uzuner et al. (Uzuner et al., 2012). We used B-cubed (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF (Luo, 2005) as the evaluation metrics in our experiments. Choice of Parameters: We use cross-validation on the training data to determine the system parameters. In Equation (1), we set k1 = 100. With this choice of k1 , distance term becomes significant only if the scores given by pairwise classifier for different mention pairs differ by less than 0.01. Since all our constraints are important to be enforced, we chose pl = 100 in Equation (2) for all values of l . This choice of penalty parameters makes all the constraints hard. 8 Results Table 1 compares the performance of four systems (1) Baseline (B),"
C12-1081,P10-1142,0,0.0158471,"unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating other factors like distance between mentions, several constraints etc. during the inference step. There has been an increasing interest in knowledge-rich coreference resolution (Uryupina et al., 2011; Rahman and Ng, 2011; Bryl et al., 2010; Ng, 2010; Ponzetto and Strube, 2006; Bean and Riloff, 2004). Wikipedia is one of the most common knowledge resources that have been used by researchers. However, Wikipedia is not very good for clinical text because it doesn’t have sufficient coverage of medical terms and also lacks precision. In this paper, we used domain-specific knowledge sources like UMLS, MeSH and SNOMED CT to improve coreference resolution in clinical 1339 domain. One of the earliest works in coreference resolution in clinical domain is that of Zheng et al. (Zheng et al., 2011). In this work, authors review recent advances in gen"
C12-1081,P02-1014,0,0.0785046,"PROB. Since PRON mentions can corefer with the mentions of other types, there are no separate pronoun (PRON) chains. 3 Coreference Model In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, generate a set of edges such that any two mentions that belong in the same equivalence class are connected by some path in the graph. We construct this entity-mention graph by finding out the best antecedent of each given mention (anaphor) such that the antecedent belongs to the same equivalence class as the anaphor. The “Best-Link” strategy (Ng and Cardie, 2002; Bengtson and Roth, 2008; Chang et al., 2011) for selecting the antecedent of a mention chooses as the antecedent that candidate which gets the maximum score according to a pairwise coreference function pc . We extend the “Best-Link” strategy by including several constraints in its objective function as shown below. 3.1 Decision Model: Constrained Best-Link Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential, we generate a coreference graph Gd according to the following decisi"
C12-1081,N06-1025,0,0.0266608,"ed setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating other factors like distance between mentions, several constraints etc. during the inference step. There has been an increasing interest in knowledge-rich coreference resolution (Uryupina et al., 2011; Rahman and Ng, 2011; Bryl et al., 2010; Ng, 2010; Ponzetto and Strube, 2006; Bean and Riloff, 2004). Wikipedia is one of the most common knowledge resources that have been used by researchers. However, Wikipedia is not very good for clinical text because it doesn’t have sufficient coverage of medical terms and also lacks precision. In this paper, we used domain-specific knowledge sources like UMLS, MeSH and SNOMED CT to improve coreference resolution in clinical 1339 domain. One of the earliest works in coreference resolution in clinical domain is that of Zheng et al. (Zheng et al., 2011). In this work, authors review recent advances in general purpose coreference re"
C12-1081,D08-1068,0,0.0161603,"spection, we found that our system made recall errors only on very difficult mention pairs. Predicting coreference relation among such mention pairs requires a lot of reasoning. 10 Related Work For news text, several different architectures have been proposed for coreference resolution. Systems have been developed which allow for entity-level features or features over sets of noun phrases (Culotta et al., 2007). Such methods generally involve some kind of global inference which is difficult to implement and may also be intractable. Research (Finkel and Manning, 2008; Haghighi and Klein, 2007; Poon and Domingos, 2008) has also been carried out to explore how to reconcile pairwise decisions to form coherent clusters. However, pairwise models with rich knowledge base have been shown to be very successful in both supervised and unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating other factors like distance b"
C12-1081,W11-1901,0,0.0170316,"Recently, entity-based models for coreference resolution have been proposed. Such approaches try to directly model the entities in the text and usually involve some kind of global inference and tend to be quite complex. However, most of the best results on coreference resolution were achieved with simpler architectures which use a pairwise classifier between mentions and a decoding strategy like “closest-first” or “best-link” to first find the best antecedent for every mention. This step is then followed up by an inference procedure in which coreference chains are formed (Chang et al., 2011; Pradhan et al., 2011). In this paper, we extend the “best-link” model to include several constraints derived from surfaceform of the mentions and the context in which they appear. Another contribution of this paper is to show the use of domain-specific knowledge sources (like UMLS1 , MetaMap), mention parsing and clinical descriptors (obtained from medical ontologies) in deriving the features which are helpful for coreference resolution. In clinical Information Extraction (IE), researchers often map clinical text to UMLS concepts (Zheng et al., 2012; Rink et al., 2012). But such mapping alone doesn’t allow an IE s"
C12-1081,D10-1048,0,0.0158771,"odnari et al., 2012) and Jindal et al. (Jindal and Roth, 2012) also use a pairwise classification technique for clinical coreference resolution and use UMLS to get some of their semantic features. However, they don’t use the concepts’ parents information available in UMLS. Uzuner et al. (Uzuner et al., 2012) give a brief overview of several systems which participated in 2012 i2b2 coreference challenge. Most of the systems submitted in the challenge were rule-based. Rink et al. (Rink et al., 2012) used a multi-pass sieve architecture which is similar to the one developed by Raghunathan et al. (Raghunathan et al., 2010). Xu et al. (Xu et al., 2012) developed an effective strategy for pronoun resolution where they first determined the type of the pronoun and then chose the closest preceding concept of the same type as the antecedent. All these works assumed mentions’ boundaries (along with their types) to be given just like ours. Conclusion Electronic Health Records are becoming increasingly important and their automatic analysis lies at the heart of several applications. This paper presented a system for coreference resolution for EHRs. In this paper, we proposed a rich model for selecting the best anteceden"
C12-1081,P11-1082,0,0.0159411,"very successful in both supervised and unsupervised setups (Bengtson and Roth, 2008; Haghighi and Klein, 2009). An important step in such models is to find the antecedent for each mention. For selecting the antecedent, “best-link” decoding strategy has been shown to give better results than “closest-first”. In this paper, we extended the “best-link” strategy used by researchers by incorporating other factors like distance between mentions, several constraints etc. during the inference step. There has been an increasing interest in knowledge-rich coreference resolution (Uryupina et al., 2011; Rahman and Ng, 2011; Bryl et al., 2010; Ng, 2010; Ponzetto and Strube, 2006; Bean and Riloff, 2004). Wikipedia is one of the most common knowledge resources that have been used by researchers. However, Wikipedia is not very good for clinical text because it doesn’t have sufficient coverage of medical terms and also lacks precision. In this paper, we used domain-specific knowledge sources like UMLS, MeSH and SNOMED CT to improve coreference resolution in clinical 1339 domain. One of the earliest works in coreference resolution in clinical domain is that of Zheng et al. (Zheng et al., 2011). In this work, authors"
C12-1081,M95-1005,0,0.107014,", namely Discharge and Progress records. All records have been fully de-identified and manually annotated for coreference. This gave us a total of 4 datasets. We would refer to these datasets as Part, Beth, PitD and PitP in the following discussion. The total number of documents in the training set of Part, Beth, PitD and PitP are 136, 115, 119 and 122 respectively. Test set of Part, Beth, PitD and PitP contains 94, 79, 77 and 72 documents respectively. For more information about the datasets, please refer to Uzuner et al. (Uzuner et al., 2012). We used B-cubed (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF (Luo, 2005) as the evaluation metrics in our experiments. Choice of Parameters: We use cross-validation on the training data to determine the system parameters. In Equation (1), we set k1 = 100. With this choice of k1 , distance term becomes significant only if the scores given by pairwise classifier for different mention pairs differ by less than 0.01. Since all our constraints are important to be enforced, we chose pl = 100 in Equation (2) for all values of l . This choice of penalty parameters makes all the constraints hard. 8 Results Table 1 compares the performance of four syste"
C16-1183,araki-etal-2014-detecting,0,0.0449042,"assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created the Event Coreference Bank (ECB), the first dataset with both within and acros"
C16-1183,W99-0201,0,0.878543,"ument. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset. 1 Introduction Understanding events is crucial to natural language understanding and has applications ranging from question answering (Berant et al., 2014; Narayanan and Harabagiu, 2004), to causal reasoning (Do et al., 2011; Chambers and Jurafsky, 2008) to headline generation (Sun et al., 2015). The task of Cross Document Event Coreference (CDEC) determines if two event mentions (which belong to different documents) refer to the same event (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Figure 1 shows an example of two events whose mentions co-refer across 3 documents, O’Brien fired! [King] decided to ﬁre [O’Brien] , who s;ll has two years lea on the contract he signed when he was hired by the [Sixers] on April 21 , 2004. Cheeks hired! Up the coast, the [Philly Sixers] canned [Jim O’Brien] ( who had been under ﬁre all season ) and quickly hired [Mo Cheeks]. [76ers] have relieved [Maurice Cheeks] of his coaching responsibili;es . [Cheeks] was brought aaer [Jim O&apos;Brien] was relieved of his du;es. Figure 1: Event coreference across 3 documents. Even"
C16-1183,P98-1013,0,0.0408752,"erative Clustering (SAC) Performs greedy agglomerative clustering, using a learned pairwise classifier to score if two cluster of mentions are coreferent. Inter-cluster similarity score is computed as the average of the similarity score of their mentions. Like Lemma-δ, two clusters are considered for linking if the document similarity exceeds a threshold (we use 0.3 as above). We re-implemented the pairwise classifier of Yang et al. (2015) using the same feature set. Mention heads were found using dependency parses obtained by Stanford CoreNLP (Manning et al., 2014). We identify the Framenet (Baker et al., 1998) frame evoked by an event mention, we use the SEMAFOR (Das and Smith, 2011). For identifying the arguments for an event mention we use Illinois-SRL (Punyakanok et al., 2008). The pairwise classifier was trained using the Illinois-SL package (Chang et al., 2015). 7 Experiments We first show the limitations of using current evaluation settings by showing how they can produce a wide range of results for the same baseline system. Next, we show the value of isolating the performance on predicting cross document coreference links. Finally, we assess the quality of the dataset and describe how we sem"
C16-1183,P10-1143,0,0.155545,"son and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created the Event Coreference Bank (ECB), the first dataset with both within and across document event coreference annotations. ECB contained 482 documents, obtained from the Google News archive. Bejan and Harabagiu (2010) also showed encouraging results on ECB with several unsupervised Bayesian approaches. Later, ECB was augmented by Lee et al. (2012), to include entity level coreference annotations as well. On the new dataset, which they named EECB, they showed how entity and within document event coreference can benefit from making joint coreference decisions. Using EECB, Wolfe et al. (201"
C16-1183,D08-1031,1,0.842567,"ich enables accurate evaluation of the cross document coreference performance (§ 4.1). Lifting these assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagi"
C16-1183,P08-1090,0,0.0409532,"ent of CDEC systems. Our new evaluation setting better reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset. 1 Introduction Understanding events is crucial to natural language understanding and has applications ranging from question answering (Berant et al., 2014; Narayanan and Harabagiu, 2004), to causal reasoning (Do et al., 2011; Chambers and Jurafsky, 2008) to headline generation (Sun et al., 2015). The task of Cross Document Event Coreference (CDEC) determines if two event mentions (which belong to different documents) refer to the same event (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Figure 1 shows an example of two events whose mentions co-refer across 3 documents, O’Brien fired! [King] decided to ﬁre [O’Brien] , who s;ll has two years lea on the contract he signed when he was hired by the [Sixers] on April 21 , 2004. Cheeks hired! Up the coast, the [Philly Sixers] canned [Jim O’Brien] ( who had been under ﬁre all season ) and quic"
C16-1183,cybulska-vossen-2014-using,0,0.682564,"oth within and across document event coreference annotations. ECB contained 482 documents, obtained from the Google News archive. Bejan and Harabagiu (2010) also showed encouraging results on ECB with several unsupervised Bayesian approaches. Later, ECB was augmented by Lee et al. (2012), to include entity level coreference annotations as well. On the new dataset, which they named EECB, they showed how entity and within document event coreference can benefit from making joint coreference decisions. Using EECB, Wolfe et al. (2015) formulated event coreference as an predicate alignment problem. Cybulska and Vossen (2014) noted that ECB and EECB did not have enough lexical diversity, thus oversimplifying the cross document coreference task. To get around this, they augmented the ECB corpus with 502 documents and released a larger corpus with event coreference annotations, named ECB+. While there have been other works which use multi-modal supervision signals (Zhang et al., 2015) for CDEC, at present, ECB+ is the sole public dataset with cross document coreference annotations for events, leading to its popularity (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Yang et al., 2015). The most recent work usi"
C16-1183,W15-0801,0,0.623625,"reference as an predicate alignment problem. Cybulska and Vossen (2014) noted that ECB and EECB did not have enough lexical diversity, thus oversimplifying the cross document coreference task. To get around this, they augmented the ECB corpus with 502 documents and released a larger corpus with event coreference annotations, named ECB+. While there have been other works which use multi-modal supervision signals (Zhang et al., 2015) for CDEC, at present, ECB+ is the sole public dataset with cross document coreference annotations for events, leading to its popularity (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Yang et al., 2015). The most recent work using the ECB+ corpus is that of Yang et al. (2015), who developed a novel Bayesian clustering framework, which clusters event within and across documents, by modeling the clustering process as a Hierarchical Distance Dependent Chinese Restaurant Process (HDDCRP). 3 Cross Document Event Coreference We view CDEC as a clustering task aimed at event-specific information aggregation. The clustering generated by a CDEC system allows one to examine all appearances of an event over a large corpus, shedding light on how the same event gets described in differ"
C16-1183,W03-0502,0,0.0515986,"e Cheeks] of his coaching responsibili;es . [Cheeks] was brought aaer [Jim O&apos;Brien] was relieved of his du;es. Figure 1: Event coreference across 3 documents. Event mentions are shown in bold and its participants are enclosed by brackets, []. Note that there are multiple firing events, but only a few co-refer. An efficient CDEC system enables corpus-level aggregation of event attributes, which can prove valuable for tasks such as information extraction and aggregation (Humphreys et al., 1997; Zhang et al., 2015), topic detection and tracking (Allan et al., 1998), multi-document summarization (Daniel et al., 2003) and knowledge discovery (Mayfield et al., 2009). In this work, we analyze whether existing CDEC evaluations reflect a system’s ability to predict cross document links. Past works have adopted different evaluation methodologies which either make simplifying assumptions about the coreference task, such as ignoring singletons, or overlook certain coreference mistakes made by a CDEC system (as discussed in §4). Furthermore, under existing evaluations, a system which only predicts within document coreference links can score higher than a system which This work is licensed under a Creative Commons"
C16-1183,P11-1144,0,0.025738,"learned pairwise classifier to score if two cluster of mentions are coreferent. Inter-cluster similarity score is computed as the average of the similarity score of their mentions. Like Lemma-δ, two clusters are considered for linking if the document similarity exceeds a threshold (we use 0.3 as above). We re-implemented the pairwise classifier of Yang et al. (2015) using the same feature set. Mention heads were found using dependency parses obtained by Stanford CoreNLP (Manning et al., 2014). We identify the Framenet (Baker et al., 1998) frame evoked by an event mention, we use the SEMAFOR (Das and Smith, 2011). For identifying the arguments for an event mention we use Illinois-SRL (Punyakanok et al., 2008). The pairwise classifier was trained using the Illinois-SL package (Chang et al., 2015). 7 Experiments We first show the limitations of using current evaluation settings by showing how they can produce a wide range of results for the same baseline system. Next, we show the value of isolating the performance on predicting cross document coreference links. Finally, we assess the quality of the dataset and describe how we semi-automatically detected different types of annotation errors. Since we foc"
C16-1183,D11-1027,1,0.800127,"accurate assessment of CDEC systems. Our new evaluation setting better reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset. 1 Introduction Understanding events is crucial to natural language understanding and has applications ranging from question answering (Berant et al., 2014; Narayanan and Harabagiu, 2004), to causal reasoning (Do et al., 2011; Chambers and Jurafsky, 2008) to headline generation (Sun et al., 2015). The task of Cross Document Event Coreference (CDEC) determines if two event mentions (which belong to different documents) refer to the same event (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Figure 1 shows an example of two events whose mentions co-refer across 3 documents, O’Brien fired! [King] decided to ﬁre [O’Brien] , who s;ll has two years lea on the contract he signed when he was hired by the [Sixers] on April 21 , 2004. Cheeks hired! Up the coast, the [Philly Sixers] canned [Jim O’Brien] ( who had been u"
C16-1183,P07-1029,0,0.010758,"ize the incorrect across topic (and sub-topic) links made by the Lemma baseline. However, the S IMPLE -C DEC and P URE -C DEC evaluations show that Lemma-δ is a stronger baseline. For future comparisons, using Lemma-δ as a baseline is more appropriate. 7.3 The Annotation Quality of ECB+ Evaluating with singletons also helped in discovering annotation errors in the dataset. In addition to identifying annotation errors, as described below, we found that several documents were partially annotated,6 which is consistent with a similar observation made in (Liu et al., 2014). We used the approach of Goldberg and Elhadad (2007) to semi-automatically detect annotation errors, by training an anchored SVM. First, for each pair of mention (mi , mj ) in the training data, we added a unique anchor feature aij , thus making the data linearly separable. Next, we trained a SVM classifier on all of the data with a high penalty parameter C. The classifier uses the anchor features to memorize the hard to classify examples, which are either genuine hard coreference pairs, or incorrect annotations. By thresholding the features weights for the anchor features |aij |&gt; δ (we use δ = 0.95), we generated a short-list of these hard cas"
C16-1183,P14-2076,0,0.0257117,"NLL average, which is average of MUC, B3 and CEAFe F-scores (Pradhan et al., 2011). Following previous work, we report CoNLL F1 and also the F-score averaged across all metrics. Coreference Scorers For running evaluations under the Y CF and B& H setting we use the standard within-document coreference scorer of Pradhan et al. (2014). However, in the S IMPLE -C DEC and P URE C DEC setting, creating a single meta-document for the entire test split leads to a document with over 8000 mentions, which causes runtime issues for metrics like CEAFe and Blanc with Pradhan et al. (2014)’s scorer.4 We use Hachey et al. (2014)’s scorer instead, which provides more efficient implementations.5 6 Baselines We evaluate the following event coreference baseline models under the different evaluation settings, Lemma In this model two events are coreferent if the head lemmas of their event mentions match. Lemma-WD In this model we consider two event mentions coreferent if their head lemmas match and they belong to the same document. This is the within document variant of the Lemma model. Lemma-δ In this model two event mentions are considered coreferent only if their head lemmas match and the tf-idf document similarity of t"
C16-1183,W97-1311,0,0.568356,"ixers] canned [Jim O’Brien] ( who had been under ﬁre all season ) and quickly hired [Mo Cheeks]. [76ers] have relieved [Maurice Cheeks] of his coaching responsibili;es . [Cheeks] was brought aaer [Jim O&apos;Brien] was relieved of his du;es. Figure 1: Event coreference across 3 documents. Event mentions are shown in bold and its participants are enclosed by brackets, []. Note that there are multiple firing events, but only a few co-refer. An efficient CDEC system enables corpus-level aggregation of event attributes, which can prove valuable for tasks such as information extraction and aggregation (Humphreys et al., 1997; Zhang et al., 2015), topic detection and tracking (Allan et al., 1998), multi-document summarization (Daniel et al., 2003) and knowledge discovery (Mayfield et al., 2009). In this work, we analyze whether existing CDEC evaluations reflect a system’s ability to predict cross document links. Past works have adopted different evaluation methodologies which either make simplifying assumptions about the coreference task, such as ignoring singletons, or overlook certain coreference mistakes made by a CDEC system (as discussed in §4). Furthermore, under existing evaluations, a system which only pre"
C16-1183,D12-1045,0,0.140735,"uation of the cross document coreference performance (§ 4.1). Lifting these assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created t"
C16-1183,liu-etal-2014-supervised,0,0.178914,".1). Lifting these assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created the Event Coreference Bank (ECB), the first dataset with b"
C16-1183,P14-2005,0,0.0775947,"el metric, B3 calculates precision and recall for each mention by measuring the proportion of overlap between the predicted and gold coreference chains. The final score is an average over the scores for all mentions. Although it overcomes the limitations of MUC, it uses mentions of the same entity (coreference chain) more than once. CEAFe (Luo, 2005) An entity-level metric which first finds an optimal alignment between entities in the key to the entities in the response by maximizing an entity similarity objective. This alignment is then used to calculate the CEAF precision and recall. Blanc (Luo et al., 2014) First described in (Recasens and Hovy, 2011) and later extended in (Luo et al., 2014). Blanc is based on the Rand Index (Rand, 1971), and computes two F-scores, one evaluating the quality of coreference decisions (Pairwise) and another evaluating the quality of the non-coreference 3 While evaluation should not rely on the corpus layout, we do not in any way suggest that this is the ideal approach for performing coreference. Indeed, considering coreference decisions over the entire corpus will be prohibitively expensive. 1953 decisions (Pairwise-Negative). The Pairwise (PW) and the Pairwise-Ne"
C16-1183,H05-1004,0,0.415613,"insertions or edge-deletions required to obtain the gold clustering (key) from the predicted clustering (response). A known limitation of MUC is that it does not reward a system for correctly identifying singletons. B3 (Bagga and Baldwin, 1998) A mention-level metric, B3 calculates precision and recall for each mention by measuring the proportion of overlap between the predicted and gold coreference chains. The final score is an average over the scores for all mentions. Although it overcomes the limitations of MUC, it uses mentions of the same entity (coreference chain) more than once. CEAFe (Luo, 2005) An entity-level metric which first finds an optimal alignment between entities in the key to the entities in the response by maximizing an entity similarity objective. This alignment is then used to calculate the CEAF precision and recall. Blanc (Luo et al., 2014) First described in (Recasens and Hovy, 2011) and later extended in (Luo et al., 2014). Blanc is based on the Rand Index (Rand, 1971), and computes two F-scores, one evaluating the quality of coreference decisions (Pairwise) and another evaluating the quality of the non-coreference 3 While evaluation should not rely on the corpus lay"
C16-1183,P14-5010,0,0.00428789,"reduces to the Lemma baseline. Supervised Agglomerative Clustering (SAC) Performs greedy agglomerative clustering, using a learned pairwise classifier to score if two cluster of mentions are coreferent. Inter-cluster similarity score is computed as the average of the similarity score of their mentions. Like Lemma-δ, two clusters are considered for linking if the document similarity exceeds a threshold (we use 0.3 as above). We re-implemented the pairwise classifier of Yang et al. (2015) using the same feature set. Mention heads were found using dependency parses obtained by Stanford CoreNLP (Manning et al., 2014). We identify the Framenet (Baker et al., 1998) frame evoked by an event mention, we use the SEMAFOR (Das and Smith, 2011). For identifying the arguments for an event mention we use Illinois-SRL (Punyakanok et al., 2008). The pairwise classifier was trained using the Illinois-SL package (Chang et al., 2015). 7 Experiments We first show the limitations of using current evaluation settings by showing how they can produce a wide range of results for the same baseline system. Next, we show the value of isolating the performance on predicting cross document coreference links. Finally, we assess the"
C16-1183,C04-1100,0,0.0269628,"which overcomes these limitations, and allows for an accurate assessment of CDEC systems. Our new evaluation setting better reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset. 1 Introduction Understanding events is crucial to natural language understanding and has applications ranging from question answering (Berant et al., 2014; Narayanan and Harabagiu, 2004), to causal reasoning (Do et al., 2011; Chambers and Jurafsky, 2008) to headline generation (Sun et al., 2015). The task of Cross Document Event Coreference (CDEC) determines if two event mentions (which belong to different documents) refer to the same event (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Figure 1 shows an example of two events whose mentions co-refer across 3 documents, O’Brien fired! [King] decided to ﬁre [O’Brien] , who s;ll has two years lea on the contract he signed when he was hired by the [Sixers] on April 21 , 2004. Cheeks hired! Up the coast, the [Philly Sixers]"
C16-1183,C02-1139,0,0.07491,"sing a new setting which enables accurate evaluation of the cross document coreference performance (§ 4.1). Lifting these assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this dire"
C16-1183,K15-1002,1,0.869048,"s document coreference performance (§ 4.1). Lifting these assumptions also allowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created the Event Coreference"
C16-1183,D16-1038,1,0.855143,"lowed us to semi-automatically identify several inconsistent annotations in the dataset. As the current evaluation dataset is the only available dataset with cross document coreference annotations, it is prudent to improve its annotation quality. We describe these annotation errors in § 7.3. 2 Related Work Work on event coreference deals primarily with coreference within document, mostly building on insights gained from the entity coreference literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Lee et al., 2012; Peng et al., 2015). Recent approaches (Liu et al., 2014; Araki et al., 2014; Peng et al., 2016) have shown improvements in within document event coreference by exploiting event specific sub-structure (viz. sub-event or information propagation to arguments) and new event representations. In comparison, cross document event coreference has been a less well-studied problem. Early work on cross document event coreference was done by Bagga and Baldwin (1999), who showed preliminary results on small exploratory datasets. To encourage research in this direction, Bejan and Harabagiu (2010) created the Event Coreference Bank (ECB), the first dataset with both within and across document event cor"
C16-1183,W11-1901,0,0.0815359,"Missing"
C16-1183,P14-2006,0,0.0872642,"-score. Each of the above metrics have some drawbacks. B3 and CEAF scores rapidly approach 100 if many singletons are present, and the same can drive the Blanc-PWN score to dominate the Blanc-PW score (Recasens and Hovy, 2011). This is why the entity coreference literature reports the CoNLL average, which is average of MUC, B3 and CEAFe F-scores (Pradhan et al., 2011). Following previous work, we report CoNLL F1 and also the F-score averaged across all metrics. Coreference Scorers For running evaluations under the Y CF and B& H setting we use the standard within-document coreference scorer of Pradhan et al. (2014). However, in the S IMPLE -C DEC and P URE C DEC setting, creating a single meta-document for the entire test split leads to a document with over 8000 mentions, which causes runtime issues for metrics like CEAFe and Blanc with Pradhan et al. (2014)’s scorer.4 We use Hachey et al. (2014)’s scorer instead, which provides more efficient implementations.5 6 Baselines We evaluate the following event coreference baseline models under the different evaluation settings, Lemma In this model two events are coreferent if the head lemmas of their event mentions match. Lemma-WD In this model we consider tw"
C16-1183,J08-2005,1,0.739269,"similarity score is computed as the average of the similarity score of their mentions. Like Lemma-δ, two clusters are considered for linking if the document similarity exceeds a threshold (we use 0.3 as above). We re-implemented the pairwise classifier of Yang et al. (2015) using the same feature set. Mention heads were found using dependency parses obtained by Stanford CoreNLP (Manning et al., 2014). We identify the Framenet (Baker et al., 1998) frame evoked by an event mention, we use the SEMAFOR (Das and Smith, 2011). For identifying the arguments for an event mention we use Illinois-SRL (Punyakanok et al., 2008). The pairwise classifier was trained using the Illinois-SL package (Chang et al., 2015). 7 Experiments We first show the limitations of using current evaluation settings by showing how they can produce a wide range of results for the same baseline system. Next, we show the value of isolating the performance on predicting cross document coreference links. Finally, we assess the quality of the dataset and describe how we semi-automatically detected different types of annotation errors. Since we focus here on evaluation and not on developing an end-to-end CDEC system, we use the gold event menti"
C16-1183,P15-1045,0,0.0636745,"r reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset. 1 Introduction Understanding events is crucial to natural language understanding and has applications ranging from question answering (Berant et al., 2014; Narayanan and Harabagiu, 2004), to causal reasoning (Do et al., 2011; Chambers and Jurafsky, 2008) to headline generation (Sun et al., 2015). The task of Cross Document Event Coreference (CDEC) determines if two event mentions (which belong to different documents) refer to the same event (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Figure 1 shows an example of two events whose mentions co-refer across 3 documents, O’Brien fired! [King] decided to ﬁre [O’Brien] , who s;ll has two years lea on the contract he signed when he was hired by the [Sixers] on April 21 , 2004. Cheeks hired! Up the coast, the [Philly Sixers] canned [Jim O’Brien] ( who had been under ﬁre all season ) and quickly hired [Mo Cheeks]. [76ers] have reliev"
C16-1183,M95-1005,0,0.607965,"r links between documents. Proposal 2 (P URE -C DEC ) In this setting, we first preprocess each document so that all within document mentions of the same event are reduced to a single meta-mention in that document. In this way, we discount the within document coreference links and focus on the cross document coreference links. Then, we follow the S IMPLE -C DEC setting to evaluate a single meta-document M generated from the preprocessed system response and the gold key documents. 5 Evaluation Metrics We briefly describe the coreference evaluation metrics that are used in our experiments. MUC (Vilain et al., 1995) A link-level metric, MUC counts the minimum number of edge-insertions or edge-deletions required to obtain the gold clustering (key) from the predicted clustering (response). A known limitation of MUC is that it does not reward a system for correctly identifying singletons. B3 (Bagga and Baldwin, 1998) A mention-level metric, B3 calculates precision and recall for each mention by measuring the proportion of overlap between the predicted and gold coreference chains. The final score is an average over the scores for all mentions. Although it overcomes the limitations of MUC, it uses mentions of"
C16-1183,N15-1002,0,0.223941,"Missing"
C16-1183,Q15-1037,0,0.372095,"lignment problem. Cybulska and Vossen (2014) noted that ECB and EECB did not have enough lexical diversity, thus oversimplifying the cross document coreference task. To get around this, they augmented the ECB corpus with 502 documents and released a larger corpus with event coreference annotations, named ECB+. While there have been other works which use multi-modal supervision signals (Zhang et al., 2015) for CDEC, at present, ECB+ is the sole public dataset with cross document coreference annotations for events, leading to its popularity (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Yang et al., 2015). The most recent work using the ECB+ corpus is that of Yang et al. (2015), who developed a novel Bayesian clustering framework, which clusters event within and across documents, by modeling the clustering process as a Hierarchical Distance Dependent Chinese Restaurant Process (HDDCRP). 3 Cross Document Event Coreference We view CDEC as a clustering task aimed at event-specific information aggregation. The clustering generated by a CDEC system allows one to examine all appearances of an event over a large corpus, shedding light on how the same event gets described in different documents. We fi"
C16-1183,D15-1020,0,0.069481,"en] ( who had been under ﬁre all season ) and quickly hired [Mo Cheeks]. [76ers] have relieved [Maurice Cheeks] of his coaching responsibili;es . [Cheeks] was brought aaer [Jim O&apos;Brien] was relieved of his du;es. Figure 1: Event coreference across 3 documents. Event mentions are shown in bold and its participants are enclosed by brackets, []. Note that there are multiple firing events, but only a few co-refer. An efficient CDEC system enables corpus-level aggregation of event attributes, which can prove valuable for tasks such as information extraction and aggregation (Humphreys et al., 1997; Zhang et al., 2015), topic detection and tracking (Allan et al., 1998), multi-document summarization (Daniel et al., 2003) and knowledge discovery (Mayfield et al., 2009). In this work, we analyze whether existing CDEC evaluations reflect a system’s ability to predict cross document links. Past works have adopted different evaluation methodologies which either make simplifying assumptions about the coreference task, such as ignoring singletons, or overlook certain coreference mistakes made by a CDEC system (as discussed in §4). Furthermore, under existing evaluations, a system which only predicts within document"
C16-1183,J14-2004,0,\N,Missing
C16-1183,C98-1013,0,\N,Missing
C16-1183,D14-1159,0,\N,Missing
C16-1285,W04-2412,0,0.118381,"Missing"
C16-1285,C10-1018,1,0.835462,"val constituents = t.getView(ViewNames.TOKENS).getConstituents // Go through a sliding window of tokens constituents.sliding(3)._forall { cons: List[Constituent] =&gt; POSTaggerPairwise on (cons(0), cons(1)).second === POSTaggerPairwise on ( cons(1), cons(2)).first } } 4.3 Entity-Relation extraction This task is for labeling entities and recognizing semantic relations among them. It requires making several local decisions (identifying named entities in the sentence) to support the relation identification. The models we represent here are inspired some well-known previous work (Zhou et al., 2005; Chan and Roth, 2010). The nodes in our models consists of Sentences, Mentions and Relations. 4.3.1 Features and Constraints For the entity extraction classifier, we define various lexical features for each mention – head word, POStags, words and POStags in a context window. Also, we incorporate some features based on gazetteers for organization, vehicle, weapons, geographic locations, proper names and collective nouns. The relation extraction classifier uses lexical, collocation and dependency-based features from the baseline implementation in Chan and Roth (2010). We also use features from the brown word cluster"
C16-1285,J93-2004,0,0.0549242,"Missing"
C16-1285,J08-2005,1,0.947422,"m a sentence node to all contained predicate nodes in the sentence and then apply the constraint to all of those predicates. Each constraint imposes the argumentTypeLearner to assign a legal argument type to each candidate argument or does not count it as an argument at all, i.e., to assign none value to the argument type. The feature templates are instances of Learnable in Saul and in fact they are treated as local classifiers. The script of Figure 3 shows the ArgumentType template. The Constraints are specified by means of first-order logical expressions. We use the constraints specified in Punyakanok et al. (2008) in our models. The script in Figure 4, shows an example expressing the legal argument constraints for a sentence. 4.1.3 Model Configurations Programming for learning and inference configurations in Saul is simply composing the basic building blocks of the language, that is, feature and constraint templates in different ways. Local models. Training local models is as easy as calling the train function over each specified feature template separately (e.g. ArgTypeLearner.train()). The test on these models also is simply done by calling test for each template (e.g. ArgTypeLearner.test()). In addi"
C16-1285,W09-1119,1,0.651657,"se and fine labels follow a strict hierarchy, we leverage this information to boost the prediction of the fine-grained classifier by constraining its prediction upon the (more reliable) coarse-grained relation classifier. 4.3.2 Model Configuration Entity type classifier. For the entity type task, we train two independent classifiers - one for coarse-label and the second for the fine-grained entity type. We generate the candidates for entities by taking all nouns and possessive pronouns, base noun phrases, selective chunks from the shallow parse and named entities annotated by the NE tagger of Ratinov and Roth (2009). Relation type classifier. For the relation types, we train two independent classifiers - coarse-grained relation type label and fine-grained relation type label. We use features from our unified data-model which are properties defined on the relations node in the data-model graph. We also incorporate the Relation Hierarchy constraint during inference so that the predictions of both classifiers are coherent. We report some of our results in Table 3. 5 Related Work This work has been done in the context of Saul, a recently developed declarative learning based programming language. DeLBP is a n"
C16-1285,P98-2186,1,0.511923,"t train function and provide the list of all declared constraint classifiers as parameters. The results of some versions of these models are shown in Table 1. The experimental settings, the data and the train/test splits are according to (Punyakanok et al., 2008) and the results are comparable. As the results show the models that use constraints are the best performing ones. For SRL the global background knowledge on the arguments in IBT setting did not improve the results. 4.2 Part-Of-Speech Tagging This is perhaps the most often used application in ML for NLP. We use the setting proposed by Roth and Zelenko (1998) as the basis for our experiments. The graph of an example sentence is shown in Figure 1. We model the problem as a single-node graph representing constituents in sentences. We make use of context window features and hence our graph has edges between each token and its context window. This enables us to define contextual features by traversing the relevant edges to access tokens in the context. The following code uses the gold POS-tag label (POSLabel) of the two tokens before the current token during training and POS-tag classifier’s prediction (POSTaggerKnown) of the two tokens before the cur"
C16-1285,L16-1645,1,0.822309,"these components have been specified, the programmer can easily choose which templates to use for learning (training) and inference (prediction). In this way the global objective is generated automatically for different training and testing paradigms in the spectrum of local to global models. One advantage of programming in Saul is that one can define a generic data-model for various tasks in each application domain. In this paper, we enrich Saul with an NLP data-model based on E DISON, a recently-introduced NLP library which contains raw data readers, data structures and feature extractors (Sammons et al., 2016) and use it as a collection of Sensors to easily generate the data-model from the raw data. In Saul, a Sensor is a ‘black-box’ function that can generate nodes, edges and properties in the graph. An example of a sensor for generating nodes and edges is a sentence tokenizer which receives a sentence and generates its tokens. Here, we will provide some examples of data-model declaration language but more details are available on-line2 . In the rest of the paper, we walk through the tasks of Semantic Role Labeling (SRL), Part-of-Speech (POS) tagging and Entity-Relation (ER) extraction and show ho"
C16-1285,W04-3212,0,0.0645782,"which are defined as classifiers (feature templates) and global constraints (constraint templates) in Saul. SRL has four main feature templates: 1) Predicate template: connects an input constituent to a single label lisP red . The input features of this template are generated based on the properties of the constituents φconstituent . The candidate generator of this template is a filter that takes all constituents whose pos-tag is VP; 2) Argument template: connects a pair of constituents to a linked label lisArg . The candidate generator of this template is a set of rules that are suggested by Xue and Palmer (2004); 3) ArgumentType template: connects a pair of constituents to the linked label largT ype . Same Xue-Palmer heuristics are used; 4) ArgumentTypeCorrelations template: connects two pairs of pairs of constituent (i.e. relations between relations) to their join linked label. The candidates are the pairs of Xue-Palmer candidates. 3034 ates. That being said, the comn Saul is presented by a relafactor graph. We exemplify when discussing the real probctions. node in the graph and using Sensors each sentence 461 will be connected to it components which are Con462 stituents derived from a constituent p"
C16-1285,P05-1053,0,0.028629,"TextAnnotation =&gt; val constituents = t.getView(ViewNames.TOKENS).getConstituents // Go through a sliding window of tokens constituents.sliding(3)._forall { cons: List[Constituent] =&gt; POSTaggerPairwise on (cons(0), cons(1)).second === POSTaggerPairwise on ( cons(1), cons(2)).first } } 4.3 Entity-Relation extraction This task is for labeling entities and recognizing semantic relations among them. It requires making several local decisions (identifying named entities in the sentence) to support the relation identification. The models we represent here are inspired some well-known previous work (Zhou et al., 2005; Chan and Roth, 2010). The nodes in our models consists of Sentences, Mentions and Relations. 4.3.1 Features and Constraints For the entity extraction classifier, we define various lexical features for each mention – head word, POStags, words and POStags in a context window. Also, we incorporate some features based on gazetteers for organization, vehicle, weapons, geographic locations, proper names and collective nouns. The relation extraction classifier uses lexical, collocation and dependency-based features from the baseline implementation in Chan and Roth (2010). We also use features from"
C16-1285,J92-4003,0,\N,Missing
C16-1285,C98-2181,1,\N,Missing
C16-2031,D13-1184,1,0.781797,"multilingual Wikipedia dump and existing training data for English NER. The system is online at http://cogcomp.cs.illinois.edu/page/demo_view/xl_wikifier 1 Motivation Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia. For English text, this problem has been studied extensively. (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013) It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. Recently, there has also been interest in the cross-lingual setting of Wikification: given a mention from a document written in a non-English language, the goal is to find the corresponding title in the English Wikipedia. This task is driven partly by the fact that a lot of information around the world may be written in a foreign language for which there are limited linguistic resources and, specifically, no English translation technology. Instead"
C16-2031,D07-1074,0,0.0168847,"The only resources required to train the proposed system are the multilingual Wikipedia dump and existing training data for English NER. The system is online at http://cogcomp.cs.illinois.edu/page/demo_view/xl_wikifier 1 Motivation Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia. For English text, this problem has been studied extensively. (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013) It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. Recently, there has also been interest in the cross-lingual setting of Wikification: given a mention from a document written in a non-English language, the goal is to find the corresponding title in the English Wikipedia. This task is driven partly by the fact that a lot of information around the world may be written in a foreign language for which there are limited linguistic"
C16-2031,P11-1138,1,0.730363,"oposed system are the multilingual Wikipedia dump and existing training data for English NER. The system is online at http://cogcomp.cs.illinois.edu/page/demo_view/xl_wikifier 1 Motivation Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia. For English text, this problem has been studied extensively. (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013) It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. Recently, there has also been interest in the cross-lingual setting of Wikification: given a mention from a document written in a non-English language, the goal is to find the corresponding title in the English Wikipedia. This task is driven partly by the fact that a lot of information around the world may be written in a foreign language for which there are limited linguistic resources and, specifically, no English translat"
C16-2031,N12-1052,0,0.0876386,"Missing"
C16-2031,Q16-1011,1,0.897804,"Figure 1: Screen shot of Illinois Cross-Lingual Wikifier. Figure 2: Pipeline of Illinois Cross-Lingual Wikifier. 2 System Description Figure 1 shows the web interface of our system. The bottom part is its output. The extracted named entities (in blue) are hyperlinked to the corresponding English Wikipedia pages. If the cursor points to a mention (e.g., “Verenigde Staten”), the corresponding English title and the entity type will be shown. Our system is based on two components that we proposed recently: a cross-lingual NER model (Tsai et al., 2016) and a cross-lingual mention grounding model (Tsai and Roth, 2016). Figure 2 shows an overview of the system. Given some text in a non-English language, the cross-lingual NER model extracts named entity mentions and the cross-lingual mention grounding model finds the corresponding English Wikipedia titles for each mention. 2.1 Cross-Lingual Named Entity Recognition We use the direct transfer NER model proposed in Tsai et al. (2016). This model can be trained on one or several languages, depending on the availability of training data, and can be applied to other Wikipedia languages without changing anything in the model. The key idea is that the cross-lingual"
C16-2031,K16-1022,1,0.907583,"emonstrations, pages 146–150, Osaka, Japan, December 11-17 2016. Figure 1: Screen shot of Illinois Cross-Lingual Wikifier. Figure 2: Pipeline of Illinois Cross-Lingual Wikifier. 2 System Description Figure 1 shows the web interface of our system. The bottom part is its output. The extracted named entities (in blue) are hyperlinked to the corresponding English Wikipedia pages. If the cursor points to a mention (e.g., “Verenigde Staten”), the corresponding English title and the entity type will be shown. Our system is based on two components that we proposed recently: a cross-lingual NER model (Tsai et al., 2016) and a cross-lingual mention grounding model (Tsai and Roth, 2016). Figure 2 shows an overview of the system. Given some text in a non-English language, the cross-lingual NER model extracts named entity mentions and the cross-lingual mention grounding model finds the corresponding English Wikipedia titles for each mention. 2.1 Cross-Lingual Named Entity Recognition We use the direct transfer NER model proposed in Tsai et al. (2016). This model can be trained on one or several languages, depending on the availability of training data, and can be applied to other Wikipedia languages without chan"
C16-2031,N16-1029,0,0.0661189,"Missing"
C16-2031,E06-1002,0,\N,Missing
C18-1254,P98-1013,0,0.556951,"ut’, sense number 01, has the licit semantic roles of Arg0, Arg1, and Arg2, which are putter, thing put, and where put, respectively. It is not the case that all of these are present in every use of the verb put in a corpus.3 2.2 Preposition Semantic Role Labeling Labeling preposition semantic roles helps with NLP tasks by providing additional shallow semantic information about prepositions and their semantic relation to other words in a sentence. 2 For current information on corpora with Propbank annotations, see https://propbank.github.io. Senses are from the index of Propbank and FrameNet (Baker et al., 1998) for English at http://verbs.colorado. edu/propbank/framesets-english-aliases/. 3 3006 The preposition semantic role labeler is the one described in Srikumar and Roth (2011) and Srikumar (2013). This role labeler was used due to the relatively small number of observed preposition semantic roles, and its integration with the CogComp NLP pipeline used in the current project.4 Each preposition has the potential semantic roles associated with it of GOVERNOR and GOVERNED. The governed argument in the phrase ‘to the store’ is ‘the store’. It is generally a noun phrase that follows the preposition. T"
C18-1254,bonial-etal-2014-propbank,0,0.0175209,"ines for all decisions. Additional decisions that came up are listed in the specifications.11 Annotators were instructed to make a single-pass of the data. If the output parse interfered with labeling semantic roles, annotators were instructed to change the parse according to the Penn Treebank Guidelines. In order to check verb senses and semantic role labels, an abbreviated version of Propbank (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) labels appeared in the Jubilee window, and annotators were able to use a link to the Propbank entry in the Unified Verb Index (Bonial et al., 2014; Bonial et al., forthcoming), if additional information was needed. If a verb sense did not appear to be present in Propbank, annotators consulted a list of previous decisions and, if it was present there, used that decision on semantic role labels. If the verb sense was neither in Propbank nor the list of previous decisions, annotators made note of the sense. However, annotators were instructed to try to use the previous senses as much as possible.12 The preposition semantic roles, based on those in Srikumar and Roth (2011), had brief example descriptions in the Frameset View window. 4.1 Spe"
C18-1254,P10-1101,1,0.909281,"sylvania Northbrook, IL USA danroth@seas.upenn.edu sfranco@imo-online.com Abstract This paper describes the augmentation of an existing corpus of child-directed speech. The resulting corpus is a gold-standard labeled corpus for supervised learning of semantic role labels in adult-child dialogues. Semantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown, 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the κ score for sense is 72.6, for the number of semantic-role-"
C18-1254,choi-etal-2010-propbank-instance,0,0.022212,"sociated with it of GOVERNOR and GOVERNED. The governed argument in the phrase ‘to the store’ is ‘the store’. It is generally a noun phrase that follows the preposition. The governor of the preposition can be a verb that takes a preposition as an argument. For example, in the sentence ‘Take the cart to the store’, the preposition ‘to’ has the verb ‘take’ as a governor. The governor can also be a noun phrase. For example, in the sentence ‘Give me the horse with the blue mane.’ the governor of ‘with’ is ‘the horse’. 3 Jubilee Annotation Tool The annotation tool was based on the Jubilee tool by (Choi et al., 2010)5 and the modified version is available at https://gitlab-beta.engr.illinois.edu/babysrl-group/jubilee. The original annotation tool used the Penn Treebank annotations (Mitchell et al., 1993) and Propbank’s framesets,6 and after an initial automatic SRL annotation phase, it allowed the annotators to modify the predicate sense and assign the associated semantic roles to constituents of the sentence. We extended the tool in several ways to accommodate the annotation of children’s utterances and prepositional SRL, and provided other improvements for the convenience of the annotators. A summary of"
C18-1254,P02-1031,0,0.533978,"rpus for supervised learning of semantic role labels in adult-child dialogues. Semantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown, 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the κ score for sense is 72.6, for the number of semantic-role-bearing arguments, the κ score is 77.4, for identical semantic role labels on a given argument, the κ score is 91.1, for the span of semantic role labels, and the κ for agreement is 93.9. The sense and number of arguments"
C18-1254,kingsbury-palmer-2002-treebank,0,0.725604,"s a gold-standard labeled corpus for supervised learning of semantic role labels in adult-child dialogues. Semantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown, 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the κ score for sense is 72.6, for the number of semantic-role-bearing arguments, the κ score is 77.4, for identical semantic role labels on a given argument, the κ score is 91.1, for the span of semantic role labels, and the κ for agreement is 93.9. The sense"
C18-1254,meyers-etal-2004-annotating,0,0.0796065,"creativecommons.org/licenses/by/4.0/ 1 Corpora and documentation are available at https://childes.talkbank.org. License details: http:// 3004 Proceedings of the 27th International Conference on Computational Linguistics, pages 3004–3014 Santa Fe, New Mexico, USA, August 20-26, 2018. Semantic role labeling (SRL) is a common task in NLP. For each predicate, an SRL system identifies sentence constituents and assigns to them argument (e.g., agent, patient) or adjunct (e.g., locative, manner) roles. Usually SRL refers to labeling verb semantic roles, but it has been extended to nominal predicates (Meyers et al., 2004), as well as prepositions (Srikumar and Roth, 2011; Srikumar, 2013; Schneider, 2016). SRL has proven useful in areas such as question answering and textual entailment. Annotated data sets for training and evaluating the performance of SRL systems are time-consuming to construct, but new types of annotated data are important for modeling early language acquisition, and for testing the ability of SRL systems to generalize across varieties of language use. The corpus described in this paper augments portions of an existing partial annotation of child-directed speech corpora, as described in Conno"
C18-1254,J93-2004,0,0.0775219,"es the availability and licensing of the corpus, and section 7 concludes the paper. 2 Semantic Role Labels 2.1 Verb Semantic Role Labeling with Propbank Propbank (Gildea and Palmer, 2002; Kingsbury and Palmer, 2002; Palmer et al., 2005) provides resources for labeling semantic roles for verbs. The original Propbank corpus included a large hand-annotated corpus of semantic verb-argument relations, and extensive guidelines for annotating verb semantic roles in new corpora (Bonial et al., forthcoming). Propbank added semantic role labels to sentences parsed according to Penn Treebank Guidelines (Mitchell et al., 1993). Other corpora have been annotated with Propbank verb senses and semantic roles, including discourses and SMS.2 For each verb, Propbank lists a set of senses for the verb and the licit semantic arguments for that sense. The list of semantic arguments includes core arguments like the agent and patient of transitive verbs as well as directional and locational phrases that commonly occur with the verb. Propbank annotations involve annotating the span of each of a verb’s arguments. For example, for the verb ‘put’, sense number 01, has the licit semantic roles of Arg0, Arg1, and Arg2, which are pu"
C18-1254,J05-1004,0,0.895633,"ing of semantic role labels in adult-child dialogues. Semantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown, 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the κ score for sense is 72.6, for the number of semantic-role-bearing arguments, the κ score is 77.4, for identical semantic role labels on a given argument, the κ score is 91.1, for the span of semantic role labels, and the κ for agreement is 93.9. The sense and number of arguments was often open to mult"
C18-1254,W07-0604,0,0.0476758,"ed corpus is available for use in CHAT (MacWhinney, 2000) and XML format. 1 Introduction The study of human language acquisition has greatly benefited from the availability of corpora of language use to, by, and around young children. The CHILDES project (MacWhinney, 2000) makes available transcribed corpora of adult-child dialogue in English and in a growing set of other languages.1 In recent years, annotations have been added to some CHILDES corpora, including part-of-speech tagging, syntactic parsing, and the identification of grammatical roles (Pearl and Sprouse, 2013; Sagae et al., 2010; Sagae et al., 2007). In the present paper, we describe an ongoing project that adds a new layer of annotation to selected CHILDES corpora, a hand-checked corpus of semantic role labels that provides a shallow semantic analysis of sentences’ predicate-argument structure. Our goal is to support the development of computational models of language acquisition that explore how children come to interpret sentences, assigning semantic roles to sentence constituents to determine who does what to whom in each sentence. An additional goal is to provide new resources for testing the ability of trained NLP systems to genera"
C18-1254,D11-1012,1,0.125242,"emantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown, 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines (Kingsbury and Palmer, 2002; Gildea and Palmer, 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the κ score for sense is 72.6, for the number of semantic-role-bearing arguments, the κ score is 77.4, for identical semantic role labels on a given argument, the κ score is 91.1, for the span of semantic role labels, and the κ for agreement is 93.9. The sense and number of arguments was often open to multiple interpretations in child speech, due to the rapidly ch"
C98-2181,W98-0717,1,0.760811,"back-propagation algorithm on the training stage. The input nodes of the network usually correspond to the tags of the words surrounding the word being tagged. The performance of the algorithms is comparable to that of HMM methods. In this paper, we address the POS problem with no unknown words (the closed world assumption) from the standpoint of SNOW. That is, we represent a POS tagger as a network of linear separators and use Winnow for learning weights of the network. The S N O W approach has been successfully applied to other problems of natural language processing(Golding and Roth, 1998; Krymolowski and Roth, 1998; Roth, 1998). However, this problem offers additional challenges to the S N O W architecture and algorithms. First, we are trying to learn a multi-class predictor, where the number of classes is unusually large(about 50) for such learning problems. Second, evaluating hypothesis in testing is done in a presence of attribute noise. The reason is that input features of the network are computed with respect to parts of speech of words, which are initially assigned from a lexicon. We address the first problem by restricting the parts of speech a tag for a word is selected from. Second problem is a"
C98-2181,C94-1027,0,0.168595,"sible POS tagging of the sentence one of which, d e t e r m i n e r , noun, m o d a l - v e r b , verb, respectively, is correct. The problem has numerous application in information retrieval, machine translation, speech recognition, and appears t o be an important intermediate stage in many natural language understanding related inferences. In recent years, a number of approaches have been tried for solving the problem. The most notable methods are based on Hidden Markov Models(HMM)(Kupiec, 1992; Schiitze, 1995), transformation rules(Brill, 1995; Brill, 1997), and multi-layer neural networks(Schmid, 1994). HMM taggers use manually tagged training data to compute statistics on features. For example, they can estimate lexical probabilities P r o b ( w o r d l t a g ) and contextual probabilities P r o b ( t a g l p r e v i o u s n tags). On the testing stage, the taggers conduct a search in the space of POS tags to arrive at the most probable POS labeling with respect to the computed statistics. T h a t is, given a sentence, the taggers assign in the sentence a sequence of tags that maximize the product of lexical and contextual probabilities over all words in the sentence. Transformation based"
C98-2181,E95-1020,0,0.0518257,"Missing"
C98-2181,J95-4004,0,\N,Missing
clarke-etal-2012-nlp,D11-1012,1,\N,Missing
clarke-etal-2012-nlp,W09-1119,1,\N,Missing
clarke-etal-2012-nlp,J08-2005,1,\N,Missing
clarke-etal-2012-nlp,D08-1031,1,\N,Missing
clarke-etal-2012-nlp,P05-1022,0,\N,Missing
clarke-etal-2012-nlp,P06-4018,0,\N,Missing
clarke-etal-2012-nlp,P11-1138,1,\N,Missing
clarke-etal-2012-nlp,W02-0109,0,\N,Missing
clarke-etal-2012-nlp,N10-1115,0,\N,Missing
D08-1031,N07-1011,0,0.429854,"to evaluate the impact of structural innovations. To this end, we combine an effective coreference classification model with a strong set of features, and present an ablation study to show the relative impact of a variety of features. As we show, this combination of a pairwise model and strong features produces a 1.5 percent294 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294–303, c Honolulu, October 2008. 2008 Association for Computational Linguistics age point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. 2 A Pairwise Coreference Model Given a document and a set of mentions, coreference resolution is the task of grouping the mentions into equivalence classes, so that each equivalence class contains exactly those mentions that refer to the same discourse entity. The number of equivalence classes is not specified in advance, but is bounded by the number of mentions. In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, ge"
D08-1031,N07-1030,0,0.515896,"Missing"
D08-1031,H05-1003,0,0.168961,"Missing"
D08-1031,H05-1083,0,0.0553031,"98) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 2007). 3 In the following description, the term head means the head noun phrase of a mention; the extent is the largest noun phrase headed by the head noun phrase. 3.1 The type of a mention indicates whether it is a proper noun, a common noun, or a pronoun. This feature, when conjoined with others, allows us to give different weight to a feature depending on whether it is being applied to a proper name or a pronoun. For our experiments in Section 5, we use gold mention types as is done by Culotta et al. (2007) and Luo and Zitouni (2005). Note that in the experiments described in Section 6 we predict the mention types as described there and do not use any gold data. The mention type feature is used in all experiments. 3.2 2 LBJ code is available at http://L2R.cs.uiuc.edu/ ˜cogcomp/asoftware.php?skey=LBJ 3 The package of all features used is available at http://L2R.cs.uiuc.edu/˜cogcomp/asoftware. php?skey=LBJ#features. 296 String Relation Features String relation features indicate whether two strings share some property, such as one being the substring of another or both sharing a modifier word. Features are listed in Table 1."
D08-1031,P04-1018,0,0.487517,"Missing"
D08-1031,C02-1139,0,0.576057,"pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential (see Section 2.2), we generate a coreference graph Gd according to the Best-Link decision model (Ng and Cardie, 2002b) as follows: For each mention m in document d, let Bm be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmax(pc(b, m)). b∈Bm If pc(a, m) is above a threshold chosen as described 295 in Section 4.4, we add the edge (a, m) to the coreference graph Gd . The resulting graph contains connected components, each representing one equivalence class, with all the mentions in the component referring to the same entity. This technique permits us to learn to detect some links between mentions while being agnostic about whether other mentions are linked, and ye"
D08-1031,P02-1014,0,0.907308,"pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential (see Section 2.2), we generate a coreference graph Gd according to the Best-Link decision model (Ng and Cardie, 2002b) as follows: For each mention m in document d, let Bm be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmax(pc(b, m)). b∈Bm If pc(a, m) is above a threshold chosen as described 295 in Section 4.4, we add the edge (a, m) to the coreference graph Gd . The resulting graph contains connected components, each representing one equivalence class, with all the mentions in the component referring to the same entity. This technique permits us to learn to detect some links between mentions while being agnostic about whether other mentions are linked, and ye"
D08-1031,J01-4004,0,0.986938,"the same discourse entity. The number of equivalence classes is not specified in advance, but is bounded by the number of mentions. In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, generate a set of edges such that any two mentions that belong in the same equivalence class are connected by some path in the graph. We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al., 2001). To decide whether two mentions should be linked in the graph, we learn a pairwise coreference function pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are corefer"
D08-1031,M95-1005,0,0.960586,"cted cluster and in m’s true cluster, pm is the size of the predicted cluster containing m, and tm is the size of m’s true cluster. Finally, d represents a document from the set D, and N is the total number of mentions in D. B-Cubed F-Score has the advantage of being able to measure the impact of singleton entities, and of giving more weight to the splitting or merging of larger entities. It also gives equal weight to all types of entities and mentions. For these reasons, we report our results using B-Cubed F-Score. MUC F-Score We also provide results using the official MUC scoring algorithm (Vilain et al., 1995). The MUC F-score is also the harmonic mean of precision and recall. However, the MUC precision counts precision errors by computing the minimum number of links that must be added to ensure that all mentions referring to a given entity are connected in the graph. Recall errors are the number of links that must be removed to ensure that no two mentions referring to different entities are connected in the graph. 4 The code is available at http://L2R.cs.uiuc.edu/ ˜cogcomp/tools.php 4.4 Learning Algorithm Details We train a regularized average perceptron using examples selected as described in Sec"
D08-1037,N06-1046,0,0.0152546,"problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key contribution is that we use a c"
D08-1037,P07-1083,0,0.333194,"address this problem. The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropriately. This has been shown to be difficult for language pairs which are very different, such as English and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an o"
D08-1037,P08-2014,1,0.734752,"common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropriately. This has been shown to be difficult for language pairs which are very different, such as English and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an optimization problem over a s"
D08-1037,P08-1045,0,0.204128,"Missing"
D08-1037,N06-1011,1,0.0874405,"ng English NEs to Hebrew. 1 Figure 1: Named entities transliteration pairs in English and Hebrew and the character level mapping between the two names. The Hebrew names can be romanized as eeta-l-ya and a-ya Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities. Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al., 2008). It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward task; however in many cases a consistent deterministic mapping between characters does not exist; rather, the mapping depends on the context the characters appear in and on transliteration conventions which may change across domains. Figure 1 exhibits two examples of NE transliterations in English and Hebrew, with the correct mapping across the two scripts. Although the two Hebrew names share a common prefix1 , this pr"
D08-1037,P06-1103,1,0.0915542,"ng English NEs to Hebrew. 1 Figure 1: Named entities transliteration pairs in English and Hebrew and the character level mapping between the two names. The Hebrew names can be romanized as eeta-l-ya and a-ya Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities. Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al., 2008). It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward task; however in many cases a consistent deterministic mapping between characters does not exist; rather, the mapping depends on the context the characters appear in and on transliteration conventions which may change across domains. Figure 1 exhibits two examples of NE transliterations in English and Hebrew, with the correct mapping across the two scripts. Although the two Hebrew names share a common prefix1 , this pr"
D08-1037,W05-0618,0,0.0331978,"character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key contribution is that we use a constrained optimization approach also to determine"
D08-1037,W05-0639,0,0.134871,"ze it as an optimization problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key cont"
D08-1037,W04-2401,1,0.930743,"lish and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an optimization problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter35"
D08-1037,W06-1630,0,0.358201,"all our example the Hebrew script is shown left-to-right to simplify the visualization of the transliteration mapping. 353 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 353–362, c Honolulu, October 2008. 2008 Association for Computational Linguistics In recent years, as it became clear that solutions that are based on linguistics rules are not satisfactory, machine learning approaches have been developed to address this problem. The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are"
D08-1037,D07-1001,0,\N,Missing
D09-1100,D08-1082,0,0.023721,"work also considered the supervision signal obtained by interpreting natural language in the context of a formal domain. Branavan et al. (2009) use feedback from a world model as a supervision signal. Chen and Mooney (2008) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervised generative model that accounts for both syntax and lexical choice. The semantic abstract is represented as a set of predicate logic formulae, which are applied as higher-order features for learning. We demonstrate that these features improve learning performance, and that both the lexica"
D09-1100,P09-1010,0,0.123404,"tic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This contrasts to previous work which interprets “directions” and thus assumes a direct correspondence between text and world state (Branavan et al., 2009; Chen and M"
D09-1100,C92-2106,0,0.223445,"Missing"
D09-1100,W05-0602,0,0.495566,"tation is not due merely to the added expressivity of our features. The third row compares against N O - SYNTAX, a crippled version of our model that incorporates lexical features but not the syntactic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be"
D09-1100,W00-1308,0,0.0113873,"owest, highest higher, sequence, sequential black, red, color suit, club, diamond, spade, heart onto bottom, available, top empty obtained from the Internet. Due to the popularity of the Microsoft implementation of Freecell, instructions often contain information specific to playing Freecell on a computer. We manually removed sentences which did not focus on the card aspects of Freecell (e.g., how to set up the board and information regarding where to click to move cards). In order to use our semantic abstraction model, the instructions were part-of-speech tagged with the Stanford POS Tagger (Toutanova and Manning, 2000) and dependency parses were obtained using Malt (Nivre, 2006). Table 1: Predicates in the Freecell world model, with natural language glosses obtained from the development set text. Glosses Our reading to learn setting requires a small set of glosses, which are surface forms commonly used to represent predicates from the world model. We envision an application scenario in which a designer manually specifies a few glosses for each predicate. However, for the purposes of evaluation, it would be unprincipled for the experimenters to handcraft the ideal set of glosses. Instead, we gathered a devel"
D09-1100,P07-1121,0,0.146856,"Missing"
D09-1100,P06-1085,0,0.0178021,"forward. For word slots to which no literals are aligned, the lexical item is drawn from a language model θ, estimated from the entire document collection. For slots to which at least one literal is aligned, we construct a language model φ in which the probability mass is divided equally among all glosses of aligned predicates. The language model θ is used as a backoff, so that there is a strong bias in favor of generating glosses, but some probability mass is reserved for the other lexical items. 1 There are many recent applications of Dirichlet processes in natural language processing, e.g. Goldwater et al. (2006). 961 4 Inference To compute the probability of a parsed sentence given a formula, we sum over alignments, This section describes a sampling-based inference procedure for obtaining a set of formulae f that explain the observed text s and dependency structures t. We perform Gibbs sampling over the formulae assigned to each sentence. Using the Chinese Restaurant Process interpretation of the Dirichlet Process (Aldous, 1985), we marginalize π, the infinite multinomial over all possible formulae: at each sampling step we select either an existing formula, or stochastically generate a new formula."
D09-1100,P09-1011,0,0.199814,"erely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning. This will allow us to directly evaluate the contribution of the extracted relations for learning. We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no label"
D09-1100,W05-0600,0,\N,Missing
D10-1075,W06-1615,0,0.402207,"also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 1 Focuses on P (X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target d"
D10-1075,W04-3237,0,0.115617,"he problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different fra"
D10-1075,D09-1047,0,0.0154504,"d word-cluster information. Notice that because of combining two classes of adaption algorithms, our approach is significantly better than these two systems10 . Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation. Here the data contains multiple prepositions where each preposition has many different senses. The goal is to predict the right sense for a given preposition in the testing data. The source domain is the SemEval 2007 preposition WSD Task and the target domain is from the dataset annotated in (Dahlmeier et al., 2009). Our feature design mainly comes from (Tratz and Hovy, 2009) (who do not evaluate their system on our target data). As our un10 The work (Ratinov and Roth, 2009) also combines their system with several document-level features. While it is possible to add these features in our system, we do not include any global features for the sake of simplicity. Note that our system is competitive to (Ratinov and Roth, 2009) even though our system does not use global features. Systems Our NER FM09 RR09 RR09 + global Cluster? y n y y TGT? y y n n P.F1 84.1 79.98 N/A N/A T.F1 86.5 N/A 83.2 86.2 Table 3: Comp"
D10-1075,P07-1033,0,0.443142,"Missing"
D10-1075,N09-1068,0,0.461514,"big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different frameworks, even though they might use exactly t"
D10-1075,P09-1056,0,0.0714695,"g an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 1 Focuses on P (X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms"
D10-1075,P07-1034,0,0.0802003,"urce and target labeled data to work. We denote n as the total number of features3 and m is the number of the “domains”, where one of the domains is the target domain. The FE framework creates a global weight vector in Rn(m+1) , an extended space for all domains. The representation x of the t-th domain is mapped by Φt (x) ∈ Rn(m+1) . In the extended space, the first n features consist of the “shared” block, which is always active across all tasks. The (t+1)-th block (the (nt+1)-th to the (nt+n)-th features) is a “specific” block, and is only active when 2 Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. However, little analysis on the interaction of the two aspects is discussed in that paper 3 We assume that the number of features in each domain is equal. 769 extracting examples from the task t. More formally, 2 6 Φt (x) = 4 |{z} x shared blocks z } |{ 0...0 3 blocks z } |{ 7 0 . . . 0 5 . (1) (m−t) (t−1) x |{z} specific A single weight vector w ¯ is obtained by training on the modified labeled data {yit , Φt (xti )}m t=1 . Given that this framework only extends the feature space, in this paper, we also call it"
D10-1075,P08-1068,0,0.0295145,"ngle model on the pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4 Our gazetteers comes from (Ratinov and R"
D10-1075,W09-1119,1,0.819377,"pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4 Our gazetteers comes from (Ratinov and Roth, 2009). Framework Unl"
D10-1075,N09-3017,0,0.0459385,"Missing"
D10-1075,J92-4003,0,\N,Missing
D10-1075,W03-0419,0,\N,Missing
D10-1094,P01-1005,0,0.0269081,"er is organized as follows. First, we describe related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural language by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for contexts"
D10-1094,W07-1604,0,0.113414,"Missing"
D10-1094,C10-2031,0,0.0269314,"iency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For e"
D10-1094,W07-1607,0,0.0131151,"Missing"
D10-1094,C08-1022,0,0.390829,"Missing"
D10-1094,I08-1059,0,0.106651,"used on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or al"
D10-1094,N10-1019,0,0.7264,"akes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a pot"
D10-1094,han-etal-2010-using,0,0.541148,"pted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction."
D10-1094,P03-2026,0,0.189556,"spelling errors – More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/"
D10-1094,J08-2005,1,0.730673,"lso comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic 8 ThreshAll is not possible with this training option, as the system never proposes a correction that is not in L1ConfSet(pi ). 9 LBJ code is available at http://cogcomp.cs. illinois.edu/page/software 967 Regression, while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applications (Punyakanok et al., 2008). 6 Results and Discussion Table 4 shows performance of the four systems by the source language. For each source language, the methods that restrict candidate sets in training or testing outperform the baseline system NegAll-Clean-ThreshAll that does not restrict candidate sets. The NegAll-ErrorL1-NoThresh system performs better than the other three systems for all languages, except for Italian. In fact, for the Czech speaker data, all systems other than NegAll-ErrorL1NoThresh, have a precision and a recall of 0, since no errors are detected10 . Source lang. CH CZ IT RU SP System Acc. P R NegA"
D10-1094,W10-1004,1,0.824013,"ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco"
D10-1094,N10-1018,1,0.795238,"ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco"
D10-1094,C08-1109,0,0.28795,"r a set of prepositions for the preposition correction problem. Each occurrence of a candidate word in text is represented as a vector of features. A classifier is trained on a large corpus of error-free text. Given text to correct, for each word in text that belongs to the confusion set the classifier is used to predict the most likely candidate in the confusion set given the word’s context. In the same spirit, models for correcting ESL errors are generally trained on well-formed native text. Han et al. (2006) train a maximum entropy model to correct article mistakes. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. Gamon et al. (2008) train a decision tree model and a language model to correct errors in article and preposition usage. Bergsma et al. (2009) propose a Na¨ıve Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prep"
D10-1094,P10-2065,0,0.367897,"1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the no"
D10-1107,abad-etal-2010-resource,0,0.0288388,") as in the following example, taken from a textual entailment challenge dataset: T: Nigeria’s NDLEA has seized 80 metric tonnes of cannabis in one of its largest ever hauls, officials say. (without additional information) contradict a similar statement with respect to Japan since these are different countries, as in the following: T: A strong earthquake struck off the southern tip of Taiwan at 12:26 UTC, triggering a warning from Japan’s Meteorological Agency that a 3.3 foot tsunami could be heading towards Basco, in the Philippines. H: An earthquake strikes Japan. Several recent TE studies (Abad et al., 2010; Sammons et al., 2010) suggest to isolate TE phenomena, such as recognizing taxonomic relations, and study them separately; they discuss some of characteristics of phenomena such as contradiction from a similar perspective to ours, but do not provide a solution. In this paper, we present TAxonomic RElation Classifier (TAREC), a system that classifies taxonomic relations between a given pair of terms using a machine learning based classifier. An integral part of TAREC is also our inference model that makes use of relational constraints to enforce coherency among several related predictions. TA"
D10-1107,P08-1004,0,0.0152447,"rticles in Wikipedia for each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). Moreover, an Open IE system cannot control the extracted relations and this is essential when identifying taxonomic relations. Recently, (Baroni and Lenci, 2010) described a general framework of distributional semantic models that extracts significant contexts of given terms from large corpora. Consequently, a term can be represented by a vector of con"
D10-1107,J10-4006,0,0.0399631,"of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when dealing with targeted terms not being covered."
D10-1107,P08-1079,0,0.0120035,"each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). Moreover, an Open IE system cannot control the extracted relations and this is essential when identifying taxonomic relations. Recently, (Baroni and Lenci, 2010) described a general framework of distributional semantic models that extracts significant contexts of given terms from large corpora. Consequently, a term can be represented by a vector of contexts in which it frequently 1"
D10-1107,N07-1030,0,0.0163015,"k. Figure 2 shows some n-term networks consisting of two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). 1104 Bill Clinton Blue car manufacturer BMW z z z w physical quantities v w z temperature x y George W. President Bush (a) x y x Green Honda Red (b) length y x y Toyota Celcius meter (c) (d) Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1 , z2 , ..., zm } be a set of additional terms. For a sub"
D10-1107,C92-2082,0,0.450019,"semantic representation of the terms and determines the taxonomic relations between them. This classifier will make use of existing knowledge bases in multiple ways, but will provide significantly larger coverage and more precise results. We make use of a dynamic resource such as Wikipedia to guarantee increased coverage without changing our model and also perform normalization-to-Wikipedia to find appropriate Wikipedia replacements for outside-Wikipedia terms. Moreover, stationary resources are usually brittle because of the way most of them are built: using local relational patterns (e.g. (Hearst, 1992; Snow et al., 2005)). Infrequent terms are less likely to be covered, and some relations may not be supported well by these methods because their corresponding terms rarely appear in close proximity (e.g., an Israeli tennis player Dudi Sela and Roger Federrer). Our approach uses search techniques to gather relevant Wikipedia pages of input terms and performs a learning-based classification w.r.t. to the features extracted from these pages as a way to get around this brittleness. Motivated by the needs of NLP applications such as RTE, QA, Summarization, and the compositionality argument allude"
D10-1107,P08-1119,0,0.0835548,"tions that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when dealing with targeted"
D10-1107,C08-1066,0,0.0292307,"rsity of Illinois at Urbana-Champaign Urbana, IL 61801, USA {quangdo2,danr}@illinois.edu Abstract al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007)."
D10-1107,W09-3714,0,0.0219613,"ampaign Urbana, IL 61801, USA {quangdo2,danr}@illinois.edu Abstract al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). Determining whether two terms"
D10-1107,P08-1003,0,0.0265022,"Missing"
D10-1107,P06-1015,0,0.442928,"s. 1 Introduction Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when"
D10-1107,J08-2005,1,0.809849,"rm networks consisting of two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). 1104 Bill Clinton Blue car manufacturer BMW z z z w physical quantities v w z temperature x y George W. President Bush (a) x y x Green Honda Red (b) length y x y Toyota Celcius meter (c) (d) Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1 , z2 , ..., zm } be a set of additional terms. For a subset Z ∈ Z, we construct a"
D10-1107,W04-2401,1,0.920995,"e model that makes use of relational constraints to enforce coherency among several related predictions. TAREC does not aim at building or extracting a hierarchical structure of concepts and relations, but rather to directly recognize taxonomic relations given a pair of terms. Target terms are represented using vector of features that are extracted from retrieved corresponding Wikipedia pages. In addition, we make use of existing stationary ontologies to find related terms to the target terms, and classify those too. This allows us to make use of a constraint-based inference model (following (Roth and Yih, 2004; Roth and Yih, 2007) that enforces coherency of decisions across related pairs (e.g., if x is-a y and y is-a z, it cannot be that x is a sibling of z). In the rest of the paper, after discussing related work in Section 2, we present an overview of TAREC in Section 3. The learning component and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 H: Nigeria seizes 80 tonnes of drugs. Similarly, it is important to know of a sibling relation to infer that a statement about Taiwan may 1100 Related Work"
D10-1107,P10-1122,1,0.804642,"ng example, taken from a textual entailment challenge dataset: T: Nigeria’s NDLEA has seized 80 metric tonnes of cannabis in one of its largest ever hauls, officials say. (without additional information) contradict a similar statement with respect to Japan since these are different countries, as in the following: T: A strong earthquake struck off the southern tip of Taiwan at 12:26 UTC, triggering a warning from Japan’s Meteorological Agency that a 3.3 foot tsunami could be heading towards Basco, in the Philippines. H: An earthquake strikes Japan. Several recent TE studies (Abad et al., 2010; Sammons et al., 2010) suggest to isolate TE phenomena, such as recognizing taxonomic relations, and study them separately; they discuss some of characteristics of phenomena such as contradiction from a similar perspective to ours, but do not provide a solution. In this paper, we present TAxonomic RElation Classifier (TAREC), a system that classifies taxonomic relations between a given pair of terms using a machine learning based classifier. An integral part of TAREC is also our inference model that makes use of relational constraints to enforce coherency among several related predictions. TAREC does not aim at bui"
D10-1107,P06-1101,0,0.845995,"lment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). Determining whether two terms in text have an ancestor relation (e.g. Toyota and car) or a sibling relation (e.g. Toyota and Honda) is an essential component of textual inference in NLP applications such as Question Answering, Summarization, and Recognizing Textual Entailment. Significant work has been done on developing stationary knowledge sources that could potentially support these tasks, but these resources often suffer from low coverage, noise, and are inflexible when needed to support terms that are not identical to those placed in them, m"
D10-1107,O08-4001,0,0.0565356,"Missing"
D10-1107,N09-1033,0,0.0127728,"ta-I). This dataset contains both terms with Wikipedia pages (e.g. George W. Bush) and nonWikipedia terms (e.g. hindu mysticism). Pairs of terms are generated by randomly pairing semantic class names and instances. We generate disjoint training and test sets of 8,000 and 12,000 pairs of terms, respectively. We call the test set of this 5 However, YAGO by itself is weaker than our approach in identifying taxonomic relations (see Section 6.) 6 These relations are defined in the YAGO ontology. dataset Test-I. Dataset-II is generated from 44 semantic classes of more than 10,000 instances used in (Vyas and Pantel, 2009)7 . The original semantic classes and instances were extracted from Wikipedia lists. This data, therefore, only contains terms with corresponding Wikipedia pages. We also generate disjoint training and test sets of 8,000 and 12,000 pairs of terms, respectively, and call the test set of this dataset Test-II.8 Several semantic class names in the original data are written in short forms (e.g. chemicalelem, proglanguage). We expand these names to some meaningful names which are used by all systems in our experiments. For example, terroristgroup is expanded to terrorist group, terrorism. Table 1 sh"
D10-1107,P95-1026,0,0.392173,"Missing"
D11-1012,W04-3220,0,0.0143075,"ptation problems of differing vocabularies and unseen features. 6 Discussion and Related work Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and p"
D11-1012,W04-2412,0,0.126022,"Missing"
D11-1012,W05-0620,0,0.0433094,"Missing"
D11-1012,P05-1022,0,0.0125627,"ints described above, which can be transformed to linear (in)equalities. We denote these constraints as C SRL . In addition to C SRL which were defined by Punyakanok et al. (2008), we also have the constraints linking the predictions of the identifier and classifier: C I vv,i,∅ + vv,i = 1; ∀v, i. (2) Inference in our baseline SRL system is, thus, the maximization of the objective defined in (1) subject to constraints C SRL , the identifier-classifier constraints defined in (2) and the restriction of the variables to take values in {0, 1}. To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the 6 The primary advantage of using ILP for inference is that this representation enables us to add arbitrary coherence constraints between the phenomena. If the underlying optimization problem itself is tractable, then so is the corresponding integer program. However, other approaches to solve the constrained maximization problem can also be used for inference. same feature representation as in the original system. We trained the classifiers on the standard Propbank training set using the one-vs-all extension of the average Perceptron algorithm. As with the preposition roles, we"
D11-1012,W02-1001,0,0.0154384,"ll the joint constraints. 4.2 Learning to rescale the individual systems Given the individual models and the constraints, we only need to learn the scaling parameters λpZ . Note that the number of scaling parameters is the total number of labels. When we jointly predict verb SRL and preposition role, we have 22 preposition roles (from table 3), one SRL identifier label and 54 SRL argument classifier labels. Thus we learn only 77 parameters for our joint model. This means that we only need a very small dataset that is jointly annotated with all the phenomena. We use the Structure Perceptron of Collins (2002) to learn the scaling weights. Note that for learning the scaling weights, we need each label to be associated with a real-valued feature. Given an assignment of the inference variables v, the value of the feature corresponding to the label Z of task p is given by the sum of scores of all parts in the structure for p that P p have been assigned this label, i.e. vZ,y ·ΘpZ,y . This yp feature is computed for the gold and the predicted structures and is used for updating the weights. 5 Experiments In this section, we describe our experimental setup and evaluate the performance of our approach. Th"
D11-1012,D09-1047,0,0.591094,"MPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the l"
D11-1012,N09-1037,0,0.0294931,"ing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensib"
D11-1012,P07-1072,0,0.0204688,"ponents to define our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include systems that predict structures for other linguistic phenomena because we do not retrain the underlying systems. The semantic relations can be enriched by incorporating more linguistic phenomena such as nominal SRL, defined by the Nombank annotation scheme of Meyers et al. (2004), the preposition function analysis of O’Hara and Wiebe (2009) and noun compound analysis as defined by Girju (2007) and Girju et al. 138 (2009) and others. This presents an exciting direction for future work. 7 Conclusion This paper presents a strategy for extending semantic role labeling without the need for extensive retraining or data annotation. While standard semantic role labeling focuses on verb and nominal relations, sentences can express relations using other lexical items also. Moreover, the different relations interact with each other and constrain the possible structures that they can take. We use this intuition to define a joint model for inference. We instantiate our model using verb semantic"
D11-1012,N10-1115,0,0.0272943,"Missing"
D11-1012,W08-2122,0,0.0135404,"s on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scaling pa"
D11-1012,C10-2052,0,0.334161,"Missing"
D11-1012,P08-1068,0,0.0199219,"IPIENT S PECIES T EMPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along"
D11-1012,S07-1005,0,0.455254,"e company calculated the price trends on the major stock markets on Monday. 3.1 Preposition Relations Prepositions indicate a relation between the attachment point of the preposition and its object. As we have seen, the same preposition can indicate different types of relations. In the literature, the polysemy of prepositions is addressed by The Preposition Project1 of Litkowski and Hargraves (2005), which is a large lexical resource for English that labels prepositions with their sense. This sense inventory formed the basis of the SemEval-2007 task of preposition word sense disambiguation of Litkowski and Hargraves (2007). In our example, the first on 1 http://www.clres.com/prepositions.html 131 would be labeled with the sense 8(3) which identifies the object of the preposition as the topic, while the second instance would be labeled as 17(8), which indicates that argument is the day of the occurrence. The preposition sense inventory, while useful to identify the fine grained distinctions between preposition usage, defines a unique sense label for each preposition by indexing the definitions of the prepositions in the Oxford Dictionary of English. For example, in the phrase at noon, the at would be labeled wit"
D11-1012,P09-1039,0,0.0234663,"software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let viI denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment X X C max ΘC ΘIi viI (1) i,a vi,a + vC ,vI i,a i Here, vC and vI denote all the argument classifier and identifier vari"
D11-1012,W04-2705,0,0.354793,"ithout retraining the individual models. 1 (1) The field goal by Brien changed the game in the fourth quarter. Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). Verb centered semantic role labeling would identify the arguments of the predicate change as (a) The field goal by Brien (A0, the causer of the change), (b) the game (A1, the thing changing), and (c) in the fourth quarter (temporal modifier). However, this does not tell us that the scorer of the field goal was Brien, which is expressed by the preposition by. Also, note that the in indicates a temporal relation, which overlaps with the verb’s analysis. In this paper, we propose an extension of the standard semantic role labeling task to include relations expressed by lexical items other than"
D11-1012,N09-1018,0,0.0242326,"sifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that"
D11-1012,J09-2002,0,0.398169,"Missing"
D11-1012,J05-1004,0,0.864474,"between their predictions, we show improvements in the performance of both tasks without retraining the individual models. 1 (1) The field goal by Brien changed the game in the fourth quarter. Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). Verb centered semantic role labeling would identify the arguments of the predicate change as (a) The field goal by Brien (A0, the causer of the change), (b) the game (A1, the thing changing), and (c) in the fourth quarter (temporal modifier). However, this does not tell us that the scorer of the field goal was Brien, which is expressed by the preposition by. Also, note that the in indicates a temporal relation, which overlaps with the verb’s analysis. In this paper, we propose an extension of the standard sema"
D11-1012,J08-2005,1,0.958117,"bank. We use this system as our independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4 The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to gene"
D11-1012,W09-1119,1,0.665769,"sense prediction on the prepositions that have been annotated for the Penn Treebank dataset. Role ACTIVITY ATTRIBUTE B ENEFICIARY C AUSE C ONCOMITANT E ND C ONDITION E XPERIENCER I NSTRUMENT L OCATION M EDIUM O F C OMMUNICATION N UMERIC /L EVEL O BJECT O F V ERB OTHER PART W HOLE PARTICIPANT /ACCOMPANIER P HYSICAL S UPPORT P OSSESSOR P ROFESSIONAL A SPECT R ECIPIENT S PECIES T EMPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Aver"
D11-1012,W06-1616,0,0.0231071,"cogcomp.cs.illinois.edu/page/software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let viI denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment X X C max ΘC ΘIi viI (1) i,a vi,a + vC ,vI i,a i Here, vC and vI denote all the argument clas"
D11-1012,rizzolo-roth-2010-learning,1,0.631772,"belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the labels in the Treebank dataset. According to this labeling scheme, the first on in our running example will be labeled T OPIC and the second one will 2 This dataset does not annotate all prepositions and restricts itself mainly to prepositions that start a Propbank argument. The data is available at http://nlp.comp.nus. edu.sg/corpora 3"
D11-1012,W04-2401,1,0.776726,"t classifier predictions – the identifier should predict that a candidate is an argument if, and only if, the argument classifier does not predict the label ∅. This change is in keeping with the idea of using joint inference to combine independently 5 The verb SRL system be downloaded from http:// cogcomp.cs.illinois.edu/page/software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Simila"
D11-1012,D10-1001,0,0.016691,"d semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scaling parameters. Our approach is conceptually similar to that of Rush et al. (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. They applied this idea to jointly predict dependency and phrase structure parse trees and on the task of predicting full parses together with part-ofspeech tags. The main difference in our approach is that we treat the scaling problem as a separate learning problem in itself and train a joint model specifically for re-scaling the output of the trained systems. The SRL combination system of Surdeanu et al. (2007) studied the combination of three different SRL s"
D11-1012,J08-2002,0,0.0194954,"ur independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4 The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of"
D11-1012,N09-3017,0,0.276284,"Missing"
D11-1012,W04-3212,0,0.0526208,"05). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of argument candidates for each predicate. The identification stage uses a classifier to prune the candidates. In the argument classification step, the candidates that remain after the identification step are assigned scores for the SRL arguments using a multiclass classifier. One of the labels of the classifier is ∅, which indicates that the candidate is, in fact, not an argument. The inference step produces a combined prediction for all argument candidates of a verb proposition by enforcing global constraints. The inference enforces the following structural"
D11-1012,S07-1051,0,0.131782,"Missing"
D11-1012,J05-1003,0,\N,Missing
D11-1027,P08-1090,0,0.362138,"Missing"
D11-1027,P09-1068,0,0.0962789,"Missing"
D11-1027,W09-4303,0,0.087962,"Missing"
D11-1027,W03-1210,0,0.840013,"Missing"
D11-1027,P08-1030,0,0.0478501,"Missing"
D11-1027,de-marneffe-etal-2006-generating,0,0.00501991,"Missing"
D11-1027,rizzolo-roth-2010-learning,1,0.755349,"Missing"
D11-1027,W04-2401,1,0.356693,"that are automatically collected from an unannotated corpus. We show that our approach achieves better performance than two approaches: one based on a frequently used metric that measures association, and another based on the effect-control-dependency (ECD) metric described in a prior work (Riaz and Girju, 2010). • We leverage on the interactions between event causality prediction and discourse relations prediction. We combine these knowledge sources through a global inference procedure, which we formalize via an Integer Linear Programming (ILP) framework as a constraint optimization problem (Roth and Yih, 2004). This allows us to easily define appropriate constraints to ensure that the causality and discourse predictions are coherent with each other, thereby improving the performance of causality identification. 2 “protest”, etc., and examples of the arguments of “attacked” could be its subject and object nouns. To measure the causality association between a pair of events ei and ej (in general, ei and ej could be extracted from the same or different documents), we should use information gathered about their predicates and arguments. A simple approach would be to directly calculate the pointwise mut"
D11-1027,P10-1143,0,\N,Missing
D12-1062,W06-1623,0,0.176951,"d as follows: X xheI,1i = 1, ∀e ∈ E (7) I∈I Our model also enforces reflexivity and transitivity constraints on the relations among event mentions as follows: yhei ej ,ri − yhej ei ,ˆri = 0, ∀ei ej = (ei , ej ) ∈ EE, i 6= j (8) yhei ej ,r1 i + yhej ek ,r2 i − yhei ek ,r3 i ≤ 1, ∀ei ej , ej ek , ei ek ∈ EE, i 6= j 6= k (9) The equality constraints in (8) encode reflexive property of event-event relations, where the relation rˆ denotes the inversion of the relation r. The set  of possible (r, rˆ) pairs is defined as follows: (¯b, a ¯), (¯ a, ¯b), (¯ o, o¯), (¯ n, n ¯ ) . Following the work of (Bramsen et al., 2006; Chambers and Jurafsky, 2008), we encode transitive closure of relations between event mentions with inequality constraints in (9), which states that if the pair (ei , ej ) has a certain relation r1 , and the pair (ej , ek ) has the relation r2 , then the relation r3 must be satisfied between ei and ek . Examples of such triple (r1 , r2 , r3 ) include (¯b, ¯b, ¯b) and (¯ a, a ¯, a ¯). Finally, to capture the interactions between our local pairwise classifiers we add the following constraints: xhei Ik ,1i + xhej Il ,1i − yhei ej ,¯bi ≤ 1, ∀ei Ik , ej Il ∈ EI, ∀ei ej ∈ EE, Ik precedes Il , i 6="
D12-1062,D08-1073,0,0.184215,"e1 | I1 e2 t2 • t3 • | | I2 t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the correct time of occurrence assoc"
D12-1062,W09-4303,0,0.0549401,"29 Prec. 20.86 CE−E Rec. 32.81 F1 25.03 Prec. 27.06 62.70 47.88 34.50 47.88 43.29 47.88 40.46 41.42 42.42 48.04 40.96 44.14 51.58 44.65 38.46 47.96 42.13 46.01 50.88 50.88 50.88 50.88 50.88 50.88 43.86 48.04 52.65 62.45 47.46 54.05 47.37 49.46 51.77 56.67 49.17 52.47 46.37 46.37 46.37 46.37 46.37 46.37 40.83 42.09 45.28 52.50 42.60 46.47 43.60 44.23 45.83 49.44 44.49 46.42 Overall Rec. F1 33.05 29.16 Table 2: Performance under various evaluation settings. All figures are averaged scores from 5-fold cross-validation experiments. periment, we re-trained the event coreference system described in Chen et al. (2009) on all articles in the ACE 2005 corpus, excluding the 20 articles used in our data set. The performance of these systems are shown in the fourth group of the results in Table 2. The results show that by using a learned event coreference system, we achieved the same improvement trends as with gold event coreference. However, we did not obtain significant improvement when comparing with global inference without event coreference information. This result shows that the performance of an event coreference system can have a significant impact on the overall performance. While this suggests that a"
D12-1062,W04-3250,0,0.0500438,"Missing"
D12-1062,P06-1095,0,0.107888,"stem. 1 • −∞ | t1 • e1 | I1 e2 t2 • t3 • | | I2 t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the cor"
D12-1062,C08-1108,0,0.0253528,"s (Verhagen et al., 2007; Verhagen et al., 2010). In these challenges, several temporal-related tasks were defined including the tasks of identifying the temporal relation between an event mention and a temporal expression in the same sentence, and recognizing temporal relations of pairs of event mentions in adjacent sentences. However, with several restrictions imposed to these tasks, the developed systems were not practical. Recently, there has been much work attempting to leverage Allen’s interval algebra of temporal relations to enforce global constraints on local predictions. The work of Tatu and Srikanth (2008) used global relational constraints to not only expand the training data but also identifies temporal inconsistencies to improve local classifiers. They used greedy search to select the most appropriate configuration of temporal relations among events and temporal expressions. For exact inferences, Bramsen et al. (2006), Chambers and Jurafsky (2008), Denis and Muller (2011), and Talukdar et al. (2012) formulated the temporal reasoning problem in an ILP. However, the inference models in their work were not a joint model involving multiple local classifiers but only one local classifier was invo"
D12-1062,S07-1014,0,0.118715,"esult shows that the performance of an event coreference system can have a significant impact on the overall performance. While this suggests that a better event coreference system could potentially help the task more, it also opens the question whether event coreference can be benefited from our local classifiers through the use of a joint inference framework. We would like to leave this for future investigations. 5.4 Previous Work-Related Experiments We also performed experiments using the same setting as in (Yoshikawa et al., 2009), which followed the guidelines of the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010), on our saturated data. Several assumptions were made to simplify the task. For example, only main events in adjacent sentences are considered when identifying event-event relations. See (Yoshikawa et al., 2009) for more details. We performed 5-fold cross validation without event coreference. Overall, the system achieved 29.99 F1 for the local classifiers and 34.69 when the global inference is used. These results are better than the baseline but underperform our full models where those simplification assumptions are 685 not imposed, as shown in Table 2, indicating the"
D12-1062,P09-1046,0,0.357113,"t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the correct time of occurrence associated with each event is"
D12-1062,N12-3008,1,0.89997,"and normalize two time intervals which are explicitly written, including January 1, 1993 → [199301-01 00:00:00, 1993-01-01 23:59:59] and February 7, 2000 → [2000-02-07 00:00:00, 2000-02-07 23:59:59]. Moreover, an explicit interval can also be formed by one or more separate explicit temporal expressions. In the example above, the connective term between relates the two expressions to form a single time interval: between January 1, 1993 and February 7, 2000 → [1993-01-01 00:00:00, 200002-07 23:59:59]. To extract explicit time intervals from text, we use the time interval extractor described in Zhao et al. (2012). Implicit intervals are time intervals that are not explicitly mentioned in the text. We observed that there are events that cannot be assigned to any precise time interval but are roughly known to occur in the past or in the future relative to the Document Creation Time (DCT) of the article. We introduce two implicit time intervals to represent the past and the future events as (−∞, t− DCT ] and [t+ , +∞), respectively. In addition, we also alDCT low an event mention to be assigned into the entire timeline, which is denoted by (−∞, +∞) if we cannot identify its time of occurrence. We also co"
D12-1062,S10-1010,0,\N,Missing
D12-1102,D11-1003,0,0.0859535,"Missing"
D12-1102,P07-1036,1,0.82544,"work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabeled dependency parsing). For each yi , let ci ∈ &lt; denote the weight associated with it. We denote the entire collection of weights by the vector c, forming the objective for this ILP. Not all assignments to these variables are valid. Without loss of generality, these con"
D12-1102,P06-2019,0,0.149821,"e ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function. The goal of inference is to select the maximum spanning tree of this weighted graph. 2.1 Problem Formulation In this work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabe"
D12-1102,W02-1001,0,0.105845,"l of inference is to select the maximum spanning tree of this weighted graph. 2.1 Problem Formulation In this work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabeled dependency parsing). For each yi , let ci ∈ &lt; denote the weight associated with it. We denote the entire collection of weights by the vector c, forming t"
D12-1102,P09-1039,0,0.120855,"to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that we are running inference for a large number of examples help us improve the time compl"
D12-1102,H05-1066,0,0.423866,"ral approximation schemes which can provide further speedup. We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that"
D12-1102,W05-0639,0,0.0237842,"as structured prediction problems, where the goal is to jointly assign values to many inference variables while accounting for possible dependencies among them. This decision task is a combinatorial optimization problem and can be solved using a dynamic programming approach if the structure permits. In general, the inference problem can be formulated and solved as integer linear programs (ILPs). Following (Roth and Yih, 2004) Integer linear programs have been used broadly in NLP. For example, (Riedel and Clarke, 2006) and (Martins et al., 2009) addressed the problem of dependency parsing and (Punyakanok et al., 2005; Punyakanok et al., 2008) dealt with semantic role labeling with this technique. In this section, we will use the ILP formulation of dependency parsing to introduce notation. The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al., 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function. The goal of inference is to select the maximum spanning tree of this weighted"
D12-1102,J08-2005,1,0.957658,"d theoretically guarantee the optimality of the solution. Furthermore, in some cases, even when the conditions are not satisfied, we can reuse previous solutions with high probability of being correct. Given the extensive use of integer linear programs for structured prediction in Natural Language Processing over the last few years, these ideas can be applied broadly to NLP problems. We instantiate our improved inference approaches in the structured prediction task of semantic role labeling, where we use an existing implementation and a previous trained model that is based on the approach of (Punyakanok et al., 2008). We merely modify the inference pro1114 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1114–1124, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics cess to show that we can realize the theoretical gains by making fewer calls to the underlying ILP solver. Algorithm Theorem 1 Theorem 2 Theorem 3 Speedup 2.44 2.18 2.50 Table 1: The speedup for semantic role labeling corresponding to the three theorems described in this paper. These theorems guarantee the optimality"
D12-1102,W06-1616,0,0.316377,"instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that we are running inference for a large number of examples help us"
D12-1102,W04-2401,1,0.80548,"We instantiate these schemes for the task of semantic role labeling (Section 4). Section 5 discusses related work and future research directions. 2 Motivation Many NLP tasks can be phrased as structured prediction problems, where the goal is to jointly assign values to many inference variables while accounting for possible dependencies among them. This decision task is a combinatorial optimization problem and can be solved using a dynamic programming approach if the structure permits. In general, the inference problem can be formulated and solved as integer linear programs (ILPs). Following (Roth and Yih, 2004) Integer linear programs have been used broadly in NLP. For example, (Riedel and Clarke, 2006) and (Martins et al., 2009) addressed the problem of dependency parsing and (Punyakanok et al., 2005; Punyakanok et al., 2008) dealt with semantic role labeling with this technique. In this section, we will use the ILP formulation of dependency parsing to introduce notation. The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al., 2005) to use ILP for inference. The key idea is to bui"
D12-1102,D10-1001,0,0.0817059,"Missing"
D12-1113,D08-1031,1,0.621295,"s (Vadas and Curran, 2008). 3) Let Q = {(mi , mj )}i6=j , be the queue of mention pairs approximately sorted by “easy-first” (Goldberg and Elhadad, 2010). 4) Let G be a partial clustering graph. 5) While Q is not empty - Extract a pair p = (mi , mj ) from Q. - Using the knowledge attributes of mi and mj as well as the structure of G, classify whether p is co-referent. - Update G with the classification decision. 6) Construct an end clustering from G. Figure 2: High-level system architecture. ing learning-based multi-sieve approach, we improve the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, 2 B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimental results in Sec. 6 and conclude with discussion in Sec. 7. We conclude the introduction by giving a highlevel overview of our system in Fig. 2. 2 Baseline System In this work, we are using the state-of-the-art system of (Bengtson and Roth, 2008), which relies on a pairwise scoring function pc to assign an ordered pair of mentions a probability that they are coreferential. It uses a rich set of features including: string edit distance, gender match, whether the mentions appear"
D12-1113,E06-1002,0,0.0545438,"Missing"
D12-1113,D07-1074,0,0.0168777,"Missing"
D12-1113,N07-1011,0,0.169697,"6.1 Experiments and Results do not work well on transcribed text. Therefore, we discard all the transcripts. Our criteria was simple. The ACE annotators have marked the named entities both in newswire and in the transcripts. We kept only those documents which contained named entities (according to manual ACE annotation) and at least 1/3 of the named entities started with a capital letter. After this pre-processing step, we were left with 275 out of the original 336 training documents, and 42 out of the 107 testing documents. For the experiments throughout this paper, following Culotta et al. (Culotta et al., 2007) and much other work, to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. However, we do not use the gold named entity types such as person/location/facility etc. available in the data. In all experiments we automatically split words and sentences, and annotate the text with part-of-speech tags, named entities and cross-link concepts from the text to Wikipedia using publicly available tools. Data We use the official ACE 2004 English training data (NIST, 2004). We started with the data split used in (Culotta et al., 20"
D12-1113,N10-1115,0,0.10124,"mbiguation to Wikipedia. We then extract attributes from the cross-linked Wikipedia pages (described in Sec. 3.1), assign these attributes to the document mentions (Sec. 3.2) and develop knowledge-rich compatibility metric between mentions (Sec. 3.3)2 . (2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009; Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise “co-reference” vs. “non-coreference” decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. In our running example, the decision of whether {[vessel]}m1 refers to {[Kursk]}m2 is made before the decision of whether {[vessel]}m1 refers to {Norwegian [ship]}m4 since decisions in the same sentence are believed to be easier than cross-sentence ones."
D12-1113,D09-1120,0,0.326245,"ailable system for context1234 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1234–1244, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics sensitive disambiguation to Wikipedia. We then extract attributes from the cross-linked Wikipedia pages (described in Sec. 3.1), assign these attributes to the document mentions (Sec. 3.2) and develop knowledge-rich compatibility metric between mentions (Sec. 3.3)2 . (2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009; Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise “co-reference” vs. “non-coreference” decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as addit"
D12-1113,N10-1061,0,0.241964,"Missing"
D12-1113,H05-1004,0,0.0989373,"luence the weight the features get (essentially leading to a different regularization), which in turn leads to slightly different results. 1242 Recall that the Best-Link algorithm applies transitive closure on the graph generated by thresholding the pairwise co-reference scoring function pc. The lower the threshold on the positive prediction, the lower is the precision and the higher is the recall. In Fig. 4 we compare the end clustering quality across a variety of thresholds and for various system flavors using three metrics: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005)13 . The purpose of this comparison is to see the impact of the knowledge and the prediction features on the final output and to see whether the performance gains are due to (mis-)tuning of one of the systems or are they consistent across a variety of thresholds. The end performance of the baseline system on our training/testing split peaks at around 78.39 MUC, 83.03 B 3 and 77.52 CEAF, which is higher (e.g. 3 B 3 F1 points) than the originally reported result on the entire dataset (which includes the transcripts). This is expected, since well-formed text is easier to process than transcripts."
D12-1113,P02-1014,0,0.330168,", whether the heads are synonyms in WordNet etc. The function pc is modeled using regularized averaged perceptron for a tuned number of training rounds, learning rate and margin. For the end system, we keep these parameters intact, our only modifications will be adding knowledge-rich features and adding intermediate classification sieves to the training and the inference, which we will discuss in the following sections. At inference time, given a document d and a pairwise co-reference scoring function pc, (Bengtson and Roth, 2008) generate a graph Gd according to the Best-Link decision model (Ng and Cardie, 2002) as follows. For each mention m in document d, let Bm be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmaxb∈Bm (pc(b, m)). We will add the edge (a, m) to Gd if pc(a, m) predicts the pair to be co-referent with a confidence exceeding a chosen threshold, then we take the transitive closure3 . The properties of the Best-Link inference are illustrated in Fig. 3. At this stage, we ask the reader to ignore the knowledge attributes at the bottom of the figure. Let us assume that the pairwise classifier labeled the mentions (m2 , m5 ) co-referent because"
D12-1113,N06-1025,0,0.582568,"Missing"
D12-1113,D10-1048,0,0.559929,"1234 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1234–1244, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics sensitive disambiguation to Wikipedia. We then extract attributes from the cross-linked Wikipedia pages (described in Sec. 3.1), assign these attributes to the document mentions (Sec. 3.2) and develop knowledge-rich compatibility metric between mentions (Sec. 3.3)2 . (2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009; Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise “co-reference” vs. “non-coreference” decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes avai"
D12-1113,P11-1082,0,0.581805,"lities assigned to the titles it appears in is less than 7, we mark the token compatible with these nationalities. This allows us to identify Ivan Lebedev as Russian and Ronen Ben-Zohar as Israeli, even though Wikipedia may not contain pages for these specific people. 6 (Nastase and Strube, 2008) analyze a set of categories S assigned to Wikipedia page p jointly, hence the same category expression can be interpreted differently, depending on S. 1237 3.2 Injecting Knowledge Attributes Once we have extracted the knowledge attributes of Wikipedia pages, we need to inject them into the mentions. (Rahman and Ng, 2011) used YAGO for similar purposes, but noticed that knowledge injection is often noisy. Therefore they used YAGO only for mention pairs where one mention was an NE of type PER/LOC/ORG and the other was a common noun. This implies that all MISC NEs were discarded, and all NE-NE pairs were discarded as well. We also note that (Rahman and Ng, 2011) reports low utility of FrameNet-based features. In fact, when incrementally added to other features in cluster-ranking model the FrameNet-based features sometimes led to performance drops. This observation has motivated our choice of high-precision lowre"
D12-1113,P11-1138,1,0.0637618,"l NE-NE pairs were discarded as well. We also note that (Rahman and Ng, 2011) reports low utility of FrameNet-based features. In fact, when incrementally added to other features in cluster-ranking model the FrameNet-based features sometimes led to performance drops. This observation has motivated our choice of high-precision lowrecall heuristic in Sec. 3.1 and will motivate us to add features conservatively when building attribute compatibility metric in Sec. 3.3. Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011)7 , a context-sensitive system for disambiguation to Wikipedia. Using context-sensitive disambiguation to Wikipedia as well as high-precision set of knowledge attributes allows us to inject the knowledge to more mention pairs when compared to (Rahman and Ng, 2011). Our exact heuristic for injecting knowledge attributes to mentions is as follows: Named Entities with Wikipedia Disambiguation If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge attributes from p. GLOW allows us to map “Ephraim Sneh” to http://en.wikipedia.org/wiki/Efraim Sneh and to assi"
D12-1113,P08-1039,0,0.0651762,"We describe our approach for leveraging possibly contradicting predictions in Sec. 5. (4) By adding word-knowledge features and us2 The extracted attributes and the related sources are available for public download http://cogcomp.cs.illinois.edu/Data/ Ace2004CorefWikiAttributes.zip reat 1235 Input: document d; mentions M = {m1 , . . . , mN } 1) For each mi ∈ M , assign it a Wikipedia page pi in a context-sensitive way (pi may be null). - If pi 6= null: extract knowledge attributes from pi and assign to m. - Else extract knowledge attributes directly from m via noun-phrase parsing techniques (Vadas and Curran, 2008). 3) Let Q = {(mi , mj )}i6=j , be the queue of mention pairs approximately sorted by “easy-first” (Goldberg and Elhadad, 2010). 4) Let G be a partial clustering graph. 5) While Q is not empty - Extract a pair p = (mi , mj ) from Q. - Using the knowledge attributes of mi and mj as well as the structure of G, classify whether p is co-referent. - Update G with the classification decision. 6) Construct an end clustering from G. Figure 2: High-level system architecture. ing learning-based multi-sieve approach, we improve the performance of the state-of-the-art system of (Bengtson and Roth, 2008) b"
D12-1113,M95-1005,0,0.0943027,"m. While the SFC features do not add new information, they influence the weight the features get (essentially leading to a different regularization), which in turn leads to slightly different results. 1242 Recall that the Best-Link algorithm applies transitive closure on the graph generated by thresholding the pairwise co-reference scoring function pc. The lower the threshold on the positive prediction, the lower is the precision and the higher is the recall. In Fig. 4 we compare the end clustering quality across a variety of thresholds and for various system flavors using three metrics: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005)13 . The purpose of this comparison is to see the impact of the knowledge and the prediction features on the final output and to see whether the performance gains are due to (mis-)tuning of one of the systems or are they consistent across a variety of thresholds. The end performance of the baseline system on our training/testing split peaks at around 78.39 MUC, 83.03 B 3 and 77.52 CEAF, which is higher (e.g. 3 B 3 F1 points) than the originally reported result on the entire dataset (which includes the transcripts). This is expected, since wel"
D12-1138,H05-1120,0,0.0260438,"major components (Brill and Moore, 2000). Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. Query spelling correction, a special form of the problem, has received much attention in recent years. Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. Research in this direction includes utilizing large web corpora and query log (Chen et al., 2007; Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005), employing large-scale n-gram models, training phrase-based error model from clickthrough data (Sun et al., 2010) and developing additional features (Gao et al., 2010). Query alteration/refinement is a very relevant topic to query spelling correction. The goal of query alteration/refinement is to modify the ineffective query so that it could . Researches on this track include query expansion (Xu and Croft, 1996; Qiu and Frei, 1993; Mitra et al., 1998), query contraction(Kumaran and Allan, 2008; Bendersky and Croft, 2008; Kumaran and Carvalho, 2009) and other types of query reformulations for"
D12-1138,P00-1037,0,0.0896922,"e weights trained by LS-SVM can be used to search for more candidate corrections. The improvement in recall at different levels over the noisy channel model demonstrates that our model is superior even when used in the two-stage approach.. 2 Related Work Spelling correction has a long history (Levenshtein, 1966). Traditional techniques were on small scale and depended on having a small trusted lexicons (Kukich, 1992). Later, statistical generative models were shown to be effective in spelling correction, where a source language model and an error model were identified as two major components (Brill and Moore, 2000). Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. Query spelling correction, a special form of the problem, has received much attention in recent years. Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. Research in this direction includes utilizing large web corpora and query log (Chen et al., 2007; Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005), employing large"
D12-1138,N10-1066,1,0.919448,"lling correction using a discriminative model without loss of efficiency. More specifically, we want 1) a learning process that is aware of the search phase and interacts with its result; 2) an efficient search algorithm that is able to incorporate the learned model and guide the search to the target spelling correction. In this paper, we propose a new discriminative model for query correction that maintains the advantage of a discriminative model in accommodating flexible combination of features and naturally incorporates an efficient search algorithm in learning and inference. Similarly to (Chang et al., 2010) we collapse a two stage process into a single discriminatively trained process, by considering the output of the first stage as an intermediate latent representation for the joint learning process. Specifically, we make use of the latent structural SVM (LS-SVM) (Yu and Joachims, 2009) formulation. We formulate the problem query spelling correction as a multi1512 class classification problem on structured inputs and outputs. The advantage of the structural SVM model is that it allows task specific, customizable solutions for the inference problem. This allows us to adapt the model to make it w"
D12-1138,D07-1019,0,0.0179427,"del and an error model were identified as two major components (Brill and Moore, 2000). Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. Query spelling correction, a special form of the problem, has received much attention in recent years. Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. Research in this direction includes utilizing large web corpora and query log (Chen et al., 2007; Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005), employing large-scale n-gram models, training phrase-based error model from clickthrough data (Sun et al., 2010) and developing additional features (Gao et al., 2010). Query alteration/refinement is a very relevant topic to query spelling correction. The goal of query alteration/refinement is to modify the ineffective query so that it could . Researches on this track include query expansion (Xu and Croft, 1996; Qiu and Frei, 1993; Mitra et al., 1998), query contraction(Kumaran and Allan, 2008; Bendersky and Croft, 2008; Kumaran and Carvalho"
D12-1138,W04-3238,0,0.0292115,"del were identified as two major components (Brill and Moore, 2000). Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. Query spelling correction, a special form of the problem, has received much attention in recent years. Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. Research in this direction includes utilizing large web corpora and query log (Chen et al., 2007; Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005), employing large-scale n-gram models, training phrase-based error model from clickthrough data (Sun et al., 2010) and developing additional features (Gao et al., 2010). Query alteration/refinement is a very relevant topic to query spelling correction. The goal of query alteration/refinement is to modify the ineffective query so that it could . Researches on this track include query expansion (Xu and Croft, 1996; Qiu and Frei, 1993; Mitra et al., 1998), query contraction(Kumaran and Allan, 2008; Bendersky and Croft, 2008; Kumaran and Carvalho, 2009) and other types of"
D12-1138,W06-2929,0,0.0312877,"correction, usually only one correction is valid for an input query. Therefore, we apply the 0-1 loss to our model:  ∆(c, cˆ) = 0 c = cˆ 1 c= 6 cˆ (10) Given this loss function, the loss augmented inference problem can be solved easily with an algorithm similar to Algorithm 1. This is done by initializing the loss to be 1 at the beginning of each search path. During the search procedure, we check if the loss decreases to 0 given the correction string so far. If this is the case, we decreases the score by 1 and add the path back to the priority queue. More advanced functions may also be used (Dreyer et al., 2006), which may lead to better training performance. We plan to further study different loss functions in our future work. The inference of the latent alignment variable can be solved with dynamic programming, as the number of possible alignments is limited given the query and the correction. 4 Features In the following discussions, we will describe how the features in our discriminative model are developed under the guidance of the monotonicity constraint. 4.1 Source Probability and Transformation Probability We know from empirical experience that the source probability and the transformation pro"
D12-1138,P11-1042,0,0.0181009,"any approaches have been proposed to perform discriminative training of the model (McCallum et al., 2000; Lafferty, 2001). However, these approaches mostly deal with a relatively small search space where the number of candidates at each step is limited (e.g. POS tagging). A typically used search algorithm is dynamic programming. In spelling correction, however, the search space is much bigger and the existing approaches featuring dynamic programming are difficult to be applied. Structural learning and latent structural learning has been studied a lot in NLP in recent years(Chang et al., 2010; Dyer et al., 2011), and has been 1513 shown to be useful in a range of NLP applications from Textual Entailment, Paraphrasing and Transliteration (Chang et al., 2010) to sentiment analysis (Yessenalina et al., 2010). Work has also been done on integrating discriminative learning in search. Freitag and Khadivi used a perceptron algorithm to train for sequence alignment problem. A beam search algorithm was utilized in the search (Freitag and Khadivi, 2007). Daume et al. proposed the Searn framework for search based structural prediction (Daume et al., 2009). Our model differs from the Searn framework in that it l"
D12-1138,D07-1025,0,0.0300977,"g dynamic programming are difficult to be applied. Structural learning and latent structural learning has been studied a lot in NLP in recent years(Chang et al., 2010; Dyer et al., 2011), and has been 1513 shown to be useful in a range of NLP applications from Textual Entailment, Paraphrasing and Transliteration (Chang et al., 2010) to sentiment analysis (Yessenalina et al., 2010). Work has also been done on integrating discriminative learning in search. Freitag and Khadivi used a perceptron algorithm to train for sequence alignment problem. A beam search algorithm was utilized in the search (Freitag and Khadivi, 2007). Daume et al. proposed the Searn framework for search based structural prediction (Daume et al., 2009). Our model differs from the Searn framework in that it learns to make global decisions rather than accumulating local decisions. The global decision was made possible by an efficient search algorithm. Query spelling correction also shares many similarities with statistical machine translation (SMT). Sun et al. (2010) has formulated the problem within an SMT framework. However, SMT usually involves more complex alignments, while in query spelling correction search is the more challenging part"
D12-1138,C10-1041,0,0.0687803,"mind. The probability is usually modeled by a language model estimated from a sizable corpus. The transformation probability p(q|c) measures how likely it is that q is the output given that c has been formed by the user. This probability can be either heuristic-based (edit distance) or learned from samples of well aligned corrections. One problem with the noisy channel model is that there is no weighting for the two kinds of probabilities, and since they are estimated from different sources, there are usually issues regarding their scale and comparability, resulting in suboptimal performance (Gao et al., 2010). Another limitation of this generative model is that it is not able to take advantage of additional useful features. 1511 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1511–1521, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics A discriminative model may solve these problems by adding the flexibility of using features and applying weights. But training such a model is not easy. The difficulty is that the output space of query correction is enormous, as the can"
D12-1138,C90-2036,0,0.524797,"Missing"
D12-1138,P10-1028,0,0.0635535,"e spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. Query spelling correction, a special form of the problem, has received much attention in recent years. Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. Research in this direction includes utilizing large web corpora and query log (Chen et al., 2007; Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005), employing large-scale n-gram models, training phrase-based error model from clickthrough data (Sun et al., 2010) and developing additional features (Gao et al., 2010). Query alteration/refinement is a very relevant topic to query spelling correction. The goal of query alteration/refinement is to modify the ineffective query so that it could . Researches on this track include query expansion (Xu and Croft, 1996; Qiu and Frei, 1993; Mitra et al., 1998), query contraction(Kumaran and Allan, 2008; Bendersky and Croft, 2008; Kumaran and Carvalho, 2009) and other types of query reformulations for bridging the vocabulary gap (Wang and Zhai, 2008). (Guo et al., 2008) proposed a unified model to perform a broad"
D12-1138,D10-1102,0,0.0255613,"h space where the number of candidates at each step is limited (e.g. POS tagging). A typically used search algorithm is dynamic programming. In spelling correction, however, the search space is much bigger and the existing approaches featuring dynamic programming are difficult to be applied. Structural learning and latent structural learning has been studied a lot in NLP in recent years(Chang et al., 2010; Dyer et al., 2011), and has been 1513 shown to be useful in a range of NLP applications from Textual Entailment, Paraphrasing and Transliteration (Chang et al., 2010) to sentiment analysis (Yessenalina et al., 2010). Work has also been done on integrating discriminative learning in search. Freitag and Khadivi used a perceptron algorithm to train for sequence alignment problem. A beam search algorithm was utilized in the search (Freitag and Khadivi, 2007). Daume et al. proposed the Searn framework for search based structural prediction (Daume et al., 2009). Our model differs from the Searn framework in that it learns to make global decisions rather than accumulating local decisions. The global decision was made possible by an efficient search algorithm. Query spelling correction also shares many similarit"
D13-1057,P12-1041,0,0.0241582,"Yu and Joachims (2009) formulates coreference with latent spanning trees. However, their approach has no directionality between mentions, whereas our latent structure captures the natural left-to-right ordering of mentions. In our experiments (Sec. 5), we show that our technique vastly outperforms both the spanning tree and the correlational clustering techniques. We also compare with (Fernandes et al., 2012) and the publicly available Stanford coreference system (Raghunathan et al., 2010; Lee et al., 2011), a state-of-theart rule-based system. Finally, some research (Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2011a) has tried to integrate world knowledge from webbased statistics or knowledge bases into a coreference system. World knowledge is potentially useful for resolving coreference and can be injected into our system in a straightforward way via the constraints framework. We will show an example of incorporating our system with name-entity and WordNet-based similarity metric (Q. Do, 2009) in Sec. 5. Including massive amount of information from knowledge resources is not the focus of this paper and may distort the comparison with other relevant models but our results indicate t"
D13-1057,D08-1031,1,0.798393,"ls proposed in the literature. 1 Introduction Coreference resolution is a challenging task, that involves identification and clustering of noun phrases mentions that refer to the same real-world entity. Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. The most popular of these frameworks is the pairwise mention model (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions and previously identified entities (that is, between mentions and clusters). While such models are potentially more expressive, they are largely based on heuristics to achieve computational tractability. This paper focuses on a novel and principled machine learning framework that pushes t"
D13-1057,W12-4504,0,0.0279359,"Missing"
D13-1057,P06-2019,0,0.00973685,"e resources is not the focus of this paper and may distort the comparison with other relevant models but our results indicate that this is doable in our model, and may provide significant improvements. 3 Latent Left-Linking Model with Constraints In this section, we describe our Constrained Latent Left-Linking Model (CL3 M). CL3 M is inspired by a few ideas from the literature: (a) the popular BestLeft-Link inference approach to coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008), and (b) the injection of domain knowledge-based constraints for structured prediction (Roth and Yih, 2004; Clarke and Lapata, 2006; Chang et al., 2012b; Ganchev et al., 2010; Koo et al., 2010; Pascal and Baldridge, 2009). We first introduce the notion of a pairwise mention-scorer, then introduce our Left-Linking Model (L3 M), and finally describe how to inject constraints into our model. Let d be a document with md mentions. Mentions are denoted solely using their indices, ranging from 1 to md . A coreference clustering C for document 603 d is a collection of disjoint sets partitioning the set {1, . . . , md }. We represent C as a binary function with C(i, j) = 1 if mentions i and j are coreferent, otherwise C(i, j) = 0."
D13-1057,N07-1011,0,0.189366,"thm on ACE and when evaluated on the gold mentions of Ontonotes. We show that CL3 M performs particularly well on clusters containing named entity mentions, which are more important for many information extraction applications. In the rest of this section, after describing our experimental setting, we provide careful analysis of our algorithms and compare them to competitive coreference approaches in the literature. 5.1 Experimental Setup Datasets: ACE 2004 contains 443 documents — we used a standard split of these documents into 268 training, 68 development, and 106 testing documents used by Culotta et al. (2007) and Bengtson and Roth (2008). OntoNotes-5.0 dataset, released for the CoNLL 2012 Shared Task (Pradhan et al., 2012), is by far the largest annotated corpus on coreference. It contains 3,145 annotated documents drawn from a wide variety of sources — newswire, bible, broadcast transcripts, magazine articles, and web blogs. We report results on both development set and test set. To test on the development set, we further split the training data into training and development sets. Classifier details: For each of the pairwise approaches, we assume the pairwise score is given by w·φ(·, ·)+t where φ"
D13-1057,N07-1030,0,0.0389526,"ike previous best-link techniques, learning in our case is performed jointly with decoding — we present a novel latent structural SVM approach, optimized using a fast stochastic gradient-based technique. Furthermore, we present a probabilistic generalization of L3 M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. We augment this model with a temperature-like parameter (Samdani et al., 2012) to provide additional flexibility. CL3 M augments L3 M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). This capability is very desirable as shown by the success of the rule-based deterministic approach of Raghunathan et al. (2010) in the CoNLL shared task 2011 (Pradhan et al., 2011). In L3 M, domain-specific constraints are incorporated into learning and inference in a straightforward way. CL3 M scores a mention’s contribution to its cluster by combining the corresponding score 601 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics of the underlyin"
D13-1057,D08-1069,0,0.108073,"examples. Similar approaches to training and, additionally, decoupling the training stage from the clustering stage were used by other systems. In this paper, we formalize the learning problem of the best-left-link model as a structured 602 prediction problem and analyze our system with detailed experiments. Furthermore, we generalize this approach by considering multiple pairwise left-links instead of just the best link, efficiently capturing the notion of a mention-to-cluster link. Many techniques in the coreference literature break away from the mention pair-based, best-leftlink paradigm. Denis and Baldridge (2008) and Ng (2005) learn a local ranker to rank the mention pairs based on their compatibility. While these approaches achieve decent empirical performance, it is unclear why these are the right ways to train the model. Some techniques consider a more expressive model by using features defined over mentioncluster or cluster-cluster (Rahman and Ng, 2011c; Stoyanov and Eisner, 2012; Haghighi and Klein, 2010). For these models, the inference and learning algorithms are usually complicated. Very recently, Durrett et al. (2013) propose a probabilistic model which enforces structural agreement constrain"
D13-1057,P13-1012,0,0.0277076,"rature break away from the mention pair-based, best-leftlink paradigm. Denis and Baldridge (2008) and Ng (2005) learn a local ranker to rank the mention pairs based on their compatibility. While these approaches achieve decent empirical performance, it is unclear why these are the right ways to train the model. Some techniques consider a more expressive model by using features defined over mentioncluster or cluster-cluster (Rahman and Ng, 2011c; Stoyanov and Eisner, 2012; Haghighi and Klein, 2010). For these models, the inference and learning algorithms are usually complicated. Very recently, Durrett et al. (2013) propose a probabilistic model which enforces structural agreement constraints between specified properties of mention cluster when using a mention-pair model. This approach is very related to the probabilistic extension of our method as both models attempt to leverage entity-level information from mention-pair features. However, our approach is simpler because it directly considers the probabilities of multiple links. Furthermore, while their model performs only slightly better than the Stanford rule-based system (Lee et al., 2011), we significantly outperform this system. Most importantly, o"
D13-1057,W12-4502,0,0.0691342,"Missing"
D13-1057,N10-1112,0,0.051977,"t al., 2012). Consequently, as γ → 0, P r[c ⊙ i; d, w] in Eq. 8 approaches a Kronecker delta function centered on the cluster containing the max-scoring mention, thus reducing to the best-link case of L3 M. Thus, PL3 M, when tuning the value of γ, is a strictly more general model than L3 M. Learning with PL3 M We use a likelihood-based approach to learning with PL3 M, and first compute the probability P r[C; d, w] of generating a clustering C, given w. We then learn w by minimizing the regularized negative log-likelihood of the data, augmenting the partition function with a loss-based margin (Gimpel and Smith, 2010). We omit the details of likelihood computation due to lack of space. With PL3 M, we again follow a stochastic gradient descent technique instead of CCCP for the same reasons mentioned in Sec. 3.3. The stochastic gradient (subgradient when γ = 0) w.r.t. mention i in document d is given by ∇LL(w)id ∝ X 0≤j&lt;i X pj φ(i, j) − p′j φ(i, j) + λw, 0≤j&lt;i where pj and p′j , j = 0, . . . , i − 1, are non-negative weights that sum to one and are given by 1 pj p′j = = e γ (w·φ(i,j)+δ(Cd ,i,j)) P 1 0≤k&lt;i e γ (w·φ(i,k)+δ(Cd ,i,k)) and Cd (i, j)Zi (d, w, γ) P r[j ← i; d, w] . Zi (Cd ; d, w, γ) Interestingly,"
D13-1057,N10-1061,0,0.407886,"the same real-world entity. Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. The most popular of these frameworks is the pairwise mention model (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions and previously identified entities (that is, between mentions and clusters). While such models are potentially more expressive, they are largely based on heuristics to achieve computational tractability. This paper focuses on a novel and principled machine learning framework that pushes the state-ofthe-art while operating at a mention-pair granularity. We present two models — the Latent Left-Linking Model (L3 M), and a version of that is augmented with domain"
D13-1057,P11-1115,0,0.0109421,"score on the test set and is competitive with the best system participated in the CoNLL shared task 2012. Performance on named entities: The coreference annotation in Ontonotes 5.0 includes various types of mentions. However, not all mention types are equally interesting. In particular, clusters which contain at least one proper name or a named entity mention are more important for information extraction tasks like Wikification (Mihalcea and Csomai, 2007; Ratinov et al., 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), and entity linking and knowledge based population (Ji and Grishman, 2011). Inspired by this, we compare our system to the best systems in the CoNLL shared task of 2011 (Stanford (Lee et al., 2011)) and 2012 (Fernandes (Fernandes et al., 2012)) on the following specific tasks on Ontonotes-5.0. • ENT-C: Evaluate the system on clusters that contain at least one proper name mention. We generate the gold annotation and system outputs by using the gold and predicted name entity tag annotations provided by the CoNLL shard task 2012. That is, if a cluster does not include any name entity mention, then it will be removed from the final clustering. • PER-C: As in the constru"
D13-1057,D10-1125,0,0.0117174,"rison with other relevant models but our results indicate that this is doable in our model, and may provide significant improvements. 3 Latent Left-Linking Model with Constraints In this section, we describe our Constrained Latent Left-Linking Model (CL3 M). CL3 M is inspired by a few ideas from the literature: (a) the popular BestLeft-Link inference approach to coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008), and (b) the injection of domain knowledge-based constraints for structured prediction (Roth and Yih, 2004; Clarke and Lapata, 2006; Chang et al., 2012b; Ganchev et al., 2010; Koo et al., 2010; Pascal and Baldridge, 2009). We first introduce the notion of a pairwise mention-scorer, then introduce our Left-Linking Model (L3 M), and finally describe how to inject constraints into our model. Let d be a document with md mentions. Mentions are denoted solely using their indices, ranging from 1 to md . A coreference clustering C for document 603 d is a collection of disjoint sets partitioning the set {1, . . . , md }. We represent C as a binary function with C(i, j) = 1 if mentions i and j are coreferent, otherwise C(i, j) = 0. Let s(C; w, d) be the score of a given clustering C for a gi"
D13-1057,W11-1902,0,0.265959,"arning algorithms are usually complicated. Very recently, Durrett et al. (2013) propose a probabilistic model which enforces structural agreement constraints between specified properties of mention cluster when using a mention-pair model. This approach is very related to the probabilistic extension of our method as both models attempt to leverage entity-level information from mention-pair features. However, our approach is simpler because it directly considers the probabilities of multiple links. Furthermore, while their model performs only slightly better than the Stanford rule-based system (Lee et al., 2011), we significantly outperform this system. Most importantly, our model obtains state-of-the-art performance on OntoNotes-5.0 while still operating at the mention-pair granularity. We believe that this is due to our novel and principled structured prediction framework which results in accurate (and efficient) training. Several structured prediction techniques have been applied to coreference resolution in the machine learning literature. For example, McCallum and Wellner (2003) and Finley and Joachims (2005) model coreference as a correlational clustering problem (Bansal et al., 2002) on a comp"
D13-1057,J13-4004,0,0.0261873,"cribed in the paper can be treated as features, due to their high precision, treating them as hard constraints (set ρ to a high value) is a safe and direct way to inject human knowledge into the learning model. Moreover, our framework allows a constraint to use information from previous decisions (such as “cannot-link” constraints). Treating such constraints as features will complicate the learning model. 5.2 Performance of the End-to-End System We compare our system with the top systems reported in the CoNLL shared task 2012 as well as with the Stanford’s publicly released rule-based system (Lee et al., 2013; Lee et al., 2011), which won the CoNLL 2011 Shared Task (Pradhan et al., 2011). Note that all the systems use the same annotations (e.g., gender prediction, part-of-speech tags, name entity tags) provided by the shared task organizers. However, each system implements its own mention detector and pipelines the identified mentions into the coreference clustering component. Moreover, different systems use a different set of features. In order to partially control for errors on mention detection and better evaluate the clustering component in our coreference system, we will also present results"
D13-1057,H05-1004,0,0.105274,"L3 M, we do stochastic gradient descent with 5 passes over the data. Empirically, we observe that this is enough to generate a stable model. For PL3 M (Sec. 4), we tune the value of γ using the development set picking the best γ from {0.0, 0.2, . . . , 1.0}. Recall that when γ = 0, PL3 M is the same as L3 M. We refer to L3 M and PL3 M with incorporating constraints during inference as CL3 M and CPL3 M (Sec. 3.4), respectively. Metrics: We compare the systems using three popular metrics for coreference — MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). Following, the CoNLL shared tasks (Pradhan et al., 2012), we use the average F1 scores of these three metrics as the main metric of comparison. Features: We build our system on the publicly available Illinois-Coref system1 primarily because it contains a rich set of features presented in Bengtson and Roth (2008) and Chang et al. (2012a) (the latter adds features for pronominal anaphora resolution). We also compare with the Best-Left-Link approach described by Bengtson and Roth (2008). Constraints: We consider the following constraints in CL3 M and CPL3 M. • SameSpan: two mentions must be lin"
D13-1057,W12-4511,0,0.0209694,"Missing"
D13-1057,P02-1014,0,0.758914,"tured prediction models proposed in the literature. 1 Introduction Coreference resolution is a challenging task, that involves identification and clustering of noun phrases mentions that refer to the same real-world entity. Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. The most popular of these frameworks is the pairwise mention model (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions and previously identified entities (that is, between mentions and clusters). While such models are potentially more expressive, they are largely based on heuristics to achieve computational tractability. This paper focuses on a novel and principled machine learni"
D13-1057,W11-1901,0,0.168456,"based technique. Furthermore, we present a probabilistic generalization of L3 M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. We augment this model with a temperature-like parameter (Samdani et al., 2012) to provide additional flexibility. CL3 M augments L3 M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). This capability is very desirable as shown by the success of the rule-based deterministic approach of Raghunathan et al. (2010) in the CoNLL shared task 2011 (Pradhan et al., 2011). In L3 M, domain-specific constraints are incorporated into learning and inference in a straightforward way. CL3 M scores a mention’s contribution to its cluster by combining the corresponding score 601 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics of the underlying L3 M model with that from a set of constraints. Most importantly, in our experiments on benchmark coreference datasets, we show that CL3 M, with just five constraints, compares fav"
D13-1057,W12-4501,0,0.613088,"on for Computational Linguistics of the underlying L3 M model with that from a set of constraints. Most importantly, in our experiments on benchmark coreference datasets, we show that CL3 M, with just five constraints, compares favorably with other, more complicated, state-of-the-art algorithms on a variety of evaluation metrics. Overall, the main contribution of this paper is a principled machine learning model operating at mention-pair granularity, using easy to implement constraint-augmented inference and learning, that yields competitive results on coreference resolution on Ontonotes-5.0 (Pradhan et al., 2012) and ACE 2004 (NIST, 2004). 2 Related Work The idea of Latent Left-linking Model (L3 M) is inspired by a popular inference approach to coreference which we call the Best-Left-Link approach (Ng and Cardie, 2002; Bengtson and Roth, 2008). In the best-left-link strategy, each mention i is connected to the best antecedent mention j with j &lt; i (i.e. a mention occurring to the left of i, assuming a leftto-right reading order), thereby creating a left-link. The “best” antecedent mention is the one with the highest pairwise score, wij ; furthermore, if wij is below some threshold, say 0, then i is not"
D13-1057,D10-1048,0,0.201316,"SVM approach, optimized using a fast stochastic gradient-based technique. Furthermore, we present a probabilistic generalization of L3 M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. We augment this model with a temperature-like parameter (Samdani et al., 2012) to provide additional flexibility. CL3 M augments L3 M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). This capability is very desirable as shown by the success of the rule-based deterministic approach of Raghunathan et al. (2010) in the CoNLL shared task 2011 (Pradhan et al., 2011). In L3 M, domain-specific constraints are incorporated into learning and inference in a straightforward way. CL3 M scores a mention’s contribution to its cluster by combining the corresponding score 601 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics of the underlying L3 M model with that from a set of constraints. Most importantly, in our experiments on benchmark coreference datasets, we show"
D13-1057,P11-1082,0,0.299066,"y. Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. The most popular of these frameworks is the pairwise mention model (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions and previously identified entities (that is, between mentions and clusters). While such models are potentially more expressive, they are largely based on heuristics to achieve computational tractability. This paper focuses on a novel and principled machine learning framework that pushes the state-ofthe-art while operating at a mention-pair granularity. We present two models — the Latent Left-Linking Model (L3 M), and a version of that is augmented with domain knowledge-based cons"
D13-1057,D12-1113,1,0.690954,"her approach proposed by Yu and Joachims (2009) formulates coreference with latent spanning trees. However, their approach has no directionality between mentions, whereas our latent structure captures the natural left-to-right ordering of mentions. In our experiments (Sec. 5), we show that our technique vastly outperforms both the spanning tree and the correlational clustering techniques. We also compare with (Fernandes et al., 2012) and the publicly available Stanford coreference system (Raghunathan et al., 2010; Lee et al., 2011), a state-of-theart rule-based system. Finally, some research (Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2011a) has tried to integrate world knowledge from webbased statistics or knowledge bases into a coreference system. World knowledge is potentially useful for resolving coreference and can be injected into our system in a straightforward way via the constraints framework. We will show an example of incorporating our system with name-entity and WordNet-based similarity metric (Q. Do, 2009) in Sec. 5. Including massive amount of information from knowledge resources is not the focus of this paper and may distort the comparison with other relevant models bu"
D13-1057,P11-1138,1,0.130081,"but this difference disappears when we use our system with constraints, CL3 M. Although our system is much simple, it achieves the best B 3 score on the test set and is competitive with the best system participated in the CoNLL shared task 2012. Performance on named entities: The coreference annotation in Ontonotes 5.0 includes various types of mentions. However, not all mention types are equally interesting. In particular, clusters which contain at least one proper name or a named entity mention are more important for information extraction tasks like Wikification (Mihalcea and Csomai, 2007; Ratinov et al., 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), and entity linking and knowledge based population (Ji and Grishman, 2011). Inspired by this, we compare our system to the best systems in the CoNLL shared task of 2011 (Stanford (Lee et al., 2011)) and 2012 (Fernandes (Fernandes et al., 2012)) on the following specific tasks on Ontonotes-5.0. • ENT-C: Evaluate the system on clusters that contain at least one proper name mention. We generate the gold annotation and system outputs by using the gold and predicted name entity tag annotations provided by the CoNLL shard task 2012. T"
D13-1057,W04-2401,1,0.811506,"2008). However, unlike previous best-link techniques, learning in our case is performed jointly with decoding — we present a novel latent structural SVM approach, optimized using a fast stochastic gradient-based technique. Furthermore, we present a probabilistic generalization of L3 M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. We augment this model with a temperature-like parameter (Samdani et al., 2012) to provide additional flexibility. CL3 M augments L3 M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). This capability is very desirable as shown by the success of the rule-based deterministic approach of Raghunathan et al. (2010) in the CoNLL shared task 2011 (Pradhan et al., 2011). In L3 M, domain-specific constraints are incorporated into learning and inference in a straightforward way. CL3 M scores a mention’s contribution to its cluster by combining the corresponding score 601 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational"
D13-1057,N12-1087,1,0.94801,"ring mention to its left, much like the existing best-left-link inference models (Ng and Cardie, 2002; Bengtson and Roth, 2008). However, unlike previous best-link techniques, learning in our case is performed jointly with decoding — we present a novel latent structural SVM approach, optimized using a fast stochastic gradient-based technique. Furthermore, we present a probabilistic generalization of L3 M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. We augment this model with a temperature-like parameter (Samdani et al., 2012) to provide additional flexibility. CL3 M augments L3 M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). This capability is very desirable as shown by the success of the rule-based deterministic approach of Raghunathan et al. (2010) in the CoNLL shared task 2011 (Pradhan et al., 2011). In L3 M, domain-specific constraints are incorporated into learning and inference in a straightforward way. CL3 M scores a mention’s contribution to its cluster by combining the corresponding score 601 Proceedings of the 2013 Conference on Empirical Methods in Natural L"
D13-1057,J01-4004,0,0.365572,"well as some structured prediction models proposed in the literature. 1 Introduction Coreference resolution is a challenging task, that involves identification and clustering of noun phrases mentions that refer to the same real-world entity. Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. The most popular of these frameworks is the pairwise mention model (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions and previously identified entities (that is, between mentions and clusters). While such models are potentially more expressive, they are largely based on heuristics to achieve computational tractability. This paper focuses on a novel and prin"
D13-1057,C12-1154,0,0.503875,"rwise left-links instead of just the best link, efficiently capturing the notion of a mention-to-cluster link. Many techniques in the coreference literature break away from the mention pair-based, best-leftlink paradigm. Denis and Baldridge (2008) and Ng (2005) learn a local ranker to rank the mention pairs based on their compatibility. While these approaches achieve decent empirical performance, it is unclear why these are the right ways to train the model. Some techniques consider a more expressive model by using features defined over mentioncluster or cluster-cluster (Rahman and Ng, 2011c; Stoyanov and Eisner, 2012; Haghighi and Klein, 2010). For these models, the inference and learning algorithms are usually complicated. Very recently, Durrett et al. (2013) propose a probabilistic model which enforces structural agreement constraints between specified properties of mention cluster when using a mention-pair model. This approach is very related to the probabilistic extension of our method as both models attempt to leverage entity-level information from mention-pair features. However, our approach is simpler because it directly considers the probabilities of multiple links. Furthermore, while their model"
D13-1057,P09-1074,0,0.0291876,"dent mention is the one with the highest pairwise score, wij ; furthermore, if wij is below some threshold, say 0, then i is not connected to any antecedent mention. The final clustering is a transitive closure of these “best” links. The intuition behind best-left-link strategy is based on how humans read and decipher coreference links – they mostly rely on information to the left of the mention when deciding whether to add it to a previously constructed cluster or not. This strategy has been successful and commonly used in coreference resolution (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). However, most works have developed ad-hoc approaches to implement this idea. For instance, Bengtson and Roth (2008) train a model w on binary training data generated by taking for each mention, the closest antecedent coreferent mention as a positive example, and all the other mentions as negative examples. Similar approaches to training and, additionally, decoupling the training stage from the clustering stage were used by other systems. In this paper, we formalize the learning problem of the best-left-link model as a structured 602 prediction problem and analyze our system with detailed exp"
D13-1057,M95-1005,0,0.668001,"but use a tuned value (tuned on a development set) during testing. For learning with L3 M, we do stochastic gradient descent with 5 passes over the data. Empirically, we observe that this is enough to generate a stable model. For PL3 M (Sec. 4), we tune the value of γ using the development set picking the best γ from {0.0, 0.2, . . . , 1.0}. Recall that when γ = 0, PL3 M is the same as L3 M. We refer to L3 M and PL3 M with incorporating constraints during inference as CL3 M and CPL3 M (Sec. 3.4), respectively. Metrics: We compare the systems using three popular metrics for coreference — MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). Following, the CoNLL shared tasks (Pradhan et al., 2012), we use the average F1 scores of these three metrics as the main metric of comparison. Features: We build our system on the publicly available Illinois-Coref system1 primarily because it contains a rich set of features presented in Bengtson and Roth (2008) and Chang et al. (2012a) (the latter adds features for pronominal anaphora resolution). We also compare with the Best-Left-Link approach described by Bengtson and Roth (2008). Constraints: We consider the fol"
D13-1057,W12-4513,1,\N,Missing
D13-1057,W11-1904,1,\N,Missing
D13-1074,P06-1032,0,0.122418,"Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence"
D13-1074,D07-1001,0,0.0255979,"nnotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), and (2) a model that join"
D13-1074,D12-1052,0,0.55997,"tatistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts with the noun number error: a correction system"
D13-1074,W13-1703,0,0.196576,"Missing"
D13-1074,W11-2838,0,0.488,"e, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault a"
D13-1074,W12-2006,0,0.262607,"nsiders interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010"
D13-1074,C08-1022,0,0.23815,"Missing"
D13-1074,I08-1059,0,0.0301249,"011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentenc"
D13-1074,N10-1019,0,0.206966,"al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues"
D13-1074,P03-2026,0,0.294222,"atical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. B"
D13-1074,P08-1021,0,0.0200433,"ction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts wi"
D13-1074,P11-2089,1,0.912829,"Missing"
D13-1074,de-marneffe-etal-2006-generating,0,0.0113291,"Missing"
D13-1074,D11-1022,0,0.0261226,"ve data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)),"
D13-1074,W13-3601,0,0.246487,"Missing"
D13-1074,P11-1094,0,0.0522924,"not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University"
D13-1074,J08-2005,1,0.732485,"ry recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, ("
D13-1074,W04-2401,1,0.908343,": (1) Data: until very recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear progra"
D13-1074,W10-1004,1,0.869357,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,D10-1094,1,0.833405,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,N10-1018,1,0.898646,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,P11-1093,1,0.877471,"the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Na¨ıve Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 2 ∅ denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. Example “They believe that such situation must be avoided.” “Nevertheless , electric cars is still regarded as a great trial innovation.” “Every students have appointments with the head of the department.” Predictions made by the Illinois system such situation → such a situations cars is → car are No change Table 3: Examples of predictions of the Illinois system that combines independently-trained models. The words that are selected as inpu"
D13-1074,C08-1109,0,0.0160111,"riff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleadi"
D13-1074,P13-1143,0,0.170403,"e input from participants. The number of mistakes in the revised test data is slightly higher. contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett"
D13-1074,W12-2032,1,\N,Missing
D13-1074,W13-3602,1,\N,Missing
D13-1074,W11-2843,1,\N,Missing
D13-1184,E06-1002,0,0.549288,"n par with the top tier systems despite not being trained on the given data. The LCC system used sophisticated clustering algorithms trained on the TAC development set (Monahan et al., 2011). The second-ranked MS-MLI system relied on topic modeling, external web search engine logs as well as training on the development data (Cucerzan, 2011). This shows the robustness of our methods as well as the general importance of understanding textual relations in the task of Entity Linking and Wikification. 6 Related Work and Discussion Earlier works on Wikification formulated the task as a WSD problem (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007) and focused primarily on training a model using local context. Later, various global statistical approaches were proposed to emphasize different coherence measures between the titles of the disambiguated mentions in the same doc1795 ument (Cucerzan, 2007; Milne and Witten, 2008; Ratinov et al., 2011). Built on top of the statistical models, our work focuses on leveraging deeper understanding of the text to more effectively and accurately utilize existing knowledge. We have demonstrated that, by incorporating textual relations and semantic knowledge as linguistic co"
D13-1184,P11-1056,1,0.77062,"ikipedia dump we used. These examples further illustrate that, along with understanding the relation expressed in the text, we 1788 need to access background knowledge sources and to deal with variability in surface representation across the text, Wikipedia, and knowledge, in order to reliably address the Wikification problem. In this paper we focus on understanding those natural language constructs that will allow eliminating these “obvious” (to a human reader) mistakes from Wikification. In particular, we focus on resolving coreference and a collection of local syntacticosemantic relations (Chan and Roth, 2011); better understanding the relational structure of the text allows us to generate title candidates more accurately given the text, rank these candidates better and determine when a mention in text has no corresponding title in Wikipedia and should be mapped to NIL, a key problem in Wikification. Moreover, it allows us to access external knowledge based resources more effectively in order to support these decisions. We incorporate the outcome of our relational analysis, along with the associated features extracted from external sources and the “standard” wikification statistical features, into"
D13-1184,D07-1074,0,0.988502,"blem, and show that the ability to identify relations in text helps both candidate generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task. 1 Introduction Wikification (D2W), the task of identifying concepts and entities in text and disambiguating them into their corresponding Wikipedia page, is an important step toward supporting deeper textual understanding, by augmenting the ability to ground text in existing knowledge and facilitating knowledge expansion. D2W has been studied extensively recently (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010; Ratinov et al., 2011) and has already found broad applications in NLP, Information Extraction, and Knowledge Acquisition from text, from coreference resolution (Ratinov and Roth, 2012) to entity linking and knowledge population (Ellis et al., 2011; Ji et al., 2010; Cucerzan, 2011). Given a document D containing a set of concept and entity mentions M ( referred to later as surface), the goal of Wikification is to find the most accurate mapping from mentions to Wikipedia titles T ; this mapping needs to take into"
D13-1184,D12-1113,1,0.652026,"on (D2W), the task of identifying concepts and entities in text and disambiguating them into their corresponding Wikipedia page, is an important step toward supporting deeper textual understanding, by augmenting the ability to ground text in existing knowledge and facilitating knowledge expansion. D2W has been studied extensively recently (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010; Ratinov et al., 2011) and has already found broad applications in NLP, Information Extraction, and Knowledge Acquisition from text, from coreference resolution (Ratinov and Roth, 2012) to entity linking and knowledge population (Ellis et al., 2011; Ji et al., 2010; Cucerzan, 2011). Given a document D containing a set of concept and entity mentions M ( referred to later as surface), the goal of Wikification is to find the most accurate mapping from mentions to Wikipedia titles T ; this mapping needs to take into account our understanding of the text as well as background knowledge that is often needed to determine the most appropriate title. We also allow a special NIL title that captures all mentions that are outside Wikipedia. Earlier approaches treated this task as a word"
D13-1184,P11-1138,1,0.41968,"ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task. 1 Introduction Wikification (D2W), the task of identifying concepts and entities in text and disambiguating them into their corresponding Wikipedia page, is an important step toward supporting deeper textual understanding, by augmenting the ability to ground text in existing knowledge and facilitating knowledge expansion. D2W has been studied extensively recently (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010; Ratinov et al., 2011) and has already found broad applications in NLP, Information Extraction, and Knowledge Acquisition from text, from coreference resolution (Ratinov and Roth, 2012) to entity linking and knowledge population (Ellis et al., 2011; Ji et al., 2010; Cucerzan, 2011). Given a document D containing a set of concept and entity mentions M ( referred to later as surface), the goal of Wikification is to find the most accurate mapping from mentions to Wikipedia titles T ; this mapping needs to take into account our understanding of the text as well as background knowledge that is often needed to determine"
D13-1184,W04-2401,1,0.529102,"represents the initial score for the kth candidate title being chosen for mention mi . For a pair of titles (tki , tlj ), we denote the confidence of (k,l) finding a relation between them by wij . Its value depends on the textual relation type and on how coherent it is with our existing knowledge. Our goal is to find the best assignment to variables eki , such that it satisfies some legitimacy (hard) constraints and the soft constraints dictated by the (k,l) relational constraints (via scores wij ). To accomplish that we define our objective function as a Constrained Conditional Model (CCM) (Roth and Yih, 2004; Chang et al., 2012) that is used to reward or (k,l) penalize a pair of candidates tki , tlj by wij when they are chosen in the same document. Specifically, we choose the assignment ΓD that optimizes: ΓD = arg max Γ s.t. (k,l) XX i ski eki + XX i,j k (k,l) (k,l) wij rij k,l ∈ {0, 1} Integral constraints eki ∈ {0, 1} P ∀i k eki = 1 Integral constraints rij (k,l) 2rij ≤ eki + elj Unique solution Relation definition Note that as in most NLP problems, the problem is very sparse, resulting in a tractable ILP that is solved quickly by off-the-shelf ILP packages (Gurobi Optimization, 2013). In our c"
D13-1186,W06-1623,0,0.0140749,"urse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILPs and it enables us to easily incorporate soft con"
D13-1186,P07-1036,1,0.807375,"state-ofthe-art BK system as we vary the size of the training data, indicating the robustness of the joint inference procedure. 1 Please note that the results reported in Table 2 can not be directly compared with those reported in the challenge because we only had a fraction of the original training and testing data. 1812 Discussion and Related Work In this paper, we chose to train a rather simple sequential model (using CRF), and focused on incorporating global constraints only at inference time2 . While it is possible to jointly train the model with the global constraints (as illustrated by Chang et al. (2007), Mann and McCallum (2007), Mann and McCallum (2008), Ganchev et al. (2010) etc.), this process will be a lot less efficient, and prior work (Roth and Yih, 2005) has shown that it may not be beneficial. Roth and Yih (2004, 2007) suggested the use of integer programs to model joint inference in a fully supervised setting. Our paper follows their conceptual approach. However, they used only hard constraints in their inference formulation. Chang et al. (2012) extended the ILP formulation and used soft constraints within the Constrained Conditional Model formulation (Chang, 2011). However, their i"
D13-1186,W11-1904,1,0.787689,"sive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILPs and it enables us to easily incorporate soft constraints into the inference procedure. Our experimental results show that soft constraints indeed give much better performance than hard constr"
D13-1186,D07-1001,0,0.016821,"writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILPs and it enables us to easily incorporate soft constraints into the inference procedure. Our experimental results show that sof"
D13-1186,N07-1030,0,0.0614569,"Missing"
D13-1186,W04-3250,0,0.0180658,"uch better than hard constraints (BKC-HARD). the constraint. We observe from Figures 3a and 3b that for Type-2 and Type-3 constraints, global maxima is attained at ρ2 = 0.6 and ρ3 = 0.3 respectively. Hard vs Soft Constraints Table 4 compares the performance of BKC-HARD with that of BKC. First 3 rows in this table show the performance of both systems for the individual categories (TEST, TRE and PROB). The fourth row shows the overall score of both systems. BKC outperformed BKCHARD on all the categories by statistically significant differences at p = 0.05 according to Bootstrap Resampling Test (Koehn, 2004). For the OVERALL category, BKC improved over BKC-HARD by 1.0 F1 points. Effect of Training Data Size on Performance 5 86 BKC BK 85 F1−score 84 83 82 81 80 40 50 60 70 80 90 100 110 120 130 Training Data Size (# clinical reports) Figure 4: This figure shows the effect of training data size on performance of concept recognition. 4.2.2 Comparing with state-of-the-art baseline In the 2010 i2b2/VA shared task, majority of top systems were CRF-based models, motivating the use of CRF as our baseline. Table 2 compares the performance of 4 systems: B, BK, BC and BKC. As pointed out before, our BK syst"
D13-1186,P08-1099,0,0.0269984,"f the training data, indicating the robustness of the joint inference procedure. 1 Please note that the results reported in Table 2 can not be directly compared with those reported in the challenge because we only had a fraction of the original training and testing data. 1812 Discussion and Related Work In this paper, we chose to train a rather simple sequential model (using CRF), and focused on incorporating global constraints only at inference time2 . While it is possible to jointly train the model with the global constraints (as illustrated by Chang et al. (2007), Mann and McCallum (2007), Mann and McCallum (2008), Ganchev et al. (2010) etc.), this process will be a lot less efficient, and prior work (Roth and Yih, 2005) has shown that it may not be beneficial. Roth and Yih (2004, 2007) suggested the use of integer programs to model joint inference in a fully supervised setting. Our paper follows their conceptual approach. However, they used only hard constraints in their inference formulation. Chang et al. (2012) extended the ILP formulation and used soft constraints within the Constrained Conditional Model formulation (Chang, 2011). However, their implementation performed only approximate inference."
D13-1186,W05-0618,0,0.0246326,"and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILPs and it enables us to easil"
D13-1186,C04-1197,1,0.842081,"Missing"
D13-1186,J08-2005,1,0.764902,"e UMLS (Url1, 2013), MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILP"
D13-1186,W06-1616,0,0.0310059,"ives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference problem. IQPs is a richer family of models than ILPs and it enables us to easily incorporate soft constraints into the inference procedure. Our experimen"
D13-1186,W04-2401,1,0.883449,"Knowledge captured in medical ontologies like UMLS (Url1, 2013), MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the latter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference formulation. Integer Linear Programming (ILP) based approaches have been used for global inference in many works (Roth and Yih, 2004; Punyakanok et al., 2004; Punyakanok et al., 2008; Marciniak and Strube, 2005; Bramsen et al., 2006; Barzilay and Lapata, 2006; Riedel and Clarke, 2006; Clarke and Lapata, 2007; Clarke and Lapata, 2008; Denis et al., 2007; Chang et al., 2011). However, in most of these works, researchers have focussed only on hard constraints while formulating the inference problem. Formulating all the constraints as hard constraints is not always desirable because the constraints are not perfect in many cases. In this paper, we propose Integer Quadratic Programs (IQPs) as a way of formulating the inference p"
D13-1186,N06-1046,0,\N,Missing
D13-1186,P05-1045,0,\N,Missing
D13-1186,N12-1008,0,\N,Missing
D15-1102,N13-1122,0,0.0171977,"Missing"
D15-1102,W07-1009,0,0.487584,"task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Approach Mentions and Their Combinations Typically, a mention that appears in a natural language sentence consists of a contiguous sequence of natural language words. Consider a sentence that consists of n words where each word is indexed with its position in the sentence. A mention m can be uniquely represented with a tuple hbm , em , τ i, where bm and em are the indices of the first and"
D15-1102,P08-1024,0,0.0328942,"∂wk i X − fk (xi , yi ) + 2λwk 3.5 The features that we use are inspired by the work of (Carreras et al., 2002). Specifically, we consider the following features defined over the inputs: • Words (and POS tags, if available) that appear around the current word (with position information), with a window of size 3. (3) • Word n-grams (and POS n-grams, if available) that contain the current word (with position information), for n = 2, 3, 4. i where wk is the weight of the k-th feature fk . We note that unlike many recent latent-variable approaches to structured prediction (Petrov and Klein, 2007; Blunsom et al., 2008), we are able to represent each of our outputs y with a single fullyobserved structure. Thus, our objective function essentially defines a standard regularized softmax regression model, and is therefore convex (Boyd and Vandenberghe, 2004), where a global optimum can be found. The objective function defined in Equation 2 can be optimized with standard gradient-based methods. We used L-BFGS (Liu and Nocedal, 1989) as our optimization method. 3.4 Features • Bag of words around the current word, with a window of size 5. • Word pattern features 2 . Note that these are the indicator functions defin"
D15-1102,P11-1055,0,0.00896767,"ian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Approach Mentions and Their Combinations Typically, a mention that appears in a natural language sentence consists of a contiguous sequence of natural language words. Consider a sentence that consists of n words where each word is indexed with its position in the sentence. A mention m can be uniquely represented with a tuple hbm , em , τ i, where bm and em are the indices of the first and last word of the mention, respectively, and τ is its semantic class (type). We can see that for a given sentence consisting of n words, there are altogether tn(n + 1)/2 possible"
D15-1102,W01-1812,0,0.0540666,"ingle hyperedge, while the two brown links that connect two I nodes and one X node form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combina"
D15-1102,W02-2004,0,0.0186569,"Missing"
D15-1102,D13-1057,1,0.777883,"Missing"
D15-1102,P14-1038,0,0.108717,"s amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the number of words in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for downlo"
D15-1102,N13-1006,0,0.0152784,"m fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into"
D15-1102,P13-1008,0,0.0296153,"ctly representing exponentially many possible mentions enables a mention’s boundaries, type and head information to be jointly learned in a single framework. The model scales linearly with respect to the number of words in the input sentence, and performs exact learning where a unique global optimum can be found. Empirically, we have demonstrated the effectiveness of such a model across several standard datasets. Future work include explorations of efficient algorithms for other information extraction tasks, such as joint mention and relation extraction (Li and Ji, 2014) and event extraction (Li et al., 2013). Our system and code can be downloaded from http://statnlp.org/research/ie/. Results on C ONLL2003 To understand how well our model works on datasets where mentions or entities do not overlap with one another, we conducted additional experiments on the standard dataset used in the C O NLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), where the named entities strictly do not overlap with one another. We compared our system’s performance against that of a baseline version of the state-of-the-art Illinois NER system (Ratinov and Roth, 2009). Their system performed sequential prediction"
D15-1102,D13-1052,0,0.0269047,"ode form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combination. These paths altogether form a unique sub-hypergraph of the original hy"
D15-1102,P15-2121,1,0.761848,"hat connect two I nodes and one X node form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combination. These paths altogether"
D15-1102,J01-1005,0,0.0110782,"using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented sev"
D15-1102,N04-4028,0,0.0372638,"mean of the precision (P ) and recall (R) scores, where precision is the ratio between the number of correctly predicted mentions and the total number of predicted mentions, and recall is the ratio between the number of correctly predicted mentions and the total number of gold mentions. We will also adopt these metrics in our evaluations later. Unfortunately, the model only optimizes its objective function defined in Equation 2, which is the negative (regularized) joint log-likelihood. Previous work showed it was possible to optimize the F measure in a log-linear model (Suzuki et al., 2006). Culotta and McCallum (2004) also proposed a method for optimizing information extraction performance based on confidence estimation. Their work is based on linear-chain CRF and estimate the confidence of extracted fields based on marginal probabilities. The technique is not directly applicable to our task where a hypergraph representation is used to encode overlapping mentions. In this work, we used a very simple and intuitive technique for optimizing the F measure. The idea is to further tune the weight of a single parameter – mention penalty based on the development set, after the training process completes. 4 Experim"
D15-1102,P09-1113,0,0.0284313,"Missing"
D15-1102,E14-1048,0,0.0154488,"essing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the number of words in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model"
D15-1102,D09-1015,0,0.776009,"vely studied in the past few decades by the community, such a semantic tagging task presents several additional new challenges. First, a mention can consist of multiple words, so its length can be arbitrarily long. Second, the mentions can overlap with one another. Popular models used for POS tagging, such as linear-chain conditional random fields (Lafferty et al., 2001) or semi-Markov conditional random fields (Sarawagi and Cohen, 2004) have difficulties coping with these issues. While approaches on addressing these issues exist, current algorithms typically suffer from high time complexity (Finkel and Manning, 2009) and are therefore difficult to scale to large datasets. On the other hand, the problem of designing efficient and scalable models for mention extraction and classification from natural language texts becomes increasingly important in this era where a large volume of textual data is becoming available on the Web every day – users need systems which are able to scale to extremely large datasets to support efficient semantic analysis for timely decisonmaking. In this paper, we tackle the above-mentioned issue by introducing a novel model for joint mention extraction and classification. We make t"
D15-1102,P14-1037,0,0.021832,"al attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the numbe"
D15-1102,W03-0425,0,0.0127339,"g our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for download from http://statnlp.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 20"
D15-1102,N04-1001,0,0.134894,"t the same time identify their semantic classes (e.g., person, organization, etc). Such a task is often known as named entity recognition and classification (NERC), one of the standard tasks in information extraction (IE). While such a task focuses on the extraction and classification of entities in the texts which are named, recently researchers also showed interest in a closely related task – mention extraction and classification/typing. Unlike a named entity, a mention is typically defined as a reference to an entity in natural language text that can be either named, nominal or pronominal (Florian et al., 2004). The task of mention detection and tracking has received substantial attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from soc"
D15-1102,W09-1119,1,0.942365,"p.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entitie"
D15-1102,D11-1141,0,0.0137711,"be either named, nominal or pronominal (Florian et al., 2004). The task of mention detection and tracking has received substantial attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions wit"
D15-1102,P06-1028,0,0.00967452,"efined as the harmonic mean of the precision (P ) and recall (R) scores, where precision is the ratio between the number of correctly predicted mentions and the total number of predicted mentions, and recall is the ratio between the number of correctly predicted mentions and the total number of gold mentions. We will also adopt these metrics in our evaluations later. Unfortunately, the model only optimizes its objective function defined in Equation 2, which is the negative (regularized) joint log-likelihood. Previous work showed it was possible to optimize the F measure in a log-linear model (Suzuki et al., 2006). Culotta and McCallum (2004) also proposed a method for optimizing information extraction performance based on confidence estimation. Their work is based on linear-chain CRF and estimate the confidence of extracted fields based on marginal probabilities. The technique is not directly applicable to our task where a hypergraph representation is used to encode overlapping mentions. In this work, we used a very simple and intuitive technique for optimizing the F measure. The idea is to further tune the weight of a single parameter – mention penalty based on the development set, after the training"
D15-1102,W03-0419,0,0.0555241,"Missing"
D15-1102,P13-1106,0,0.0117956,"cting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Ap"
D15-1102,P02-1060,0,0.279921,"rds in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for download from http://statnlp.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-superv"
D15-1202,N06-1046,0,0.0112253,"ations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P , which mentions n quantities q1 , q2 , . . . , qn . Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this work problems that can b"
D15-1202,D14-1159,0,0.0194536,"2014) might be the most related to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning se"
D15-1202,P06-2019,0,0.00930189,"by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P , which mentions n quantities q1 , q2 , . . . , qn . Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in th"
D15-1202,N10-1115,0,0.0155436,"evant information from the problem text. To this end, we introduce the concept of a quantity schema which we extract for each quantity in the problem’s text. Along with the question asked, the quantity 1747 schemas provides all the information needed to solve most arithmetic problems. A quantity schema for a quantity q in problem P consists of the following components. 1. Associated Verb For each quantity q, we detect the verb associated with it. We traverse up the dependency tree starting from the quantity mention, and choose the first verb we reach. We used the easy first dependency parser (Goldberg and Elhadad, 2010). 1. Unit features: Most questions specifically mention the object whose amount needs to be computed, and hence questions provide valuable clue as to which quantities can be irrelevant. We add a feature for whether the unit of quantity q is present in the question tokens. Also, we add a feature based on whether the units of other quantities have better matches with question tokens (based on the number of tokens matched), and one based on the number of quantities which have the maximum number of matches with the question tokens. 2. Subject of Associated Verb We detect the noun phrase, which act"
D15-1202,D14-1058,0,0.345499,"atively straightforward text, and seemingly simple semantics. Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem. Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with The solution involves understanding that the number of shelves needs to be summed up, and that the total number of shelves needs to be multiplied by the number of books each shelf can hold. In addition, one has to understand that the number “2” is not a direct part of the solution of the problem. While a solution to these problems eventually requires composing multi-step numeric expressions from text, we believe that directly predicting this complex expression from text is not feasible. At the heart of our technical approach is the novel notion of an Expression Tree. W"
D15-1202,P14-1026,0,0.46567,"annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem. However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem. In contrast, our system does not have any such restrictions, effectively handling problems mentioning multiple quantities and requiring multiple steps. Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most related to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant"
D15-1202,W05-0639,0,0.0175947,"i et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P , which mentions n quantities q1 , q2 , . . . , qn . Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once"
D15-1202,J08-2005,1,0.429037,"answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P , which mentions n quantities q1 , q2 , . . . , qn . Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, a"
D15-1202,W04-2401,1,0.733815,"not used in creating the solution. A binary classifier trained to predict whether a quantity q is relevant or not (Section 4.3), can provide these scores. For an expression E, let I(E) be the set of all quantities in P which are not used in expression E. Let T be a monotonic expression tree for E. We define Score(E) of an expression E in terms of the above scoring functions and a scaling parameter wI RR as follows: X Score(E) =wI RR I RR(q)+ (3) q∈I(E) X PAIR(qi , qj , LCA (qi , qj , T )) qi ,qj ∈I(E) / Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012). Our objective function makes use of the scores returned by I RR(·) and PAIR(·) to determine the expression tree and is constrained by legitimacy and background knowledge constraints, detailed below. 1. Positive Answer: Most arithmetic problems asking for amounts or number of objects usually have a positive number as an answer. Therefore, while searching for the best scoring expression, we reject expressions generating negative answer. 2. Integral Answer: Problems with questions such as ‘how many” usually expect integral solutions. (4) Once we have a top k list of candida"
D15-1202,Q15-1001,1,0.595436,"ts concise and relatively straightforward text, and seemingly simple semantics. Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem. Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with The solution involves understanding that the number of shelves needs to be summed up, and that the total number of shelves needs to be multiplied by the number of books each shelf can hold. In addition, one has to understand that the number “2” is not a direct part of the solution of the problem. While a solution to these problems eventually requires composing multi-step numeric expressions from text, we believe that directly predicting this complex expression from text is not feasible. At the heart of our technical approach is the novel notion"
D16-1038,W99-0201,0,0.402049,"present events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-re"
D16-1038,P10-1143,0,0.037571,"rison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009) is an early work based on supervised learning. We also report HDP-Coref results as an unsupervised baseline (Bejan and Harabagiu, 2010), which utilizes nonparametric Bayesian models. Moreover, we create another unsupervised event co-reference baseline (Type+SharedMen): we treat events of the same type which share at least one co-referent entity (inside event arguments) as coreferred. On TAC-KBP corpus, we report results from the top ranking system of the TAC-2015 Event Nugget Evaluation Task as TAC-TOP. We name our event mention detection module in MSEP similarity-based event mention detection MSEP-EMD system. For event co-reference, the proposed similarity based co-reference detection MSEP-Coref method has a number of variat"
D16-1038,D08-1031,1,0.422316,"Thus, when we encounter missing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We als"
D16-1038,P15-2061,0,0.0576145,"vector representation is illustrated in Fig. 2. If there are missing event arguments, we set the corresponding vector to be “NIL” (we set each position as “NaN”). We also augment the event vector representation by concatenating more text fragments to enhance the interactions between the action and other arguments, as shown in Fig. 3. Essentially, we flatten the event structure to preserve the alignment of event arguments so that the structured information can be reflected in our vector space. 2.2 Event Mention Detection Motivated by the seed-based event trigger labeling technique employed in Bronstein et al. (2015), we turn to ACE annotation guidelines for event examples described under each event type label. For instance, the ACE-2005 guidelines list the example “Mary Smith joined Foo Corp. in June 1998.” for label “START-POSITION”. Altogether, we collect 172 event examples from 33 event types (5 each on average).4 We can then get vector representations for these example events following the procedures in Sec. 2.1. We define the event type representation as the numerical average of all vector representations corresponding to example events under that type. We use the similarity between an event candida"
D16-1038,J92-4003,0,0.527401,"verted index for each word to search the Wikipedia corpus. The text fragment representation is thus a weighted combination of the concept vectors corresponding to its words. We use the same setting as in Chang et al. (2008) to filter out pages with fewer than 100 words and those containing fewer than 5 hyperlinks. To balance between the effectiveness of ESA representations and its cost, we use the 200 concepts with the highest weights. Thus, we convert each text fragment to a very sparse vector of millions of dimensions (but we just store 200 non-zero values). Brown Cluster BC was proposed by Brown et al. (1992) as a way to support abstraction in NLP tasks, measuring words’ distributional similarities. This method generates a hierarchical tree of word clusters by evaluating the word co-occurrence based on a n-gram model. Then, paths traced from root to 396 leaves can be used as word representations. We use the implementation by Song and Roth (2014), generated over the latest Wikipedia dump. We set the maximum tree depth to 20, and use a combination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use"
D16-1038,W09-3208,0,0.0573058,"rge body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how w"
D16-1038,W09-4303,0,0.0956789,"CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009) is an early work based on supervised learning. We also report HDP-Coref results as an unsupervised baseline (Bejan and Harabagiu, 2010), which utilizes nonparametric Bayesian models. Moreover, we create another unsupervised event co-reference baseline (Type+SharedMen): we treat events of the same type which share at least one co-referent entity (inside event arguments) as coreferred. On TAC-KBP corpus, we report results from the top ranking system of the TAC-2015 Event Nugget Evaluation Task as TAC-TOP. We name our event mention detection module in MSEP similarity-based event mention detectio"
D16-1038,P15-1017,0,0.241504,"the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from the text and then average, element-wise, all remaining word vectors to produce the resulting vector representation of the text fragment. 4 Experiments 4.1 Datasets ACE The ACE-2005 English corpus (NIST, 2005) contains fine-grained event annotations, including event trigger, argument, entity, and time-stamp annotations. We select 40 documents from newswire articles for event detection evaluation and the rest for training (same as Chen et al. (2015)). We do 10fold cross-validation for event co-reference. TAC-KBP The TAC-KBP-2015 corpus is annotated with event nuggets that fall into 38 types and coreference relations between events. 7 We use the train/test data split provided by the official TAC6 https://levyomer.wordpress.com/2014/04/25/dependencybased-word-embeddings 7 The event ontology of TAC-KBP (based on ERE annotation) is almost the same to that of ACE. To adapt our system to the TAC-KBP corpus, we use all ACE event seeds of “Contact.Phone-Write” for “Contact.Correspondence” and separate ACE event seeds of “Movement.Transport” into"
D16-1038,D13-1184,1,0.762027,"e use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We also augment the argument type set with NER labels: PER (person) and ORG (organization). We add either of the NER labels if we detect such a named entity. For each pair of events, we check event arguments agentsub and agentobj respectively. If none of the types for the aligned event ar"
D16-1038,D12-1062,1,0.747638,"in research on events, including the dif400 ficulty to annotate events and their relations. At the heart of our approach is the design of structured vector representations for events which, as we show, supports a good level of generalization within and across domains. The resulting approach outperforms state-of-art supervised methods on some of the key metrics, and adapts significantly better to a new domain. One of the key research directions is to extend this unsupervised approach to a range of other relations among events, including temporal and causality relations, as is (Do et al., 2011; Do et al., 2012). Acknowledgments The authors would like to thank Eric Horn for comments that helped to improve this work. This material is based on research sponsored by the US Defense Advanced Research Projects Agency (DARPA) under agreements FA8750-13-2-000 and HR0011-15-2-0025. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or imp"
D16-1038,P13-2083,0,0.0206993,"using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed"
D16-1038,P11-1113,0,0.0283793,"ANC — 74.0 75.1 68.7 72.9 71.7 72.3 72.8 72.5 73.1 73.5 AVG 75.7 74.4 75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity co"
D16-1038,W13-1203,0,0.197967,"er Science and Engineering, Hong Kong University of Science and Technology 1 {hpeng7,danr}@illinois.edu, 2 yqsong@cse.ust.hk Abstract a minimum, determining whether two snippets of text represent the same event or not – the event coreference problem. Events have been studied for years, but they still remain a key challenge. One reason is that the frame-based structure of events necessitates addressing multiple coupled problems that are not easy to study in isolation. Perhaps an even more fundamental difficulty is that it is not clear whether our current set of events’ definitions is adequate (Hovy et al., 2013). Thus, given the complexity and fundamental difficulties, the current evaluation methodology in this area focuses on a limited domain of events, e.g. 33 types in ACE 2005 (NIST, 2005) and 38 types in TAC KBP (Mitamura et al., 2015). Consequently, this allows researchers to train supervised systems that are tailored to these sets of events and that overfit the small domain covered in the annotated data, rather than address the realistic problem of understanding events in text. An important aspect of natural language understanding involves recognizing and categorizing events and the relations a"
D16-1038,E12-1029,0,0.0303396,"7 72.9 71.7 72.3 72.8 72.5 73.1 73.5 AVG 75.7 74.4 75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work foll"
D16-1038,P16-1025,0,0.0528716,"y model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how well their system performed in each of these two cases. Huang et al. (2016) also considered ACE (Cross-Validation) SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC 47.1 42.1 40.2 B3 59.9 60.3 58.6 CEAFe 58.7 59.0 57.4 BLANC 44.4 44.1 43.8 AVG 52.5 51.4 50.0 TAC-KBP (Test Data) TAC-TOP SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC — 34.9 33.1 30.2 B3 — 44.2 44.6 43.9 CEAFe — 39.6 39.7 38.7 BLANC — 37.1 36.8 35.7 AVG 39.1 39.0 38.5 37.1 Table 6: Event Co-reference End-To-End Results. Train Event Detection In Domain NW Out of Domain DF In Domain DF Out of Domain NW Event Co-referenc"
D16-1038,W97-1311,0,0.0998694,"mantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and"
D16-1038,P08-1030,0,0.187651,"equal number of documents. When trained on NW and tested on DF, supervised methods encounter out-of-domain situations. However, the MSEP system can adapt well.12 Table 7 shows that MSEP outperforms supervised methods in out-of-domain situations for both tasks. The differences are statistically significant with p < 0.05. 5 Related Work Event detection has been studied mainly in the newswire domain as the task of detecting event triggers and determining event types and arguments. Most earlier work has taken a pipeline approach where local classifiers identify triggers first, and then arguments (Ji and Grishman, 2008; Liao and 12 Note that the supervised method needs to be re-trained and its parameters re-tuned while MSEP does not need training and its cut-off threshold is fixed ahead of time using event examples. ACE (Cross-Validation) Graph Joint Supervised SupervisedBase SupervisedExtend Type+SharedMen Unsupervised HDP-Coref MSEP-CorefESA MSEP-CorefBC MSEP-CorefW2V MSEP MSEP-CorefDEP MSEP-CorefESA+AUG MSEP-CorefESA+AUG+KNOW MSEP-CorefESA+AUG+KNOW (GA) MUC — 74.8 73.6 74.9 59.1 — 65.9 65.0 65.1 65.9 67.4 68.0 68.8 B3 — 92.2 91.6 92.8 83.2 83.8 91.5 89.8 90.1 92.3 92.6 92.9 92.5 CEAFe 84.5 87.0 85.9 87.1"
D16-1038,D12-1045,0,0.153158,"d event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how well their system performed in each of these two cases. Huang et al. (2016) also considered ACE (Cross-Validation) SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC 47.1 42.1 40.2 B3 59.9 60.3 58.6 CEAFe 58.7 59.0 57.4 BLANC 44.4 44.1 43.8 AVG 52.5 51"
D16-1038,P14-2050,0,0.0137596,"mbination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use the skip-gram tool by Mikolov et al. (2013) over the latest Wikipedia dump, resulting in word vectors of dimensionality 200. Dependency-Based Embedding DEP is the generalization of the skip-gram model with negative sampling to include arbitrary contexts. In particular, it deals with dependency-based contexts, and produces markedly different embeddings. DEP exhibits more functional similarity than the original skip-gram embeddings (Levy and Goldberg, 2014). We directly use the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from the text and then average, element-wise, all remaining word vectors to produce the resulting vector representation of the text fragment. 4 Experiments 4.1 Datasets ACE The ACE-2005 English corpus (NIST, 2005) contains fine-grained event annotations, including event trigger, argument, entity, and time-stamp annotations. We select 40 documents from newswire articles for event detection evaluation and the rest for"
D16-1038,P13-1008,0,0.0905247,"75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy"
D16-1038,P10-1081,0,0.0389618,"Missing"
D16-1038,W15-0807,0,0.0469874,"respectively. We use SSED and SupervisedBase as the supervised modules for comparison. For event detection, we compare F1 scores of span plus type match while we report the average F1 scores for event co-reference. the problem of event clustering. They represented event structures based on AMR (Abstract Meaning Representation) and distributional semantics, and further generated event schemas composing event triggers and argument roles. Recently, TAC has organized Event Nugget Detection and Co-reference Evaluations, resulting in interesting works, some of which contributed to our comparisons (Liu et al., 2015; Mitamura et al., 2015; Hsi et al., 2015; Sammons et al., 2015). 6 Conclusion This paper proposes a novel event detection and co-reference approach with minimal supervision, addressing some of the key issues slowing down progress in research on events, including the dif400 ficulty to annotate events and their relations. At the heart of our approach is the design of structured vector representations for events which, as we show, supports a good level of generalization within and across domains. The resulting approach outperforms state-of-art supervised methods on some of the key metrics, and a"
D16-1038,H05-1004,0,0.0920777,"ng. 4.2 use augmented ESA vector representation (AUG)8 , and whether we use knowledge during co-reference inference (KNOW). We also develop a supervised event co-reference system following the work of Sammons et al. (2015), namely SupervisedBase . We also add additional event vector representations9 as features to this supervised system and get SupervisedExtend . 4.3 For event detection, we use standard precision, recall and F1 metrics. For event co-reference, we compare all systems using standard F1 metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009)"
D16-1038,N13-1090,0,0.0185368,"straction in NLP tasks, measuring words’ distributional similarities. This method generates a hierarchical tree of word clusters by evaluating the word co-occurrence based on a n-gram model. Then, paths traced from root to 396 leaves can be used as word representations. We use the implementation by Song and Roth (2014), generated over the latest Wikipedia dump. We set the maximum tree depth to 20, and use a combination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use the skip-gram tool by Mikolov et al. (2013) over the latest Wikipedia dump, resulting in word vectors of dimensionality 200. Dependency-Based Embedding DEP is the generalization of the skip-gram model with negative sampling to include arbitrary contexts. In particular, it deals with dependency-based contexts, and produces markedly different embeddings. DEP exhibits more functional similarity than the original skip-gram embeddings (Levy and Goldberg, 2014). We directly use the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from"
D16-1038,W15-0809,0,0.0436271,"t coreference problem. Events have been studied for years, but they still remain a key challenge. One reason is that the frame-based structure of events necessitates addressing multiple coupled problems that are not easy to study in isolation. Perhaps an even more fundamental difficulty is that it is not clear whether our current set of events’ definitions is adequate (Hovy et al., 2013). Thus, given the complexity and fundamental difficulties, the current evaluation methodology in this area focuses on a limited domain of events, e.g. 33 types in ACE 2005 (NIST, 2005) and 38 types in TAC KBP (Mitamura et al., 2015). Consequently, this allows researchers to train supervised systems that are tailored to these sets of events and that overfit the small domain covered in the annotated data, rather than address the realistic problem of understanding events in text. An important aspect of natural language understanding involves recognizing and categorizing events and the relations among them. However, these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task, resulting in supervised systems that attempt to learn complex models from small amounts of dat"
D16-1038,P02-1014,0,0.0350973,"ec(a)k #|a 6= NIL| . Thus, when we encounter missing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set"
D16-1038,C04-1197,1,0.184492,"ion, time and sentence/clause. Figure 3: Augmented event vector representation. Event vector is the concatenation of vectors corresponding to basic event vector representation, agentsub + action, agentobj + action, location + action and time + action. Here, “+” means that we first put text fragments together and then convert the combined text fragment into an ESA vector. event types. Event arguments are largely entity mentions or temporal/spatial arguments. They serve as specific roles in events, similarly to SRL arguments that are assigned role labels for predicates. We use the Illinois SRL (Punyakanok et al., 2004) tool to pre-process the text. We evaluate the SRL coverage on both event triggers and event arguments, shown in Table 2.3 For event triggers, we only focus on recall since we expect the event mention detection module to filter out most non-trigger predicates. Results show a good coverage of SRL predicates and arguments on event triggers and arguments. Even though we only get approximate event arguments, it is easier and more reliable to categorize them into five abstract roles, than to determine the exact role label with respect to event triggers. We identify the five most important and abstr"
D16-1038,P10-2029,0,0.0229096,"issing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We also augment the argument t"
D16-1038,M95-1005,0,0.701236,"supervised methods. For MSEP, we only need to run on each corpus once for testing. 4.2 use augmented ESA vector representation (AUG)8 , and whether we use knowledge during co-reference inference (KNOW). We also develop a supervised event co-reference system following the work of Sammons et al. (2015), namely SupervisedBase . We also add additional event vector representations9 as features to this supervised system and get SupervisedExtend . 4.3 For event detection, we use standard precision, recall and F1 metrics. For event co-reference, we compare all systems using standard F1 metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determ"
D16-1038,N12-3008,1,0.853533,".9 85.7 73.5 82.3 F1 — — — 88.0 81.9 86.4 TAC KBP Predicates over Triggers SRL Args over Event Args Verb-SRL Nom-SRL All Verb-SRL Nom-SRL All Precision — — — 89.8 88.2 89.5 Recall 90.6 85.5 88.1 83.6 69.9 81.0 F1 — — — 86.6 78.0 85.0 Table 2: Semantic role labeling coverage. We evaluate both “Predicates over Triggers” and “SRL Arguments over Event Arguments”. “All” stands for the combination of Verb-SRL and Nom-SRL. The evaluation is done on all data. ists. 5) We set the SRL temporal argument as event time. If there is no such SRL label, we then use the Illinois Temporal Expression Extractor (Zhao et al., 2012) to find the temporal argument within an event’s sentence/clause. 6) We allow one or more missing event arguments among agentsub , agentobj , location or time, but require actions to always exist. Given the above structured information, we convert each event component to its corresponding vector representation, discussed in detail in Section 3. We then concatenate the vectors of all components together in a specific order: action, agentsub , agentobj , location, time and sentence/clause. We treat the whole sentence/clause, to which the “action” belongs, as context, and we append its correspond"
D16-1117,S13-1045,0,0.0449965,"Missing"
D16-1117,W10-2903,1,0.916684,"Missing"
D16-1117,D14-1058,0,0.234522,"Missing"
D16-1117,P14-1026,0,0.364085,"Missing"
D16-1117,D13-1161,0,0.0655092,"Missing"
D16-1117,H05-1066,0,0.21043,"Missing"
D16-1117,P98-2186,1,0.45389,"Missing"
D16-1117,D15-1202,1,0.844849,"Missing"
D16-1117,Q15-1001,1,0.900907,"Missing"
D16-1117,P13-1045,0,0.0355972,"Missing"
D16-1117,P07-1121,0,0.0228591,"Missing"
D16-1117,P14-1005,0,\N,Missing
D16-1117,C98-2181,1,\N,Missing
D17-1108,S13-2002,0,0.465304,"Missing"
D17-1108,S07-1025,0,0.0340257,"take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction The goal of the tempora"
D17-1108,W06-1623,0,0.552887,": original human annotations. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This"
D17-1108,P14-2082,0,0.527329,"Missing"
D17-1108,D08-1073,0,0.897267,"tions. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is co"
D17-1108,S13-2012,0,0.195424,"rs et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events. State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime ripping monitor cascaded ordered hurt BEFORE INCLUDED BEFORE NO RELATION Figure 2: The human-annotation for Ex1 provided in TE3, where many TLINKs are missing due to the annotation difficulty. Solid lines: original human annotations. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for"
D17-1108,Q14-1022,0,0.813971,"ansitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008). Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning). In parallel to the work presented here, Leeu"
D17-1108,P07-2044,0,0.361833,"hat it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extract"
D17-1108,chang-manning-2012-sutime,0,0.0250965,"m solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is conceptually similar"
D17-1108,P07-1036,1,0.241964,"Missing"
D17-1108,W02-1001,0,0.0486901,"Missing"
D17-1108,D12-1062,1,0.835673,"ue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them. The task is challenging because it often requires global consid"
D17-1108,S13-2015,0,0.27149,"attempts by Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events. State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime ripping monitor cascaded ordered hurt BEFORE INCLUDED BEFORE NO RELATION Figure 2: The human-annotation for Ex1 provided in TE3, where many TLINKs are missing due to the annotation difficulty. Solid lines: original human annotations. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it i"
D17-1108,P14-1135,0,0.186219,"Missing"
D17-1108,E17-1108,0,0.399805,"014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008). Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning). In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations. Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses. More importantly, they compared structured learning to local baselines, while we find that the comparison between structured learning and L+I is more interesting and important for 1028 understanding the effect of global considerations in the learning phase. In difference from existing methods, we also discus"
D17-1108,S15-2134,0,0.438791,"lem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them. The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of t"
D17-1108,P06-1095,0,0.839474,"or human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) t"
D17-1108,C08-3012,0,0.438508,"roves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008). Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning). In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations. Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses. More importantly, they compared structured learnin"
D17-1108,N12-3008,1,0.874361,"Missing"
D17-1108,rizzolo-roth-2010-learning,1,0.904507,"Missing"
D17-1108,W04-2401,1,0.253201,"ion for Ex1 provided in TE3, where many TLINKs are missing due to the annotation difficulty. Solid lines: original human annotations. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations. (Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global co"
D17-1108,S10-1071,0,0.0668609,"Missing"
D17-1108,P11-2061,0,0.141548,"m the others (see Table 1 in Mani et al. (2006)). For example, relations such as immediately before or immediately after barely exist in a corpus compared to before and after. 2. Due to the ambiguity in natural language, determining relations like before and immediately before can be a difficult task itself (Chambers et al., 2014). In this work, we follow the reduced set of temporal relation types used in CAEVO (Chambers et al., 2014): before, after, includes, is included, equal, and vague. 2.2 Quality of A Temporal Graph The most recent evaluation metric in TE3, i.e., the temporal awareness (UzZaman and Allen, 2011), is adopted in this work. Specifically, let Gsys and Gtrue be two temporal graphs from the system prediction and the ground truth, respectively. The precision and recall of temporal awareness are defined as follows. P = + |G− sys ∩ Gtrue | |G− sys | , R= + |G− true ∩ Gsys | |G− true | where G+ is the closure of graph G, G− is the reduction of G, “∩” is the intersection between TLINKs in two graphs, and |G |is the number of TLINKs in G. The temporal awareness metric better captures how “useful” a temporal graph is. For example, if system 1 produces ripping is before hurt and hurt is before mon"
D17-1108,S13-2001,0,0.744537,"earning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e.,"
D17-1108,S07-1014,0,0.454898,". This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem. 1 Introduction Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing. The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-"
D17-1168,E14-1023,0,0.0442061,"Missing"
D17-1168,I13-1171,0,0.00789247,"field of narrative understanding. Apart from event-centric understanding of narrative plots (Lehnert, 1981; McIntyre and Lapata, 2010; Goyal et al., 2010; Elsner, 2012; Finlayson, 2012), recent methods have focused on understanding narratives from the perspective of characters (Wilensky, 1978) mentioned in them. These methods study character personas (Bamman et al., 2013, 2014) or Proppian (Propp, 1968) roles (Valls-Vargas et al., 2014, 2015), inter-character relationships (Iyyer et al., 2016; Chaturvedi et al., 2016, 2017), and social networks of characters (Elson et al., 2010; Elson, 2012; Agarwal et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and"
D17-1168,P98-1013,0,0.407141,"atman, 1980). For instance, in Figure 1 Wesley got angry → Wesley bit her hand → Wesley was scolded describes a more coherent sequence of events, as compared to Wesley got angry → Wesley bit her hand → Wesley got a cookie Prior work in script-learning attempts to model such prototypical sequence of events (usually captured through verbs). For this task, we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful. Peng and Roth (2016) recently proposed a neural SemLM approach, to model such sequence of events using a language model of FrameNet (Baker et al., 1998) frames that are evoked in the given text. It represents an event using the corresponding predicate frame and its sense, obtained using a Sematic Role Labeler (Punyakanok et al., 2004). It also extends the frame definition to include explicit discourse markers (such as but, and) since they model relationships between frames. For example, in Fig1604 ure 1, the SemLM representation for the last sentence of the context is ‘Get.01-and-bit.01’. Here, ‘01’ indicates specific predicate senses for verbs ‘get’ and ‘bit’ with ‘and’ being a discourse marker. Also, it produces ‘scold.01’ and ‘give.01’ for"
D17-1168,D13-1178,0,0.0186671,"e structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee,"
D17-1168,P13-1035,0,0.0511263,"Missing"
D17-1168,P14-1035,0,0.0258596,"short story out of provided options. Introduction Narratives are a fundamental part of human language and culture. They serve as vehicles to share experiences, information and goals. For these reasons, automatically understanding stories is an interesting but challenging task for Computational Linguists (Mani, 2012). Story comprehension involves not only an array of NLP capabilities, but also some common sense knowledge and an understanding of normative social behavior (Charniak, 1972). Past research has focused on various aspects of story understanding such as identifying character personas (Bamman et al., 2014; Valls-Vargas et al., 2015), interpersonal relationships (Chaturvedi, 2016), plotpatterns (Jockers, 2013), narrative structures (Finlayson, 2012). There has also been an interest in predicting what is expected to happen next in a piece of text (Chambers and Jurafsky, 2008). Human readers are good at filling-in-the-gaps or inferring information that is not explicitly stated in the text. However, computers are not yet able to match their performance on predicting what could be the likely next step in a given sequence of events described in a story. Recently, Mostafazadeh et al. (2016) introduce"
D17-1168,J08-1001,0,0.0164162,"hich are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alte"
D17-1168,N04-1015,0,0.0176676,"nian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story. 5 Conclusion Story c"
D17-1168,D13-1185,0,0.0340312,"tical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 20"
D17-1168,P09-1068,0,0.116716,"Missing"
D17-1168,P08-1090,0,0.653614,"enging task for Computational Linguists (Mani, 2012). Story comprehension involves not only an array of NLP capabilities, but also some common sense knowledge and an understanding of normative social behavior (Charniak, 1972). Past research has focused on various aspects of story understanding such as identifying character personas (Bamman et al., 2014; Valls-Vargas et al., 2015), interpersonal relationships (Chaturvedi, 2016), plotpatterns (Jockers, 2013), narrative structures (Finlayson, 2012). There has also been an interest in predicting what is expected to happen next in a piece of text (Chambers and Jurafsky, 2008). Human readers are good at filling-in-the-gaps or inferring information that is not explicitly stated in the text. However, computers are not yet able to match their performance on predicting what could be the likely next step in a given sequence of events described in a story. Recently, Mostafazadeh et al. (2016) introduced the story-cloze task for testing this ability, albeit without the aspect of language generation. This task requires choosing the correct ending to a given four sentences long story (also referred to as context) from two provided alternatives. Fig. 1 shows an example story"
D17-1168,N13-1104,0,0.0436317,"ve been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches"
D17-1168,E12-1065,0,0.0610485,"Missing"
D17-1168,P10-1015,0,0.00584503,"rk is most closely related to the field of narrative understanding. Apart from event-centric understanding of narrative plots (Lehnert, 1981; McIntyre and Lapata, 2010; Goyal et al., 2010; Elsner, 2012; Finlayson, 2012), recent methods have focused on understanding narratives from the perspective of characters (Wilensky, 1978) mentioned in them. These methods study character personas (Bamman et al., 2013, 2014) or Proppian (Propp, 1968) roles (Valls-Vargas et al., 2014, 2015), inter-character relationships (Iyyer et al., 2016; Chaturvedi et al., 2016, 2017), and social networks of characters (Elson et al., 2010; Elson, 2012; Agarwal et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 20"
D17-1168,E14-1006,0,0.0217013,"Missing"
D17-1168,D10-1008,0,0.0234642,"Missing"
D17-1168,J95-2003,0,0.7912,"ding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes th"
D17-1168,N16-1180,1,0.132214,"Missing"
D17-1168,E12-1034,0,0.0383605,"on et al., 2010; Elson, 2012; Agarwal et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, F"
D17-1168,P04-1050,0,0.0473704,"ting events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, idea"
D17-1168,N15-1185,0,0.0381542,"nding. Apart from event-centric understanding of narrative plots (Lehnert, 1981; McIntyre and Lapata, 2010; Goyal et al., 2010; Elsner, 2012; Finlayson, 2012), recent methods have focused on understanding narratives from the perspective of characters (Wilensky, 1978) mentioned in them. These methods study character personas (Bamman et al., 2013, 2014) or Proppian (Propp, 1968) roles (Valls-Vargas et al., 2014, 2015), inter-character relationships (Iyyer et al., 2016; Chaturvedi et al., 2016, 2017), and social networks of characters (Elson et al., 2010; Elson, 2012; Agarwal et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for scrip"
D17-1168,P03-1069,0,0.131385,"ently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story. 5 Conclusion Story comprehension is a complex Natural Language Understanding tas"
D17-1168,P11-1100,0,0.0321269,"Missing"
D17-1168,D12-1106,0,0.0295068,"model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story. 5 Conclusion Story comprehension is a complex Natural Language Understanding task involving linguistic intelligence as well as a semantic and social kno"
D17-1168,P10-1158,0,0.0236956,"Missing"
D17-1168,W14-1606,0,0.0143159,"ses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theo"
D17-1168,1985.tmi-1.17,0,0.187263,") or Proppian (Propp, 1968) roles (Valls-Vargas et al., 2014, 2015), inter-character relationships (Iyyer et al., 2016; Chaturvedi et al., 2016, 2017), and social networks of characters (Elson et al., 2010; Elson, 2012; Agarwal et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned pro"
D17-1168,N16-1098,0,0.494061,"racter personas (Bamman et al., 2014; Valls-Vargas et al., 2015), interpersonal relationships (Chaturvedi, 2016), plotpatterns (Jockers, 2013), narrative structures (Finlayson, 2012). There has also been an interest in predicting what is expected to happen next in a piece of text (Chambers and Jurafsky, 2008). Human readers are good at filling-in-the-gaps or inferring information that is not explicitly stated in the text. However, computers are not yet able to match their performance on predicting what could be the likely next step in a given sequence of events described in a story. Recently, Mostafazadeh et al. (2016) introduced the story-cloze task for testing this ability, albeit without the aspect of language generation. This task requires choosing the correct ending to a given four sentences long story (also referred to as context) from two provided alternatives. Fig. 1 shows an example story consisting of a short context, and two ending options. In this work we address this story-cloze task. While the short nature and third person narrative style of these stories help us circumvent the problem of speaker identification and processing long 1603 Proceedings of the 2017 Conference on Empirical Methods in"
D17-1168,P15-1019,0,0.0475727,"Missing"
D17-1168,P16-1028,1,0.637548,": For a story, or any piece of text, to be coherent, it needs to describe a meaningful or ‘mutually entailing’ sequence of events (Chatman, 1980). For instance, in Figure 1 Wesley got angry → Wesley bit her hand → Wesley was scolded describes a more coherent sequence of events, as compared to Wesley got angry → Wesley bit her hand → Wesley got a cookie Prior work in script-learning attempts to model such prototypical sequence of events (usually captured through verbs). For this task, we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful. Peng and Roth (2016) recently proposed a neural SemLM approach, to model such sequence of events using a language model of FrameNet (Baker et al., 1998) frames that are evoked in the given text. It represents an event using the corresponding predicate frame and its sense, obtained using a Sematic Role Labeler (Punyakanok et al., 2004). It also extends the frame definition to include explicit discourse markers (such as but, and) since they model relationships between frames. For example, in Fig1604 ure 1, the SemLM representation for the last sentence of the context is ‘Get.01-and-bit.01’. Here, ‘01’ indicates spe"
D17-1168,D14-1162,0,0.116281,"or each option, we first align each of its topic-words with the most similar topic-word in one of the context-sentences, while defining the alignment score as this similarity value. We measure similarity between two words using the cosine similarity of their vector space representations (using 2 The reported segmentation process made sense from qualitative analysis on a random sample, and also led to superior performance compared to alternate strategies. 3 Polarities of ‘negated’ word were reversed (determined from neg dependency relation in the corresponding sentence). 1605 pretrained GloVe (Pennington et al., 2014) vectors). We then quantify the topical-closeness of an ending option with the context using averaged alignment score of its topic-words4 . For each ending option, we extract one feature whose value is this topical-closeness with the context. As before, we also include a binary comparative feature. 2.2 Hidden Coherence Model Sec. 2.1 described the three semantic consistency aspects and the corresponding features. We now describe our model which uses these features (represented as f~co in the rest of this paper) to identify the (in)coherent ending-option. The model is also ~ co , which will dep"
D17-1168,E14-1024,0,0.0547333,"et al., 2013, 2014; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). 4.2 Events-centered learning: Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Ba"
D17-1168,D08-1020,0,0.0232035,"ce: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story. 5 Conclusion Story comprehension is a complex Natural Language Understanding task involving linguistic intelligence as well as a semantic and social knowledge of the real world. This paper studies story"
D17-1168,C04-1197,1,0.532147,"Wesley bit her hand → Wesley got a cookie Prior work in script-learning attempts to model such prototypical sequence of events (usually captured through verbs). For this task, we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful. Peng and Roth (2016) recently proposed a neural SemLM approach, to model such sequence of events using a language model of FrameNet (Baker et al., 1998) frames that are evoked in the given text. It represents an event using the corresponding predicate frame and its sense, obtained using a Sematic Role Labeler (Punyakanok et al., 2004). It also extends the frame definition to include explicit discourse markers (such as but, and) since they model relationships between frames. For example, in Fig1604 ure 1, the SemLM representation for the last sentence of the context is ‘Get.01-and-bit.01’. Here, ‘01’ indicates specific predicate senses for verbs ‘get’ and ‘bit’ with ‘and’ being a discourse marker. Also, it produces ‘scold.01’ and ‘give.01’ for the correct and incorrect endings respectively. We train this language model using a log bilinear language model (Mnih and Hinton, 2007) on a collection of unannotated short stories ("
D17-1168,P06-2103,0,0.0126268,"and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006), syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011). The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story. 5 Conclusion Story comprehension is a complex Natural Language Understanding task involving linguistic int"
D17-1168,N15-1001,0,0.033377,"rafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework (Grosz et al., 1995) and focus on entities and their syntactic roles (Karamanis, 2003; Karamanis et al., 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Elsner and Charniak, 2008). Other approaches measure coherence using topic drift within a domain (Barzilay and Lee, 2004; Fung and Ngai, 2006), co-occurrence of wo"
D17-1168,H05-1044,0,0.0340227,"Missing"
D17-1168,P10-1100,0,0.0127976,"antic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts (Schank and Abelson, 1977; Mooney and DeJong, 1985). Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Orr et al., 2014; Pichotta and Mooney, 2014). Such methods for script-learning also include Bayesian approaches (Bejan, 2008; Frermann et al., 2014), sequence alignment algorithms (Regneri et al., 2010) and neural networks (Modi and Titov, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016). There has also been work on representing events in a structured manner using schemas, which are learned probabilistically (Chambers, 2013; Cheung et al., 2013; 1610 Nguyen et al., 2015), using graphs (Balasubramanian et al., 2013) or neural approaches (Titov and Khoddam, 2015). Recently, Ferraro and Durme (2016) presented a unified Bayesian model for scripts and frames. 4.3 Textual Coherence: Our work is also related to the study of coherence in discourse. A significant amount of prior wor"
D17-1168,W17-0907,0,0.0241821,"al. (2016) into validation and test sets of 1871 instances each for the Story-Cloze Task, and were used for training and evaluating our model. 3.2 Baselines We use the following baselines in our experiments: DSSM: (Mostafazadeh et al., 2016) It trains two deep neural networks (Huang et al., 2013) to project the context and the ending-options into the same vector space. Based on these vector representations, it predicts the ending-option with the largest cosine similarity with the context. Msap: The task addressed in this paper was also a shared task for an EACL’17 workshop and this baseline (Schwartz et al., 2017) represents the best performance reported on its leaderboard (Mostafazadeh et al., 2017). It trains a logistic regression based on stylistic and languagemodel based features. LR: Our next baseline is a simple logistic regression model which is agnostic to the fact that there are multiple types of aspects. Given a context and ending-options, it predicts the answer using the same features (Sec. 2.1) as the Hidden Coherence model but clubs them all into one feature-vector. Majority Vote: This ensemble method uses the features extracted for each of the K = 3 aspects, to train K separate logistic r"
D17-1168,C98-1013,0,\N,Missing
D17-1168,P08-2011,0,\N,Missing
D17-1168,W17-0906,0,\N,Missing
D17-1269,D16-1153,0,0.180432,"nd NER, and showed that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters were induced using large parallel corpora. Building on this work, T¨ackstr¨om (2012) focuses solely on NER, and includes experiments on self-training and multi-source transfer for NER. Tsai and Roth (2016) link words and phrases to entries in Wikipedia and use page categories as features. They showed that these wikifier features are strong language independent features. We build on this work, and use these features in our experiments. Bharadwaj et al. (2016) build a transfer model using phonetic features instead of lexical features. These features are not strictly languageindependent, but can work well when languages share vocabulary but with spelling variations, as in the case of Turkish, Uzbek, and Uyghur. 2.3 Others In a technique similar to ours, Carreras et al. (2003) use Spanish resources for Catalan NER. They translate the features in the weight vector, which has the flavor of a language independent model with the lexical features of a projection model. Our work is a natural extension of this paper, but explores these techniques on many mo"
D17-1269,E03-1038,0,0.158514,"Missing"
D17-1269,P11-1061,0,0.029267,"having almost no target language resources. 2 Related Work There are two main branches of work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religio"
D17-1269,D14-1096,0,0.0433053,"et language resources. 2 Related Work There are two main branches of work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the"
D17-1269,R11-1017,0,0.658382,"work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist in a large number of languages, but the d"
D17-1269,E14-1049,0,0.0172242,"Missing"
D17-1269,kamholz-etal-2014-panlex,0,0.0220943,"solution. Religious texts, such as the Bible and the Koran, exist in many languages, but the unique domain makes them hard to use. This leaves the vast majority of the world’s languages with no general method for NER. We propose a simple solution that requires only minimal resources. We translate annotated data in a high-resource language into a low-resource language, using just a lexicon.2 We refer to this as cheap translation, because in general, lexicons are much cheaper and easier to find than parallel text (Mausam et al., 2010). One of the biggest efforts at gathering lexicons is Panlex (Kamholz et al., 2014), which has lexicons for 10,000 language varieties available to download today. The quality and size of these dic1 https://www.ethnologue.com/enterprise-faq/how-manylanguages-world-are-unwritten-0 2 We use the terms ‘lexicon’ and ‘dictionary’ interchangeably. 2536 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2536–2545 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tionaries may vary, but in Section 5.3 we showed that even small dictionaries can give improvements. If there is no dictionary, or if the q"
D17-1269,P12-1073,0,0.223822,"o main branches of work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist in a large number o"
D17-1269,P07-2045,0,0.0115992,"Missing"
D17-1269,W01-0504,0,0.264115,"Missing"
D17-1269,K16-1022,1,0.653022,"dataset in a different language. Lexical or lexical-derived features are typically not used unless there is significant vocabulary overlap between languages. T¨ackstr¨om et al. (2012) experiments with direct transfer of dependency parsing and NER, and showed that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters were induced using large parallel corpora. Building on this work, T¨ackstr¨om (2012) focuses solely on NER, and includes experiments on self-training and multi-source transfer for NER. Tsai and Roth (2016) link words and phrases to entries in Wikipedia and use page categories as features. They showed that these wikifier features are strong language independent features. We build on this work, and use these features in our experiments. Bharadwaj et al. (2016) build a transfer model using phonetic features instead of lexical features. These features are not strictly languageindependent, but can work well when languages share vocabulary but with spelling variations, as in the case of Turkish, Uzbek, and Uyghur. 2.3 Others In a technique similar to ours, Carreras et al. (2003) use Spanish resources"
D17-1269,N16-1072,1,0.029984,"fication on a dataset in a different language. Lexical or lexical-derived features are typically not used unless there is significant vocabulary overlap between languages. T¨ackstr¨om et al. (2012) experiments with direct transfer of dependency parsing and NER, and showed that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters were induced using large parallel corpora. Building on this work, T¨ackstr¨om (2012) focuses solely on NER, and includes experiments on self-training and multi-source transfer for NER. Tsai and Roth (2016) link words and phrases to entries in Wikipedia and use page categories as features. They showed that these wikifier features are strong language independent features. We build on this work, and use these features in our experiments. Bharadwaj et al. (2016) build a transfer model using phonetic features instead of lexical features. These features are not strictly languageindependent, but can work well when languages share vocabulary but with spelling variations, as in the case of Turkish, Uzbek, and Uyghur. 2.3 Others In a technique similar to ours, Carreras et al. (2003) use Spanish resources"
D17-1269,D11-1006,0,0.144505,"hods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist in a large number of languages, but the domain is too far removed from typical target domains (such as newswire) to be useful. As a simple example, the Bible contains"
D17-1269,H01-1035,0,0.516069,"w a good score, despite having almost no target language resources. 2 Related Work There are two main branches of work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions"
D17-1269,D16-1135,0,0.102007,"NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist in a large number of languages, but the domain is too far remove"
D17-1269,D08-1063,0,0.100405,"language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist in a large number of languages, but the domain is too far removed from typical target domains (such as newswire"
D17-1269,W09-1119,1,0.237956,"Tamil, and Yoruba.5 We use the same set of test documents as used in Tsai et al. (2016). We also use Hindi and Malayalam data from FIRE 2013,6 pre-processed to contain only PER, ORG, and LOC tags. While several of these languages are decidedly high-resource, we limit the resources used in order to show that our techniques will work in truly low-resource settings. In practice, this means generating training data where high-quality manually annotated data is already available, and using dictionaries where translation is available. 4.3 NER Model In all of our work we use the Illinois NER system (Ratinov and Roth, 2009) with standard features (forms, capitalization, affixes, word prior, word after, etc.) as our base model. We train Brown clusters on the entire Wikipedia dump for any given language (again, any monolingual corpus will do), and include the multilingual gazetteers and wikifier features proposed in Tsai et al. (2016). 5 Experiments We performed two different sets of experiments: first translating only from English, then translating from additional languages selected to be similar to the target language. 5.1 Translation from English We start by translating from the highest resourced language in th"
D17-1269,Q14-1005,0,0.0495817,"elated Work There are two main branches of work in crosslingual NLP: projection across parallel data, and language independent methods. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011; Ni and Florian, 2016), mention detection (Zitouni and Florian, 2008), and parsing (Hwa et al., 2005; McDonald et al., 2011). For NER, the received wisdom is that parallel projection methods work very well, although there is no consensus on the necessary size of the parallel corpus. Most approaches require millions of sentences, with a few exceptions which require thousands. Accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences. Religious texts (such as the Bible and the Koran) exist i"
D17-1269,W12-1908,0,0.357903,"Missing"
D17-1269,N12-1052,0,0.361155,"Missing"
D17-1269,W02-2024,0,0.0679052,"1 Lexicons We use lexicons provided by (Rolston and Kirchhoff, 2016), which are harvested from PanLex, Wiktionary, and various other sources. There are 103 lexicons, each mapping between English and a target language. These vary in size from 56K entries to 1.36M entries, as shown in the second row of Table 2. There are also noisy translations. Some entries consist of a single English letter, some are morphological endings, others are misspellings, others are obscure translations of metaphors, and still others are just wrong. 4.2 Datasets We use data from CoNLL2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The 4 languages represented are English, German, Spanish, and Dutch. All training is on the train set, and testing is on the test set (TestB). The evaluation metric for all experiments is phrase level F1, as explained in Tjong Kim Sang (2002). In order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008), and LORELEI projects. From LORELEI, we use Turkish and Hausa 4 From REFLEX, we use Bengali, Tamil, and Yoruba.5 We use the same set of test documents as used in Tsai et al. (2016). We also use Hindi and"
D17-1284,W10-3503,0,0.0259316,") Wikifier (Ratinov et al., 2011), an unsupervised linker that uses hand-crafted features to rank candidates, (3) Vinculum (Ling et al., 2015), a modular, unsupervised pipeline system, (4) AIDA (Hoffart et al., 2011), a supervised linker trained on CoNLL data and uses hand-crafted features, and (5) BerkCNN (FrancisLandau et al., 2016), a recent neural supervised approach that has variants that use hand-crafted features. Evaluation Setup We evaluate our approach on the following four datasets: CoNLL-YAGO (Hoffart et al., 2011), ACE 2004 (NIST, 2004; Ratinov et al., 2011), ACE 2005 (NIST, 2005; Bentivogli et al., 2010), and Wikipedia (Ratinov et al., 2011). For each of these datasets, we use the standard test/development splits, but do not use any information from the training splits. End-to-end entity linking systems such as Vinculum and Wikifier perform an NER-style F1 evaluation where CoNLL Test Dev ACE05 Wiki Plato (Sup) Plato (Semi-Sup) AIDA* BerkCNN:Sparse* BerkCNN:CNN* BerkCNN:Full* 79.7 86.4 81.8 74.9 81.2 85.5 86.91 - 83.6 84.5 89.9 81.5 75.7 82.2 Priors Model C Model CD Model CT Model CDT Model CDTE 68.5 81.4 81.0 82.3 82.5 82.9 70.9 83.4 83.2 83.9 85.6 84.9 81.1 83.7 85.8 86.5 86.8 85.6 78.1 86.1"
D17-1284,E06-1002,0,0.0628074,"s of information used about the entities. Many existing approaches use links and information from Wikipedia as the only source of supervision to build the entity linking system. These approaches use sparse entity and mention-context representations, such as, based on the Wikipedia categories (Cucerzan, 2007), weighted bag of words in the entity description and mention context (Kulkarni et al., 2009; Ratinov et al., 2011), hand crafted features based on partial string matches, punctuations in entity name (McNamee et al., 2009), etc. Heuristics (Mihalcea and Csomai, 2007) or linear classifiers (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011; McNamee et al., 2009) are used over these features to rank entity candidates for linking. Recently, neural models have been proposed as a way to support better generalization over the sparse features; e.g., using feedforward networks on bag-of-words of the entity context (He et al., 2013), or using entity-class information from KB (Sun et al., 2015). Some models ignore the entity’s description on Wikipedia, but rather, only rely on the context from links to learn entity representations (Lazic et al., 2015), or use a pipeline of existing annotators to fil"
D17-1284,D13-1184,1,0.898979,"edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015)"
D17-1284,D07-1074,0,0.125256,"∗ Work performed while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the short"
D17-1284,E17-1013,0,0.0171427,"t optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Embedding Entity Information Knowledge bases contain different kinds of information about entities such as textual description, linked mentions (in Wikipedia), and types (in Freebase). For accurate linking, it is often necessary to combine information from these various sources. Here, we describe our model that encodes information a"
D17-1284,Q14-1037,0,0.200923,"oth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these as"
D17-1284,N16-1150,0,0.44249,"match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015), do not make use of the local context of the mention (“plays” and “match”) while others, such as Berke"
D17-1284,P16-1059,0,0.0570157,"Landau et al., 2016), that uses CNNs operating over different granularity of entity and mention contexts, also follows this training regime and trains separate models for each dataset. Such approaches can be prohibitive in many applications as it encourages the model to over-fit to the peculiarities of different datasets and domains. Other forms of information, apart from descriptions, and context from linked data, are also utilized for linking. Many approaches perform joint inference over the linking decisions in a document (Milne and Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Globerson et al., 2016), identify mentions that do not link to any existing entity (NIL) (Bunescu and Pasca, 2006; Ratinov et al., 2011), and cluster NIL-mentions (Wick et al., 2013; Lazic et al., 2015) to discover new entities. Few approaches jointly model entity linking, and other related NLP tasks to improve linking, such as, coreference resolution (Hajishirzi et al., 2013), relational inference (Cheng and Roth, 2013), and joint coreference with typing (Durrett and Klein, 2014). In our model, we use fine-grained type information of the entity as an auxiliary distant supervision to improve mention-context represen"
D17-1284,D13-1029,0,0.0281151,"rmation, apart from descriptions, and context from linked data, are also utilized for linking. Many approaches perform joint inference over the linking decisions in a document (Milne and Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Globerson et al., 2016), identify mentions that do not link to any existing entity (NIL) (Bunescu and Pasca, 2006; Ratinov et al., 2011), and cluster NIL-mentions (Wick et al., 2013; Lazic et al., 2015) to discover new entities. Few approaches jointly model entity linking, and other related NLP tasks to improve linking, such as, coreference resolution (Hajishirzi et al., 2013), relational inference (Cheng and Roth, 2013), and joint coreference with typing (Durrett and Klein, 2014). In our model, we use fine-grained type information of the entity as an auxiliary distant supervision to improve mention-context representation but do not use intermediate typing decisions for linking. Many approaches that learn entity embeddings for other applications have also been proposed, 2682 India_cricket_team ve Ldesc description embedding Description Encoder The Indian cricket team, also known as Team India, represents India in international cricket. Letype Ltext Description Obje"
D17-1284,P13-2006,0,0.755536,"s to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015), do not make use of the local context of the mention ("
D17-1284,D11-1072,0,0.376212,"Missing"
D17-1284,Q15-1023,1,0.167138,"ty, India cricket team, one needs to use ∗ Work performed while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an"
D17-1284,spitkovsky-chang-2012-cross,0,0.223056,"of each mention with the vector representations of each entity candidate, and combine the results from the two sources for making linking decisions. A typical KB contains millions of entities, which makes it prohibitively expensive to compute a similarity score between each mention and all entities in the KB. Prior work has shown that, for a given mention, aggressively pruning the set of possible entities to a small subset hurts performance only negligibly, while making the linker extremely efficient. For each mention m, we generate a set of candidate entities Cm = {cj } ⊂ E using CrossWikis (Spitkovsky and Chang, 2012), a dictionary computed from a Google crawl of the web that stores the frequency with which a mention links to a particular entity. To generate Cm we choose the top−30 entities for each mention string, and normalize this frequency across the chosen candidates to compute Pprior (e|m). In the literature, such a dictionary is often built from the anchor links in Wikipedia (Ratinov et al., 2011; Hoffart et al., 2011) but Ling et al. (2015) show using CrossWikis gives improved prior scores and candidate recall. For each mention m, we use our learned mention-context encoder from § 3.1 to encode the"
D17-1284,C16-1218,0,0.177748,"ers for the different sources of information about the entity, and encourage the entity embedding to be similar to all of the encoded representations. A key requirement for information extraction systems is their ability to work across texts from 1 The source code and the datasets are available at https://nitishgupta.github.io/neural-el 2681 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2681–2690 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics various domains. Some methods (Francis-Landau et al., 2016; Nguyen et al., 2016; Hoffart et al., 2011) train parameters on domain-specific linked data, thus hampering their ability to generalize to new domains. By only making use of indirect supervision that is available in Wikipedia/Freebase, we refrain from using domain specific training data, and produce a domain-independent linking system. Our comprehensive evaluation on recent entity linking benchmarks reveals that the resulting entity linker compares favorably to state-of-the-art systems across datasets, even those that have handengineered features or use dataset-specific training. We hence show that our model not"
D17-1284,D14-1162,0,0.115857,"is Wikipedia (dump dated 2016/09/20). We use existing links in Wikipedia, with the anchors as mentions, and links as the true entity, as input to the context encoder (see § 3.1). As the description of each entity (§ 3.2), we use the first 100 tokens of the entity’s Wikipedia page 2685 (same as Francis-Landau et al. (2016)). To obtain entity types (see § 3.3), we extract the types for each entity from Freebase and map them to the 112 fine-grained types introduced by Ling and Weld (2012). For context and description encoders, we use pre-trained 300-dimensional case-sensitive word embeddings by Pennington et al. (2014) as the first layer that is not updated during training. Hyper-parameters We perform coarse-grained tuning of the hyper-parameters using a fraction of the training data. The vectors for the entities, types, contexts, and descriptions are of size d = 200. The size of the local context encoder LSTM hidden layer l, local context output, and the document-context encoder output Dm is set to 100(= l = Dm ). The document context vocabulary contains |VG |= 1.5 million strings. We use dropout (Srivastava et al., 2014) with a probability of 0.4. Additionally, we use word-dropout where we replace a rando"
D17-1284,P11-1138,1,0.821844,"d while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several exis"
D17-1284,P16-1136,0,0.0154861,"r sports_team Figure 1: Overview of the Model (§ 3): Each entity has a Wikipedia description, linked mentions in Wikipedia (only one shown), and fine-grained types from Freebase (only one shown). We encode local and document-level mention contexts (§ 3.1), entity-description (§ 3.2), and fine-grained entity-types (§ 3.3 & § 3.4). Joint optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Emb"
D17-1284,N16-1103,0,0.0132203,"Overview of the Model (§ 3): Each entity has a Wikipedia description, linked mentions in Wikipedia (only one shown), and fine-grained types from Freebase (only one shown). We encode local and document-level mention contexts (§ 3.1), entity-description (§ 3.2), and fine-grained entity-types (§ 3.3 & § 3.4). Joint optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Embedding Entity Inform"
D17-1284,E17-1119,0,0.0913543,"t|e) |E| 0 e∈E 3.4 t∈Te t ∈T / e Type-Aware Context Representation, T Apart from being able to represent the types of the entities, it is also important for our linker to be able to represent the type information at the mention level. In the example in Fig. 1, although the mention “India” is prominently used to refer to the country, it is evident from the sentence that it refers to a Sports Team. The context-encoder captures this information in an unstructured manner, thus it will be useful for the encoder to directly utilize this supervision. This is a similar setup as Ling et al. (2015) and Shimaoka et al. (2017) that use noisy distant supervision to train a fine-grained type predictor for mentions. In order for the context encoders, and type embeddings to directly inform each other, we introduce an objective Lmtype between every vm and vt if type t belongs to Te for the entity e that m refers to. This objective is similar to Letype from § 3.3. 3.5 Learning Unified Entity Representations In the sections above we described different encoder models to capture entity-context information (local- and document-level), entity-description from a KB, and fine-grained types in a single entity representation vec"
D17-1284,Q15-1036,0,\N,Missing
D17-1284,E17-1058,0,\N,Missing
D18-1010,D14-1059,0,0.089761,"d over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference. Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as claim verification problem, if we treat (question, correct answer) as a claim. However, little work has studied how well a QA system can identify all the answer evidence. Only a few works considered improving the evidence presentation in claim verification problems. 2 By “two-wing optimization”, we mean that t"
D18-1010,D12-1050,0,0.0172774,"he claims are shared by two modules. Then we update sji , the representation of word sji , by element-wise addition: sji = sji ⊕ cji (12) This step enables the word sji to “see” all related clues from Se . The reason we add sji and cji is motivated by a simple experience: Assume the claim “Lily lives in the biggest city in Canada”, and one sentence contains a clue “· · · Lily lives in Toronto · · · ” and another sentence contains a clue “· · · Toronto is Canada’s largest city· · · ”. The most simple yet effective approach to aggregating the two clues is to sum up their representation vectors (Blacoe and Lapata, 2012) (we do not concatenate them, as those clues have no consistent textual order across different sji ). After updating the representation of each word in si , we perform the aforementioned “singlechannel fine-grained representation” between the updated si and the claim x, generating [e, x]. 4 4.1 Experiments Setup Dataset. In this work, we use FEVER (Thorne et al., 2018). The claims in FEVER were generated from the introductory parts of about 50K Loss function. For the claim verification input (Se , x), we forward its representation [e, x] to a 109 83.67 ... Wiki page retrieval4 . For each claim"
D18-1010,P09-2015,1,0.903428,"nrih bjrbn areb ahofjrf no enough info false Marilyn Monroe worked with Warner Brothers true Canada is the second largest country ... conﬂicting Figure 1, aims at identifying text snippets in the corpus that act as evidence that supports or refutes the claim. This problem has broad applications. For example, knowledge bases (KB), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), can be augmented with a new relational statement such as “(Afghanistan, is source of, Kushan Dynasty)”. This needs to be first verified by a claim verification process and supported by evidence (Roth et al., 2009; Chaganty et al., 2017). More broadly, claim verification is a key component in any technical solution addressing recent concerns about the trustworthiness of online content (Vydiswaran et al., 2011; Pasternack and Roth, 2013; Hovy et al., 2013). In both scenarios, we care about whether or not a claim holds, and seek reliable evidence in support of this decision. Evidential claim verification requires that we address three challenges. First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim. This differs from the conventio"
D18-1010,N18-1074,0,0.352786,"efine the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference. Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as claim verification problem, if we treat (question,"
D18-1010,D17-1109,0,0.0233167,"fjrf no enough info false Marilyn Monroe worked with Warner Brothers true Canada is the second largest country ... conﬂicting Figure 1, aims at identifying text snippets in the corpus that act as evidence that supports or refutes the claim. This problem has broad applications. For example, knowledge bases (KB), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), can be augmented with a new relational statement such as “(Afghanistan, is source of, Kushan Dynasty)”. This needs to be first verified by a claim verification process and supported by evidence (Roth et al., 2009; Chaganty et al., 2017). More broadly, claim verification is a key component in any technical solution addressing recent concerns about the trustworthiness of online content (Vydiswaran et al., 2011; Pasternack and Roth, 2013; Hovy et al., 2013). In both scenarios, we care about whether or not a claim holds, and seek reliable evidence in support of this decision. Evidential claim verification requires that we address three challenges. First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim. This differs from the conventional textual entailment ("
D18-1010,W14-2508,0,0.22289,"decision space Y for the claim verification. In the optimal condition, a one-hot vector over Y indicates which decision to make towards the claim, and a binary vector over S indicates a subset of sentences Se (in blue in Figure 2) to act as evidence. Prior work mostly approached this problem as a pipeline procedure – first, given a claim x, determine Se by some similarity matching; then, conduct textual entailment over (Se , x) pairs. Our framework, T WOW ING OS, optimizes the two 2 Related Work Most work focuses on the dataset construction while lacking advanced models to handle the problem. Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a simila"
D18-1010,P17-1171,0,0.0502516,"presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference. Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as claim verification problem, if we treat (question, correct answer) as a claim. However, little work has studied how well a QA system can identify all the answer evidence. Only a few works considered improving the evidence presentation in claim verification problems. 2 By “two-wing optimization”, we mean that the same object, i.e., the claim, is mapped into two target spaces in a joint optimization scheme. 106 To start, a piece of text t (t ∈ S ∪ {x}) is represented as a sequence of l hidden states, forming a feature map T ∈ Rd×l , where d is the dime"
D18-1010,N16-1138,0,0.0913405,"s which decision to make towards the claim, and a binary vector over S indicates a subset of sentences Se (in blue in Figure 2) to act as evidence. Prior work mostly approached this problem as a pipeline procedure – first, given a claim x, determine Se by some similarity matching; then, conduct textual entailment over (Se , x) pairs. Our framework, T WOW ING OS, optimizes the two 2 Related Work Most work focuses on the dataset construction while lacking advanced models to handle the problem. Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some w"
D18-1010,P17-2067,0,0.193172,"some similarity matching; then, conduct textual entailment over (Se , x) pairs. Our framework, T WOW ING OS, optimizes the two 2 Related Work Most work focuses on the dataset construction while lacking advanced models to handle the problem. Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, pred"
D18-1010,N13-1132,0,0.0291168,"tes the claim. This problem has broad applications. For example, knowledge bases (KB), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), can be augmented with a new relational statement such as “(Afghanistan, is source of, Kushan Dynasty)”. This needs to be first verified by a claim verification process and supported by evidence (Roth et al., 2009; Chaganty et al., 2017). More broadly, claim verification is a key component in any technical solution addressing recent concerns about the trustworthiness of online content (Vydiswaran et al., 2011; Pasternack and Roth, 2013; Hovy et al., 2013). In both scenarios, we care about whether or not a claim holds, and seek reliable evidence in support of this decision. Evidential claim verification requires that we address three challenges. First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim. This differs from the conventional textual entailment (TE) problem (Dagan et al., 2013) as here we first look for the premises given a hypothesis. Clearly, the evidence one seeks depends on the claim, as well as on the eventual entailment Introduction A claim, e.g., “Marilyn M"
D18-1010,W16-0103,1,0.896779,"Missing"
D18-1010,I17-1097,0,0.0131747,"e the problem. Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning. Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference. Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as"
D18-1010,D16-1244,0,0.191308,"Missing"
D18-1046,P17-1183,0,0.105968,"ent to develop generation models which can handle input for which the transliteration does not belong in N . Our Work We show that a weak generation model can be iteratively improved using constrained discovery. In particular, our work uses a weak generation model to discover new training pairs, using constraints to drive the bootstrapping. Our generation model is inspired by the success of sequence to sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2015) for string transduction tasks like inflection and derivation generation (Faruqui et al., 2016; Cotterell et al., 2017; Aharoni and Goldberg, 2017; Makarov et al., 2017). Our bootstrapping framework can be viewed as an instance of constraint driven learning (Chang et al., 2007, 2012). 3 Transliteration Generation with Hard Monotonic Attention - Seq2Seq(HMA) We view generation as a string transduction task and use a sequence to sequence (Seq2Seq) generation model that uses hard monotonic attention (Aharoni and Goldberg, 2017), henceforth referred to as Seq2Seq(HMA). During generation, Seq2Seq(HMA) directly models the monotonic source-to-target sequence alignments, using a pointer that attends to a single input character at a time. Monoto"
D18-1046,W16-2711,0,0.0172234,"eneration model, giving it wide applicability. Through case studies, we showed that collecting an adequate seed list is practical with a few hours of annotation. The benefit of incorporating our transliteration approach in a downstream task, namely candidate generation, was also demonstrated. Finally, we discussed some of the inherent challenges of learning transliteration and the deficits of existing training sets. There are several interesting directions for future work. Performing model combination, either by developing hybrid transliteration models (Nicolai et al., 2015) or by ensembling (Finch et al., 2016), can further improve low resource transliteration. Jointly leveraging similarities between related languages, such as writing systems or phonetic properties (Kunchukuttan et al., 2018), also shows promise for low-resource settings. Our analysis suggests value in revisiting “transliteration in context” approaches (Goto et al., 2003; Hermjakob et al., 2008), especially for languages like Hebrew. We would also like to expand on the analyses provided in §7 which uncover challenges inherent to the transliteration task, particularly the impact of the native/foreign distinction in the train and test"
D18-1046,2003.mtsummit-papers.17,0,0.163802,"t challenges of learning transliteration and the deficits of existing training sets. There are several interesting directions for future work. Performing model combination, either by developing hybrid transliteration models (Nicolai et al., 2015) or by ensembling (Finch et al., 2016), can further improve low resource transliteration. Jointly leveraging similarities between related languages, such as writing systems or phonetic properties (Kunchukuttan et al., 2018), also shows promise for low-resource settings. Our analysis suggests value in revisiting “transliteration in context” approaches (Goto et al., 2003; Hermjakob et al., 2008), especially for languages like Hebrew. We would also like to expand on the analyses provided in §7 which uncover challenges inherent to the transliteration task, particularly the impact of the native/foreign distinction in the train and test data, the difficulties posed by specific scripts or pairs of scripts, and how these impact both backand forward-transliteration. Recent work from Merhav and Ash (2018) suggests many useful analyses that we would like to incorporate. Acknowledgments The authors thank Mitch Marcus, Snigdha Chaturvedi, Stephen Mayhew, Nitish Gupta, D"
D18-1046,P04-1021,0,0.192055,"a single native informant. Second, in §8.2 we show that our approach benefits a typical downstream application, namely candidate generation for cross-lingual entity linking, by improving recall on two low-resource languages – Tigrinya and Macedonian. We also present an analysis (§7) of the inherent challenges of transliteration, and the trade-off between native (i.e., source) and foreign (i.e., target) vocabulary. 2 Related Work We briefly review the limitations of existing generation and discovery approaches, and provide an overview of how our work addresses them. Transliteration Generation (Haizhou et al., 2004; Jiampojamarn et al., 2009; Ravi and Knight, 2009; Jiampojamarn et al., 2010; Finch et al., 2015, inter alia) requires generous amount of name pairs (≈5-10k) in order to learn to map words in the source script to the target script. While some approaches (Irvine et al., 2010; Tsai and Roth, 2018) use Wikipedia inter-language links to identify name pairs for supervision, a truly lowresource language (like Tigrinya) is likely to have limited Wikipedia presence as well. Transliteration Discovery (Sproat et al., 2006; Chang et al., 2009) is considerably easier than generation, owing to the smaller"
D18-1046,N09-1034,1,0.733026,"of how our work addresses them. Transliteration Generation (Haizhou et al., 2004; Jiampojamarn et al., 2009; Ravi and Knight, 2009; Jiampojamarn et al., 2010; Finch et al., 2015, inter alia) requires generous amount of name pairs (≈5-10k) in order to learn to map words in the source script to the target script. While some approaches (Irvine et al., 2010; Tsai and Roth, 2018) use Wikipedia inter-language links to identify name pairs for supervision, a truly lowresource language (like Tigrinya) is likely to have limited Wikipedia presence as well. Transliteration Discovery (Sproat et al., 2006; Chang et al., 2009) is considerably easier than generation, owing to the smaller search space. However, discovery often uses features derived from resources that are unavailable for low-resource languages, like comparable corpora (Sproat et al., 2006; Klementiev and Roth, 2008). A key limitation of discovery is the assumption that the correct transliteration(s) is in the list of candidates N . Since discovery models always pick something from N , they can produce false positives, if no correct transliteration is present in N . To overcome this, it is prudent to develop generation models which can handle input fo"
D18-1046,P07-1036,1,0.859562,"eneration model can be iteratively improved using constrained discovery. In particular, our work uses a weak generation model to discover new training pairs, using constraints to drive the bootstrapping. Our generation model is inspired by the success of sequence to sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2015) for string transduction tasks like inflection and derivation generation (Faruqui et al., 2016; Cotterell et al., 2017; Aharoni and Goldberg, 2017; Makarov et al., 2017). Our bootstrapping framework can be viewed as an instance of constraint driven learning (Chang et al., 2007, 2012). 3 Transliteration Generation with Hard Monotonic Attention - Seq2Seq(HMA) We view generation as a string transduction task and use a sequence to sequence (Seq2Seq) generation model that uses hard monotonic attention (Aharoni and Goldberg, 2017), henceforth referred to as Seq2Seq(HMA). During generation, Seq2Seq(HMA) directly models the monotonic source-to-target sequence alignments, using a pointer that attends to a single input character at a time. Monotonic attention is a natural fit for transliteration because even though the number of characters needed to represent a sound in the"
D18-1046,2010.amta-papers.12,0,0.267302,"on with Constrained Discovery for Low-Resource Languages Shyam Upadhyay Jordan Kodner University of Pennsylvania University of Pennsylvania Philadelphia, PA Philadelphia, PA shyamupa@seas.upenn.edu jkodner@seas.upenn.edu Abstract 2008) which involves selecting an appropriate transliteration for a word from a list of candidates. This work develops transliteration generation approaches for low-resource languages. Existing transliteration generation models require supervision in the form of source-target name pairs (≈5-10k), which are often collected from names in Wikipedia inter-language links (Irvine et al., 2010). However, most languages that use non-Latin scripts are under-represented in terms of such resources. Table 1 illustrates this issue, and the extra coverage one can achieve by extending to low-resource languages. A model that requires 50k name pairs as supervision can only support 6 languages, while one that just needs 500 could support 56. For a model to be widely applicable, it must function in low-resource settings. Generating the English transliteration of a name written in a foreign script is an important and challenging step in multilingual knowledge acquisition and information extracti"
D18-1046,D17-1074,0,0.0268114,"vercome this, it is prudent to develop generation models which can handle input for which the transliteration does not belong in N . Our Work We show that a weak generation model can be iteratively improved using constrained discovery. In particular, our work uses a weak generation model to discover new training pairs, using constraints to drive the bootstrapping. Our generation model is inspired by the success of sequence to sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2015) for string transduction tasks like inflection and derivation generation (Faruqui et al., 2016; Cotterell et al., 2017; Aharoni and Goldberg, 2017; Makarov et al., 2017). Our bootstrapping framework can be viewed as an instance of constraint driven learning (Chang et al., 2007, 2012). 3 Transliteration Generation with Hard Monotonic Attention - Seq2Seq(HMA) We view generation as a string transduction task and use a sequence to sequence (Seq2Seq) generation model that uses hard monotonic attention (Aharoni and Goldberg, 2017), henceforth referred to as Seq2Seq(HMA). During generation, Seq2Seq(HMA) directly models the monotonic source-to-target sequence alignments, using a pointer that attends to a single input"
D18-1046,P13-1153,0,0.161981,"s to crosslingual candidate generation for entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.1 1 Introduction Transliteration is the process of transducing names from one writing system to another (e.g., ओबामा in Devanagari to Obama in Latin script) while preserving their pronunciation (Knight and Graehl, 1998; Karimi et al., 2011). In particular, back-transliteration from foreign languages to English has applications in multilingual knowledge acquisition tasks including named entity recognition (Darwish, 2013) and information retrieval (Virga and Khudanpur, 2003). Two tasks feature prominently in the transliteration literature: generation (Knight and Graehl, 1998) which involves producing an appropriate transliteration for a given word in an open-ended way, and discovery (Sproat et al., 2006; Klementiev and Roth, 1 Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu # Name Pairs in Wikipedia Languages Scripts > 50, 000 > 10, 000 > 5, 000 Previous Work > 1, 000 Our Approach > 500 >0 6 18 24 45 56 93 5 14 15 22 23 30 Table 1: Cumulative number of person name pairs in Wikipedia"
D18-1046,N16-1077,0,0.0273127,"is present in N . To overcome this, it is prudent to develop generation models which can handle input for which the transliteration does not belong in N . Our Work We show that a weak generation model can be iteratively improved using constrained discovery. In particular, our work uses a weak generation model to discover new training pairs, using constraints to drive the bootstrapping. Our generation model is inspired by the success of sequence to sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2015) for string transduction tasks like inflection and derivation generation (Faruqui et al., 2016; Cotterell et al., 2017; Aharoni and Goldberg, 2017; Makarov et al., 2017). Our bootstrapping framework can be viewed as an instance of constraint driven learning (Chang et al., 2007, 2012). 3 Transliteration Generation with Hard Monotonic Attention - Seq2Seq(HMA) We view generation as a string transduction task and use a sequence to sequence (Seq2Seq) generation model that uses hard monotonic attention (Aharoni and Goldberg, 2017), henceforth referred to as Seq2Seq(HMA). During generation, Seq2Seq(HMA) directly models the monotonic source-to-target sequence alignments, using a pointer that a"
D18-1046,W15-3909,0,0.0234345,"application, namely candidate generation for cross-lingual entity linking, by improving recall on two low-resource languages – Tigrinya and Macedonian. We also present an analysis (§7) of the inherent challenges of transliteration, and the trade-off between native (i.e., source) and foreign (i.e., target) vocabulary. 2 Related Work We briefly review the limitations of existing generation and discovery approaches, and provide an overview of how our work addresses them. Transliteration Generation (Haizhou et al., 2004; Jiampojamarn et al., 2009; Ravi and Knight, 2009; Jiampojamarn et al., 2010; Finch et al., 2015, inter alia) requires generous amount of name pairs (≈5-10k) in order to learn to map words in the source script to the target script. While some approaches (Irvine et al., 2010; Tsai and Roth, 2018) use Wikipedia inter-language links to identify name pairs for supervision, a truly lowresource language (like Tigrinya) is likely to have limited Wikipedia presence as well. Transliteration Discovery (Sproat et al., 2006; Chang et al., 2009) is considerably easier than generation, owing to the smaller search space. However, discovery often uses features derived from resources that are unavailable"
D18-1046,W09-3504,0,0.0697064,"Missing"
D18-1046,W10-2405,0,0.0231468,"efits a typical downstream application, namely candidate generation for cross-lingual entity linking, by improving recall on two low-resource languages – Tigrinya and Macedonian. We also present an analysis (§7) of the inherent challenges of transliteration, and the trade-off between native (i.e., source) and foreign (i.e., target) vocabulary. 2 Related Work We briefly review the limitations of existing generation and discovery approaches, and provide an overview of how our work addresses them. Transliteration Generation (Haizhou et al., 2004; Jiampojamarn et al., 2009; Ravi and Knight, 2009; Jiampojamarn et al., 2010; Finch et al., 2015, inter alia) requires generous amount of name pairs (≈5-10k) in order to learn to map words in the source script to the target script. While some approaches (Irvine et al., 2010; Tsai and Roth, 2018) use Wikipedia inter-language links to identify name pairs for supervision, a truly lowresource language (like Tigrinya) is likely to have limited Wikipedia presence as well. Transliteration Discovery (Sproat et al., 2006; Chang et al., 2009) is considerably easier than generation, owing to the smaller search space. However, discovery often uses features derived from resources"
D18-1046,N07-1047,0,0.0575328,"Missing"
D18-1046,P12-1073,0,0.0307231,"for Armenian). We collected about 600 and 500 annotated pairs respectively for Armenian and Punjabi. Table 5 shows that the performance of the models trained on the annotated data is comparable to that on the standard test corpora for other languages. This show that our approach is robust to human inconsistencies and regional spelling variations, and that obtaining an adequate seed list is possible with just a few hours of manual annotation. 8.2 Candidate Generation (CG) Since transliteration is an intermediate step in many downstream multilingual information extraction tasks (Darwish, 2013; Kim et al., 2012; Jeong et al., 1999; Virga and Khudanpur, 2003; Chen et al., 2006), it is possibly to gauge its performance extrinsically by the impact it has on such tasks. We use the task of candidate generation (CG), which is a key step in cross-lingual entity linking. The goal of cross-lingual entity linking (McNamee et al., 2011; Tsai and Roth, 2016; Upadhyay et al., 2018) is to ground spans of text written in any language to an entity in a knowledge base (KB). For instance, grounding [Chicago] in the following German sentence to Chicago_(band).8 [Chicago] wird in Woodstock aufzutreten. The role of CG i"
D18-1046,J98-4003,0,0.515796,"tators in a matter of hours. This opens the task to languages for which large number of training examples are unavailable. We evaluate transliteration generation performance itself, as well the improvement it brings to crosslingual candidate generation for entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.1 1 Introduction Transliteration is the process of transducing names from one writing system to another (e.g., ओबामा in Devanagari to Obama in Latin script) while preserving their pronunciation (Knight and Graehl, 1998; Karimi et al., 2011). In particular, back-transliteration from foreign languages to English has applications in multilingual knowledge acquisition tasks including named entity recognition (Darwish, 2013) and information retrieval (Virga and Khudanpur, 2003). Two tasks feature prominently in the transliteration literature: generation (Knight and Graehl, 1998) which involves producing an appropriate transliteration for a given word in an open-ended way, and discovery (Sproat et al., 2006; Klementiev and Roth, 1 Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu # Name"
D18-1046,Q18-1022,0,0.0217699,"ncorporating our transliteration approach in a downstream task, namely candidate generation, was also demonstrated. Finally, we discussed some of the inherent challenges of learning transliteration and the deficits of existing training sets. There are several interesting directions for future work. Performing model combination, either by developing hybrid transliteration models (Nicolai et al., 2015) or by ensembling (Finch et al., 2016), can further improve low resource transliteration. Jointly leveraging similarities between related languages, such as writing systems or phonetic properties (Kunchukuttan et al., 2018), also shows promise for low-resource settings. Our analysis suggests value in revisiting “transliteration in context” approaches (Goto et al., 2003; Hermjakob et al., 2008), especially for languages like Hebrew. We would also like to expand on the analyses provided in §7 which uncover challenges inherent to the transliteration task, particularly the impact of the native/foreign distinction in the train and test data, the difficulties posed by specific scripts or pairs of scripts, and how these impact both backand forward-transliteration. Recent work from Merhav and Ash (2018) suggests many us"
D18-1046,W16-2701,0,0.496839,"hm 1 in Aharoni and Goldberg (2017), with the characterlevel alignment between x1:n and y1:m being generated using the algorithm in Cotterell et al. (2016). Inference Strategies We describe an unconstrained and a constrained inference strategy to select the best transliteration yˆ from a beam {yi }ki=1 of transliteration hypotheses, sorted in descending order by likelihood. The constrained strategy use a name dictionary N , to guide the inference. These strategies are applicable to any generation model. Other Strategies in Previous Work A related constrained inference strategy was proposed by Lin et al. (2016), who use a entity linking system (Wang et al., 2015) to correct and re-rank hypotheses, using any available context to aid hypothesis correction. Our constrained inference strategy is much simpler, requiring only a name dictionary N . We experimentally show that our approach outperforms that of Lin et al. (2016). 4 Low-Resource Bootstrapping Low-resource languages will have a limited number of name pairs for training a generation model. To learn a good generation model in this setting, we propose a new bootstrapping algorithm, that uses constrained discovery to mine name pairs to re-train the"
D18-1046,I11-1029,0,0.258422,"and regional spelling variations, and that obtaining an adequate seed list is possible with just a few hours of manual annotation. 8.2 Candidate Generation (CG) Since transliteration is an intermediate step in many downstream multilingual information extraction tasks (Darwish, 2013; Kim et al., 2012; Jeong et al., 1999; Virga and Khudanpur, 2003; Chen et al., 2006), it is possibly to gauge its performance extrinsically by the impact it has on such tasks. We use the task of candidate generation (CG), which is a key step in cross-lingual entity linking. The goal of cross-lingual entity linking (McNamee et al., 2011; Tsai and Roth, 2016; Upadhyay et al., 2018) is to ground spans of text written in any language to an entity in a knowledge base (KB). For instance, grounding [Chicago] in the following German sentence to Chicago_(band).8 [Chicago] wird in Woodstock aufzutreten. The role of CG in cross-lingual entity linking is to create a set of plausible entities given a string while ensuring the correct KB entity belongs to that set. For the above German sentence, it would provide a list of possible KB entities for the string Chicago: Chicago_(band), Chicago_(city), Chicago_(font), etc., so that entity lin"
D18-1046,C18-1053,0,0.0693477,"properties (Kunchukuttan et al., 2018), also shows promise for low-resource settings. Our analysis suggests value in revisiting “transliteration in context” approaches (Goto et al., 2003; Hermjakob et al., 2008), especially for languages like Hebrew. We would also like to expand on the analyses provided in §7 which uncover challenges inherent to the transliteration task, particularly the impact of the native/foreign distinction in the train and test data, the difficulties posed by specific scripts or pairs of scripts, and how these impact both backand forward-transliteration. Recent work from Merhav and Ash (2018) suggests many useful analyses that we would like to incorporate. Acknowledgments The authors thank Mitch Marcus, Snigdha Chaturvedi, Stephen Mayhew, Nitish Gupta, Dan Deutsch, and the anonymous reviewers for their useful comments. We are grateful to the Armenian and Punjabi annotators for help with the case studies. This work was supported under DARPA LORELEI by Contract HR0011-15-2-0025, Agreement HR0011-15-2-0023 with DARPA, and an NDSEG fellowship for the second author. Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the"
D18-1046,W15-3911,0,0.0177855,"our bootstrapping algorithm admits any generation model, giving it wide applicability. Through case studies, we showed that collecting an adequate seed list is practical with a few hours of annotation. The benefit of incorporating our transliteration approach in a downstream task, namely candidate generation, was also demonstrated. Finally, we discussed some of the inherent challenges of learning transliteration and the deficits of existing training sets. There are several interesting directions for future work. Performing model combination, either by developing hybrid transliteration models (Nicolai et al., 2015) or by ensembling (Finch et al., 2016), can further improve low resource transliteration. Jointly leveraging similarities between related languages, such as writing systems or phonetic properties (Kunchukuttan et al., 2018), also shows promise for low-resource settings. Our analysis suggests value in revisiting “transliteration in context” approaches (Goto et al., 2003; Hermjakob et al., 2008), especially for languages like Hebrew. We would also like to expand on the analyses provided in §7 which uncover challenges inherent to the transliteration task, particularly the impact of the native/for"
D18-1046,N09-1005,0,0.0275026,"w that our approach benefits a typical downstream application, namely candidate generation for cross-lingual entity linking, by improving recall on two low-resource languages – Tigrinya and Macedonian. We also present an analysis (§7) of the inherent challenges of transliteration, and the trade-off between native (i.e., source) and foreign (i.e., target) vocabulary. 2 Related Work We briefly review the limitations of existing generation and discovery approaches, and provide an overview of how our work addresses them. Transliteration Generation (Haizhou et al., 2004; Jiampojamarn et al., 2009; Ravi and Knight, 2009; Jiampojamarn et al., 2010; Finch et al., 2015, inter alia) requires generous amount of name pairs (≈5-10k) in order to learn to map words in the source script to the target script. While some approaches (Irvine et al., 2010; Tsai and Roth, 2018) use Wikipedia inter-language links to identify name pairs for supervision, a truly lowresource language (like Tigrinya) is likely to have limited Wikipedia presence as well. Transliteration Discovery (Sproat et al., 2006; Chang et al., 2009) is considerably easier than generation, owing to the smaller search space. However, discovery often uses featu"
D18-1046,P06-1010,0,0.28999,"m to another (e.g., ओबामा in Devanagari to Obama in Latin script) while preserving their pronunciation (Knight and Graehl, 1998; Karimi et al., 2011). In particular, back-transliteration from foreign languages to English has applications in multilingual knowledge acquisition tasks including named entity recognition (Darwish, 2013) and information retrieval (Virga and Khudanpur, 2003). Two tasks feature prominently in the transliteration literature: generation (Knight and Graehl, 1998) which involves producing an appropriate transliteration for a given word in an open-ended way, and discovery (Sproat et al., 2006; Klementiev and Roth, 1 Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu # Name Pairs in Wikipedia Languages Scripts > 50, 000 > 10, 000 > 5, 000 Previous Work > 1, 000 Our Approach > 500 >0 6 18 24 45 56 93 5 14 15 22 23 30 Table 1: Cumulative number of person name pairs in Wikipedia inter-language links. While previous approaches for transliteration generation were applicable to only 24 languages (spanning 15 scripts), our approach is applicable to 56 languages (23 scripts). When counting scripts we exclude variants (e.g., all Cyrillic scripts and variants count a"
D18-1046,N16-1072,1,0.772389,"variations, and that obtaining an adequate seed list is possible with just a few hours of manual annotation. 8.2 Candidate Generation (CG) Since transliteration is an intermediate step in many downstream multilingual information extraction tasks (Darwish, 2013; Kim et al., 2012; Jeong et al., 1999; Virga and Khudanpur, 2003; Chen et al., 2006), it is possibly to gauge its performance extrinsically by the impact it has on such tasks. We use the task of candidate generation (CG), which is a key step in cross-lingual entity linking. The goal of cross-lingual entity linking (McNamee et al., 2011; Tsai and Roth, 2016; Upadhyay et al., 2018) is to ground spans of text written in any language to an entity in a knowledge base (KB). For instance, grounding [Chicago] in the following German sentence to Chicago_(band).8 [Chicago] wird in Woodstock aufzutreten. The role of CG in cross-lingual entity linking is to create a set of plausible entities given a string while ensuring the correct KB entity belongs to that set. For the above German sentence, it would provide a list of possible KB entities for the string Chicago: Chicago_(band), Chicago_(city), Chicago_(font), etc., so that entity linking can select the b"
D18-1046,D18-1270,1,0.778046,"obtaining an adequate seed list is possible with just a few hours of manual annotation. 8.2 Candidate Generation (CG) Since transliteration is an intermediate step in many downstream multilingual information extraction tasks (Darwish, 2013; Kim et al., 2012; Jeong et al., 1999; Virga and Khudanpur, 2003; Chen et al., 2006), it is possibly to gauge its performance extrinsically by the impact it has on such tasks. We use the task of candidate generation (CG), which is a key step in cross-lingual entity linking. The goal of cross-lingual entity linking (McNamee et al., 2011; Tsai and Roth, 2016; Upadhyay et al., 2018) is to ground spans of text written in any language to an entity in a knowledge base (KB). For instance, grounding [Chicago] in the following German sentence to Chicago_(band).8 [Chicago] wird in Woodstock aufzutreten. The role of CG in cross-lingual entity linking is to create a set of plausible entities given a string while ensuring the correct KB entity belongs to that set. For the above German sentence, it would provide a list of possible KB entities for the string Chicago: Chicago_(band), Chicago_(city), Chicago_(font), etc., so that entity linking can select the band. Foreign scripts pos"
D18-1046,W03-1508,0,0.114617,"entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.1 1 Introduction Transliteration is the process of transducing names from one writing system to another (e.g., ओबामा in Devanagari to Obama in Latin script) while preserving their pronunciation (Knight and Graehl, 1998; Karimi et al., 2011). In particular, back-transliteration from foreign languages to English has applications in multilingual knowledge acquisition tasks including named entity recognition (Darwish, 2013) and information retrieval (Virga and Khudanpur, 2003). Two tasks feature prominently in the transliteration literature: generation (Knight and Graehl, 1998) which involves producing an appropriate transliteration for a given word in an open-ended way, and discovery (Sproat et al., 2006; Klementiev and Roth, 1 Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu # Name Pairs in Wikipedia Languages Scripts > 50, 000 > 10, 000 > 5, 000 Previous Work > 1, 000 Our Approach > 500 >0 6 18 24 45 56 93 5 14 15 22 23 30 Table 1: Cumulative number of person name pairs in Wikipedia inter-language links. While previous approaches for t"
D18-1046,D15-1081,0,0.0812639,"terlevel alignment between x1:n and y1:m being generated using the algorithm in Cotterell et al. (2016). Inference Strategies We describe an unconstrained and a constrained inference strategy to select the best transliteration yˆ from a beam {yi }ki=1 of transliteration hypotheses, sorted in descending order by likelihood. The constrained strategy use a name dictionary N , to guide the inference. These strategies are applicable to any generation model. Other Strategies in Previous Work A related constrained inference strategy was proposed by Lin et al. (2016), who use a entity linking system (Wang et al., 2015) to correct and re-rank hypotheses, using any available context to aid hypothesis correction. Our constrained inference strategy is much simpler, requiring only a name dictionary N . We experimentally show that our approach outperforms that of Lin et al. (2016). 4 Low-Resource Bootstrapping Low-resource languages will have a limited number of name pairs for training a generation model. To learn a good generation model in this setting, we propose a new bootstrapping algorithm, that uses constrained discovery to mine name pairs to re-train the generation model. Our algorithm requires a small (≈5"
D18-1046,P97-1017,0,\N,Missing
D18-1231,E17-1075,0,0.522235,"ork (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In"
D18-1231,P18-1009,0,0.0978441,"ng and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervisedlearning paradigm an"
D18-1231,W16-3002,0,0.0607381,"Missing"
D18-1231,C02-1130,0,0.395516,"mains across a broad range of coarse- and fine-grained typing datasets, and it outperforms these systems in out-of-domain settings. 2 Related Work Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutu2066 1 https://github.com/CogComp/zoe ally exclusive types, has a considerable amount of work (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road to"
D18-1231,W03-0425,0,0.233223,"Missing"
D18-1231,C96-1079,0,0.44365,"not require training on entity-typing-specific supervised data. • The proposed system outperforms existing zero-shot entity typing systems. • Our system is competitive with fullysupervised systems in their respective domains across a broad range of coarse- and fine-grained typing datasets, and it outperforms these systems in out-of-domain settings. 2 Related Work Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutu2066 1 https://github.com/CogComp/zoe ally exclusive types, has a considerable amount of work (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka"
D18-1231,N06-2015,0,0.0452626,"Missing"
D18-1231,D12-1082,0,0.0607887,"Missing"
D18-1231,W02-0109,0,0.269391,"rained types (Ling and Weld, 2012). It is understood that semantic typing is a key component in many natural language understanding tasks, including Question Answering (Toral et al., 2005; Li and Roth, 2005) and Textual Entailment (Dagan et al., 2010, 2013). Consequently, the ability to type mentions semantically across domains and text genres, and to use a flexible type hierarchy, is essential for solving many important challenges. Nevertheless, most commonly used approaches and systems for semantic typing (e.g., C ORE NLP (Manning et al., 2014), C OG C OMP NLP (Khashabi et al., 2018), NLTK (Loper and Bird, 2002), S PAC Y) are trained in a supervised fashion and rely on high quality, taskspecific annotation. Scaling such systems to other domains and to a larger set of entity types faces fundamental restrictions. Coarse typing systems, which are mostly fully supervised, are known to fit a single dataset very well. However, their performance drops significantly on different text genres and even new data sets. Moreover, adding a new coarse type requires manual annotation and retraining. For finetyping systems, people have adopted a distantsupervision approach. Nevertheless, the number of types used is sm"
D18-1231,C16-1017,0,0.147162,"set. The work of Yuan and Downey (2018) can also be seen in the same spirit, i.e., systems that rely on a form of representation of the labels. In a broader sense, such works–including ours– are part of a more general line of work on zeroshot learning (Chang et al., 2008; Palatucci et al., 2009; Norouzi et al., 2013; Romera-Paredes and Torr, 2015; Song and Roth, 2014). Our work can Approach Zero-shot? Use labeled data? ATTENTIVE (Shimaoka et al., 2017) No Yes AAA (Abhishek et al., 2017) No Yes NFETC- HIER ( R ) (Xu and Barbosa, 2018) No Yes AFET (Ren et al., 2016) No Yes (partial) P ROTO LE (Ma et al., 2016) Yes Prototype Embedding Yes (partial) OT YPER (Yuan and Downey, 2018) Yes Word Embedding Yes (partial) (Huang et al., 2016) Yes Concept-embedding Clustering No Z OE (ours) Yes Type-Compatible Concepts No Table 1: Comparison of recent work on entity typing. Our system does not require any labeled data for entity typing; therefore it works on new datasets without retraining. be thought of as the continuation of the same research direction. A critical step in the design of zero-shot systems is the characterization of the output space. For supervised systems, the output representations are trivia"
D18-1231,P14-5010,0,0.00247939,"types (Tjong Kim Sang and De Meulder, 2003) to over a hundred fine-grained types (Ling and Weld, 2012). It is understood that semantic typing is a key component in many natural language understanding tasks, including Question Answering (Toral et al., 2005; Li and Roth, 2005) and Textual Entailment (Dagan et al., 2010, 2013). Consequently, the ability to type mentions semantically across domains and text genres, and to use a flexible type hierarchy, is essential for solving many important challenges. Nevertheless, most commonly used approaches and systems for semantic typing (e.g., C ORE NLP (Manning et al., 2014), C OG C OMP NLP (Khashabi et al., 2018), NLTK (Loper and Bird, 2002), S PAC Y) are trained in a supervised fashion and rely on high quality, taskspecific annotation. Scaling such systems to other domains and to a larger set of entity types faces fundamental restrictions. Coarse typing systems, which are mostly fully supervised, are known to fit a single dataset very well. However, their performance drops significantly on different text genres and even new data sets. Moreover, adding a new coarse type requires manual annotation and retraining. For finetyping systems, people have adopted a dist"
D18-1231,E99-1001,0,0.222777,"y-typing-specific supervised data. • The proposed system outperforms existing zero-shot entity typing systems. • Our system is competitive with fullysupervised systems in their respective domains across a broad range of coarse- and fine-grained typing datasets, and it outperforms these systems in out-of-domain settings. 2 Related Work Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutu2066 1 https://github.com/CogComp/zoe ally exclusive types, has a considerable amount of work (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and B"
D18-1231,U08-1016,0,0.0353465,"FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervisedlearning paradigm and instead merely rely on a general entity linking corpus and the signals in Wikipedia. Prior work has already shown the importance of Wikipedia information for NER. Tsai et al. (2016a) use a cross-lingual W IKIFIER to facilitate cross-lingual NER. However, they do not explicitly address the case where the target entity does not exist in Wikipedia. The zero-shot paradigm for entity ty"
D18-1231,E09-1070,0,0.0630158,"Missing"
D18-1231,N18-1202,0,0.0269558,"KS. For this step, assume we have a representation that encodes the sentential information anchored on the mention. We denote this mention-aware context representation as SentRep(s|m). We define a measure of consistency between a concept c and a mention m in a sentence s: Consistency(c, s, m) = cosine(SentRep(s|m), ConceptRep(c)), (1) where ConceptRep(c) is representation of a concept: ConceptRep(c) ,   avgs SentRep(s|c) s ∈ W IKI L INKS, c ∈ s) , which is the average vector of the representation of all the sentences in W IKI L INKS that describe the given concept. We use pre-trained ELM O (Peters et al., 2018), a state-of-the-art contextual and mentionaware word representation. In order to generate SentRep(s|m), we run ELM O on sentence s, where the tokens of the mention m are concatenated with “ ”, and retrieve its ELM O vector as SentRep(s|m). According to the consistency measure, we select the top `ELM O concepts for each mention. We call this set of concepts CELM O . 3.3 Surface-Based Concept Generation While context often is a key signal for typing, one should not ignore the information included in the surface form of the mentions. If the corresponding concept or entity exists in Wikipedia, ma"
D18-1231,W09-1119,1,0.618062,"o-shot entity typing systems. • Our system is competitive with fullysupervised systems in their respective domains across a broad range of coarse- and fine-grained typing datasets, and it outperforms these systems in out-of-domain settings. 2 Related Work Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutu2066 1 https://github.com/CogComp/zoe ally exclusive types, has a considerable amount of work (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mos"
D18-1231,P11-1138,1,0.728582,"ur algorithm does not require a given mention to be in Wikipedia; in fact, in many cases (such as nominal mentions) the mentions are not available in Wikipedia. We hypothesize that any entity possible in English corresponds to some type-compatible entities in Wikipedia. We can then rely mostly on the context to reveal a set of compatible titles, those that are likely to share semantic types with the target mention. The fact that our system is not required to ground to the exact concept is a key difference between our grounding and “standard” Wikification approaches (Mihalcea and Csomai, 2007; Ratinov et al., 2011). As a consequence, while entity linking approaches rely heavily on priors associated with the surface forms and do not consider those that do not link to Wikipedia titles, our system mostly relies on context, regardless of whether the grounding actually exists or not. Figure 1 shows a high-level visualization of our system. Given a mention, our system grounds it into type-compatible entries in Wikipedia. The target mention “Oarnniwsf,” is not in Wikipedia, yet it is grounded to entities with approximately correct types. In addition, while some of the grounded Wikipedia entries are inaccurate"
D18-1231,D16-1144,0,0.199671,"With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervisedlearning paradigm and instead merely rely on a general entity linking corpus and the signals in Wikipedia. Prior work has already shown the importance of Wikipedia information for NER. Tsai et al. (2016a) use a cross-lingual W IKIFIER to facilitate cross-lingual NER. However, they do not explicitly address the case where the target entity does not exist in Wikipedia. The zero-shot paradigm for entity typing has only recently bee"
D18-1231,sekine-etal-2002-extended,0,0.149836,"of coarse- and fine-grained typing datasets, and it outperforms these systems in out-of-domain settings. 2 Related Work Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutu2066 1 https://github.com/CogComp/zoe ally exclusive types, has a considerable amount of work (Grishman and Sundheim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing"
D18-1231,E17-1119,0,0.33391,"eim, 1996; Mikheev et al., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not"
D18-1231,W03-0419,0,0.723349,"Missing"
D18-1231,K16-1022,1,0.90464,"rds zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervisedlearning paradigm and instead merely rely on a general entity linking corpus and the signals in Wikipedia. Prior work has already shown the importance of Wikipedia information for NER. Tsai et al. (2016a) use a cross-lingual W IKIFIER to facilitate cross-lingual NER. However, they do not explicitly address the case where the target entity does not exist in Wikipedia. The zero-shot paradigm for entity typing has only recently been studied. Yogatama et al. (2015) proposed an embedding representation for userdefined features and labels, which facilitates information sharing among labels and reduces the dependence on the labels observed in the training set. The work of Yuan and Downey (2018) can also be seen in the same spirit, i.e., systems that rely on a form of representation of the labels. I"
D18-1231,N18-1002,0,0.630179,"l., 1999; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Ratinov and Roth, 2009). There have been many proposals to scale the systems to support a bigger type space (Fleischman and Hovy, 2002; Sekine et al., 2002). This direction was followed by the introduction of datasets with large label-sets, either manually annotated like BBN (Weischedel and Brunstein, 2005) or distantly supervised like FIGER (Ling and Weld, 2012). With larger datasets available, supervised-learning systems were proposed to learn from the data (Yosef et al., 2012; Abhishek et al., 2017; Shimaoka et al., 2017; Xu and Barbosa, 2018; Choi et al., 2018). Such systems have achieved remarkable success, mostly when restricted to their observed domain and labels. There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers (Nadeau et al., 2006), or using the anchor texts in Wikipedia (Nothman et al., 2008, 2009). Ren et al. (2016) project labels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervised"
D18-1231,P15-2048,0,0.0821171,"bels in highdimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervisedlearning paradigm and instead merely rely on a general entity linking corpus and the signals in Wikipedia. Prior work has already shown the importance of Wikipedia information for NER. Tsai et al. (2016a) use a cross-lingual W IKIFIER to facilitate cross-lingual NER. However, they do not explicitly address the case where the target entity does not exist in Wikipedia. The zero-shot paradigm for entity typing has only recently been studied. Yogatama et al. (2015) proposed an embedding representation for userdefined features and labels, which facilitates information sharing among labels and reduces the dependence on the labels observed in the training set. The work of Yuan and Downey (2018) can also be seen in the same spirit, i.e., systems that rely on a form of representation of the labels. In a broader sense, such works–including ours– are part of a more general line of work on zeroshot learning (Chang et al., 2008; Palatucci et al., 2009; Norouzi et al., 2013; Romera-Paredes and Torr, 2015; Song and Roth, 2014). Our work can Approach Zero-shot? Use"
D18-1231,C12-2133,0,0.109332,"Missing"
D18-1270,Q16-1031,0,0.195692,"h we refer as its document context. X ELMS is trained using grounded mentions in multiple languages (English and Tamil in Figure 2a), which can be derived from Wikipedia (§4.1). 2.1 Mention Context Representation To learn from mention contexts in multiple languages, we generate mention context representations using a language-agnostic mention context encoder. An overview of the mention context encoder is shown in Figure 2b. Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders. Multilingual Word Embeddings (Ammar et al., 2016b; Smith et al., 2017; Duong et al., 2017) jointly encode words in multiple (≥2) languages in the same vector space such that semantically similar words in the same language, and translationally equivalent words in different languages are close (per cosine similarity). Multilingual embeddings generalize bilingual embeddings, which do the same for two languages only. We use FAST T EXT (Bojanowski et al., 2017; Smith et al., 2017), which aligns monolingual embeddings of multiple languages in the same space using a small dictionary (∼2500 pairs) from each language to English. Both monolingual emb"
D18-1270,Q17-1010,0,0.0205654,"s shown in Figure 2b. Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders. Multilingual Word Embeddings (Ammar et al., 2016b; Smith et al., 2017; Duong et al., 2017) jointly encode words in multiple (≥2) languages in the same vector space such that semantically similar words in the same language, and translationally equivalent words in different languages are close (per cosine similarity). Multilingual embeddings generalize bilingual embeddings, which do the same for two languages only. We use FAST T EXT (Bojanowski et al., 2017; Smith et al., 2017), which aligns monolingual embeddings of multiple languages in the same space using a small dictionary (∼2500 pairs) from each language to English. Both monolingual embeddings and the dictionary can be easily obtained for languages with limited resources. We denote the multilingual word embeddings for a set of tokens {w1 , w2 , · · · , wn } by w1:n = {w1 , w2 , · · · , wn }, where each wi ∈ Rd . r <latexit sha1_base64=""juw6eQUUFM3i37Bw8Ysq5tbV1Ao="">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh"
D18-1270,D13-1184,1,0.837827,"e prior probabilities for candidates are unlikely to exist, in contrast to findings in previous work that focused on high-resource languages. We also show how in low-resource settings, X ELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided. Several future research directions remain open. For all XEL approaches, the task of candidate generation is currently limited by existence of a target language Wikipedia and remains a key challenge. A joint inference framework which enforces coherent predictions (Cheng and Roth, 2013; Globerson et al., 2016; Ganea and Hofmann, 2017) could also lead to further improvements for XEL. Similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings. Acknowledgments The authors would like to thank Snigdha Chaturvedi, Anne Cocos, Stephen Mayhew, ChenTse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch, John Hewitt and the anonymous EMNLP reviewers for their useful comments and suggestions. This work was supported by Contract HR001115-2-0025 and Agreement HR0011-15-2-0023 with the US Defense Advanced Research Pr"
D18-1270,E17-1084,0,0.155017,"S is trained using grounded mentions in multiple languages (English and Tamil in Figure 2a), which can be derived from Wikipedia (§4.1). 2.1 Mention Context Representation To learn from mention contexts in multiple languages, we generate mention context representations using a language-agnostic mention context encoder. An overview of the mention context encoder is shown in Figure 2b. Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders. Multilingual Word Embeddings (Ammar et al., 2016b; Smith et al., 2017; Duong et al., 2017) jointly encode words in multiple (≥2) languages in the same vector space such that semantically similar words in the same language, and translationally equivalent words in different languages are close (per cosine similarity). Multilingual embeddings generalize bilingual embeddings, which do the same for two languages only. We use FAST T EXT (Bojanowski et al., 2017; Smith et al., 2017), which aligns monolingual embeddings of multiple languages in the same space using a small dictionary (∼2500 pairs) from each language to English. Both monolingual embeddings and the dictionary can be easily o"
D18-1270,I11-1029,0,0.288882,"the respective Wikipedias. Tamil Wikipedia only has 9 mentions referring to Liverpool_F.C., whereas English Wikipedia has 5303 such mentions. Clearly, there is a need to augment the limited contextual evidence in low-resource languages with evidence from high-resource languages like English. Tamil sentence translates to “Suarez plays for [Liverpool] and Uruguay.” Introduction Entity Linking (EL) systems ground entity mentions in text to entries in Knowledge Bases (KB), such as Wikipedia (Mihalcea and Csomai, 2007). Recently, the task of Cross-lingual Entity Linking (XEL) has gained attention (McNamee et al., 2011; Ji et al., 2015; Tsai and Roth, 2016) with the goal of grounding entity mentions written in any language to the English Wikipedia. For instance, Figure 1 shows a Tamil (a language with >70 million speakers) and an English mention (shown [enclosed]) 1 Code at www.github.com/shyamupa/xelms Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu and their mention contexts. XEL involves grounding the Tamil mention (which translates to ‘Liverpool’) to the football club Liverpool_F.C., and not the city or the university. XEL enables knowledge acquisition directly from documents"
D18-1270,P17-1178,0,0.35072,"andidate generation outputs a list of candidate entities C(m) = {e1 , e2 , · · · , eK } of size at most K (we use K=20), each associated with a prior probability Pprior (ei |m) indicating the probability of m referring to ei , given only m’s surface. Pprior is estimated from counts over the training mentions. We adopt Tsai and Roth (2016)’s candidate generation strategy with some minor modifications (Appendix A). Using other approaches like CrossWikis (Spitkovsky and Chang, 2012), lead to consistently worse recall. We note that transliteration based candidate generation (McNamee et al., 2011; Pan et al., 2017; Tsai and Roth, 2018; Upadhyay et al., 2018) can further improve recall. 3.2 Inference We combine the context conditional entity probability Pcontext (e |m) (eq. 5) and prior probability Pprior (e |m) by taking their union: Pmodel (e |m) = Pprior (e |m) + Pcontext (e |m) − Pprior (e |m) × Pcontext (e |m) Inference for the mention m picks the entity, 2 We use the type vocabulary Γ from Ling and Weld (2012), which contains 112 fine-grained types (|Γ |= 112) 2489 eˆ = arg max Pmodel (e |m) e∈C(m) (8) 3.3 Training Objective When only training the mention context encoder and entity vectors, we min"
D18-1270,N16-1150,0,0.0436688,"Missing"
D18-1270,P11-1138,1,0.881353,"al., 2011). The test set was collected by using parallel document collections, and then crowd-sourcing the ground truths. All the test mentions in this dataset consists of person-names only. TH-Test A subset of the dataset used in (Tsai and Roth, 2016), derived from Wikipedia.3 The mentions in the dataset fall in two categories – easy and hard, where hard mentions are those for which the most likely candidate according to the prior probability (i.e., arg max Pprior (e |m)) is not the correct title. Indeed, most Wikipedia mentions can be correctly linked by selecting the most likely candidate (Ratinov et al., 2011). We use all the hard mentions from Tsai and Roth (2016)’s test splits for each language, and collectively call this subset TH-T EST. TAC15-Test TAC 2015 (Ji et al., 2015) dataset for Chinese and Spanish. It contains documents from discussion forum articles and news. We evaluate all models using linking accuracy on gold mentions, and assume gold mentions are provided at test time. Table 2 summarizes the different domains of the evaluation datasets. Tuning We avoid any dataset-specific tuning, instead tuning on a development set and applying the same parameters across all datasets. All tunable"
D18-1270,D17-1277,0,0.0305214,"ely to exist, in contrast to findings in previous work that focused on high-resource languages. We also show how in low-resource settings, X ELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided. Several future research directions remain open. For all XEL approaches, the task of candidate generation is currently limited by existence of a target language Wikipedia and remains a key challenge. A joint inference framework which enforces coherent predictions (Cheng and Roth, 2013; Globerson et al., 2016; Ganea and Hofmann, 2017) could also lead to further improvements for XEL. Similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings. Acknowledgments The authors would like to thank Snigdha Chaturvedi, Anne Cocos, Stephen Mayhew, ChenTse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch, John Hewitt and the anonymous EMNLP reviewers for their useful comments and suggestions. This work was supported by Contract HR001115-2-0025 and Agreement HR0011-15-2-0023 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release"
D18-1270,P16-1059,0,0.050512,"for candidates are unlikely to exist, in contrast to findings in previous work that focused on high-resource languages. We also show how in low-resource settings, X ELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided. Several future research directions remain open. For all XEL approaches, the task of candidate generation is currently limited by existence of a target language Wikipedia and remains a key challenge. A joint inference framework which enforces coherent predictions (Cheng and Roth, 2013; Globerson et al., 2016; Ganea and Hofmann, 2017) could also lead to further improvements for XEL. Similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings. Acknowledgments The authors would like to thank Snigdha Chaturvedi, Anne Cocos, Stephen Mayhew, ChenTse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch, John Hewitt and the anonymous EMNLP reviewers for their useful comments and suggestions. This work was supported by Contract HR001115-2-0025 and Agreement HR0011-15-2-0023 with the US Defense Advanced Research Projects Agency (DARPA). A"
D18-1270,D17-1284,1,0.939415,"ector g and the entity vector e, Pcontext (e |m) = exp(gT e) P exp(gT e0 ) (5) e0 ∈C(m) where C(m) denotes all candidate entities of the mention m (§3.1 explains how C(m) is generated). We minimize the negative log-likelihood of Pcontext (e |m) with respect to the gold entity e∗ against the candidate entities C(m), and call it the Entity-Context loss (EC-L OSS), EC-L OSS = − log P (e∗ |m) Pcontext Pcontext (e0 |m) (6) e0 ∈C(m) 2.2 Including Type Information Incorporating the fine-grained types of a mention m can help rank entities of the appropriate type higher than others (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018). For instance, knowing the correct type of mention [Liverpool] as sports_team and constraining linking to entities with the relevant type, encourages disambiguation to the correct entity. To make the mention context representation g type-aware, we predict the set of fine-grained types of m, T(m) = {t1 , ..., t|T(m) |} using g. Each ti belongs to a pre-defined type vocabulary Γ.2 The probability of a type t belonging to T(m) given the mention context is defined as P(t |m) = σ(tT g), where σ is the sigmoid function and t is the learnable embedding for type t. We define"
D18-1270,spitkovsky-chang-2012-cross,0,0.041827,"ate generation identifies a small number of plausible entities for a mention m to avoid brute force comparison with all KB entities. Given m, candidate generation outputs a list of candidate entities C(m) = {e1 , e2 , · · · , eK } of size at most K (we use K=20), each associated with a prior probability Pprior (ei |m) indicating the probability of m referring to ei , given only m’s surface. Pprior is estimated from counts over the training mentions. We adopt Tsai and Roth (2016)’s candidate generation strategy with some minor modifications (Appendix A). Using other approaches like CrossWikis (Spitkovsky and Chang, 2012), lead to consistently worse recall. We note that transliteration based candidate generation (McNamee et al., 2011; Pan et al., 2017; Tsai and Roth, 2018; Upadhyay et al., 2018) can further improve recall. 3.2 Inference We combine the context conditional entity probability Pcontext (e |m) (eq. 5) and prior probability Pprior (e |m) by taking their union: Pmodel (e |m) = Pprior (e |m) + Pcontext (e |m) − Pprior (e |m) × Pcontext (e |m) Inference for the mention m picks the entity, 2 We use the type vocabulary Γ from Ling and Weld (2012), which contains 112 fine-grained types (|Γ |= 112) 2489 eˆ"
D18-1270,N16-1072,1,0.211886,"dia only has 9 mentions referring to Liverpool_F.C., whereas English Wikipedia has 5303 such mentions. Clearly, there is a need to augment the limited contextual evidence in low-resource languages with evidence from high-resource languages like English. Tamil sentence translates to “Suarez plays for [Liverpool] and Uruguay.” Introduction Entity Linking (EL) systems ground entity mentions in text to entries in Knowledge Bases (KB), such as Wikipedia (Mihalcea and Csomai, 2007). Recently, the task of Cross-lingual Entity Linking (XEL) has gained attention (McNamee et al., 2011; Ji et al., 2015; Tsai and Roth, 2016) with the goal of grounding entity mentions written in any language to the English Wikipedia. For instance, Figure 1 shows a Tamil (a language with >70 million speakers) and an English mention (shown [enclosed]) 1 Code at www.github.com/shyamupa/xelms Dan Roth University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu and their mention contexts. XEL involves grounding the Tamil mention (which translates to ‘Liverpool’) to the football club Liverpool_F.C., and not the city or the university. XEL enables knowledge acquisition directly from documents in any language, without resorting to"
D18-1270,D18-1046,1,0.775939,"andidate entities C(m) = {e1 , e2 , · · · , eK } of size at most K (we use K=20), each associated with a prior probability Pprior (ei |m) indicating the probability of m referring to ei , given only m’s surface. Pprior is estimated from counts over the training mentions. We adopt Tsai and Roth (2016)’s candidate generation strategy with some minor modifications (Appendix A). Using other approaches like CrossWikis (Spitkovsky and Chang, 2012), lead to consistently worse recall. We note that transliteration based candidate generation (McNamee et al., 2011; Pan et al., 2017; Tsai and Roth, 2018; Upadhyay et al., 2018) can further improve recall. 3.2 Inference We combine the context conditional entity probability Pcontext (e |m) (eq. 5) and prior probability Pprior (e |m) by taking their union: Pmodel (e |m) = Pprior (e |m) + Pcontext (e |m) − Pprior (e |m) × Pcontext (e |m) Inference for the mention m picks the entity, 2 We use the type vocabulary Γ from Ling and Weld (2012), which contains 112 fine-grained types (|Γ |= 112) 2489 eˆ = arg max Pmodel (e |m) e∈C(m) (8) 3.3 Training Objective When only training the mention context encoder and entity vectors, we minimize the EC-L OSS averaged over all training"
D18-1270,Q15-1023,0,0.31847,"using its context vector g and the entity vector e, Pcontext (e |m) = exp(gT e) P exp(gT e0 ) (5) e0 ∈C(m) where C(m) denotes all candidate entities of the mention m (§3.1 explains how C(m) is generated). We minimize the negative log-likelihood of Pcontext (e |m) with respect to the gold entity e∗ against the candidate entities C(m), and call it the Entity-Context loss (EC-L OSS), EC-L OSS = − log P (e∗ |m) Pcontext Pcontext (e0 |m) (6) e0 ∈C(m) 2.2 Including Type Information Incorporating the fine-grained types of a mention m can help rank entities of the appropriate type higher than others (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018). For instance, knowing the correct type of mention [Liverpool] as sports_team and constraining linking to entities with the relevant type, encourages disambiguation to the correct entity. To make the mention context representation g type-aware, we predict the set of fine-grained types of m, T(m) = {t1 , ..., t|T(m) |} using g. Each ti belongs to a pre-defined type vocabulary Γ.2 The probability of a type t belonging to T(m) given the mention context is defined as P(t |m) = σ(tT g), where σ is the sigmoid function and t is the learnable embedding f"
D18-1345,W99-0612,0,0.488364,"axis) with different levels of CLM perplexities (x axis). The entity CLM gives a low average perplexity and small variance to entity tokens (left), while giving non-entity tokens much higher perplexity and higher variance (right). Introduction In English, there is strong empirical evidence that the character sequences that make up proper nouns tend to be distinctive. Even divorced of context, a human reader can predict that “hoekstenberger” is an entity, but “abstractually”2 is not. Some NER research explores the use of characterlevel features including capitalization, prefixes and suffixes (Cucerzan and Yarowsky, 1999; Ratinov and Roth, 2009), and character-level models (CLMs) (Klein et al., 2003) to improve the performance of NER, but to date there has been no systematic study isolating the utility of CLMs in capturing distinctions between name and non-name tokens in English or across other languages. We conduct an experimental assessment of the discriminative power of CLMs for a range of lan1 The code and resources for this publication can be found at: https://cogcomp.org/page/publication_ view/846 2 Not a real name or a real word. guages: English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Taga"
D18-1345,W03-0428,0,0.300777,"rage perplexity and small variance to entity tokens (left), while giving non-entity tokens much higher perplexity and higher variance (right). Introduction In English, there is strong empirical evidence that the character sequences that make up proper nouns tend to be distinctive. Even divorced of context, a human reader can predict that “hoekstenberger” is an entity, but “abstractually”2 is not. Some NER research explores the use of characterlevel features including capitalization, prefixes and suffixes (Cucerzan and Yarowsky, 1999; Ratinov and Roth, 2009), and character-level models (CLMs) (Klein et al., 2003) to improve the performance of NER, but to date there has been no systematic study isolating the utility of CLMs in capturing distinctions between name and non-name tokens in English or across other languages. We conduct an experimental assessment of the discriminative power of CLMs for a range of lan1 The code and resources for this publication can be found at: https://cogcomp.org/page/publication_ view/846 2 Not a real name or a real word. guages: English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Tagalog. These languages use a variety of scripts and orthographic conventions (for e"
D18-1345,N16-1030,0,0.275152,"uishing name tokens from non-name tokens, as illustrated by Figure 1, which shows perplexity histograms from a CLM trained on entity tokens. Our models use only individual tokens, but perform extremely well in spite of taking no account of word context. We then assess the utility of directly adding simple features based on this CLM implementation to an existing NER system, and show that they have a significant positive impact on performance across many of the languages we tried. By adding very simple CLM-based features to the system, our scores approach those of a state-of-the-art NER system (Lample et al., 2016) across multiple languages, demonstrating both the unique importance and the broad utility of this approach. 3073 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3073–3077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Language Entity Train Non-entity Entity Test Non-entity English Amharic Arabic Bengali Farsi Hindi Somali Tagalog 29,450 5,886 7,640 15,288 4,547 5,565 6,467 11,525 170,524 46,641 52,968 108,592 50,084 69,267 51,034 102,894 7,194 2,077 1,754 4,573 1,608 1,947 1,967 3,186 38,554 16,2"
D18-1345,P16-1028,1,0.842389,"s.” For example, “Obama” is an entity token, and is split into “O b a m a”. From these examples we learn a score measuring how likely it is that a sequence of characters forms an entity. At test time, we also split each word into characters and determine perplexity using the entity and nonentity CLMs. We assign the label corresponding to the lower perplexity CLM. We experiment with four different kinds of language model: N-gram model, Skip-gram model, Continuous Bag-of-Words model (CBOW), and Log-Bilinear model (LB). We demonstrate that the N-gram model is best suited for this task. Following Peng and Roth (2016), we implement N-gram using SRILM (Stolcke, 2002) with order 6 and Witten-Bell discounting.3 For Skip-Gram and CBOW CLMs, we use the Gensim implementation (Rehurek and Sojka, 2010) for training and inference, and we build the LB CLM using the OxLM toolkit (Baltescu et al., 2014). 2.2 Data To determine whether name identifiability applies to languages other than English, we conduct experiments on a range of languages for which we had previously gathered resources (such as Brown clusters): English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Tagalog. 3 We experimented with different orde"
D18-1345,W09-1119,1,0.585729,"of CLM perplexities (x axis). The entity CLM gives a low average perplexity and small variance to entity tokens (left), while giving non-entity tokens much higher perplexity and higher variance (right). Introduction In English, there is strong empirical evidence that the character sequences that make up proper nouns tend to be distinctive. Even divorced of context, a human reader can predict that “hoekstenberger” is an entity, but “abstractually”2 is not. Some NER research explores the use of characterlevel features including capitalization, prefixes and suffixes (Cucerzan and Yarowsky, 1999; Ratinov and Roth, 2009), and character-level models (CLMs) (Klein et al., 2003) to improve the performance of NER, but to date there has been no systematic study isolating the utility of CLMs in capturing distinctions between name and non-name tokens in English or across other languages. We conduct an experimental assessment of the discriminative power of CLMs for a range of lan1 The code and resources for this publication can be found at: https://cogcomp.org/page/publication_ view/846 2 Not a real name or a real word. guages: English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Tagalog. These languages use"
D18-1345,L16-1521,0,0.0172072,"English, we use the original splits from the ubiquitous CoNLL 2003 English dataset (Sang and Meulder, 2003), which is a newswire dataset annotated with Person (PER), Organization (ORG), Location (LOC) and Miscellaneous (MISC). To collect the list of entities and nonentities as the training data for the Entity and Non-Entity CLMs, we sample a large number of PER/ORG/LOC and non-entities from Wikipedia, using types derived from their corresponding FreeBase entities (Ling and Weld, 2012). For all other languages, we use a subset of the corpora from the LORELEI project annotated for the NER task (Strassel and Tracey, 2016). We build our entity list using the tokens labeled as entities in the training data, and our non-entity list from the remaining tokens. These two lists are then used to train two CLMs, as described above. Our datasets vary in size of entity and non-entity tokens, as shown in Table 1. The smallest, Farsi, has 4.5K entity and 50K non-entity tokens; the largest, English, has 29K entity and 170K nonentity tokens. 3 CLM for Named Entity Identification In this section, we first show the power of CLMs for distinguishing between entity and non-entity tokens in English, and then that this power is rob"
D18-2013,D17-1108,1,0.947125,"x is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1:February 27, 1998). Example 2: A car (e1:exploded) in the middle of a group of men playing volleyball. More than 10 people have (e2:died), police said. In this paper, we present CogCompTime (see Fig. 1), a tool with both the Timex and TempRel components, which are conceptually built on Zhao et al. (2012) and Ning et al. (2017), respectively. CogCompTime is a new implementation that integrates both components and also incorporates the most recent advances in this area (Ning et al., 2018a,b,c). Two highlights are: First, CogCompTime achieves comparable performance to state-of-the-art Timex systems, but is almost two times faster than the second fastest, HeidelTime (Str¨otgen and Gertz, 2010). Second, CogCompTime improves the performance of the 1 http://cogcomp.org/page/publication_ view/844 72 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 72–77 c"
D18-2013,N12-1049,0,0.0712435,"int out directions for future work and conclude this paper. 2 plain these main modules in detail. 2.1 Timex Component Existing work on Timex extraction and normalization falls into two categories: rule-based and learning-based. Rule-based systems use regular expressions to extract Timex in text and then deterministic rules to normalize them (e.g., HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012)). Learning-based systems use classification models to chunk out Timexes in text and normalize them based on grammar parsing (e.g., UWTime (Lee et al., 2014) and ParsingTime (Angeli et al., 2012)). CogCompTime adopts a mixed strategy: we use machine learning in the Timex extraction step and rule parsing in the normalization step. This mixed strategy, while maintaining a state-of-the-art performance, significantly improves the computational efficiency of the Timex component, as we show in Sec. 3. Technically, the Timex extraction step can be formulated as a generic text chunking problem and the standard B(egin), I(nside), and O(utside) labeling scheme can be used. CogCompTime proposes TemporalChunker, by retraining Illinois-Chunker (Punyakanok and Roth, 2001) on top of the Timex chunk"
D18-2013,P18-1212,1,0.912432,"insight for temporal understanding and be useful for multiple time-aware applications. 1 Introduction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1"
D18-2013,S13-2002,0,0.358716,"age. 2.2 and prepositional phrase head, and train a sparse averaged perceptron for event extraction. 2.3 TempRel Component Temporal relations can be generally modeled by a graph (called temporal graph), where the nodes represent events and Timexes, and the edges represent TempRels. With all the nodes extracted (by previous steps), the TempRel component is to make predictions on the labels of those edges. In this paper, the label set for Event-Event TempRels is before, after, equal, and vague and for Event-Timex TempRels is equal and not-equal.2 State-of-the-art methods include, e.g., ClearTK (Bethard, 2013), CAEVO (Chambers et al., 2014), and Ning et al. (2017). The TempRel task is known to be very difficult. Ning et al. (2018c) attributes the difficulty partly to the low inter-annotator agreement (IAA) of existing TempRel datasets and proposes a new MultiAxis Temporal RElations dataset of Start-points (MATRES) with significantly improved IAA, so for the TempRel task, we have chosen MATRES as the benchmark in this paper.3 We also incorporate the recent progress of Ning et al. (2017, 2018a,b). The feature set used for TempRel is shown in Fig. 3, which contains features derived individually from e"
D18-2013,N18-1077,1,0.85256,"insight for temporal understanding and be useful for multiple time-aware applications. 1 Introduction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1"
D18-2013,S15-2136,0,0.116058,"Missing"
D18-2013,P18-1122,1,0.821326,"insight for temporal understanding and be useful for multiple time-aware applications. 1 Introduction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1"
D18-2013,Q14-1022,0,0.148481,"nal phrase head, and train a sparse averaged perceptron for event extraction. 2.3 TempRel Component Temporal relations can be generally modeled by a graph (called temporal graph), where the nodes represent events and Timexes, and the edges represent TempRels. With all the nodes extracted (by previous steps), the TempRel component is to make predictions on the labels of those edges. In this paper, the label set for Event-Event TempRels is before, after, equal, and vague and for Event-Timex TempRels is equal and not-equal.2 State-of-the-art methods include, e.g., ClearTK (Bethard, 2013), CAEVO (Chambers et al., 2014), and Ning et al. (2017). The TempRel task is known to be very difficult. Ning et al. (2018c) attributes the difficulty partly to the low inter-annotator agreement (IAA) of existing TempRel datasets and proposes a new MultiAxis Temporal RElations dataset of Start-points (MATRES) with significantly improved IAA, so for the TempRel task, we have chosen MATRES as the benchmark in this paper.3 We also incorporate the recent progress of Ning et al. (2017, 2018a,b). The feature set used for TempRel is shown in Fig. 3, which contains features derived individually from each node and jointly from a nod"
D18-2013,D16-1038,1,0.850158,"iven gold extraction, while “end-to-end” means system extraction was used. Runtimes were evaluated under the same setup. Timex Systems † HeidelTime and SUTime have no clear-cut between extraction and normalization, so even if gold Timex chunks are fed in, their extraction step cannot be easily skipped. event extraction and corresponding TempRel extraction. Second, CogCompTime currently does not incorporate an event coreference component. Since coreference is important for bridging longdistance event pairs, it is a desirable feature. We can adopt existing event coreferencing techniques such as Peng et al. (2016) in the next step. Third, CogCompTime currently only works on the mainaxis events as defined in MATRES. How to incorporate other axes, e.g., intention axis, opinion axis, and hypothesis axis, requires further investigation. of vague in MATRES, i.e., to assign vague labels when either before or after is reasonable. We think this relaxed metric is more suitable when creating timelines from temporal graphs, where an order must be picked anyhow when two events have a vague relation. When system extraction was used, the TempRel performance saw a large drop. However, the performance here, although i"
D18-2013,chang-manning-2012-sutime,0,0.285258,"n component, and the TempRel component. Following that, we provide a benchmark evaluation in Sec. 3 on the TempEval3 and the MATRES datasets (UzZaman et al., 2013; Ning et al., 2018c). Finally, we point out directions for future work and conclude this paper. 2 plain these main modules in detail. 2.1 Timex Component Existing work on Timex extraction and normalization falls into two categories: rule-based and learning-based. Rule-based systems use regular expressions to extract Timex in text and then deterministic rules to normalize them (e.g., HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012)). Learning-based systems use classification models to chunk out Timexes in text and normalize them based on grammar parsing (e.g., UWTime (Lee et al., 2014) and ParsingTime (Angeli et al., 2012)). CogCompTime adopts a mixed strategy: we use machine learning in the Timex extraction step and rule parsing in the normalization step. This mixed strategy, while maintaining a state-of-the-art performance, significantly improves the computational efficiency of the Timex component, as we show in Sec. 3. Technically, the Timex extraction step can be formulated as a generic text chunking problem and the"
D18-2013,D12-1062,1,0.915367,"rtant functionalities. It incorporates the most recent progress, achieves state-of-the-art performance, and is publicly available.1 We believe that this demo will provide valuable insight for temporal understanding and be useful for multiple time-aware applications. 1 Introduction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a r"
D18-2013,S10-1071,0,0.477269,"Missing"
D18-2013,S13-2001,0,0.659546,"mension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1:February 27, 1998). Example 2: A car (e1:exploded) in the middle of a group of men playing volleyball. More than 10 people have ("
D18-2013,L18-1086,1,0.765921,"on step can be formulated as a generic text chunking problem and the standard B(egin), I(nside), and O(utside) labeling scheme can be used. CogCompTime proposes TemporalChunker, by retraining Illinois-Chunker (Punyakanok and Roth, 2001) on top of the Timex chunk annotations provided by the TempEval3 workshop (UzZaman et al., 2013). Here a machine learning based extraction algorithm significantly improves the computational efficiency by quickly sifting out impossible text chunks, as comSystem The system pipeline of CogCompTime is shown in Fig. 2: It takes raw text as input and uses CogCompNLP (Khashabi et al., 2018) to extract features such as lemmas, part-of-speech (POS) tags, and semantic role labelings (SRL). Then CogCompTime sequentially applies the Timex component, the event extraction component, and the TempRel component. Finally, both a graph visualization and a timeline visualization are provided for the users. In the following, we will ex73 Figure 2: System pipeline of CogCompTime: It preprocesses raw text input using CogCompNLP and then applies Timex, Event Extraction, and TempRel components sequentially, with two user-friendly visualizations (i.e., graph-type visualization and timeline-type vi"
D18-2013,P14-1135,0,0.719998,"Ning et al., 2018c). Finally, we point out directions for future work and conclude this paper. 2 plain these main modules in detail. 2.1 Timex Component Existing work on Timex extraction and normalization falls into two categories: rule-based and learning-based. Rule-based systems use regular expressions to extract Timex in text and then deterministic rules to normalize them (e.g., HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012)). Learning-based systems use classification models to chunk out Timexes in text and normalize them based on grammar parsing (e.g., UWTime (Lee et al., 2014) and ParsingTime (Angeli et al., 2012)). CogCompTime adopts a mixed strategy: we use machine learning in the Timex extraction step and rule parsing in the normalization step. This mixed strategy, while maintaining a state-of-the-art performance, significantly improves the computational efficiency of the Timex component, as we show in Sec. 3. Technically, the Timex extraction step can be formulated as a generic text chunking problem and the standard B(egin), I(nside), and O(utside) labeling scheme can be used. CogCompTime proposes TemporalChunker, by retraining Illinois-Chunker (Punyakanok and"
D18-2013,S07-1014,0,0.22719,"ction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1:February 27, 1998). Example 2: A car (e1:exploded) in the middle of a group of men playing volleybal"
D18-2013,S15-2134,0,0.0848379,"We believe that this demo will provide valuable insight for temporal understanding and be useful for multiple time-aware applications. 1 Introduction Time is an important dimension when we describe the world because many facts are time-sensitive, e.g., one’s place of residence, one’s employment, or the progress of a conflict between countries. Consequently, many applications can benefit from temporal understanding in natural language, e.g., timeline construction (Do et al., 2012; Minard et al., 2015), clinical events analysis (Jindal and Roth, 2013; Bethard et al., 2015), question answering (Llorens et al., 2015), and causality inference (Ning et al., 2018a). Temporal understanding from natural language requires two basic components (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The first, also known as the Timex component, requires extracting explicit time expressions in text (i.e., “Timex”) and normalize them to a standard format. In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russi"
D18-2013,N12-3008,1,0.892262,"In Example 1, the Timex is February 27, 1998 and its normalized form is ‘1998-02-27”. Note that normalization may also require a reference time for Timexes like “tomorrow”, for which we need to Example 1: Presidents Leonid Kuchma of Ukraine and Boris Yeltsin of Russia signed an economic cooperation plan on (t1:February 27, 1998). Example 2: A car (e1:exploded) in the middle of a group of men playing volleyball. More than 10 people have (e2:died), police said. In this paper, we present CogCompTime (see Fig. 1), a tool with both the Timex and TempRel components, which are conceptually built on Zhao et al. (2012) and Ning et al. (2017), respectively. CogCompTime is a new implementation that integrates both components and also incorporates the most recent advances in this area (Ning et al., 2018a,b,c). Two highlights are: First, CogCompTime achieves comparable performance to state-of-the-art Timex systems, but is almost two times faster than the second fastest, HeidelTime (Str¨otgen and Gertz, 2010). Second, CogCompTime improves the performance of the 1 http://cogcomp.org/page/publication_ view/844 72 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonst"
D19-1332,D18-1454,0,0.0833639,"Missing"
D19-1332,P17-1152,0,0.132453,"e plausible). We design the task as a binary classification: determining whether a candidate answer is plausible according to human commonsense, since there is no absolute truth here. This is aligned with other efforts that have posed commonsense as the choice of plausible alternatives (Roemmele et al., 2011). The high quality of the resulting dataset (shown in §4) also makes us believe that the notion of plausibility here is robust. Our third contribution is that, using M C TACO as a testbed, we study the temporal commonsense understanding of the best existing NLP techniques, including ESIM (Chen et al., 2017), BERT (Devlin et al., 2019) and their variants. Results in §4 show that, despite a significant improvement over random-guess baselines, the best existing techniques are still far behind human performance on temporal commonsense understanding, indicating the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weigh"
D19-1332,W04-3205,0,0.333831,"bution is that, to the best of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation standards and datasets for temporal commons"
D19-1332,D18-1202,0,0.119562,"Missing"
D19-1332,N19-1423,0,0.442694,"e task as a binary classification: determining whether a candidate answer is plausible according to human commonsense, since there is no absolute truth here. This is aligned with other efforts that have posed commonsense as the choice of plausible alternatives (Roemmele et al., 2011). The high quality of the resulting dataset (shown in §4) also makes us believe that the notion of plausibility here is robust. Our third contribution is that, using M C TACO as a testbed, we study the temporal commonsense understanding of the best existing NLP techniques, including ESIM (Chen et al., 2017), BERT (Devlin et al., 2019) and their variants. Results in §4 show that, despite a significant improvement over random-guess baselines, the best existing techniques are still far behind human performance on temporal commonsense understanding, indicating the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weight, and strength (Forbes and"
D19-1332,P17-1025,0,0.0487531,"al., 2019) and their variants. Results in §4 show that, despite a significant improvement over random-guess baselines, the best existing techniques are still far behind human performance on temporal commonsense understanding, indicating the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weight, and strength (Forbes and Choi, 2017), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). In terms of “events” commonsense, Rashkin et al. (2018) investigated the intent and reaction of participants of an event, and Zellers et al. (2018) tried to select the most likely subsequent event. To the Measure Value # of unique questions # of unique question-answer pairs avg. sentence length avg. question length avg. answer length 1893 13,225 17.8 8.2 3.3 Category event frequency event duration event stationarity event ordering event typical time # questions avg # of candidate 433 440 279 370 371 8.5 9.4"
D19-1332,W11-0116,0,0.075043,"eceived limited attention so far. Our first contribution is that, to the best of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack"
D19-1332,N18-1023,1,0.820625,"at has seen significant progress in the past few years (Clark et al., 2018; Ostermann et al., 2018; Merkhofer et al., 2018). This area, however, has mainly focused on general natural language comprehension tasks, while we tailor it to test a specific reasoning capability, which is temporal commonsense. 3364 3 Construction of M C TACO M C TACO is comprised of 13k tuples, in the form of (sentence, question, candidate answer); please see examples in Fig. 1 for the five phenomena studied here and Table 1 for basic statistics of it. The sentences in those tuples are randomly selected from MultiRC (Khashabi et al., 2018) (from each of its 9 domains). For each sentence, we use crowdsourcing on Amazon Mechanical Turk to collect questions and candidate answers (both correct and wrong ones). To ensure the quality of the results, we limit the annotations to native speakers and use qualification tryouts. Step 1: Question generation. We first ask crowdsourcers to generate questions, given a sentence. To produce questions that need temporal commonsense to answer, we require that a valid question: (a) should ask about one of the five temporal phenomena we defined earlier, and (b) should not be solved simply by a word"
D19-1332,kingsbury-palmer-2002-treebank,0,0.282935,"). Second, we mask each individual token in a candidate answer (one at a time) and use BERT (Devlin et al., 2019) to predict replacements for each missing term; we rank those predictions by the confidence level of BERT and keep the top three. Third, for those candidates that represent events, the previously-mentioned token-level perturbations rarely lead to interesting and diverse set of candidate answers. Furthermore, it may lead to invalid phrases (e.g., “he left the house” → “he walked the house”.) Therefore, to perturb such candidates, we create a pool of 60k event phrases using PropBank (Kingsbury and Palmer, 2002), and perturb the candidate answers to be the most similar ones extracted by an information retrieval (IR) system.3 This not only guarantees that all candidates are properly phrased, it also leads to more diverse perturbations. We apply the above three techniques on non“event” candidates sequentially, in the order they were explained, to expand the candidate answer set to 20 candidates per question. A perturbation technique is used, as long as the pool of candidates is still less than 20. Note there are both correct and incorrect answers in those candidates. Step 4: Answer labeling. In this st"
D19-1332,P14-1135,0,0.131483,"stics of M C TACO. best of our knowledge, no earlier work has focused on temporal commonsense, although it is critical for event understanding. For instance, Ning et al. (2018c) argues that resolving ambiguous and implicit mentions of event durations in text (a specific kind of temporal commonsense) is necessary to construct the timeline of a story. There have also been many works trying to understand time in natural language but not necessarily the commonsense understanding of time. Most recent works include the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Lee et al., 2014), temporal relation extraction (Ning et al., 2017, 2018d), and timeline construction (Leeuwenberg and Moens, 2018). Among these, some works are implicitly on temporal commonsense, such as event durations (Williams, 2012; Vempala et al., 2018), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (GranrothWilding and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it."
D19-1332,D18-1155,0,0.165847,"it is critical for event understanding. For instance, Ning et al. (2018c) argues that resolving ambiguous and implicit mentions of event durations in text (a specific kind of temporal commonsense) is necessary to construct the timeline of a story. There have also been many works trying to understand time in natural language but not necessarily the commonsense understanding of time. Most recent works include the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Lee et al., 2014), temporal relation extraction (Ning et al., 2017, 2018d), and timeline construction (Leeuwenberg and Moens, 2018). Among these, some works are implicitly on temporal commonsense, such as event durations (Williams, 2012; Vempala et al., 2018), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (GranrothWilding and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it. Instead of working on each individual aspect of temporal commonsense, we formulate the problem as a machine readin"
D19-1332,S18-1181,0,0.0195787,"ng and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it. Instead of working on each individual aspect of temporal commonsense, we formulate the problem as a machine reading comprehension task in the format of selecting plausible responses with respect to natural language queries. This relates our work to a large body of work on question-answering, an area that has seen significant progress in the past few years (Clark et al., 2018; Ostermann et al., 2018; Merkhofer et al., 2018). This area, however, has mainly focused on general natural language comprehension tasks, while we tailor it to test a specific reasoning capability, which is temporal commonsense. 3364 3 Construction of M C TACO M C TACO is comprised of 13k tuples, in the form of (sentence, question, candidate answer); please see examples in Fig. 1 for the five phenomena studied here and Table 1 for basic statistics of it. The sentences in those tuples are randomly selected from MultiRC (Khashabi et al., 2018) (from each of its 9 domains). For each sentence, we use crowdsourcing on Amazon Mechanical Turk to c"
D19-1332,D17-1108,1,0.847323,"ier work has focused on temporal commonsense, although it is critical for event understanding. For instance, Ning et al. (2018c) argues that resolving ambiguous and implicit mentions of event durations in text (a specific kind of temporal commonsense) is necessary to construct the timeline of a story. There have also been many works trying to understand time in natural language but not necessarily the commonsense understanding of time. Most recent works include the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Lee et al., 2014), temporal relation extraction (Ning et al., 2017, 2018d), and timeline construction (Leeuwenberg and Moens, 2018). Among these, some works are implicitly on temporal commonsense, such as event durations (Williams, 2012; Vempala et al., 2018), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (GranrothWilding and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it. Instead of working on each individual aspect of t"
D19-1332,P18-1212,1,0.933819,"of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation standards and datasets for temporal commonsense, our second co"
D19-1332,N18-1077,1,0.931516,"of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation standards and datasets for temporal commonsense, our second co"
D19-1332,P18-1122,1,0.900586,"of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation standards and datasets for temporal commonsense, our second co"
D19-1332,D18-2013,1,0.946722,"of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation standards and datasets for temporal commonsense, our second co"
D19-1332,S18-1119,0,0.026208,"n events) (GranrothWilding and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it. Instead of working on each individual aspect of temporal commonsense, we formulate the problem as a machine reading comprehension task in the format of selecting plausible responses with respect to natural language queries. This relates our work to a large body of work on question-answering, an area that has seen significant progress in the past few years (Clark et al., 2018; Ostermann et al., 2018; Merkhofer et al., 2018). This area, however, has mainly focused on general natural language comprehension tasks, while we tailor it to test a specific reasoning capability, which is temporal commonsense. 3364 3 Construction of M C TACO M C TACO is comprised of 13k tuples, in the form of (sentence, question, candidate answer); please see examples in Fig. 1 for the five phenomena studied here and Table 1 for basic statistics of it. The sentences in those tuples are randomly selected from MultiRC (Khashabi et al., 2018) (from each of its 9 domains). For each sentence, we use crowdsourcing on Am"
D19-1332,D14-1162,0,0.0820145,"rmance. An expert annotator also worked on M C TACO to gain a better understanding of the human performance on it. The expert answered 100 questions (about 700 (sentence, question, answer) tuples) randomly sampled from the test set, and could only see a single answer at a time, with its corresponding question and sentence. Systems. We use two state-of-the-art systems in machine reading comprehension for this task: ESIM (Chen et al., 2017) and BERT (Devlin et al., 2019). ESIM is an effective neural model on natural language inference. We initialize the word embeddings in ESIM via either GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) to demonstrate the effect of pre-training. BERT is a state-of-the-art contextualized representation used for a broad range of tasks . We also add unit normalization to BERT, which extracts and converts temporal expressions in candidate answers to their most proper units. For example, “30 months” will be converted to “2.5 years”. To the best of our knowledge, there are no other available systems for the “stationarity”, “typical time”, and “frequency” phenomena studied here. As for “duration” and “temporal order”, there are existing systems (e.g., Vempala et al. (2"
D19-1332,N18-1202,0,0.0272048,"orked on M C TACO to gain a better understanding of the human performance on it. The expert answered 100 questions (about 700 (sentence, question, answer) tuples) randomly sampled from the test set, and could only see a single answer at a time, with its corresponding question and sentence. Systems. We use two state-of-the-art systems in machine reading comprehension for this task: ESIM (Chen et al., 2017) and BERT (Devlin et al., 2019). ESIM is an effective neural model on natural language inference. We initialize the word embeddings in ESIM via either GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) to demonstrate the effect of pre-training. BERT is a state-of-the-art contextualized representation used for a broad range of tasks . We also add unit normalization to BERT, which extracts and converts temporal expressions in candidate answers to their most proper units. For example, “30 months” will be converted to “2.5 years”. To the best of our knowledge, there are no other available systems for the “stationarity”, “typical time”, and “frequency” phenomena studied here. As for “duration” and “temporal order”, there are existing systems (e.g., Vempala et al. (2018); Ning et al. (2018b)), bu"
D19-1332,P18-1043,0,0.0322403,"iques are still far behind human performance on temporal commonsense understanding, indicating the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weight, and strength (Forbes and Choi, 2017), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). In terms of “events” commonsense, Rashkin et al. (2018) investigated the intent and reaction of participants of an event, and Zellers et al. (2018) tried to select the most likely subsequent event. To the Measure Value # of unique questions # of unique question-answer pairs avg. sentence length avg. question length avg. answer length 1893 13,225 17.8 8.2 3.3 Category event frequency event duration event stationarity event ordering event typical time # questions avg # of candidate 433 440 279 370 371 8.5 9.4 3.1 5.4 6.8 Table 1: Statistics of M C TACO. best of our knowledge, no earlier work has focused on temporal commonsense, although it is critic"
D19-1332,S10-1071,0,0.195097,"Missing"
D19-1332,D18-1006,0,0.128255,"Missing"
D19-1332,N18-2026,0,0.329004,"nt durations in text (a specific kind of temporal commonsense) is necessary to construct the timeline of a story. There have also been many works trying to understand time in natural language but not necessarily the commonsense understanding of time. Most recent works include the extraction and normalization of temporal expressions (Str¨otgen and Gertz, 2010; Lee et al., 2014), temporal relation extraction (Ning et al., 2017, 2018d), and timeline construction (Leeuwenberg and Moens, 2018). Among these, some works are implicitly on temporal commonsense, such as event durations (Williams, 2012; Vempala et al., 2018), typical temporal ordering (Chklovski and Pantel, 2004; Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (GranrothWilding and Clark, 2016; Li et al., 2018). However, existing works have not studied all five types of temporal commonsense in a unified framework as we do here, nor have they developed datasets for it. Instead of working on each individual aspect of temporal commonsense, we formulate the problem as a machine reading comprehension task in the format of selecting plausible responses with respect to natural language queries. This relates our w"
D19-1332,W12-3309,0,0.54313,"ntion so far. Our first contribution is that, to the best of our knowledge, we are the first to systematically study and quantify performance on a range of temporal commonsense phenomena. Specifically, we consider five temporal properties: duration (how long an event takes), temporal ordering (typical order of events), typical time (when an event happens), frequency (how often an event occurs), and stationarity (whether a state holds for a very long time or indefinitely). Previous work has investigated some of these aspects, either explicitly or implicitly (e.g., duration (Gusev et al., 2011; Williams, 2012) and ordering (Chklovski and Pantel, 2004; Ning et al., 2018b)), but none of them have defined or studied 3363 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3363–3369, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics all aspects of temporal commonsense in a unified framework. Kozareva and Hovy (2011) defined a few temporal aspects to be investigated, but failed to quantify performances on these phenomena. Given the lack of evaluation st"
D19-1332,P18-2102,0,0.0743428,"at, despite a significant improvement over random-guess baselines, the best existing techniques are still far behind human performance on temporal commonsense understanding, indicating the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weight, and strength (Forbes and Choi, 2017), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). In terms of “events” commonsense, Rashkin et al. (2018) investigated the intent and reaction of participants of an event, and Zellers et al. (2018) tried to select the most likely subsequent event. To the Measure Value # of unique questions # of unique question-answer pairs avg. sentence length avg. question length avg. answer length 1893 13,225 17.8 8.2 3.3 Category event frequency event duration event stationarity event ordering event typical time # questions avg # of candidate 433 440 279 370 371 8.5 9.4 3.1 5.4 6.8 Table 1: Statistics of M C TACO. best"
D19-1332,D18-1009,0,0.0279314,"ng the need for further research in order to improve the currently limited capability to capture temporal semantics. 2 Related Work Commonsense has been a very popular topic in recent years and existing NLP works have mainly investigated the acquisition and evaluation of commonsense in the physical world, including but not limited to, size, weight, and strength (Forbes and Choi, 2017), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). In terms of “events” commonsense, Rashkin et al. (2018) investigated the intent and reaction of participants of an event, and Zellers et al. (2018) tried to select the most likely subsequent event. To the Measure Value # of unique questions # of unique question-answer pairs avg. sentence length avg. question length avg. answer length 1893 13,225 17.8 8.2 3.3 Category event frequency event duration event stationarity event ordering event typical time # questions avg # of candidate 433 440 279 370 371 8.5 9.4 3.1 5.4 6.8 Table 1: Statistics of M C TACO. best of our knowledge, no earlier work has focused on temporal commonsense, although it is critical for event understanding. For instance, Ning et al. (2018c) argues that resolving ambiguou"
D19-1332,Q17-1027,0,0.102087,"Missing"
D19-1386,N16-1012,0,0.0286687,"to identify content in search results for topic keywords as part of a pipeline to generate full articles. Like us, Liu et al. (2018) also employ a two-step abstractive approach, but their goal is to generate the lead Wikipedia paragraph from all of the citations and web search results. Related Work Summarization Recent work on generic summarization has focused on single-document models on the CNN/DailyMail dataset (Nallapati et al., 2016), focusing on different neural architectures and loss functions for extractive and abstractive summarization (Cheng and Lapata, 2016; Nallapati et al., 2017; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017, inter alia). Our models were 8 Conclusion In this work, we propose the summary cloze task, a new task for studying content selection in topicfocused summarization. By narrowing the scope from the full topic-focused summarization task to one at the sentence-level, we are able to collect a large-scale summary cloze dataset from Wikipedia. Our experimental results demonstrate 3727 topic reference document On May 15 , 2011 , Welts publicly came out as gay in an interview with The New York Times . He is the first prominent American sports executive to com"
D19-1386,P06-1039,0,0.0850378,"Missing"
D19-1386,N18-1065,0,0.0310127,"Missing"
D19-1386,H05-1115,0,0.0728076,"ere is a sizable gap between the performance of the abstractive model when it uses the heuristic labels versus the extractive model for preprocessing. This indicates that improving the extractive model will provide large downstream abstractive improvements. 7 based on the successful approaches of See et al. (2017) and those described in Kedzie et al. (2018). Most work on topic-focused (also referred to as “query-focused”) summarization uses the DUC 2005 and 2006 datasets (Dang, 2005, 2006). Approaches are mostly extractive, with some work selecting sentences based on token frequency/salience (Otterbacher et al., 2005; Vanderwende et al., 2007), diversity with submodular functions (Lin and Bilmes, 2011), or using Bayesian networks (Daum´e III and Marcu, 2006). To the best of our knowledge, ours is the first work which builds end-to-end models on a large-scale dataset for topic-focused summarization. Generating Wikipedia In contrast to our work focusing on content selection for topic-focused summaries, there have been previous work interested in generating Wikipedia articles. Sauper and Barzilay (2009) try to generate articles from a small set of domains by learning to generate templates from similar articl"
D19-1386,D18-1208,0,0.0978204,"where Wa is a matrix of learned parameters. ˜ i is used In this extended version of the model, h to compute the extraction probabilities instead of hi . A graphical representation of the full extractive model is presented in Figure 3. Training & Inference To convert the abstractive clozes into extractive reference sentence labels, we follow the procedure of Nallapati et al. (2017) and greedily select reference sentences that maximize ROUGE-1 recall until no more sentences improve the score. The training objective is the weighted negative log-likelihood of the labels with the same weighting as Kedzie et al. (2018). At inference, the sentence with the highest probability of extraction is selected. Extractive Model The extractive model is based on those presented in Kedzie et al. (2018). The base model works as follows. For every sentence xi in the reference document, a sentence-level representation hi is created by a sentence encoder (e.g., by averaging word vectors, by using a recurrent neural network, etc.). Then, a sentence extractor creates a document-level representation for each sentence di , for example, by running a second RNN over the sentence-level representations. The documentlevel representa"
D19-1386,W04-1013,0,0.0591177,"his data is hard to collect from human annotators and is difficult to find occurring naturally. The cloze task requires sentence-level supervision for which we were able to collect a large-scale dataset using Wikipedia (§3). By conditioning the cloze generation on a partial summary and working at the sentence-level, we are able to get around the problem of data scarcity. Evaluation Evaluating summary cloze models is challenging; The difficulties of evaluating summarization systems (Nenkova, 2006a) still persist for the summary cloze task. We propose for the main evaluation metric to be ROUGE (Lin, 2004), which has been shown to be an effective metric for summarization (Owczarzak et al., 2012). Additionally, for abstractive models we use perplexity to quantify how likely the ground-truth cloze is according to the model. Because the cloze is only one sentence long, the chance of the ground-truth and model clozes being equally valid but very different to each other is likely higher than for other summarization tasks. 3721 In June 1989, Obama met Michelle Robinson when he was employed as a summer associate at the Chicago law firm of Sidley Austin.[62] Robinson was assigned for three months as Ob"
D19-1386,P11-1052,0,0.322244,"he context), the objective of the summary cloze task is to predict the next sentence of the summary, known as the cloze. Introduction Topic-focused multi-document summarization (MDS) has long been a goal for natural language processing (Dang, 2005, 2006). In contrast to generic summarization, topic-focused summarization systems attempt to summarize a set of reference documents with respect to a specific topic or information need. Recent research on summarization has mostly focused on generic summarization, in part due to the size of the available datasets. Work on topic-focused summarization (Lin and Bilmes, 2011; Ma et al., 2016; Feigenblat et al., 2017) is largely based on the DUC 2005 and 2006 datasets (Dang, 2005, 2006), which are orders of magnitude smaller than comparable generic summarization datasets (Nallapati et al., 2016; Grusky et al., 2018). Because the available corpora are so small, it is difficult to use them to train recent stateof-the-art summarization systems, which require large amounts of training data. Instead of focusing on the full topic-focused MDS problem, we address one aspect of the task known as content selection, the problem of deciding what information should be included"
D19-1386,C16-1143,0,0.0442911,"Missing"
D19-1386,K16-1028,0,0.16883,"age processing (Dang, 2005, 2006). In contrast to generic summarization, topic-focused summarization systems attempt to summarize a set of reference documents with respect to a specific topic or information need. Recent research on summarization has mostly focused on generic summarization, in part due to the size of the available datasets. Work on topic-focused summarization (Lin and Bilmes, 2011; Ma et al., 2016; Feigenblat et al., 2017) is largely based on the DUC 2005 and 2006 datasets (Dang, 2005, 2006), which are orders of magnitude smaller than comparable generic summarization datasets (Nallapati et al., 2016; Grusky et al., 2018). Because the available corpora are so small, it is difficult to use them to train recent stateof-the-art summarization systems, which require large amounts of training data. Instead of focusing on the full topic-focused MDS problem, we address one aspect of the task known as content selection, the problem of deciding what information should be included in the summary (Nenkova, 2006b). Narrowing the scope of the problem makes it easier to collect a largescale dataset that is tailored to the specific task and thus build upon recent work on summarization. To that end, we fo"
D19-1386,W12-2601,0,0.0339867,"urring naturally. The cloze task requires sentence-level supervision for which we were able to collect a large-scale dataset using Wikipedia (§3). By conditioning the cloze generation on a partial summary and working at the sentence-level, we are able to get around the problem of data scarcity. Evaluation Evaluating summary cloze models is challenging; The difficulties of evaluating summarization systems (Nenkova, 2006a) still persist for the summary cloze task. We propose for the main evaluation metric to be ROUGE (Lin, 2004), which has been shown to be an effective metric for summarization (Owczarzak et al., 2012). Additionally, for abstractive models we use perplexity to quantify how likely the ground-truth cloze is according to the model. Because the cloze is only one sentence long, the chance of the ground-truth and model clozes being equally valid but very different to each other is likely higher than for other summarization tasks. 3721 In June 1989, Obama met Michelle Robinson when he was employed as a summer associate at the Chicago law firm of Sidley Austin.[62] Robinson was assigned for three months as Obama's adviser at the firm, and she joined him at several group social functions but decline"
D19-1386,D14-1162,0,0.0927155,"called H EURISTIC L ABELS, serves to demonstrate how well the abstractive model would perform with a perfect extractive step. Abstractive Step Evaluation Using the preprocessed documents from various extractive models, we trained abstractive models (§4.2) to produce the ground-truth cloze, both with and without access to the context. These systems were evaluated with the F1 variant of ROUGE and perplexity. 5.4 Implementation Details Our models were implemented using PyTorch (Paszke et al., 2017) in the AllenNLP framework (Gardner et al., 2017).4 We used fixed 200-dimensional GloVe embeddings (Pennington et al., 2014). The RNNs were implemented with LSTMs (Hochreiter and Schmidhuber, 1997) of size 256. The models were trained with Adam (Kingma and Ba, 2014) using a learning rate of 1e-4 and 1e-3 and batch sizes of 32 and 16 for 290k and 62.5k iterations for the extractive and abstractive models, respectively. Following See et al. (2017), we trained the abstractive models without the coverage loss until convergence, then by 3k iterations with a coverage loss weight of 1. 6 6.1 Experimental Results Human Performance & Evaluation Table 2 shows the ROUGE and quality scores for the human-written clozes with and"
D19-1386,P09-1024,0,0.039965,"06). Approaches are mostly extractive, with some work selecting sentences based on token frequency/salience (Otterbacher et al., 2005; Vanderwende et al., 2007), diversity with submodular functions (Lin and Bilmes, 2011), or using Bayesian networks (Daum´e III and Marcu, 2006). To the best of our knowledge, ours is the first work which builds end-to-end models on a large-scale dataset for topic-focused summarization. Generating Wikipedia In contrast to our work focusing on content selection for topic-focused summaries, there have been previous work interested in generating Wikipedia articles. Sauper and Barzilay (2009) try to generate articles from a small set of domains by learning to generate templates from similar articles using an integer linear program. Similarly, Banerjee and Mitra (2016) build topic classifiers based on sections on Wikipedia to identify content in search results for topic keywords as part of a pipeline to generate full articles. Like us, Liu et al. (2018) also employ a two-step abstractive approach, but their goal is to generate the lead Wikipedia paragraph from all of the citations and web search results. Related Work Summarization Recent work on generic summarization has focused on"
D19-1386,P17-1099,0,0.457978,"long (see Table 1), we chose to use a two-step approach to build an abstractive system for the summary cloze task. Extractive Step The first step uses the same extractive model from the previous subsection to significantly reduce the amount of input text. Instead of selecting just one sentence at inference, the extractive model repeatedly selects sentences until a pre-specified number of words has been met. Then, only the sentences which were extracted are passed as input to the abstractive step. Abstractive Step For the abstractive step, we extended the Pointer-Generator + Coverage network (See et al., 2017) for single-document summarization to include the context. The Pointer-Generator network is built on a sequence-to-sequence model with attention. The reference document is encoded using an RNN, and the summary is produced using a second RNN. The model is augmented with a copy mechanism by including a soft switch that allows the attention distribution to influence the decoder’s probability distribution over the vocabulary, thus making it easier to copy words from the input. Then, the coverage mechanism discourages the attention weights from repeatedly assigning high values to the same input tok"
D19-1404,C18-1179,0,0.0665823,"Missing"
D19-1404,N19-1423,0,0.0191053,"tion anger this text expresses ? etc. situation shelter The people there etc. need ? example hypothesis word wordnet definition “?” = an active diversion requiring “?”= sports physical exertion and competition “?” = a strong emotion; a feeling that “?”= anger is oriented toward some real or supposed grievance “?” = a structure that provides privacy “?”= shelter and protection from danger Table 4: Example hypotheses we created for modeling different aspects of 0 SHOT- TC. Entailment model learning. In this work, we make use of the widely-recognized state of the art entailment technique – BERT (Devlin et al., 2019), and train it on three mainstream entailment datasets: MNLI (Williams et al., 2018), GLUE RTE (Dagan et al., 2005; Wang et al., 2019) and FEVER3 (Thorne et al., 2018), respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “nonentailment”. For our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all 0 SHOT- TC aspects. For label-partiallyunseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, t"
D19-1404,K17-1034,0,0.0281365,"s hierarchy, and the word-to-label paths in ConceptNet. Srivastava et al. (2018) assume that some natural language explanations about new labels are available. Then those explanations are parsed into formal constraints which are further combined with unlabeled data to yield new label oriented classifiers through posterior regularization. However, those explanatory statements about new labels are collected from crowd-sourcing. This limits its application in real world 0 SHOT- TC scenarios. There are a few works that study a specific zeroshot problem by indirect supervision from other problems. Levy et al. (2017) and Obamuyide and Vlachos (2018) study zero-shot relation extraction by converting it into a machine comprehension and textual entailment problem respectively. Then, a supervised system pretrained on an existing machine comprehension dataset or textual entailment dataset is used to do inference. Our work studies the 0 SHOT- TC by formulating a broader vision: datasets of multiple apsects and evaluations. Other zero-shot problems studied in NLP involve entity typing (Zhou et al., 2018), sequence labeling (Rei and Søgaard, 2018), etc. 3 Benchmark the dataset In this work, we standardize the dat"
D19-1404,C16-1252,0,0.0528178,"he-art entailment system. All datasets and codes are released. 2 Related Work Z ERO - STC was first explored by the paradigm “Dataless Classification” (Chang et al., 2008). 3915 Dataless classification first maps the text and labels into a common space by Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), then picks the label with the highest matching score. Dataless classification emphasizes that the representation of labels takes the equally crucial role as the representation learning of text. Then this idea was further developed in (Song and Roth, 2014; Chen et al., 2015; Li et al., 2016a,b; Song et al., 2016). With the prevalence of word embeddings, more and more work adopts pretrained word embeddings to represent the meaning of words, so as to provide the models with the knowledge of labels (Sappadla et al., 2016; Yogatama et al., 2017; Rios and Kavuluru, 2018; Xia et al., 2018). Yogatama et al. (2017) build generative LSTM to generate text given the embedded labels. Rios and Kavuluru (2018) use label embedding to attend the text representation in the developing of a multi-label classifier. But they report R@K, so it is unclear whether the system can really predict unseen l"
D19-1404,W18-5511,0,0.0664696,"rd-to-label paths in ConceptNet. Srivastava et al. (2018) assume that some natural language explanations about new labels are available. Then those explanations are parsed into formal constraints which are further combined with unlabeled data to yield new label oriented classifiers through posterior regularization. However, those explanatory statements about new labels are collected from crowd-sourcing. This limits its application in real world 0 SHOT- TC scenarios. There are a few works that study a specific zeroshot problem by indirect supervision from other problems. Levy et al. (2017) and Obamuyide and Vlachos (2018) study zero-shot relation extraction by converting it into a machine comprehension and textual entailment problem respectively. Then, a supervised system pretrained on an existing machine comprehension dataset or textual entailment dataset is used to do inference. Our work studies the 0 SHOT- TC by formulating a broader vision: datasets of multiple apsects and evaluations. Other zero-shot problems studied in NLP involve entity typing (Zhou et al., 2018), sequence labeling (Rei and Søgaard, 2018), etc. 3 Benchmark the dataset In this work, we standardize the datasets for 0 SHOT- TC for three as"
D19-1404,N18-1027,0,0.0469418,"specific zeroshot problem by indirect supervision from other problems. Levy et al. (2017) and Obamuyide and Vlachos (2018) study zero-shot relation extraction by converting it into a machine comprehension and textual entailment problem respectively. Then, a supervised system pretrained on an existing machine comprehension dataset or textual entailment dataset is used to do inference. Our work studies the 0 SHOT- TC by formulating a broader vision: datasets of multiple apsects and evaluations. Other zero-shot problems studied in NLP involve entity typing (Zhou et al., 2018), sequence labeling (Rei and Søgaard, 2018), etc. 3 Benchmark the dataset In this work, we standardize the datasets for 0 SHOT- TC for three aspects: topic detection, emotion detection, and situation detection. For each dataset, we insist on two principles: i) Label-partially-unseen: A part of labels are unseen. This corresponds to Definition-Restrictive, enabling us to check the performance of unseen labels as well as seen labels. ii) Label-fullyunseen: All labels are unseen. This corresponds to Definition-Wild, enabling us to check the system performance in test-agnostic setups. 3.1 Topic detection Yahoo. We use the large-scale Yahoo"
D19-1404,D18-1352,0,0.147456,"we can not presume the availability of labeled data. Humans can easily decide the truth value of any upcoming labels because humans can interpret those aspects correctly and understand the meaning of those labels. The ultimate goal of 0 SHOT- TC should be to develop machines to catch up with humans in this capability. To this end, making sure the system can understand the described aspect and the label meanings plays a key role. Third problem. Prior work is mostly evaluated on different datasets and adopted different evaluation setups, which makes it hard to compare them fairly. For example, Rios and Kavuluru (2018) work on medical data while reporting R@K as metric; Xia et al. (2018) work on SNIPS-NLU intent detection data while only unseen intents are in the label-searching space in evaluation. In this work, we benchmark the datasets and evaluation setups of 0 SHOT- TC. Furthermore, we propose a textual entailment approach to handle the 0 SHOT- TC problem of diverse aspects in a unified paradigm. To be specific, we contribute in the following three aspects: Dataset. We provide datasets for studying three aspects of 0 SHOT- TC: topic categorization, emotion detection, and situation frame detection – an"
D19-1404,N18-1074,0,0.032236,"= sports physical exertion and competition “?” = a strong emotion; a feeling that “?”= anger is oriented toward some real or supposed grievance “?” = a structure that provides privacy “?”= shelter and protection from danger Table 4: Example hypotheses we created for modeling different aspects of 0 SHOT- TC. Entailment model learning. In this work, we make use of the widely-recognized state of the art entailment technique – BERT (Devlin et al., 2019), and train it on three mainstream entailment datasets: MNLI (Williams et al., 2018), GLUE RTE (Dagan et al., 2005; Wang et al., 2019) and FEVER3 (Thorne et al., 2018), respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “nonentailment”. For our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all 0 SHOT- TC aspects. For label-partiallyunseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, then fine-tune on the provided training data. Harsh policy in testing. Since seen labels have annotated data for training, we adopt different policies to pick up seen a"
D19-1404,N18-1101,0,0.0602625,"? example hypothesis word wordnet definition “?” = an active diversion requiring “?”= sports physical exertion and competition “?” = a strong emotion; a feeling that “?”= anger is oriented toward some real or supposed grievance “?” = a structure that provides privacy “?”= shelter and protection from danger Table 4: Example hypotheses we created for modeling different aspects of 0 SHOT- TC. Entailment model learning. In this work, we make use of the widely-recognized state of the art entailment technique – BERT (Devlin et al., 2019), and train it on three mainstream entailment datasets: MNLI (Williams et al., 2018), GLUE RTE (Dagan et al., 2005; Wang et al., 2019) and FEVER3 (Thorne et al., 2018), respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “nonentailment”. For our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all 0 SHOT- TC aspects. For label-partiallyunseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, then fine-tune on the provided training data. Harsh policy in testing. Since seen lab"
D19-1404,D18-1348,0,0.236648,"e the truth value of any upcoming labels because humans can interpret those aspects correctly and understand the meaning of those labels. The ultimate goal of 0 SHOT- TC should be to develop machines to catch up with humans in this capability. To this end, making sure the system can understand the described aspect and the label meanings plays a key role. Third problem. Prior work is mostly evaluated on different datasets and adopted different evaluation setups, which makes it hard to compare them fairly. For example, Rios and Kavuluru (2018) work on medical data while reporting R@K as metric; Xia et al. (2018) work on SNIPS-NLU intent detection data while only unseen intents are in the label-searching space in evaluation. In this work, we benchmark the datasets and evaluation setups of 0 SHOT- TC. Furthermore, we propose a textual entailment approach to handle the 0 SHOT- TC problem of diverse aspects in a unified paradigm. To be specific, we contribute in the following three aspects: Dataset. We provide datasets for studying three aspects of 0 SHOT- TC: topic categorization, emotion detection, and situation frame detection – an event level recognition problem. For each dataset, we have standard sp"
D19-1404,N19-1108,0,0.217163,"ch as topic, emotion, etc. Existing 0 SHOT- TC studies have mainly the following three problems. Introduction Supervised text classification has achieved great success in the past decades due to the availability of rich training data and deep learning techniques. However, zero-shot text classification (0 SHOT- TC) 1 https://cogcomp.seas.upenn.edu/page/ publication_view/883 First problem. The 0 SHOT- TC problem was modeled in a too restrictive vision. Firstly, most work only explored a single task, which was mainly topic categorization, e.g., (Pushp and Srivastava, 2017; Yogatama et al., 2017; Zhang et al., 2019). We argue that this is only the tiny tip of the iceberg for 0 SHOT- TC. Secondly, there is often a precondition that a part of classes are seen and their labeled instances are available to train a model, as we define here as Definition-Restrictive: Definition-Restrictive (0 SHOT- TC). Given labeled instances belonging to a set of seen classes S, 0 SHOT- TC aims at learning a classifier f (·) : X → Y , where Y = S ∪ U ; U is a set of unseen classes and belongs to the same aspect as S. In this work, we formulate the 0 SHOT- TC in a 3914 Proceedings of the 2019 Conference on Empirical Methods in"
D19-1404,D18-1231,1,0.825905,"os. There are a few works that study a specific zeroshot problem by indirect supervision from other problems. Levy et al. (2017) and Obamuyide and Vlachos (2018) study zero-shot relation extraction by converting it into a machine comprehension and textual entailment problem respectively. Then, a supervised system pretrained on an existing machine comprehension dataset or textual entailment dataset is used to do inference. Our work studies the 0 SHOT- TC by formulating a broader vision: datasets of multiple apsects and evaluations. Other zero-shot problems studied in NLP involve entity typing (Zhou et al., 2018), sequence labeling (Rei and Søgaard, 2018), etc. 3 Benchmark the dataset In this work, we standardize the datasets for 0 SHOT- TC for three aspects: topic detection, emotion detection, and situation detection. For each dataset, we insist on two principles: i) Label-partially-unseen: A part of labels are unseen. This corresponds to Definition-Restrictive, enabling us to check the performance of unseen labels as well as seen labels. ii) Label-fullyunseen: All labels are unseen. This corresponds to Definition-Wild, enabling us to check the system performance in test-agnostic setups. 3.1 Topic de"
D19-1404,P18-1029,0,0.0525602,"Missing"
D19-1642,S15-2136,0,0.0605098,"Missing"
D19-1642,S17-2093,0,0.0757102,"Missing"
D19-1642,P14-2082,0,0.0730285,"–6209, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and i"
D19-1642,Q14-1022,0,0.11706,"ejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based metho"
D19-1642,D08-1073,0,0.0942846,"2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, th"
D19-1642,P08-1090,0,0.0784798,"2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, th"
D19-1642,P07-2044,0,0.141837,"Missing"
D19-1642,P17-2001,0,0.0378383,"eh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task. A recent annotation scheme, Ning et al. (2018c), introduced the notion of multi-axis to represent the temporal structure of text, and identified that one of the sources of confusions in human annotation is asking annotators for TempRels across different axes."
D19-1642,E17-2118,0,0.298446,"ime in natural language (Do et al., 2012; UzZaman et al., 2013; Minard et al., 2015; Llorens et al., 2015; Ning et al., 2018a). However, the annotation process for TempRels is known to be time consuming and difficult even for humans, and existing datasets are usually small and/or have low inter-annotator agreements (IAA); e.g., UzZaman et al. (2013); Chambers et al. (2014); O’Gorman et al. (2016) reported Kohen’s  and F1 in the 60’s. Albeit the significant progress in deep learning nowadays, neural approaches have not been used extensively for this task, or showed only moderate improvements (Dligach et al., 2017; Lin et al., 2017; Meng and Rumshisky, 2018). We think it is important for to understand: is it because we missed a “magic” neural architecture, because the training dataset is small, or because the quality of the dataset should be improved? Recently, Ning et al. (2018c) introduced a new dataset called Multi-Axis Temporal RElations for Start-points (MATRES). MATRES is still relatively small in its size (15K TempRels), but has a higher annotation quality from its improved task definition and annotation guideline. This paper uses MATRES to show that a long short-term memory (LSTM) (Hochreiter a"
D19-1642,D12-1062,1,0.893576,"Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements o"
D19-1642,E17-1108,0,0.0305783,"tic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it"
D19-1642,D18-1155,0,0.0857154,"ed inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task. A recent annotation scheme, Ning et al. (2018c), introduced the notion of multi-axis to represent the temporal structure of text, and identified that one of the sources of confusions in human annotation is asking annotators for TempRels across different axes. When annotating only sameaxis TempRels, along with some"
D19-1642,W17-2341,0,0.219542,"e (Do et al., 2012; UzZaman et al., 2013; Minard et al., 2015; Llorens et al., 2015; Ning et al., 2018a). However, the annotation process for TempRels is known to be time consuming and difficult even for humans, and existing datasets are usually small and/or have low inter-annotator agreements (IAA); e.g., UzZaman et al. (2013); Chambers et al. (2014); O’Gorman et al. (2016) reported Kohen’s  and F1 in the 60’s. Albeit the significant progress in deep learning nowadays, neural approaches have not been used extensively for this task, or showed only moderate improvements (Dligach et al., 2017; Lin et al., 2017; Meng and Rumshisky, 2018). We think it is important for to understand: is it because we missed a “magic” neural architecture, because the training dataset is small, or because the quality of the dataset should be improved? Recently, Ning et al. (2018c) introduced a new dataset called Multi-Axis Temporal RElations for Start-points (MATRES). MATRES is still relatively small in its size (15K TempRels), but has a higher annotation quality from its improved task definition and annotation guideline. This paper uses MATRES to show that a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 19"
D19-1642,S15-2134,0,0.092908,"Missing"
D19-1642,P06-1095,0,0.110143,"Missing"
D19-1642,P18-1049,0,0.194014,"; UzZaman et al., 2013; Minard et al., 2015; Llorens et al., 2015; Ning et al., 2018a). However, the annotation process for TempRels is known to be time consuming and difficult even for humans, and existing datasets are usually small and/or have low inter-annotator agreements (IAA); e.g., UzZaman et al. (2013); Chambers et al. (2014); O’Gorman et al. (2016) reported Kohen’s  and F1 in the 60’s. Albeit the significant progress in deep learning nowadays, neural approaches have not been used extensively for this task, or showed only moderate improvements (Dligach et al., 2017; Lin et al., 2017; Meng and Rumshisky, 2018). We think it is important for to understand: is it because we missed a “magic” neural architecture, because the training dataset is small, or because the quality of the dataset should be improved? Recently, Ning et al. (2018c) introduced a new dataset called Multi-Axis Temporal RElations for Start-points (MATRES). MATRES is still relatively small in its size (15K TempRels), but has a higher annotation quality from its improved task definition and annotation guideline. This paper uses MATRES to show that a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) system can readily outp"
D19-1642,W16-1007,0,0.0601282,"ina, November 3–7, 2019. 2019 Association for Computational Linguistics (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao"
D19-1642,D17-1108,1,0.88146,"gineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether"
D19-1642,P18-1212,1,0.929309,"imed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (L"
D19-1642,N18-1077,1,0.932269,"imed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (L"
D19-1642,P18-1122,1,0.908745,"imed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (L"
D19-1642,D18-2013,1,0.930316,"imed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (L"
D19-1642,W16-5706,0,0.246697,"Missing"
D19-1642,D18-2021,0,0.0146818,"for the graphs represented by a group of related TempRels (more details in the appendix). We also report the average of those three metrics in our experiments. Table 2 compares the two different ways to handle event positions discussed in Sec. 3.1: position indicators (P.I.) and simple concatenation (Concat), both of which are followed by network (d) in Fig. 1 (i.e., without using Siamese yet). We extensively studied the usage of various pretrained word embeddings, including conventional embeddings (i.e., the medium versions of word2vec, GloVe, and FastText provided in the Magnitude package (Patel et al., 2018)) and contextualized embeddings (i.e., the original ELMo and large uncased BERT, respectively); except for the input embeddings, we kept all other parameters the same. We used cross-entropy loss and the StepLR optimizer in PyTorch that decays the learning rate by 0.5 every 10 epochs (performance not sensitive to it). Comparing to the previously used P.I. (Dligach et al., 2017), we find that, with only two exceptions (underlined in Table 2), the Concat system saw consistent gains under various embeddings 6 http://cogcomp.org/page/publication_ view/835 7 http://cogcomp.org/page/publication_ view"
D19-1642,D14-1162,0,0.0879892,"Missing"
D19-1642,N18-1202,0,0.0603908,"Missing"
D19-1642,P09-1046,0,0.0370764,"rceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the l"
D19-1642,C14-1220,0,0.0489455,"SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task. A recent annotation scheme,"
D19-1642,Y15-1009,0,0.0125359,"Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task. A recent annotation scheme, Ning et al. (2018c),"
D19-1642,P17-2035,0,0.0434402,", 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016). There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018). However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results). Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task. A recent annotation scheme, Ning et al. (2018c), introduced the notion of multi-axis to represent the temporal structure of text, and identified that one of the sources of confusions in human annotation is asking annotat"
D19-1642,S13-2001,0,0.133911,"onference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6203–6209, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et a"
D19-1642,S07-1014,0,0.102486,"203 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6203–6209, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017). Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Z"
D19-1642,C08-3012,0,0.216541,"roposed system is public2 and can serve as a strong baseline for future research. 2 Related Work Early computational attempts to TempRel extraction include Mani et al. (2006); Chambers et al. 1 For example, “explode” typically happens before “die”. https://cogcomp.org/page/publication_ view/879 2 6203 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6203–6209, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008), which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events. The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Bethard et al., 2015, 2016, 2017), and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al"
D19-1650,P02-1061,0,0.157172,"sing and the 9th International Joint Conference on Natural Language Processing, pages 6256–6261, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work This problem of robustness in casing has been studied in the context of NER and truecasing. Robustness in NER A practical, common solution to this problem is summarized by the Stanford CoreNLP system (Manning et al., 2014): train on uncased text, or use a truecaser on test data.2 We include these suggested solutions in our analysis below. In one of the few works that address this problem directly, Chieu and Ng (2002) describe a method similar to co-training for training an upper case NER, in which the predictions of a cased system are used to adjudicate and improve those of an uncased system. One difference from ours is that we are interested in having a single model that works on upper or lowercased text. When tagging text in the wild, one cannot know a priori if it is consistently cased or not. Truecasing Truecasing presents a natural solution for situations with noisy or uncertain text capitalization. It has been studied in the context of many fields, including speech recognition (Brown and Coden, 2001"
D19-1650,W11-1601,0,0.0139281,"s.3 Briefly, each sentence is split into characters (including spaces) and modeled with a 2-layer bidirectional LSTM, with a linear binary classification layer on top. We train the truecaser on a dataset from 2 https://stanfordnlp.github.io/ CoreNLP/caseless.html 3 cogcomp.org/page/publication_view/881 System Test set F1 (Susanto et al., 2016) Wikipedia 93.19 BiLSTM Wikipedia CoNLL Train CoNLL Test PTB 01-18 PTB 22-24 93.01 78.85 77.35 86.91 86.22 Table 2: Truecaser word-level performance on English data. This truecaser is trained on the Wikipedia corpus. Wikipedia refers to the test set from Coster and Kauchak (2011). CoNLL Test refers to testb. PTB is the Penn Treebank. Wikipedia, originally created for text simplification (Coster and Kauchak, 2011), but commonly used for evaluation in truecasing papers (Susanto et al., 2016). This task has the convenient property that if the data is well-formed, then supervision is free. We evaluate this truecaser on several data sets, measuring F1 on the word level (see Table 2). At test time, all text is lowercased, and case labels are predicted. First, we evaluate the truecaser on the same test set as Susanto et al. (2016) in order to show that our implementation is"
D19-1650,N19-1423,0,0.0379538,"ndards as seen in Section 3. However, experiment 5 shows that if the training data is also truecased, then the performance is good, especially in situations where the test data is known to contain no case information. Training only on uncased data gives good performance in both NER and POS – in fact the highest performance on uncased text in POS – but never reaches the overall average scores from experiment 3 or 3.5. We have repeated these experiments for NER in several different settings, including using only static embeddings, using a non-neural truecaser, and using BERT uncased embeddings (Devlin et al., 2019). While the relative performance of the experiments varied, the conclusion was the same: training on cased and uncased data produces the best results. When using uncased BERT embeddings, we 7 Application: Improving NER Performance on Twitter To further test our results, we look at the Broad Twitter Corpus4 (Derczynski et al., 2016), a dataset comprised of tweets gathered from a broad variety of genres, and including many noisy and informal examples. Since we are testing the robustness of our approach, we use a model trained on CoNLL 2003 data. Naturally, in any crossdomain experiment, one will"
D19-1650,W18-2501,0,0.016137,"th respect to the original with probability p ∈ [0, 1]. In experiment 1, p = 0. In experiment 2, p = 1. In experiment 3, p = 0.5. Our implementation is somewhat different from standard dropout in that our method is a preprocessing step, not done randomly at each epoch. 5 Experiments Before we show results, we will describe our experimental setup. We emphasize that our goal is to experiment with strong models in noisy settings, not to obtain state-of-the-art scores on any dataset. 5.1 NER We use the standard BiLSTM-CRF architecture for NER (Ma and Hovy, 2016), using an AllenNLP implementation (Gardner et al., 2018). We experiment with pre-trained contextual embeddings, ELMo (Peters et al., 2018), which are generated for each word in a sentence, and concatenated with GloVe word vectors (lowercased) (Pennington et al., 2014), and character embeddings. ELMo embeddings are trained with cased inputs, meaning that there will be some mismatch when generating embeddings for uncased text. In all experiments, we train on English CoNLL 2003 Train data (Tjong Kim Sang and De Meulder, 2003) and evaluate on the CoNLL 2003 Test data (testb). We always evaluate on two different versions: the original version, and a ver"
D19-1650,D15-1176,0,0.0683519,"Missing"
D19-1650,P16-1101,0,0.0329868,"for capitalization, where a sentence is lowercased with respect to the original with probability p ∈ [0, 1]. In experiment 1, p = 0. In experiment 2, p = 1. In experiment 3, p = 0.5. Our implementation is somewhat different from standard dropout in that our method is a preprocessing step, not done randomly at each epoch. 5 Experiments Before we show results, we will describe our experimental setup. We emphasize that our goal is to experiment with strong models in noisy settings, not to obtain state-of-the-art scores on any dataset. 5.1 NER We use the standard BiLSTM-CRF architecture for NER (Ma and Hovy, 2016), using an AllenNLP implementation (Gardner et al., 2018). We experiment with pre-trained contextual embeddings, ELMo (Peters et al., 2018), which are generated for each word in a sentence, and concatenated with GloVe word vectors (lowercased) (Pennington et al., 2014), and character embeddings. ELMo embeddings are trained with cased inputs, meaning that there will be some mismatch when generating embeddings for uncased text. In all experiments, we train on English CoNLL 2003 Train data (Tjong Kim Sang and De Meulder, 2003) and evaluate on the CoNLL 2003 Test data (testb). We always evaluate o"
D19-1650,P14-5010,0,0.00259809,"ethod for training data that results in a single model with high performance on both cased and uncased datasets. 6256 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6256–6261, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work This problem of robustness in casing has been studied in the context of NER and truecasing. Robustness in NER A practical, common solution to this problem is summarized by the Stanford CoreNLP system (Manning et al., 2014): train on uncased text, or use a truecaser on test data.2 We include these suggested solutions in our analysis below. In one of the few works that address this problem directly, Chieu and Ng (2002) describe a method similar to co-training for training an upper case NER, in which the predictions of a cased system are used to adjudicate and improve those of an uncased system. One difference from ours is that we are interested in having a single model that works on upper or lowercased text. When tagging text in the wild, one cannot know a priori if it is consistently cased or not. Truecasing Tru"
D19-1650,J93-2004,0,0.0689284,"in truecasing papers (Susanto et al., 2016). This task has the convenient property that if the data is well-formed, then supervision is free. We evaluate this truecaser on several data sets, measuring F1 on the word level (see Table 2). At test time, all text is lowercased, and case labels are predicted. First, we evaluate the truecaser on the same test set as Susanto et al. (2016) in order to show that our implementation is near to the original. Next, we measure truecasing performance on plain text extracted from the CoNLL 2003 English (Tjong Kim Sang and De Meulder, 2003) and Penn Treebank (Marcus et al., 1993) train and test sets. These results contain two types of errors: idiosyncratic casing in the gold data and failures of the truecaser. However, from the high scores in the Wikipedia experiment, we suppose that much of the score drop comes from idiosyncratic casing. This point is important: if a dataset contains idiosyncratic casing, then it is likely that NER or POS models have fit to that casing (especially with these two wildly popular datasets). As a result, truecasing, since it can’t recover these idiosyncrasies, is not likely to be the best plan. Notably, the scores on CoNLL are especially"
D19-1650,D14-1162,0,0.0789163,"od is a preprocessing step, not done randomly at each epoch. 5 Experiments Before we show results, we will describe our experimental setup. We emphasize that our goal is to experiment with strong models in noisy settings, not to obtain state-of-the-art scores on any dataset. 5.1 NER We use the standard BiLSTM-CRF architecture for NER (Ma and Hovy, 2016), using an AllenNLP implementation (Gardner et al., 2018). We experiment with pre-trained contextual embeddings, ELMo (Peters et al., 2018), which are generated for each word in a sentence, and concatenated with GloVe word vectors (lowercased) (Pennington et al., 2014), and character embeddings. ELMo embeddings are trained with cased inputs, meaning that there will be some mismatch when generating embeddings for uncased text. In all experiments, we train on English CoNLL 2003 Train data (Tjong Kim Sang and De Meulder, 2003) and evaluate on the CoNLL 2003 Test data (testb). We always evaluate on two different versions: the original version, and a version with all casing removed (e.g. everything lowercase). 5.2 POS Tagging We use a neural POS tagging model built with a BiLSTM-CRF (Ma and Hovy, 2016), and GloVe embeddings (Pennington et al., 2014), character e"
D19-1650,N18-1202,0,0.0292422,"experiment 2, p = 1. In experiment 3, p = 0.5. Our implementation is somewhat different from standard dropout in that our method is a preprocessing step, not done randomly at each epoch. 5 Experiments Before we show results, we will describe our experimental setup. We emphasize that our goal is to experiment with strong models in noisy settings, not to obtain state-of-the-art scores on any dataset. 5.1 NER We use the standard BiLSTM-CRF architecture for NER (Ma and Hovy, 2016), using an AllenNLP implementation (Gardner et al., 2018). We experiment with pre-trained contextual embeddings, ELMo (Peters et al., 2018), which are generated for each word in a sentence, and concatenated with GloVe word vectors (lowercased) (Pennington et al., 2014), and character embeddings. ELMo embeddings are trained with cased inputs, meaning that there will be some mismatch when generating embeddings for uncased text. In all experiments, we train on English CoNLL 2003 Train data (Tjong Kim Sang and De Meulder, 2003) and evaluate on the CoNLL 2003 Test data (testb). We always evaluate on two different versions: the original version, and a version with all casing removed (e.g. everything lowercase). 5.2 POS Tagging We use a"
D19-1650,D16-1225,0,0.0611221,"rtain text capitalization. It has been studied in the context of many fields, including speech recognition (Brown and Coden, 2001; Gravano et al., 2009), and machine translation (Wang et al., 2006), as the outputs of these tasks are traditionally lowercased. Lita et al. (2003) proposed a statistical, wordlevel, language-modeling based method for truecasing, and experimented on several downstream tasks, including NER. Nebhi et al. (2015) examine truecasing in tweets using a language model method and evaluate on both NER and POS. More recently, a neural model for truecasing has been proposed by Susanto et al. (2016), in which each character is associated with a label U or L, for upper and lower case respectively. This neural character-based method outperforms wordlevel language model-based prior work. 3 Truecasing Experiments We use our own implementation of the neural method described in Susanto et al. (2016) as the truecaser used in our experiments.3 Briefly, each sentence is split into characters (including spaces) and modeled with a 2-layer bidirectional LSTM, with a linear binary classification layer on top. We train the truecaser on a dataset from 2 https://stanfordnlp.github.io/ CoreNLP/caseless.h"
D19-1650,W03-0419,0,0.129512,"Missing"
D19-1650,N06-1001,0,0.282541,"or machine translation are traditionally without case. Ideally we would like a model to perform equally well on both cased and uncased text, in contrast with current models. 1 For POS tagging, this happens in tagsets that explicitly mark proper nouns, such as the Penn Treebank tagset. Prior solutions have included models trained on lowercase text, or models that automatically recover capitalization from lowercase text, known as truecasing. There has a been a substantial body of literature on the effect of truecasing applied after speech recognition (Gravano et al., 2009), machine translation (Wang et al., 2006), or social media (Nebhi et al., 2015). A few works that evaluate on downstream tasks (including NER and POS) show that truecasing improves performance, but they do not demonstrate that truecasing is the best way to improve performance. In this paper, we evaluate two foundational NLP tasks, NER and POS, on cased text and lowercased text, with the goal of maximizing the average score regardless of casing. To achieve this goal, we explore a number of simple options that consist of modifying the casing of the train or test data. Ultimately we propose a simple preprocessing method for training dat"
D19-1650,C16-1111,0,\N,Missing
E14-1038,N10-1019,0,0.102404,"e preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on"
E14-1038,P01-1005,0,0.102126,"Missing"
E14-1038,P11-1092,0,0.257009,"t with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most co"
E14-1038,P03-2026,0,0.364715,"Missing"
E14-1038,W13-3603,0,0.101105,"Missing"
E14-1038,W11-2838,0,0.179927,"Missing"
E14-1038,W12-2006,0,0.158705,"S tag and dependency of the governor of the preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of th"
E14-1038,C08-1022,0,0.168511,"Missing"
E14-1038,P08-1021,0,0.10352,"ancing.” “You ask me for some informations*/information- here they*/it are*/is.” “Nobody {has to be}*/{should be} late.” Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus. 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT section of the Gigaword corpus and contains about 2,600 of frequently-occurring words tagged as VB; for example, (3) will add shop but not shopping, but (4) will add both. For methods (3) and (4), we developed verbMorph,5 a tool that performs morphological analysis on verbs and is used to lemmatize verbs and to generate morphological variants. The module makes uses of (1) the verb lemma l"
E14-1038,I08-1059,0,0.528693,"ite Form choice that encompass the most common grammatical verb problems for ESL learners. The first two examples show mistakes on verbs that function as main verbs in a clause: sentence (1) shows an example of subject-verb Agreement error; (2) is an example of a Tense mistake where the ambiguity is between {will find} (Future tense) Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et 358 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–367, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and find (Present tense). Examples (3) and (4) display Form mistakes: confusing the infinitive and gerund forms in (3) and including an inflection on an infinitive verb in (4). This paper addresses the specific challenges of verb error correction that have not been addressed previously – identifying candidates for mistakes and determining which class of errors is pres"
E14-1038,P11-1121,0,0.147689,"svivek@cs.stanford.edu Abstract al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assume prior knowledge of the mistake type; however, identifying the specific category of a verb error is nontrivial, since the surface form of the verb may be ambiguous, especially when that verb is used incorrectly. Consider the following examples of verb mistakes: Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studi"
E14-1038,W13-3601,0,0.294354,"Missing"
E14-1038,D10-1032,0,0.0651336,"Missing"
E14-1038,W10-1004,1,0.910661,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,N10-1018,1,0.883695,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,P11-1093,1,0.953366,"a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mista"
E14-1038,P12-2039,0,0.0417645,"POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on features or on the spe"
E14-1038,P10-2065,0,0.230951,"Missing"
E14-1038,P11-1019,0,0.118417,"Missing"
E14-1038,W13-3602,1,\N,Missing
E17-5005,P11-1062,0,0.0332715,"ition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guidance, to software packages and applications. The goal of this tutorial is to introduce the computational framework to the broader ACL commuVivek Srikumar University of Utah svivek@cs.u"
E17-5005,D10-1125,0,0.0289042,"as in event recognition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guidance, to software packages and applications. The goal of this tutorial is to introduce the computational framework to the broader ACL commuVivek Srikumar Universi"
E17-5005,W04-2401,1,0.652046,"on, dependency parsing and semantic parsing. The setting is also appropriate for cases that may require making global decisions that involve multiple components, possibly pre-designed or prelearned, as in event recognition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP"
H05-1073,P97-1023,0,0.0168789,"emotional meaning, 580 such as happy or sad, matter, emotion classification probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study"
H05-1073,W05-0625,1,0.133096,"Missing"
H05-1073,P04-1045,0,0.0053032,"n order to be effective, emotion recognition must go beyond such resources; the authors note themselves that lexical affinity is fragile. The method was tested on 20 users’ preferences for an email-client, based on user-composed text emails describing short but colorful events. While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set. Whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive, e.g. (Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS). A short study by (Sugimoto et al., 2004) addresses sentence-level emotion recognition for Japanese TTS. Their model uses a composition assumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence. They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects and compute words’ emotional strength based on the ratio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects. Additionally, t"
H05-1073,W04-3253,0,0.252605,"Missing"
H05-1073,P04-1035,0,0.0487695,"Missing"
H05-1073,P02-1053,0,0.0132851,"on probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study This part covers the experimental study with a form"
H05-1073,C02-1150,1,\N,Missing
H05-1073,J04-3002,0,\N,Missing
H05-1073,P04-1034,0,\N,Missing
H05-2004,W05-0620,0,0.0327476,"Missing"
H05-2004,kingsbury-palmer-2002-treebank,0,0.0431051,"ion of lower level processors, while achieving effective real time performance. 1 Introduction Semantic parsing of sentences is believed to be an important subtask toward natural language understanding, and has immediate applications in tasks such information extraction and question answering. We study semantic role labeling (SRL), defined as follows: for each verb in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles (such as Agent, Patient or Instrument) and their adjuncts (such as Locative, Temporal or Manner). The PropBank project (Kingsbury and Palmer, 2002), which provides a large humanannotated corpus of semantic verb-argument relations, has opened doors for researchers to apply machine learning techniques to this task. The focus of the research has been on improving the performance of the SRL system by using, in addition to raw text, various syntactic and semantic information, e.g. Part of Speech (POS) tags, chunks, clauses, syntactic parse tree, and named entities, which is found crucial to the SRL system (Punyakanok et al., 2005). In order to support a real world application such as an interactive question-answering system, the ability of an"
H05-2004,W05-0625,1,0.858867,"Missing"
H05-2004,W05-0639,0,0.0165936,"Agent, Patient or Instrument) and their adjuncts (such as Locative, Temporal or Manner). The PropBank project (Kingsbury and Palmer, 2002), which provides a large humanannotated corpus of semantic verb-argument relations, has opened doors for researchers to apply machine learning techniques to this task. The focus of the research has been on improving the performance of the SRL system by using, in addition to raw text, various syntactic and semantic information, e.g. Part of Speech (POS) tags, chunks, clauses, syntactic parse tree, and named entities, which is found crucial to the SRL system (Punyakanok et al., 2005). In order to support a real world application such as an interactive question-answering system, the ability of an SRL system to analyze text in real time is a necessity. However, in previous research, the overall efficiency of the SRL system has not been considered. At best, the efficiency of an SRL system may be reported in an experiment assuming that all the necessary information has already been provided, which is not realistic. A real world scenario requires the SRL system to perform all necessary preprocessing steps in real time. The overall efficiency of SRL systems that include the pre"
H05-2004,W04-3212,0,0.0313041,"-art syntactic parser (Charniak, 2000) the output of which provides useful information for the main SRL module. The main SRL module consists of four stages: pruning, argument identification, argument classification, and inference. The following is the overview of these four stages. Details of them can be found in (Koomen et al., 2005). Pruning The goal of pruning is to filter out unlikely argument candidates using simple heuristic rules. Only the constituents in the parse tree are considered as argument candidates. In addition, our system exploits a heuristic modified from that introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. Argument Identification The argument identification stage uses binary classification to identify whether a candidate is an argument or not. We train and apply the binary classifiers on the constituents supplied by the pruning stage. Argument Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage. Inference The purpose of this stage is to incorporate some prior linguis"
H05-2004,A00-2018,0,\N,Missing
J08-2005,P98-1013,0,0.0806203,"Missing"
J08-2005,J04-4004,0,0.0226339,"Missing"
J08-2005,W04-2412,0,0.0418443,"Missing"
J08-2005,W05-0620,0,0.0974251,"Missing"
J08-2005,P01-1017,0,0.0485429,"ingsbury 2005), which provides a large human-annotated corpus of verb predicates and their arguments, has enabled researchers to apply machine learning techniques to develop SRL systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier 2003; Pradhan et al. 2003; Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004; Koomen et al. 2005). However, most systems rely heavily on full syntactic parse trees. Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak 2001) is still far from perfect. Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they do not provide as much information as a full syntactic parser, have been shown to be more robust in their speciﬁc tasks (Li and Roth 2001). This raises the very natural and interesting question of quantifying the importance of full parsing information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding SRL system. Although PropBank is built by adding semantic annotations to the constituents in the Penn Treebank syntactic"
J08-2005,W03-1006,0,0.0393928,"Missing"
J08-2005,W97-0306,1,0.150784,"Missing"
J08-2005,W01-0502,1,0.691822,"Missing"
J08-2005,W03-1008,0,0.026464,"Missing"
J08-2005,J02-3001,0,0.528607,"on features for the SRL task. The creation of PropBank was inspired by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation between syntactic and semantic information. Following this philosophy, the features aim to indicate the properties of the predicate, the constituent which is an argument candidate, and the relationship between them through the available syntactic information. We explain these features herein. For further discussion of these features, we 262 Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL refer the readers to the article by Gildea and Jurafsky (2002), which introduced these features. r r r r r r r Predicate and POS tag of predicate: indicate the lemma of the predicate verb and its POS tag. Voice: indicates passive/active voice of the predicate. Phrase type: provides the phrase type of the constituent, which is the tag of the corresponding constituent in the parse tree. Head word and POS tag of the head word: provides the head word of the constituent and its POS tag. We use the rules introduced by Collins (1999) to extract this feature. Position: describes if the constituent is before or after the predicate, relative to the position in the"
J08-2005,P02-1031,0,0.0284703,"Missing"
J08-2005,C04-1186,0,0.0639183,"Missing"
J08-2005,W04-2416,0,0.0274841,"Missing"
J08-2005,W05-0623,0,0.0290998,"Missing"
J08-2005,kingsbury-palmer-2002-treebank,0,0.289024,"Missing"
J08-2005,W05-0625,1,0.415721,"Missing"
J08-2005,W01-0706,1,0.302998,"nmaier 2003; Pradhan et al. 2003; Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004; Koomen et al. 2005). However, most systems rely heavily on full syntactic parse trees. Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak 2001) is still far from perfect. Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they do not provide as much information as a full syntactic parser, have been shown to be more robust in their speciﬁc tasks (Li and Roth 2001). This raises the very natural and interesting question of quantifying the importance of full parsing information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding SRL system. Although PropBank is built by adding semantic annotations to the constituents in the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing is for an SRL system. To the best of our knowledge, this problem was ﬁrst addressed by Gildea and Palmer (2002). In their attempt to use limited syntactic information, the parser they used wa"
J08-2005,J93-2004,0,0.0393983,"Missing"
J08-2005,W05-0628,0,0.0153512,"Missing"
J08-2005,J05-1004,0,0.721049,"Missing"
J08-2005,W05-0634,0,0.0443973,"Missing"
J08-2005,N04-1030,0,0.0125181,"Missing"
J08-2005,C04-1197,1,0.675102,"ent in Section 4.2 where the gold-standard boundaries are used with the parse trees generated by an automatic parse. In such cases, if the information on the constituent, such as phrase type, needs to be extracted, the deepest constituent that covers the whole argument will be used. For example, in Figure 1, the phrase type for by John Smith is PP, and its path feature to the predicate assume is PP↑VP↓VBN. We also use the following additional features. These features have been shown to be useful for the systems by exploiting other information in the absence of the full parse tree information (Punyakanok et al. 2004), and, hence, can be helpful in conjunction with the features extracted from a full parse tree. They also aim to encode the properties of the predicate, the constituent to be classiﬁed, and their relationship in the sentence. r r Context words and POS tags of the context words: the feature includes the two words before and after the constituent, and their POS tags. Verb class: the feature is the VerbNet (Kipper, Palmer, and Rambow 2002) class of the predicate as described in PropBank Frames. Note that a 263 Computational Linguistics Volume 34, Number 2 verb may inhabit many classes and we coll"
J08-2005,W04-2401,1,0.799617,"s were always made for each argument independently, ignoring the global information across arguments in the ﬁnal output. The purpose of the inference stage is to incorporate such information, including both linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one argument of each type.” This knowledge is useful to resolve any inconsistencies of argument classiﬁcation in order to generate ﬁnal legitimate predictions. We design an inference procedure that is formalized as a constrained optimization problem, represented as an integer linear program (Roth and Yih 2004). It takes as input the argument classiﬁers’ conﬁdence scores for each type of argument, along with a list of constraints. The output is the optimal solution that maximizes the linear sum of the conﬁdence scores, subject to the constraints that encode the domain knowledge. The inference stage can be naturally extended to combine the output of several different SRL systems, as we will show in Section 5. In this section we ﬁrst introduce the constraints and formalize the inference problem for the semantic role labeling task. We then demonstrate how we apply integer linear programming (ILP) to ge"
J08-2005,W00-0726,0,0.00973031,"Missing"
J08-2005,W04-3212,0,0.119377,"Missing"
J08-2005,P97-1003,0,\N,Missing
J08-2005,J03-4003,0,\N,Missing
J08-2005,C98-1013,0,\N,Missing
J08-2005,P03-1002,0,\N,Missing
J08-3005,P01-1005,0,0.00953516,"oW has a number of extensions such as regularization and good treatment of multiclass classiﬁcation. SNoW provides, in addition to classiﬁcation, a reliable conﬁdence in the instance prediction which facilitates its use in an inference algorithm that combines predictors to produce a coherent inference. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including part-of-speech tagging, shallow parsing, information extraction tasks, and so forth, and compared favorably with other classiﬁers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and 432 Daya, Roth, and Wintner Identifying Semitic Roots Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as the learning algorithm in this work is motivated by its good performance on other, similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package. As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental papers thereafter, most algorithms used today, from on-line variations of Winnow and Perceptron to maximum e"
J08-3005,W98-1007,0,0.0383304,"is inserted into the ﬁrst consonantal slot of the pattern, the second ﬁlls the second slot, and the third ﬁlls the last slot. See Shimron (2003) for a survey. We present a machine learning approach, augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic words. To the best of our knowledge, this is the ﬁrst application of machine learning to this problem, and one of the few attempts to directly address the non-concatenative morphology of Semitic languages using machine learning. Although there exist programs which can extract the roots of words in Arabic (Beesley 1998a, 1998b) and Hebrew (Choueka 1990), they are all dependent on labor-intensive construction of large-scale lexicons which are components of full-scale morphological analyzers. Note that the Arabic morphological analyzer of Buckwalter (2002, software documentation) only uses “word stems—rather than root and pattern morphemes—to identify lexical items.” Buckwalter further notes that “The information on root and pattern morphemes could be added to each stem entry if this were desired.” The challenge of our work is to automate this process, avoiding the bottleneck of having to laboriously list the"
J08-3005,W04-3246,1,0.337041,"Missing"
J08-3005,W01-0502,1,0.874929,"Missing"
J08-3005,W02-2010,0,0.136782,"NoW provides, in addition to classiﬁcation, a reliable conﬁdence in the instance prediction which facilitates its use in an inference algorithm that combines predictors to produce a coherent inference. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including part-of-speech tagging, shallow parsing, information extraction tasks, and so forth, and compared favorably with other classiﬁers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and 432 Daya, Roth, and Wintner Identifying Semitic Roots Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as the learning algorithm in this work is motivated by its good performance on other, similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package. As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental papers thereafter, most algorithms used today, from on-line variations of Winnow and Perceptron to maximum entropy algorithms to SVMs, perform comparably if tuned properly, and the eventual performance"
J08-3005,P05-1071,0,0.0623824,".shtml). The system can be used for practical applications or for scientiﬁc (linguistic) research, and constitutes an important addition to the growing set of resources dedicated to Semitic languages. It is one of the few attempts to directly address the non-concatenative morphology of Semitic languages and extract non-contiguous morphemes from surface forms. As a machine learning application, this work describes a set of experiments in combination of classiﬁers under constraints. The resulting insights can be used for other applications of the same techniques for similar problems (see, e.g., Habash and Rambow 2005). Furthermore, this work demonstrates that providing a data-driven classiﬁer with limited linguistic knowledge signiﬁcantly improves the classiﬁcation results. We focus on Hebrew in the ﬁrst part of this article. After sketching the linguistic data in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple, baseline, learning approach. We then propose several methods for combining the results 2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior to character n-grams for this task. 430 Daya, Roth, and Wintner Identifying Semitic Roo"
J08-3005,W05-0639,0,0.045069,"Missing"
J08-3005,P94-1025,0,0.0349979,"Missing"
J08-3005,W03-0419,0,0.0357704,"Missing"
J08-3005,W02-0506,0,\N,Missing
J17-4002,K15-1010,0,0.0507327,"Missing"
J17-4002,P15-1068,0,0.0280758,"Missing"
J17-4002,N13-1055,0,0.50182,"ower than the MT approach of Junczys-Dowmunt and Grundkiewicz (2016), the combined system uses a much weaker MT component (which scores 39.48 F0.5, when used by itself). We expect that a combined system that used a better MT component would perform significantly better than Junczys-Dowmunt and Grundkiewicz (2016) and, in addition, would handle better some types of mistakes that MT systems do not do well on. Adaptation Using Artificial Errors Several other researchers study the effect of the adaptation framework that uses artificial errors. The two most closely related studies are the works by Cahill et al. (2013) and Felice and Yuan (2014). Cahill et al. researched the effects of different training paradigms with different data sets on the preposition error correction task. They show improvements when using artificial errors, although their selection models were not optimized with respect to F-score (typically, models trained on well-edited text use a threshold, as we do in this work, because 752 Rozovskaya, Roth, and Sammons Adapting to Learner Errors with Minimal Supervision otherwise these models tend to have extremely low precision, which negatively affects the F-score). Because the original model"
J17-4002,W07-1604,0,0.0553449,"Missing"
J17-4002,P11-1092,0,0.184477,"r evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale"
J17-4002,D12-1052,0,0.0514367,"features. Parameter Tuning Ten percent of the training data is used to optimize the inflation rate for the AP-adapted models (0.8 for articles and prepositions, and 0.85 for verb agreement). 4.3 Note on Evaluation Metrics Various metrics have been proposed and used in error correction. These can be broken down roughly into metrics that compute the accuracy of the system and those that use the F-measure. The HOO competitions adopted F1, which can take into account both precision and recall of the systems; the M2 scorer used in CoNLL is also F-based but can take into account phrase-based edits (Dahlmeier and Ng 2012). Overall, accuracy and F-measure are equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an"
J17-4002,W13-1703,0,0.192291,"Missing"
J17-4002,W12-2006,0,0.0686499,"Missing"
J17-4002,W11-2838,0,0.234377,"b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale, Anisimoff, and Narroway 2012; Ng et al. 2013, 2014), makes use of a machine-learning classifier paradigm and is based on the methodology for correcting context-sensitive spelling mistakes made by native speakers. With the exception that learners and native writers exhibit errors of different types, most of the grammar and usage mistakes made by non-native speakers of English also fall into the category of context-sensitive errors that result in valid English words (e.g., articles or prepositions) being confused. Traditionally, following the work on context-sensitive spelling, classifie"
J17-4002,W07-1607,0,0.0850399,"Missing"
J17-4002,C08-1022,0,0.199451,"Missing"
J17-4002,N15-1060,0,0.0362442,"se-based edits (Dahlmeier and Ng 2012). Overall, accuracy and F-measure are equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an I-measure that is accuracy-based and Napoles et al. (2015) proposed a variation of the BLEU metric used in Machine Translation (called GLEU). Further, Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) also compare the outputs of the systems in the CoNLL shared task against human judgments and show that the need for new metrics is motivated by a lack of correlation between F1 and human judgments. However, the newly proposed GLEU metric does not fare much better in terms of correlation with human judgments than the F-measure and the I-measure. Developing a"
J17-4002,E14-3013,0,0.355184,"of Junczys-Dowmunt and Grundkiewicz (2016), the combined system uses a much weaker MT component (which scores 39.48 F0.5, when used by itself). We expect that a combined system that used a better MT component would perform significantly better than Junczys-Dowmunt and Grundkiewicz (2016) and, in addition, would handle better some types of mistakes that MT systems do not do well on. Adaptation Using Artificial Errors Several other researchers study the effect of the adaptation framework that uses artificial errors. The two most closely related studies are the works by Cahill et al. (2013) and Felice and Yuan (2014). Cahill et al. researched the effects of different training paradigms with different data sets on the preposition error correction task. They show improvements when using artificial errors, although their selection models were not optimized with respect to F-score (typically, models trained on well-edited text use a threshold, as we do in this work, because 752 Rozovskaya, Roth, and Sammons Adapting to Learner Errors with Minimal Supervision otherwise these models tend to have extremely low precision, which negatively affects the F-score). Because the original models were not optimized, it is"
J17-4002,W14-1702,0,0.0128573,"relevant confusion set. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the application of the selection training approach to ESL error correction, a model is tailored toward one mistake type (e.g., errors involving preposition usage) and is trained on well-formed native English text with features defined based on the surr"
J17-4002,W09-2112,0,0.070649,"from the FCE corpus. S and INF stand for third person singular and bare verb form, respectively. The left column shows the correct verb. Each row shows the author’s verb choices for that label and Prob(source|label). The numbers next to the targets show the count of the label (or source) in the data set. Label S (640) INF (2,192) WAS (593) WERE (157) Sources S (627) INF (2,205) WAS (601) WERE (149) 0.961 0.005 - 0.039 0.995 - 0.997 0.064 0.003 0.936 is ensured by generating artificial mistakes using the confusion matrix. The idea of using artificial errors goes back to Izumi et al. (2003) and Foster and Andersen (2009). The approach discussed here refers to the adaptation method originally proposed in Rozovskaya and Roth (2010b) and its modified version in Rozovskaya, Sammons, and Roth (2012). In Rozovskaya and Roth (2010b), artificial errors are generated using the distribution of naturally-occurring errors. This version of the approach suffers from the low recall problem, as discussed subsequently. Rozovskaya, Sammons, and Roth (2012) describe a more general method of using artificial errors for adaptation and also solve the low recall problem. This is the method we describe here. Generating Artificial Er"
J17-4002,N10-1019,0,0.42737,"d tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in te"
J17-4002,W11-1422,0,0.0179001,"ically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale, Anisimoff, and Narroway 2012; Ng et al. 2013, 2014), makes use of a machine-learning classifier paradigm and is based on the methodology for correcting co"
J17-4002,I08-1059,0,0.58607,"these methods ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these"
J17-4002,D15-1052,0,0.0329163,"Missing"
J17-4002,han-etal-2010-using,0,0.0320065,"Missing"
J17-4002,P03-2026,0,0.473253,"putational Linguistics Volume 43, Number 4 the Illinois system that implements these methods ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a na"
J17-4002,W14-1703,0,0.0208495,"et. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the application of the selection training approach to ESL error correction, a model is tailored toward one mistake type (e.g., errors involving preposition usage) and is trained on well-formed native English text with features defined based on the surrounding context. Task-specific confusion"
J17-4002,D16-1161,0,0.208624,"on the CoNLL corpus, the performance is quite poor (28.25). Thus, all other MT systems use an additional source of supervision—either the Cambridge Learner corpus (CLC), which is a larger version of FCE, or the publicly available Lang-8 corpus. The Lang-8 corpus used in the literature has several versions, depending on when it was collected and how much noise has been removed from it. The smallest version used by Hoang, Chollampatt, and Ng (2016) and Chollampatt, Taghipour, and Ng (2016) contains about 11M words; the one in Mizumoto and Matsumoto (2016) is about twice as large; the version in Junczys-Dowmunt and Grundkiewicz (2016) contains about 30M words; the MT component in Rozovskaya and Roth (2016) has 48M words. Because of these differences in size and quality, the comparisons are not fair. For instance, Junczys-Dowmunt and Grundkiewicz (2016) report an F0.5 score of 52.21 using a version closer to 50M words, which was used in 751 Computational Linguistics Volume 43, Number 4 Table 19 State-of-the-art systems on CoNLL-2014 data set. The systems are divided into three categories: classifiers, MT, and combined. For each system, we also show the size of the annotated learner data used to train the system. Depending o"
J17-4002,P08-1021,0,0.465416,"next section. These error patterns may be prominent across multiple first languages or be first-language dependent. The effect of “language transfer”—applying knowledge from the native language, when learning a foreign language—has been the subject of considerable study in the second-language acquisition literature (Odlin 1989; Gass and Selinker 1992; Montrul 2000; Montrul and Slabakova 2002; Oh and Zubizaretta 2003; Ionin, Zubizarreta, and Bautista 2008). These facts have also been confirmed empirically by studies that quantitatively examine learner corpora (Han, Chodorow, and Leacock 2006; Lee and Seneff 2008). For example, speakers of languages that do not have a determiner system (e.g., Russian) tend to make 4–5 times more article mistakes in English than speakers whose first language has articles (Rozovskaya and Roth 2010b). In addition to the error regularities due to first language influence, some confusions are much more likely to occur than others across multiple first languages. For example, regardless of the first language, ESL writers are 38 times more likely to incorrectly use “in” rather than “by” in place of the correct word “on” (Table 2, Section 3.1). Training for Correction Tasks De"
J17-4002,C12-2084,0,0.0177187,"al, contextual information, such as the POS tags of the head nouns for article errors, the surface form of the verbs for verb agreement mistakes, and semantic classes of the nouns for preposition mistakes. The adaptation method is evaluated in the context of training statistical machine translation systems for grammar correction, so that work is not directly comparable to ours. Furthermore, no improvements are reported of the baseline of training on a corpus containing natural errors. Large-scale Error-annotated Corpora Two large-scale error-annotated corpora became available recently—Lang-8 (Mizumoto et al. 2012) and corpus Wikirev (Cahill et al. 2013)—that several researchers have used in developing grammar correction systems. Similar to results reported in this work, Cahill et al. (2013) show that using these corpora with artificially generated errors or naturally occurring errors typically outperforms models trained in the selection paradigm.9 We wish to emphasize that the focus of the current work is on methods that use minimal supervision, when error-annotated data are not available, such as low-frequency errors and languages other than English. 7. Discussion and Conclusion This article addressed"
J17-4002,P13-1112,0,0.0464128,"Missing"
J17-4002,P15-2097,0,0.0254983,"equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an I-measure that is accuracy-based and Napoles et al. (2015) proposed a variation of the BLEU metric used in Machine Translation (called GLEU). Further, Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) also compare the outputs of the systems in the CoNLL shared task against human judgments and show that the need for new metrics is motivated by a lack of correlation between F1 and human judgments. However, the newly proposed GLEU metric does not fare much better in terms of correlation with human judgments than the F-measure and the I-measure. Developing a new, more appropriate metric is an involved issue that is beyond the sc"
J17-4002,W14-1701,0,0.046607,"be error-free, where each target word occurrence (e.g., peace) is treated as a positive training example for the corresponding word. Given a text to correct, for each confusable word, the task is to select the most likely candidate from the relevant confusion set. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the appl"
J17-4002,W13-3601,0,0.111029,"Missing"
J17-4002,W13-3602,1,0.925905,"Missing"
J17-4002,W14-1704,1,0.929151,"of training either on annotated or native data alone. This is because, in contrast to training on annotated ESL data, the adaptation approach only requires a small amount of annotation to estimate the parameters related to error regularities, while context parameters can be learned from native data. In sum, the proposed methods allow us to combine the advantages of training on native and annotated learner data. The proposed adaptation framework was implemented as part of the Illinois system that came first in several text correction competitions, including the prestigious CoNLL shared tasks (Rozovskaya et al. 2014, 2013). We further evaluate the proposed approaches and study the effect of adaptation when using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages. This article unifies and significantly extends material that appeared previously in Rozovskaya and Roth (2010b, 2011, 2014), and Rozovskaya, Sammons and Roth (2012). The novel contribution is concentrated in Section 5, and evaluates the adaptation approach by comparing performance when error statistics are drawn from the writer’s native (target) language data vs. from"
J17-4002,D10-1094,1,0.83317,"rror correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions"
J17-4002,N10-1018,1,0.621593,"rror correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions"
J17-4002,P11-1093,1,0.939673,"ion algorithms that we develop for this purpose. After we describe how error patterns are learned, two adaptation methods are presented. The proposed methods are designed to work with two state-of-the-art machine learning algorithms: The artificial errors adaptation method is applicable for a discriminative learning algorithm (implemented here within Averaged Perceptron – henceforth AP), and the priors adaptation method for the Naive Bayes (NB) algorithm. These two algorithms demonstrated superior performance in a study that compared several learning frameworks for ESL error correction tasks (Rozovskaya and Roth 2011). This study also included language models and other count-based methods. 3.1 Learning Error Patterns Error patterns are extracted from annotated learner data; these error patterns are referred to as error statistics. As we show here, unlike the context parameters, the error statistics are very simple, and thus we only need a small annotated sample to estimate them. Given a specific task, we collect all source/label pairs from the annotated sample, where both the source and the label belong to the confusion set, and generate two 729 Computational Linguistics Volume 43, Number 4 Table 1 Confusi"
J17-4002,Q14-1033,1,0.838077,"the frequencies of the respective target words in WikiNYT. To compare with the WikiNYT, Web1T contains on the order of 10,000 more training examples for each error type. Using the two corpora allows us to evaluate the proposed adaptation methods when applying two state-of-the-art machine learning algorithms and to demonstrate how to take advantage of the benefits provided by each data source and each machine learning framework. On WikiNYT, we train discriminatively using the AP algorithm and rich syntactic features shown to be useful for article and verb agreement errors (Lee and Seneff 2008; Rozovskaya and Roth 2014) and for preposition errors (Tetreault, Foster, and Chodorow 2010). Because of the special format of the Web1T corpus, it is difficult to generate rich feature annotations for this data, and to make use of a discriminative classifier on this corpus, as one would have to limit the surrounding context to two words on each side of the mistake. Because we wish to make use of the context features that extend beyond the two-word window, it is only possible to use count-based methods (e.g., NB or language models). We thus train the NB algorithm. Learner Data Several annotated learner data sets were m"
J17-4002,P16-1208,1,0.787987,"Missing"
J17-4002,W11-2843,1,0.841078,"nd other fine-grained adaptations, see Section 5. 4.2 Experimental Set-up Depending on what training data source is used, we refer to the models as follows: 1. Native-trained models: trained on native English data in the selection paradigm 2. ESL-trained models: trained on annotated learner data in the correction training paradigm 3. Adapted models: trained on native English data and adapted using annotated learner data Features AP models trained on WikiNYT use rich features tailored to each error type. These features were used in the components of the Illinois system in several shared tasks (Rozovskaya et al. 2011, 2013) and are presented in Appendix Tables A.1, A.2, A.3, and A.4 for convenience. The Web1T models are trained on word n-gram features. Parameter Tuning Ten percent of the training data is used to optimize the inflation rate for the AP-adapted models (0.8 for articles and prepositions, and 0.85 for verb agreement). 4.3 Note on Evaluation Metrics Various metrics have been proposed and used in error correction. These can be broken down roughly into metrics that compute the accuracy of the system and those that use the F-measure. The HOO competitions adopted F1, which can take into account bot"
J17-4002,W12-2032,1,0.918662,"Missing"
J17-4002,D14-1102,0,0.107547,"Missing"
J17-4002,C08-1109,0,0.232257,". Note that because native data do not have information about learner errors, the model can only use contextual cues. Thus, when the resulting classifier is applied to non-native text, the most appropriate preposition is selected based exclusively on the surrounding context, similar to a cloze task where one needs to “guess” a word that has been replaced by blank in a sentence. An error is flagged if this most likely candidate is different from the author’s choice (Eeg-Olofsson and Knuttson 2003; Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault and Chodorow 2008; Tetreault, Foster, and Chodorow 2010). We call this general approach to error correction the selection training paradigm. In realistic ESL situations, however, the scenario is different and there is additional information that could be used by a correction system beyond that used in the selection paradigm. The ESL learner writes a text, and typically makes mistakes on 1 The Illinois system ranked first in all metrics in the CoNLL-2013 competition and scored second and first on original and revised annotation metrics, respectively, in the 2014 competition. 2 This article unifies and significa"
J17-4002,P10-2065,0,0.102557,"Missing"
J17-4002,P11-1019,0,0.0649247,"this data, and to make use of a discriminative classifier on this corpus, as one would have to limit the surrounding context to two words on each side of the mistake. Because we wish to make use of the context features that extend beyond the two-word window, it is only possible to use count-based methods (e.g., NB or language models). We thus train the NB algorithm. Learner Data Several annotated learner data sets were made available recently, including the data set used in the HOO competition, the CoNLL data set (Dahlmeier, Ng, and Wu 2013), and the FCE data set (Yannakoudakis, Briscoe, and Medlock 2011), a subset of the Cambridge Learner corpus. Because we want to explore the effects of first language backgrounds, we use the FCE corpus in this work. This corpus contains data from learners of multiple language backgrounds, including information on the first language of the writer. The work described in Section 5 makes use of this information. We discuss other corpora and approaches in Section 6. The FCE corpus contains 1,244 essays (500,000 words) produced by learners of 16 first language backgrounds. The data set is fully corrected and error tagged. For the key adaptation experiments, we spl"
J17-4002,N16-1042,0,0.0636681,"s, as some error phenomena, such as article and verb agreement errors, are better handled using classifiers (Rozovskaya and Roth 2016). Regarding the amount of supervision, MT systems require significant annotation: All of the recently published MT systems are trained using Lang-8 corpus, which contains between 11M and 48M words of annotated learner data, depending on the corpus version and the pre-processing that was performed (Susanto, Phandi, and Ng 2014; Chollampatt, Taghipour, and Ng 2016; Hoang, Chollampatt, and Ng 2016; JunczysDowmunt and Grundkiewicz 2016; Mizumoto and Matsumoto 2016; Yuan and Briscoe 2016). Although most of the recent publications report results on the CoNLL-2014 test set, those systems are evaluated on global behavior (i.e., the whole corpus) and do not focus or evaluate performance on specific error phenomena, as we do in this work. Further, because we care about specific first-language backgrounds, CoNLL is not an appropriate data set to use: The FCE corpus is the only one that contains data from learners of multiple language backgrounds and information on the first language of the writer, used in Section 5. Therefore, in this work we make use of the FCE data set. However, w"
J17-4002,P01-1005,0,\N,Missing
J17-4002,N16-1133,0,\N,Missing
J19-4002,H05-1071,0,0.0754747,"Missing"
J19-4002,D07-1087,0,0.00946961,"cally extracting structured information from unstructured and/or semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is l"
J19-4002,W04-2504,0,0.561617,"; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes,"
J19-4002,P08-1007,0,0.0604381,"Missing"
J19-4002,W12-3014,0,0.0254481,"; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering const"
J19-4002,J87-1002,0,0.504203,"try problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 201"
J19-4002,C14-1206,0,0.049657,"Missing"
J19-4002,W16-1303,0,0.0164499,"04; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering constraints to improve th"
J19-4002,N10-1031,0,0.0263931,"Missing"
J19-4002,P09-1075,0,0.0225883,"component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Ch"
J19-4002,P12-1007,0,0.0226245,"c devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et"
J19-4002,P14-1048,0,0.380757,"to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse conside"
J19-4002,D14-1168,0,0.0268694,"ann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into"
J19-4002,W12-1622,0,0.0534271,"Missing"
J19-4002,J86-3001,0,0.739399,"isting solver for geometry problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and"
J19-4002,P14-1092,0,0.0603686,"Missing"
J19-4002,P14-1002,0,0.0243145,"nicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without m"
J19-4002,P13-1127,0,0.012602,"Language to Programs: After harvesting axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-s"
J19-4002,P14-1003,0,0.0224181,"The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting."
J19-4002,D15-1193,0,0.0155235,"semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our appr"
J19-4002,P11-1060,0,0.017064,"Missing"
J19-4002,W04-1013,0,0.00883788,"iscourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure Indicator matching the current (and parent) node of axiom mentions in respective JSON hierarchies; i.e., are both nodes mentioned as axioms, diagrams or bounding boxes? Equation Template Indicator feature that matches templates of equations detected in the axiom mentions. The template matcher is designed such that it identifies various rewritings of the same axiom equation, e.g., PA × PB = PT2 and PA × PB = PC2 could refer to the same axiom with point T in one axiom mention"
J19-4002,P16-1057,0,0.0602349,"Missing"
J19-4002,P17-1015,0,0.0126677,"ooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text have been rare beca"
J19-4002,W10-4327,0,0.0608886,"Missing"
J19-4002,P14-5010,0,0.00542909,"Missing"
J19-4002,J96-3006,0,0.282068,"ore explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 20"
J19-4002,P15-1121,0,0.0128899,"nd Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes, indentations, or quotations. It may refer to asso"
J19-4002,P15-1085,0,0.0213097,"Missing"
J19-4002,P15-1024,1,0.90458,"Missing"
J19-4002,D15-1171,0,0.193157,"es in a multimedia document (Hovy 1998) for the various stages of information extraction. Our experiments show the usefulness of all the various typographical features over and above the various lexical semantic and discourse level features considered for the task. We use our model to extract and parse axiomatic knowledge from a novel data set of 20 publicly available math textbooks. We use this structured axiomatic knowledge to build a new axiomatic solver that performs logical inference to solve geometry problems. Our axiomatic solver outperforms GEOS on all existing test sets introduced in Seo et al. (2015) as well as a new test set of geometry questions collected from these textbooks. We also performed user studies on a number of school students studying geometry who found that our axiomatic solver is more interpretable and useful compared with GEOS. 2. Background and Related Work Discourse Analysis: Discourse analysis is the analysis of semantics conveyed by a coherent sequence of sentences, propositions, or speech. Discourse analysis is taken up in a variety of disciplines in the humanities and social sciences and a number of discourse theories have been proposed (Mann and Thompson 1988; Kamp"
J19-4002,N03-1030,0,0.361741,"Missing"
J19-4002,N09-1064,0,0.0330417,"Missing"
J19-4002,K15-2002,0,0.0155504,"icking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is o"
J19-4002,P13-2123,0,0.0111238,"es (constants, predicates, and functions) across the two axioms. When comparing geometric entities, we include geometric entities derived from the associated diagrams when available. Longest Common Subsequence Real valued feature that computes the length of longest common subsequence of words between two axiom mentions normalized by the total number of words in the two mentions. Number of discourse elements Real valued feature that computes the absolute difference in the number of discourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure"
J19-4002,P17-1041,0,0.0605138,"ing axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text"
K15-1002,D08-1031,1,0.843784,"mention is inside the boundary of another mention), but mention heads never overlap. This property also simplifies the problem of mention head candidate generation. In the example above, the first “they” refers to “Multinational companies investing in China” and the second “They” refers to “Domestic manufacturers, who are also suffering”. In both cases, the mention heads are sufficient to support the decisions: ”they” refers to ”companies”, and ”They” refers to ”manufacturers”. In fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads (Bengtson and Roth, 2008). Furthermore, consider the possible mention candidate “league” (italic in the text). It is not chosen as a mention because the surrounding context is not focused on “anti-piracy league”. So, mention the CoNLL-2012 dataset is built from OntoNotes-5.0 corpus. 2 This example is chosen from the ACE-2004 corpus. 3 All features except for those that rely on modifiers. 13 Figure 1: Comparison between a traditional pipelined system and our proposed system. We split up mention detection into two steps: mention head candidate generation and (an optional) mention boundary detection. We feed mention head"
K15-1002,W11-1902,0,0.0180443,"tuency parsing information and gold named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3 M) as our mention-pair coreference model in the joint framework10 . When the CL3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention dete"
K15-1002,P14-1005,0,0.0670586,"Missing"
K15-1002,D12-1045,0,0.0370925,"hurch (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree 20 structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. Lee et al. (2012) model entity coreference and event coreference jointly; Durrett and Klein (2014) consider joint coreference and entity-linking. The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different. 6 Conclusion This paper proposes a joint inference approach to the end-to-end coreference resolution problem. By moving to identify mention heads rather than mentions, and by developing an ILP-ba"
K15-1002,P98-1034,0,0.0737942,"2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3 M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non-local features, and integration of external knowledge. NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Co"
K15-1002,H05-1004,0,0.296323,"m achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ1 = 0.25 and λ2 = 0.2. erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref. We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M. Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). We use the average F1 scores (AVG) of these three metrics as the main metric for comparison. We use the v7.0 scorer provided by CoNLL-2012 Shared Task11 . We also evaluate the mention detection performance based on precision, recall and F1 score. As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads. 4.2 Performance for Coreference Resolution Performance of coreference resolution for all systems on the ACE-2004 and CoNLL-2012 datasets is shown in Table 2 and Table 3 respectively.12 These results show that our d"
K15-1002,W11-1904,1,0.94182,"d to using gold mentions. The rest of the paper is organized as follows. We explain the joint head-coreference learning and inference framework in Sec. 2. Our mention head candidate generation module and mention boundary detection module are described in Sec. 3. We report our experimental results in Sec. 4, review related work in Sec. 5 and conclude in Sec. 6. 2 A Joint Head-Coreference Framework This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from Chang et al. (2011). The joint learning and inference model takes as input mention head candidates 4 Available at http://cogcomp.cs.illinois. edu/page/software_view/Coref 14 (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head"
K15-1002,P02-1014,0,0.492026,"Missing"
K15-1002,D13-1057,1,0.82786,"d mentions and reduce the performance gap compared to using gold mentions. The rest of the paper is organized as follows. We explain the joint head-coreference learning and inference framework in Sec. 2. Our mention head candidate generation module and mention boundary detection module are described in Sec. 3. We report our experimental results in Sec. 4, review related work in Sec. 5 and conclude in Sec. 6. 2 A Joint Head-Coreference Framework This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from Chang et al. (2011). The joint learning and inference model takes as input mention head candidates 4 Available at http://cogcomp.cs.illinois. edu/page/software_view/Coref 14 (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions joi"
K15-1002,W12-4501,0,0.10837,"r approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. MUC 78.17 63.89 64.28 65.81 67.28 70.28 71.35 71.81 72.74 B3 81.64 70.33 70.37 71.97 73.06 73.93 75.33 75.69 76.69 CEAFe 78.45 70.21 70.16 71.14 73.25 73.04 74.02 74.45 75.18 AVG 79.42 68.14 68.27 69.64 71.20 72.42 73.57 73.98 74.87 GoldM/H StanfordM PredictedM H-M-CorefM H-Joint-MM StanfordH PredictedH H-M-CorefH H-Joint-MH Table 2: Performance of coreference resolution for all s"
K15-1002,A88-1019,0,0.179276,"scover. Ratinov and Roth (2009) present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non-local features, and integration of external knowledge. NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree 20 structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. Lee et al. ("
K15-1002,W95-0107,0,0.0150189,"phrases detected are not necessarily mentions that we need to discover. Ratinov and Roth (2009) present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non-local features, and integration of external knowledge. NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree 20 structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studyi"
K15-1002,N07-1011,0,0.0168451,"eriments on the two standard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Task (Pradhan et al., 2012), contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. MUC 78.17 63.89 64.28 65.81 67.28 70.28 71.35 71.81 72.74 B3 81.64 70.33 70.37 71.97 73.06 73.93 75.33 75.69 76.69 CEAFe 78.45 70.21 70.16 71.14 73.25 73.04 74.02 74.45 75.18 AVG 79.42 68.14 68.27 69.64 71.20 72.42 73.57 73.98 74.87 GoldM/H StanfordM Predic"
K15-1002,W09-1119,1,0.767289,"ons. The sequence labeling component builds on the following assumption: Assumption Different mentions have different heads, and heads do not overlap with each other. That is, for each mi, j , we have a corresponding head ha,b where i ≤ a ≤ b ≤ j. Moreover, for another head ha0 ,b0 , we have the satisfying condition a − b0 > 0 or b − a0 &lt; 0 ∀ha,b , ha0 ,b0 . Based on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation, as shown, e.g. in Ratinov and Roth (2009). The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. The problem is then transformed into a simple, but constrained, 5-class classification problem. The BILOU-classifier shares all features with the mention head detection model described in Sec. 2.1 except for two: length of mention heads and NPMI over head boundary. For each instance, the feature vector is sparse and we use sparse perceptron (Jackson and Craven, 1996) for supervised training. We also apply a two layer prediction aggrega"
K15-1002,N07-1030,0,0.0101161,"5.49 22.60 HeadH 22.40 10.58 JointH 11.81 13.75 H-Joint-MH 34.21 24.33 Table 5: Analysis of performance improvement in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. “Head” represents the mention head candidate generation module, “Joint” represents the joint learning and inference framework, and “H-JointM” indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3 M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected ar"
K15-1002,N13-1071,0,0.0381098,"Missing"
K15-1002,D13-1203,0,0.179073,"Missing"
K15-1002,W04-2401,1,0.569564,"Missing"
K15-1002,Q14-1037,0,0.0511285,"named entity information. The parsing information9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj¨orkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL3 M) as our mention-pair coreference model in the joint framework10 . When the CL3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can"
K15-1002,D09-1015,0,0.0111072,"oth (2009) present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non-local features, and integration of external knowledge. NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree 20 structure as a representation of identifying named entities within other named entities. The most relevant study on mentions in the context of coreference was done in Recasens et al. (2013); this work studies distinguishing single mentions from coreferent mentions. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. Lee et al. (2012) model entity coreference an"
K15-1002,N06-2015,0,0.0968508,"uct negative examples as (oi−1 , ha,b , L) and (o j+1 , ha,b , R). Once trained, the binary classifier takes in the head, a token and the direction of the token relative to the head, and decides whether the token is inside or outside the mention corresponding to the head. At test time, this classifier is used around each confirmed head to determine the mention boundaries. The features used here are similar to the mention head detection model described in Sec. 2.1. 4 Experiments We present experiments on the two standard coreference resolution datasets, ACE-2004 (NIST, 2004) and OntoNotes-5.0 (Hovy et al., 2006). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets. 4.1 Experimental Setup Datasets The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007; Bengtson and Roth, 2008). The OntoNotes-5.0 dataset, which is released for the CoNLL-2012 Shared Ta"
K15-1002,D12-1114,0,0.0848512,"Missing"
K15-1002,J01-4004,0,0.337797,"ntH 19.22 15.21 H-Joint-MH 53.22 22.22 CoNLL-2012 MDER PGR(AVG) HeadM 25.04 12.16 JointM 10.45 10.44 H-Joint-MM 35.49 22.60 HeadH 22.40 10.58 JointH 11.81 13.75 H-Joint-MH 34.21 24.33 Table 5: Analysis of performance improvement in terms of Mention Detection Error Reduction (MDER) and Performance Gap Reduction (PGR) for coreference resolution on the ACE-2004 and CoNLL-2012 datasets. “Head” represents the mention head candidate generation module, “Joint” represents the joint learning and inference framework, and “H-JointM” indicates the end-to-end system. Cardie, 2002; Bengtson and Roth, 2008; Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too (Chang et al., 2011; Denis and Baldridge, 2007). In this paper, we use the Constrained Latent Left-Linking Model (CL3 M) described in Chang et al. (2013) in our experiments. The task of mention detection is closely related to Named Entity Recognition (NER). Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998)"
K15-1002,M95-1005,0,0.664278,"heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ1 = 0.25 and λ2 = 0.2. erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref. We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M. Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). We use the average F1 scores (AVG) of these three metrics as the main metric for comparison. We use the v7.0 scorer provided by CoNLL-2012 Shared Task11 . We also evaluate the mention detection performance based on precision, recall and F1 score. As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads. 4.2 Performance for Coreference Resolution Performance of coreference resolution for all systems on the ACE-2004 and CoNLL-2012 datasets is"
K15-1002,J03-4003,0,\N,Missing
K15-1002,J94-4002,0,\N,Missing
K15-1002,C98-1034,0,\N,Missing
K15-2012,J92-4003,0,0.276394,"ich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features, and 67,204 Brown cluster features. we have the correct connective and arguments provided as input). This means that these results are higher than they would be if evaluated using inputs generated"
K15-2012,P98-2186,1,0.715266,"fying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois NLP software1 and the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity features. Our system achieves an F1 score of 0.2492 in overall performance on the development set and 0.1798 on the blind test set. 1 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2 , syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. Introduction The Illinois discourse parsing system builds on existing approaches, using a series of classifiers to identify different elements of discourse structures such as argument boundaries and types along with discourse connectives and senses. In developi"
K15-2012,de-marneffe-etal-2006-generating,0,0.0264312,"Missing"
K15-2012,E14-1068,0,0.0253366,"dition to the features used by Lin et al. (2014), we also add function words to the path features. If we detect a word which is in the lexicon of function words, we replace the corresponding tag used in the path with the word’s surface string. 2.4 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify expli"
K15-2012,I05-6007,0,0.0834838,"Missing"
K15-2012,H05-2018,0,0.0627534,"o generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement f"
K15-2012,K15-2001,0,0.050462,"ignore the disjoint connectives, our results can be improved if we incorporate those missing connectives. We do not presently incorporate the argument information in connective detection and sense classification for the explicit parser. Connective detection and classification can be improved if we also incorporate more features from arguments or perform joint learning. In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 P 92.97 98.15 64.41 87.06 77.02 94.74 83.18 34.58 82.94 59.75 Cross-Validation Results We first present the cross validation results for each component using the training data (Table 1). All the results are averaged over 10-fold cross validation of all the examples we generated, using our own predicted features and our own evaluation code. Each component is evaluated in isolation, assuming the inputs are from gold data (for example: for connective classification, it is assumed 4 Discussion In this section we point to some types of errors in our system’s predictions and the complications"
K15-2012,N15-1082,1,0.659558,"licit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual"
K15-2012,prasad-etal-2008-penn,0,0.840823,"nd senses. In developing the components of this pipeline, we investigated different kinds of features to try to improve abstraction while retaining sufficient expressivity. To that end, we investigated a combination of parse and lexical (function word) features; brown clusters; and relations between verb-argument structures in consecutive sentences. 2.2 Recognizing Explicit Connectives In this section, we describe the system we developed, and introduce the features we used in To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifier to determine when a word matching an entry in the list represents an actual connective. We only focus on the connectives with consecutive tokens and ignore the non-consecutive connectives. We generate lexicosyntactic and path features associated with the 1 http://cogcomp.cs.illinois.edu/page/ software 2 http://cogcomp.cs.illinois.edu/page/ software_view/illinois-lemmatizer 2 System Description 78 Proceedings of the Nineteenth Conference on Computational Nat"
K15-2012,rizzolo-roth-2010-learning,1,0.822845,"itecture based on the description in Lin et al. (2014), then investigated features and inference approaches to improve the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). We present a system that implements an end-to-end discourse parser. The system uses a pipeline architecture with seven stages: preprocessing, recognizing explicit connectives, identifying argument positions, identifying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois NLP software1 and the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity featur"
K15-2012,C98-2181,1,\N,Missing
K16-1022,W13-3520,0,0.046061,"al., 2010), to use in open information extraction (Wu and Weld, 2010). It has 220 also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et al., 2013) are used as features in their NER model. Although the features are delexicalized, the embeddings are unique to each language, and so the model cannot transfer. Kim et al. (2012) use Wikipedia to generate parallel sentences with NE annotations. They propose a semi-CRF model for aligning entities in parallel sentences. Results are very strong on Wikipedia data. This is a hybrid approach in that it is supervised projection using Wikipedia. Our work is most closely related to Kazama and Torisawa (2007). They do NER using Wikipedia category features for each mention. However, their method for wiki"
K16-1022,D10-1075,1,0.886281,"Missing"
K16-1022,P11-1061,0,0.0339643,"ed approaches exploit the fact that, by editing Wikipedia, thousands of people have made annotations in hundreds of languages. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007"
K16-1022,P07-1033,0,0.019596,"Missing"
K16-1022,D14-1096,0,0.0493364,"the fact that, by editing Wikipedia, thousands of people have made annotations in hundreds of languages. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008;"
K16-1022,N10-1063,0,0.0129674,"mann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008; Song and Roth, 2014), to generating parallel data (Smith et al., 2010), to use in open information extraction (Wu and Weld, 2010). It has 220 also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et"
K16-1022,N12-1052,0,0.0469082,"Missing"
K16-1022,P12-1073,0,0.245259,"people have made annotations in hundreds of languages. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008; Song and Roth, 2014), to generating parallel d"
K16-1022,W12-1908,0,0.174224,"Missing"
K16-1022,D11-1006,0,0.0161099,"ojection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008; Song and Roth, 2014), to generating parallel data (Smith et al., 2010), to use in open information extraction (Wu and Weld,"
K16-1022,Q14-1019,0,0.0177004,"rthogonal and can be used together. In this paper, we focus on the second, direct transfer setting. We propose a cross-lingual NER model which is trained on annotated documents in one or multiple source languages, and can be applied to all languages in Wikipedia. The model depends on a cross-lingual wikifier, which only requires multilingual Wikipedia, no sentencealigned or word-aligned parallel text is needed. The key contribution of this paper is the development of a method that makes use of crosslingual wikification and entity linking (Tsai and Roth, 2016; Ji et al., 2015; Ji et al., 2016; Moro et al., 2014) to generate language-independent features for NER, and showing how useful this can be for training NER models with no annotation in the target language. Given a mention (sub-string) from a document written in a foreign language, the goal of cross-lingual wikification is to find the corNamed Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages). We introduce a language independent method for NER, building on cross-li"
K16-1022,W02-2024,0,0.51518,"e total number of names in the gazetteers of each language is listed in Table 2. Experiments and Analysis In this section, we conduct experiments to validate and analyze the proposed NER model. First, we show that adding wikifier features improves results on monolingual NER. Second, we show that wikifier features are strong signals in direct transfer of a trained NER model across languages. Finally, we explore the importance of Wikipedia size to the quality of wikifier features and study the use of multiple source languages. 4.1 Datasets We use data from CoNLL2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The 4 languages represented are English, German, Spanish, and Dutch, each annotated using the IOB1 labeling scheme, which we convert to the BIO labeling scheme. All training is on the train set, and testing is on the test set. The evaluation metric for all experiments is phrase level F1, as explained in (Tjong Kim Sang, 2002). In order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008) and LORELEI projects. From LORELEI, we use Turkish,2 From REFLEX, we use Bengali, Tagalog, Tamil, and Yoruba.3 While Tu"
K16-1022,N16-1072,1,0.153272,"ependent features. Note that these two approaches are orthogonal and can be used together. In this paper, we focus on the second, direct transfer setting. We propose a cross-lingual NER model which is trained on annotated documents in one or multiple source languages, and can be applied to all languages in Wikipedia. The model depends on a cross-lingual wikifier, which only requires multilingual Wikipedia, no sentencealigned or word-aligned parallel text is needed. The key contribution of this paper is the development of a method that makes use of crosslingual wikification and entity linking (Tsai and Roth, 2016; Ji et al., 2015; Ji et al., 2016; Moro et al., 2014) to generate language-independent features for NER, and showing how useful this can be for training NER models with no annotation in the target language. Given a mention (sub-string) from a document written in a foreign language, the goal of cross-lingual wikification is to find the corNamed Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages). We introduce a lan"
K16-1022,W09-1119,1,0.876724,"scalable to other languages because it requires both a trained NER and a NP chunker. 2.3 Base features Non-Lexical Previous Tags (ti−1 , ti−2 ) Tag Context (distr. for [wi , wi+1 , wi+2 ]) Lexical Forms (..., wi−1 , wi , wi+1 , ...) Affixes (prefixes and suffixes of wi ) Capitalization (wi capitalized?) Prev. Tag Pattern (ti−2 , wi−1 , wi ) Word type (capital? digits? letter?) Gazetteers Multilingual Wikipedia titles Cross-lingual Wikifier Features Freebase types of (wi−1 , wi , wi+1 ) Wikipedia categories of (wi−1 , wi , wi+1 ) Table 1: Feature groups. Base features are the features used by Ratinov and Roth (2009), the state of the art English NER model. Gazetteers and crosslingual wikifier features are described in detail in Section 3. pare against it in our experiments. Our work falls under the umbrella of direct transfer methods combined with the use of Wikipedia. We introduce wikifier features, which are truly delexicalized, and use Wikipedia as a source of information for each language. 3 Named Entity Recognition Model We use the state of the art English NER model of Ratinov and Roth (2009) as the base model. This model approaches NER as a multiclass classification problem with greedy decoding, us"
K16-1022,Q14-1005,0,0.0664558,"Wikipedia, thousands of people have made annotations in hundreds of languages. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008; Song and Roth, 2014), to gen"
K16-1022,P11-1138,1,0.453452,"Missing"
K16-1022,P10-1013,0,0.0135797,"et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Chang et al., 2008; Song and Roth, 2014), to generating parallel data (Smith et al., 2010), to use in open information extraction (Wu and Weld, 2010). It has 220 also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et al., 2013) are used as features in their NER model. Althou"
K16-1022,H01-1035,0,0.172457,"languages. Wikipediabased approaches exploit the fact that, by editing Wikipedia, thousands of people have made annotations in hundreds of languages. 2.1 Projection Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are of high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language. There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011). Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data. 2.2 Using Wikipedia Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovic"
K16-1022,N16-1029,0,0.155044,"in set, and testing is on the test set. The evaluation metric for all experiments is phrase level F1, as explained in (Tjong Kim Sang, 2002). In order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008) and LORELEI projects. From LORELEI, we use Turkish,2 From REFLEX, we use Bengali, Tagalog, Tamil, and Yoruba.3 While Turkish, Tagalog, and Yoruba each has a few non-Latin characters, Bengali and Tamil are with an entirely non-Latin script. This is a major reason for inclusion in our experiments. We use the same set of test documents as used in Zhang et al. (2016). All other documents in the REFLEX and LORELEI packages are used as the training documents in our monolingual experiments. We refer to these five languages collectively as low-resource languages. Besides PER, LOC, and ORG, some lowresource languages contain TIME tags and TTL tags, which represented titles in text, such as Secretary, President, or Minister. Since such words are not tagged in the CoNLL training data, we opted to simply remove these tags. On the other hand, there is no MISC tag in the low-resource languages. Instead, many MISC-tagged entities in the CoNLL datasets have LOC tags"
K17-1010,D14-1058,0,0.0572458,"Missing"
K17-1010,J17-2005,0,0.0450206,"mantic drift in query expansion. One key difference from general text summarization literature is that we operate on questions, which tend to have different essentiality characteristics than, say, paragraphs or news articles. As we discuss in Section 2.1, typical indicators of essentiality such as being a proper noun or a verb (for event extraction) are much less informative for questions. Similarly, while the opening sentence of a Wikipedia article is often a good summary, it is the last sentence (in multi-sentence questions) that contains the most pertinent words. In parallel to our effort, Jansen et al. (2017) recently introduced a science QA system that uses the notion of focus words. Their rule-based system incorporates grammatical structure, answer types, etc. We take a different approach by learning a supervised model using a new annotated dataset. 0.75 0.5 is by in temperature drop a sudden to usually respond animals One 0 way 0.25 Figure 1: Essentiality scores generated by our system, which assigns high essentiality to “drop” and “temperature”. erates the scores shown in Figure 1, where more weight is put on “temperature” and “sudden drop”. A QA system, when armed with such information, is ex"
K17-1010,D15-1080,1,0.790366,"Missing"
K17-1010,J92-4003,0,0.378835,"Missing"
K17-1010,P89-1010,0,0.122518,"Missing"
K17-1010,C02-1150,1,0.139557,"tests for elementary level science, the challenge is even more pronounced (Clark, 2015). Many QA systems in such domains † Most of the work was done when the first and last authors were affiliated with the University of Illinois, UrbanaChampaign. 80 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 80–89, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Chart Title 1 atically evaluated, in part due to the lack of ground truth annotations. There is related work on extracting question type information (Li and Roth, 2002; Li et al., 2007) and applying it to the design and analysis of end-to-end QA systems (Moldovan et al., 2003). The concept of term essentiality studied in this work is different, and so is our supervised learning approach compared to the typical rule-based systems for question type identification. Another line of relevant work is sentence compression (Clarke and Lapata, 2008), where the goal is to minimize the content while maintaining grammatical soundness. These approaches typically build an internal importance assignment component to assign significance scores to various terms, which is of"
K17-1010,Q13-1029,0,0.0587422,"Missing"
K17-1010,N16-1152,0,0.0198852,"Missing"
K17-1010,P16-1122,0,0.0191443,"Missing"
K17-1010,P14-2105,0,0.0536199,"Missing"
K17-1010,W16-0103,0,0.0850481,"Missing"
K17-1010,de-marneffe-etal-2006-generating,0,\N,Missing
K17-1010,C08-1061,0,\N,Missing
K17-1010,C16-1285,1,\N,Missing
K17-1019,P98-1013,0,0.605872,"raction is a delicate balance between specificity and correctness. Semantic Frames Semantic frame is defined by Fillmore (1976): frames are certain schemata or frameworks of concepts or terms which link together as a system, which impose structure or coherence on some aspect of human experience, and which may contain elements which are simultaneously parts of other such frameworks. In this work, we simplify it by defining a semantic frame as a composition of a predicate and its corresponding argument participants. The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs. Here we require the predicate to be disambiguated to a specific sense, thus each frame can be uniquely represented by its predicate sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. For example, in Ex.1, the semantic frame in Opt.1 would be abstracted as “convict.01”. We associate each of these frames with an embedding. The arguments of the frames are modeled as entities, as described next. Additionally, in accordance with the idea pro1 Though there are a number works on fine-grai"
K17-1019,D13-1178,0,0.120139,". This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014b,a; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s"
K17-1019,Q14-1029,0,0.0767864,"ral inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014b,a; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semanti"
K17-1019,D13-1185,0,0.0610179,"olution and shallow discourse parsing. This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014b,a; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Most recently, Mostafazadeh et al. (2016, 2017) proposed sto"
K17-1019,D08-1073,0,0.196259,"ure 1: Examples of short stories requiring different aspects of semantic knowledge. For all stories, Opt.1 is the correct follow-up, while Opt.2 is the contrastive wrong follow-up demonstrating the importance of each aspect. Alter. showcases an alternative correct follow-up, which requires considering different aspects of semantics jointly. frame. Clearly, “convict” is more likely than “go” to follow such sequence. This semantic knowledge can be learned through modeling frame sequences observed in a large corpus. This phenomena has already been studied in script learning works (Chatman, 1980; Chambers and Jurafsky, 2008b; Ferraro and Van Durme, 2016; Pichotta and Mooney, 2016a; Peng and Roth, 2016). However, modeling actions is not sufficient; participants in actions and their emotions are also important. In Ex. 2, Opt.2 is not a plausible answer because the story is about “football”, and it does not make sense to suddenly change the key enIntroduction Understanding a story requires understanding sequences of events. It is thus vital to model semantic sequences in text. This modeling process necessitates deep semantic knowledge about what can happen next. Since events involve actions, participants and emotio"
K17-1019,N13-1104,0,0.392043,"Missing"
K17-1019,1985.tmi-1.17,0,0.43105,"Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Our work is built upon the previous work (Peng and Roth, 2016). It generated a probabilistic model on semantic frames while taking into account discourse information, and showed applications to both co-reference resolution and shallow discourse parsing. This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van"
K17-1019,N16-1098,0,0.178499,"Missing"
K17-1019,E14-1006,0,0.118347,"Missing"
K17-1019,P15-1019,0,0.20998,"Missing"
K17-1019,K15-1002,1,0.770401,"short stories, we re-train FES-LM on the ROCStories dataset (Mostafazadeh et al., 2017) with the model trained on NYT as initialization. We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) 5 . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) (Punyakanok et al., 2004) and Part-of-Speech (POS) tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within document entity co-reference (Peng et al., 2015a) to produce co-reference chains to get the new entity 4 5 FES Representation Generation 6 Available at http://cogcomp.org/page/software/ We use the mapping file http://verbs.colorado.edu/verbindex/fn/vn-fn.xml to do it. For example, “place” and “put” with the same VerbNet sense id “9.1-2” are both mapped to the FrameNet frame “Placing”. 7 Available at https://catalog.ldc.upenn.edu/LDC2008T19 Available at http://cs.rochester.edu/nlp/rocstories/ 177 NYT ROCStories Vocabulary Size FES F E S Sequence Size #seq #token 4M 200K 7 7 1.2M 100K 15K 1K 100 98 25.4M 630K Table 2: Statistics on FES-LM vo"
K17-1019,C16-1266,0,0.124833,"(Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014b,a; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and"
K17-1019,N15-1082,1,0.819461,"short stories, we re-train FES-LM on the ROCStories dataset (Mostafazadeh et al., 2017) with the model trained on NYT as initialization. We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) 5 . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) (Punyakanok et al., 2004) and Part-of-Speech (POS) tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within document entity co-reference (Peng et al., 2015a) to produce co-reference chains to get the new entity 4 5 FES Representation Generation 6 Available at http://cogcomp.org/page/software/ We use the mapping file http://verbs.colorado.edu/verbindex/fn/vn-fn.xml to do it. For example, “place” and “put” with the same VerbNet sense id “9.1-2” are both mapped to the FrameNet frame “Placing”. 7 Available at https://catalog.ldc.upenn.edu/LDC2008T19 Available at http://cs.rochester.edu/nlp/rocstories/ 177 NYT ROCStories Vocabulary Size FES F E S Sequence Size #seq #token 4M 200K 7 7 1.2M 100K 15K 1K 100 98 25.4M 630K Table 2: Statistics on FES-LM vo"
K17-1019,P16-1028,1,0.0871182,"all stories, Opt.1 is the correct follow-up, while Opt.2 is the contrastive wrong follow-up demonstrating the importance of each aspect. Alter. showcases an alternative correct follow-up, which requires considering different aspects of semantics jointly. frame. Clearly, “convict” is more likely than “go” to follow such sequence. This semantic knowledge can be learned through modeling frame sequences observed in a large corpus. This phenomena has already been studied in script learning works (Chatman, 1980; Chambers and Jurafsky, 2008b; Ferraro and Van Durme, 2016; Pichotta and Mooney, 2016a; Peng and Roth, 2016). However, modeling actions is not sufficient; participants in actions and their emotions are also important. In Ex. 2, Opt.2 is not a plausible answer because the story is about “football”, and it does not make sense to suddenly change the key enIntroduction Understanding a story requires understanding sequences of events. It is thus vital to model semantic sequences in text. This modeling process necessitates deep semantic knowledge about what can happen next. Since events involve actions, participants and emotions, semantic knowledge about these aspects must be captured and modeled. Conside"
K17-1019,D14-1162,0,0.0775478,"Missing"
K17-1019,E12-1034,0,0.154694,"Missing"
K17-1019,E14-1024,0,0.0655828,"rk (Peng and Roth, 2016). It generated a probabilistic model on semantic frames while taking into account discourse information, and showed applications to both co-reference resolution and shallow discourse parsing. This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Rudinger et al., 2015; Pichotta and Mooney, 2016b,a). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014b,a; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scrip"
K17-1019,kingsbury-palmer-2002-treebank,0,0.129654,"r context from other sentences. Thus, entity abstraction is a delicate balance between specificity and correctness. Semantic Frames Semantic frame is defined by Fillmore (1976): frames are certain schemata or frameworks of concepts or terms which link together as a system, which impose structure or coherence on some aspect of human experience, and which may contain elements which are simultaneously parts of other such frameworks. In this work, we simplify it by defining a semantic frame as a composition of a predicate and its corresponding argument participants. The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs. Here we require the predicate to be disambiguated to a specific sense, thus each frame can be uniquely represented by its predicate sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. For example, in Ex.1, the semantic frame in Opt.1 would be abstracted as “convict.01”. We associate each of these frames with an embedding. The arguments of the frames are modeled as entities, as described next. Additionally, in accordance with the idea pro1 Tho"
K17-1019,P16-1027,0,0.348297,"s of semantic knowledge. For all stories, Opt.1 is the correct follow-up, while Opt.2 is the contrastive wrong follow-up demonstrating the importance of each aspect. Alter. showcases an alternative correct follow-up, which requires considering different aspects of semantics jointly. frame. Clearly, “convict” is more likely than “go” to follow such sequence. This semantic knowledge can be learned through modeling frame sequences observed in a large corpus. This phenomena has already been studied in script learning works (Chatman, 1980; Chambers and Jurafsky, 2008b; Ferraro and Van Durme, 2016; Pichotta and Mooney, 2016a; Peng and Roth, 2016). However, modeling actions is not sufficient; participants in actions and their emotions are also important. In Ex. 2, Opt.2 is not a plausible answer because the story is about “football”, and it does not make sense to suddenly change the key enIntroduction Understanding a story requires understanding sequences of events. It is thus vital to model semantic sequences in text. This modeling process necessitates deep semantic knowledge about what can happen next. Since events involve actions, participants and emotions, semantic knowledge about these aspects must be captur"
K17-1019,K16-1006,0,0.0141654,"t) ), t = 1, 2, · · · , k. We get multiple features depending on how long we go back in the context in terms of FES representations. Note that one sentence can contain multiple FES representations depending on how many semantic frames it has. For simplicity, we assume a single FES representation rFES k Application on News We choose shallow discourse parsing as the task to show FES-LM’s applicability on news. In particular, we evaluate on identifying the correct sense of discourse connectives (both explicit and implicit 10 We also tried Neural-LSTM (Pichotta and Mooney, 2016a) and context2vec (Melamud et al., 2016) model, but we cannot get better results. 11 The ablation study is not done for perplexity test because FES-LM with less semantic aspects yields smaller vocabulary, which naturally leads to lower perplexity. 12 The test set contains 1, 871 four-sentences long stories with two fifth sentence options for each, of which only one is correct; and we report the accuracy. 179 Base (Song et al., 2015)* SemLM (Peng and Roth, 2016) Top (Mihaylov and Frank, 2016) FES-LM (this work) FES-LM - Entity FES-LM - Sentiment CoNLL16 Test Explicit Implicit Overall 89.8 35.6 60.4 91.1 36.3 61.4 89.8 39.2 63.3 91.0"
K17-1019,C04-1197,1,0.662772,"n this section, we explain how we build FES-LM from un-annotated plain text. 4.1 Dataset and Preprocessing Dataset We first use the New York Times (NYT) Corpus4 (from year 1987 to 2007) to train FESLM. It contains over 1.8M documents in total. To fine tune the model on short stories, we re-train FES-LM on the ROCStories dataset (Mostafazadeh et al., 2017) with the model trained on NYT as initialization. We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) 5 . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) (Punyakanok et al., 2004) and Part-of-Speech (POS) tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within document entity co-reference (Peng et al., 2015a) to produce co-reference chains to get the new entity 4 5 FES Representation Generation 6 Available at http://cogcomp.org/page/software/ We use the mapping file http://verbs.colorado.edu/verbindex/fn/vn-fn.xml to do it. For example, “place” and “put” with the same VerbNet sense id “9.1-2” are both mapped to the FrameNet frame “Placin"
K17-1019,K16-2014,0,0.0634276,"as described in Peng and Roth (2016), i.e. we add additional conditional probability features generated from FES-LM into the base system. We evaluate on CoNLL16 (Xue et al., 2016) test and blind sets, following the train and development split from the Shared Task, and report F1 using the official shared task scorer. Table 4 shows the results for shallow discourse parsing with added FES-LM features. We get significant improvement over the base system(*) (based on McNemar’s Test) and outperform SemLM, which only utilizes frame information in the semantic sequences. We also rival the top system (Mihaylov and Frank, 2016) in the CoNLL16 Shared Task (connective sense classification subtask). Note that the FES-LM used here is trained on NYT corpus. The ablation study shows that entity aspect contributes less than sentiment aspect in this application. LBL 126.0 43.2 38.4 36.3 Table 3: Quality comparison of neural language models. We report results for perplexity and narrative cloze test. Both evaluations are done on the gold PropBank data (annotated with gold frames). LBL outperforms CBOW and SG on both tests. We carry out ablation studies for narrative cloze test for FES-LM without entity and sentiment aspects r"
K17-1019,P11-1082,0,0.0552194,"Missing"
K17-1019,D16-1144,0,0.0260285,"Missing"
K17-1019,P98-2186,1,0.325005,"otated plain text. 4.1 Dataset and Preprocessing Dataset We first use the New York Times (NYT) Corpus4 (from year 1987 to 2007) to train FESLM. It contains over 1.8M documents in total. To fine tune the model on short stories, we re-train FES-LM on the ROCStories dataset (Mostafazadeh et al., 2017) with the model trained on NYT as initialization. We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) 5 . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) (Punyakanok et al., 2004) and Part-of-Speech (POS) tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within document entity co-reference (Peng et al., 2015a) to produce co-reference chains to get the new entity 4 5 FES Representation Generation 6 Available at http://cogcomp.org/page/software/ We use the mapping file http://verbs.colorado.edu/verbindex/fn/vn-fn.xml to do it. For example, “place” and “put” with the same VerbNet sense id “9.1-2” are both mapped to the FrameNet frame “Placing”. 7 Available at https://catalog.ldc.upenn.edu/LDC2008T"
K17-1019,N13-1090,0,0.00976561,".4 36.3 Table 3: Quality comparison of neural language models. We report results for perplexity and narrative cloze test. Both evaluations are done on the gold PropBank data (annotated with gold frames). LBL outperforms CBOW and SG on both tests. We carry out ablation studies for narrative cloze test for FES-LM without entity and sentiment aspects respectively. 5.1 Quality of FES-LM To evaluate the modeling ability of different neural language models, we train each variant of FES-LM on NYT corpus and report perplexity and narrative cloze test results. Here, we choose the Skip-Gram (SG) model (Mikolov et al., 2013b) and Continuous-Bag-of-Words (CBOW) model (Mikolov et al., 2013a) for comparison with the LBL model. We utilize the word2vec package to implement both SG and CBOW. We set the context window size to be 10 for SG and 5 for CBOW. We employ the same experimental setting as detailed in Peng and Roth (2016). Results are shown in Table 3. They confirm that LBL model performs the best with the lowest perplexity and highest recall for narrative cloze test.10 Note that the numbers reported are not directly comparable with those in literature (Rudinger et al., 2015; Peng and Roth, 2016), as we model mu"
K17-1019,D15-1195,0,0.232992,"Missing"
K17-1019,W14-1606,0,0.0610721,"Missing"
K17-1019,K15-2012,1,0.91074,"train FESLM. It contains over 1.8M documents in total. To fine tune the model on short stories, we re-train FES-LM on the ROCStories dataset (Mostafazadeh et al., 2017) with the model trained on NYT as initialization. We use the train set of ROCStories, which contains around 100K short stories (each consists of five sentences) 5 . Preprocessing We pre-process all documents with Semantic Role Labeling (SRL) (Punyakanok et al., 2004) and Part-of-Speech (POS) tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within document entity co-reference (Peng et al., 2015a) to produce co-reference chains to get the new entity 4 5 FES Representation Generation 6 Available at http://cogcomp.org/page/software/ We use the mapping file http://verbs.colorado.edu/verbindex/fn/vn-fn.xml to do it. For example, “place” and “put” with the same VerbNet sense id “9.1-2” are both mapped to the FrameNet frame “Placing”. 7 Available at https://catalog.ldc.upenn.edu/LDC2008T19 Available at http://cs.rochester.edu/nlp/rocstories/ 177 NYT ROCStories Vocabulary Size FES F E S Sequence Size #seq #toke"
K17-1019,N15-1001,0,0.142053,"Missing"
K17-1019,H05-1044,0,0.0141337,"s”. This makes the co-reference decisions more robust on short stories.8 The entity representation re is eventually constructed as a one-hot vector for types of 5 dimensions and an additional dimension for “new entity” information. As we consider both subjects and objects of a frame, re is of 12 dimensions in total. If either one of the entities within a frame is missing from SRL annotations, we set its corresponding 6 dimensions as zeros. Sentiment Representation Generation We first determine the polarity of a word by a look-up table from two pre-trained sentiment lexicons (Liu et al., 2005; Wilson et al., 2005). We then count the number of positive words versus negative words to decide the sentiment of a piece of text as detailed in Sec. 2. This process is done on text corresponding to each frame, i.e. a sentence or a clause. Since we have two different lexicons, we get two separate one-hot sentiment vectors, each with a dimension of 3. Thus, the sentiment representation is the concatenation of the two vectors, a total dimension of 6. 4.3 Neural Language Model Training For the NYT corpus, we treat each document as a single semantic sequence while on ROCStories, we see each story as a semantic sequen"
K17-1019,K16-2001,0,0.0158482,"ing a person can start a co-reference chain in news. 9 The FES representation space can be seen as entity and sentiment infused frame embedding space. 178 CBOW SG Perplexity FES-LM 133.8 135.8 Narrative Cloze Test (Recall@30) FES-LM 38.9 37.3 FES-LM - Entity 35.3 33.1 FES-LM - Sentiment 34.9 32.8 ones). We choose Song et al. (2015), which uses a supervised pipeline approach, as our base system. We follow the same experimental setting as described in Peng and Roth (2016), i.e. we add additional conditional probability features generated from FES-LM into the base system. We evaluate on CoNLL16 (Xue et al., 2016) test and blind sets, following the train and development split from the Shared Task, and report F1 using the official shared task scorer. Table 4 shows the results for shallow discourse parsing with added FES-LM features. We get significant improvement over the base system(*) (based on McNemar’s Test) and outperform SemLM, which only utilizes frame information in the semantic sequences. We also rival the top system (Mihaylov and Frank, 2016) in the CoNLL16 Shared Task (connective sense classification subtask). Note that the FES-LM used here is trained on NYT corpus. The ablation study shows t"
K17-1019,P15-2048,0,0.0375329,"Missing"
K17-1019,W11-1913,0,\N,Missing
K17-1019,C98-1013,0,\N,Missing
K17-1019,P08-1090,0,\N,Missing
K17-1019,C98-2181,1,\N,Missing
K17-1019,W16-6003,0,\N,Missing
K17-1019,W17-0906,0,\N,Missing
K17-1019,P17-2097,0,\N,Missing
K19-1045,W18-3027,0,0.0393631,"Missing"
K19-1045,P18-1180,1,0.743288,"ed constraints in various NLP applications. Text Generation. Text generation tasks like image captioning, machine translation, sentence simplification, etc., often require that the output must contain specific words or phrases (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2017) in order to incorporate prior knowledge (e.g., the caption must have the word “chair” as the object was detected in the image). Similarly, constraints can disallow invalid sequences, such as words which do not rhyme (Ghazvininejad et al., 2016, 2018) or do not appear in a dictionary (Deutsch et al., 2018). These constraints can be represented as FSAs where all paths to an accepting state contain the required sequences and do not contain any disallowed sequences. Note that the size of the automata is not a function of the output vocabulary but the vocabulary that participates in the constraints. These constraints are described in detail in §4. Also known as a working set. 485 Sequence Tagging. Many sequence tagging problems (such as NER, shallow parsing, etc.) require that the output tags are valid under the specific tagging scheme, such as BIO, which marks each token as beginning, inside, or o"
K19-1045,N16-1024,0,0.0177268,"am search and adds parentheses to almost-valid parses.6 The algorithms are evaluated using F1 as reported by EVALB, a standard evaluation toolkit.7 However, because EVALB ignores invalid trees during evaluation (which can artificially improve the performance of models which violate constraints), we also report coverage, the percentage of valid outputs. We use a beam size of 10. Syntactic Parsing. Syntactic parsing (dependency or constituency) tasks require that the output forms a valid tree, a constraint commonly enforced using the shift-reduce algorithm (Zhu et al., 2013; Nivre et al., 2014; Dyer et al., 2016). Shiftreduce inference inspects the state of a stack to decide which next actions are valid (e.g., if the stack is empty, reduce is invalid). Shift-reduce inference is implicitly using an automaton which is the intersection of a PDA (that counts how many shift and reduce actions have occurred) and an FSA (that restricts the maximum number of actions based on the input sentence length). Semantic Role Labeling. For SRL, the unconstrained model is an off-the-shelf implementation of He et al. (2017). The input to the model is the sentence and the predicate for which the arguments need to be ident"
K19-1045,N18-2011,0,0.0218837,"Missing"
K19-1045,D16-1126,0,0.0220411,"essibility of automata by showing how they can represent commonly used constraints in various NLP applications. Text Generation. Text generation tasks like image captioning, machine translation, sentence simplification, etc., often require that the output must contain specific words or phrases (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2017) in order to incorporate prior knowledge (e.g., the caption must have the word “chair” as the object was detected in the image). Similarly, constraints can disallow invalid sequences, such as words which do not rhyme (Ghazvininejad et al., 2016, 2018) or do not appear in a dictionary (Deutsch et al., 2018). These constraints can be represented as FSAs where all paths to an accepting state contain the required sequences and do not contain any disallowed sequences. Note that the size of the automata is not a function of the output vocabulary but the vocabulary that participates in the constraints. These constraints are described in detail in §4. Also known as a working set. 485 Sequence Tagging. Many sequence tagging problems (such as NER, shallow parsing, etc.) require that the output tags are valid under the specific tagging scheme,"
K19-1045,D17-1098,0,0.0233048,"ll of the automata (line 5), it is valid and subsequently returned (line 6). Otherwise, the first violated constraint is added to W (line 8), its automaton A0 intersected with AW (line 9), and constrained inference is re-run (line 3). 4 5 3 Representing Constraints as Automata We now illustrate the expressibility of automata by showing how they can represent commonly used constraints in various NLP applications. Text Generation. Text generation tasks like image captioning, machine translation, sentence simplification, etc., often require that the output must contain specific words or phrases (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2017) in order to incorporate prior knowledge (e.g., the caption must have the word “chair” as the object was detected in the image). Similarly, constraints can disallow invalid sequences, such as words which do not rhyme (Ghazvininejad et al., 2016, 2018) or do not appear in a dictionary (Deutsch et al., 2018). These constraints can be represented as FSAs where all paths to an accepting state contain the required sequences and do not contain any disallowed sequences. Note that the size of the automata is not a function of the output"
K19-1045,J02-3001,0,0.0220512,"PropBank) [Alice Smith] gave a [flower] to [Bob] B-A0 I-A0 O O B-A0 O B-A2 duplicate A0 B-A5 I-A5 O O B-A1 O B-A2 illegal arg A5 for predicate “gave” B-A0 O O O B-A1 O B-A2 span [Alice Smith] should have single label Figure 1: An example SRL instance (input, predicate, gold tags) with different invalid tag sequences and the constraints they violate (details in §4.2). space Y (Taskar, 2004; Tsochantaridis et al., 2005). The search is restricted to set of valid y ∈ Yx ⊆ Y for x by imposing constraints during inference. Figure 1 shows examples of constraints used in Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). Currently, inference for most structured prediction problems is solved sequentially, predicting the output in a left-to-right manner (Sutskever et al., 2014; Luong et al., 2016). Such sequential inference approaches enforce constraints in different ways. For example, a shift-reduce parser consults a stack to determine which action sequences produce valid trees (Nivre et al., 2014), while an SRL model penalizes tag sequences which violate constraints during inference (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015). At present, inference algorithms are designed to handle task-specific cons"
K19-1045,W16-2409,0,0.0686236,"Missing"
K19-1045,P13-2009,0,0.0553914,"Missing"
K19-1045,P17-1044,0,0.0187761,"traint commonly enforced using the shift-reduce algorithm (Zhu et al., 2013; Nivre et al., 2014; Dyer et al., 2016). Shiftreduce inference inspects the state of a stack to decide which next actions are valid (e.g., if the stack is empty, reduce is invalid). Shift-reduce inference is implicitly using an automaton which is the intersection of a PDA (that counts how many shift and reduce actions have occurred) and an FSA (that restricts the maximum number of actions based on the input sentence length). Semantic Role Labeling. For SRL, the unconstrained model is an off-the-shelf implementation of He et al. (2017). The input to the model is the sentence and the predicate for which the arguments need to be identified, and the model outputs a tag sequence (such as Figure 1). To isolate the effect of constraints, we assume the gold predicates are available, unlike He et al. (2017). We use the standard train and development splits from the CoNLL 2005 shared task and the same model parameters as He et al. (2017). We report the CoNLL F1 score8 computed for the core arguments (namely A0 –A5 ) and use greedy inference. Semantic Parsing and Code Generation. In semantic parsing and code generation, constraints e"
K19-1045,P17-1141,0,0.0201539,"e 5), it is valid and subsequently returned (line 6). Otherwise, the first violated constraint is added to W (line 8), its automaton A0 intersected with AW (line 9), and constrained inference is re-run (line 3). 4 5 3 Representing Constraints as Automata We now illustrate the expressibility of automata by showing how they can represent commonly used constraints in various NLP applications. Text Generation. Text generation tasks like image captioning, machine translation, sentence simplification, etc., often require that the output must contain specific words or phrases (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2017) in order to incorporate prior knowledge (e.g., the caption must have the word “chair” as the object was detected in the image). Similarly, constraints can disallow invalid sequences, such as words which do not rhyme (Ghazvininejad et al., 2016, 2018) or do not appear in a dictionary (Deutsch et al., 2018). These constraints can be represented as FSAs where all paths to an accepting state contain the required sequences and do not contain any disallowed sequences. Note that the size of the automata is not a function of the output vocabulary but the voc"
K19-1045,P09-1039,0,0.115815,"Missing"
K19-1045,J05-1004,0,0.119627,"Missing"
K19-1045,D08-1068,0,0.0956608,"Missing"
K19-1045,D16-1032,0,0.0320105,"Missing"
K19-1045,N18-1119,0,0.0196221,"subsequently returned (line 6). Otherwise, the first violated constraint is added to W (line 8), its automaton A0 intersected with AW (line 9), and constrained inference is re-run (line 3). 4 5 3 Representing Constraints as Automata We now illustrate the expressibility of automata by showing how they can represent commonly used constraints in various NLP applications. Text Generation. Text generation tasks like image captioning, machine translation, sentence simplification, etc., often require that the output must contain specific words or phrases (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2017) in order to incorporate prior knowledge (e.g., the caption must have the word “chair” as the object was detected in the image). Similarly, constraints can disallow invalid sequences, such as words which do not rhyme (Ghazvininejad et al., 2016, 2018) or do not appear in a dictionary (Deutsch et al., 2018). These constraints can be represented as FSAs where all paths to an accepting state contain the required sequences and do not contain any disallowed sequences. Note that the size of the automata is not a function of the output vocabulary but the vocabulary that participa"
K19-1045,P18-1249,0,0.0291555,"Missing"
K19-1045,D17-1160,0,0.0372661,"Missing"
K19-1045,N16-1030,0,0.0308532,"Missing"
K19-1045,W04-2401,1,0.630426,"For example, a shift-reduce parser consults a stack to determine which action sequences produce valid trees (Nivre et al., 2014), while an SRL model penalizes tag sequences which violate constraints during inference (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015). At present, inference algorithms are designed to handle task-specific constraints, and there is no general formulation for constrained sequential inference. This contrasts with the state of affairs in NLP before deep learning, when constrained inference approaches used general formulations like Integer Linear Programming (ILP) (Roth and Yih, 2004; Clarke and Lapata, 2008, inter alia). We present a simple, general-purpose sequential inference algorithm that takes a model and Introduction The key challenge in structured prediction problems (like sequence tagging and parsing) is inference (also known as decoding), which involves identifying the best output structure y for an input instance x from an exponentially large search 1 All code available at https://cogcomp.seas. upenn.edu/page/publication_view/884 ∗ Equal contribution, authors listed alphabetically  Work done while at University of Pennsylvania. 482 Proceedings of the 23rd Conf"
K19-1045,P16-1057,0,0.0548788,"Missing"
K19-1045,P13-1043,0,0.0337754,"umber of preterminals at the end of beam search and adds parentheses to almost-valid parses.6 The algorithms are evaluated using F1 as reported by EVALB, a standard evaluation toolkit.7 However, because EVALB ignores invalid trees during evaluation (which can artificially improve the performance of models which violate constraints), we also report coverage, the percentage of valid outputs. We use a beam size of 10. Syntactic Parsing. Syntactic parsing (dependency or constituency) tasks require that the output forms a valid tree, a constraint commonly enforced using the shift-reduce algorithm (Zhu et al., 2013; Nivre et al., 2014; Dyer et al., 2016). Shiftreduce inference inspects the state of a stack to decide which next actions are valid (e.g., if the stack is empty, reduce is invalid). Shift-reduce inference is implicitly using an automaton which is the intersection of a PDA (that counts how many shift and reduce actions have occurred) and an FSA (that restricts the maximum number of actions based on the input sentence length). Semantic Role Labeling. For SRL, the unconstrained model is an off-the-shelf implementation of He et al. (2017). The input to the model is the sentence and the predicate"
K19-1045,N18-1203,0,0.0388546,"Missing"
K19-1045,Q15-1003,0,0.0297641,"Missing"
K19-1045,N06-1054,0,0.0394093,"ay not be necessary because it is possible for a constraint to be satisfied without it being enforced. This is the basis for active set methods (such as the cutting-plane algorithm (Kelley, 1960; Tsochantaridis et al., 2005)), which maintain a set during inference that contains currently active (i.e., enforced) constraints. When a constraint is violated by the current output, it enters (i.e., is added to) the active set. We present an active set method for imposing multiple constraints represented by a set of automata Sx in Algorithm 2. Our algorithm is inspired by the active set algorithm of Tromble and Eisner (2006) for finite-state transducers. For an instance x, Algorithm 2 maintains a active set5 W corresponding to all violated constraints so far. W is represented by the intersection AW of the relevant automata, which is initialized with an automata Σ∗ that accepts any sequence (line 1). On each iteration, the algorithm runs a constrained inference algorithm (such as Algorithm 1) that uses AW (line 3) to find an output y ˆ. Then, F IND -V IOLATION checks if y ˆ violates any of the constraints that are not currently in the active set, Sx  W (line 4). If y ˆ is accepted by all of the automata (line 5),"
K19-1045,D18-1046,1,0.883851,"Missing"
K19-1045,W04-3212,0,0.0558114,"Missing"
K19-1051,D17-1168,1,0.888659,"Jane wanted to buy a new car. She had to borrow some money from her father. ... So, on an event level, we abstract the text as “PER[new]-want.01-buy.01-ARG[new](NEU), PER[old]-have.04-borrow.01-ARG[new](NEU)”. For FES-RNNLM, the system predicts the next event as “PER[old]-sell.01-ARG[new](NEU)” since in training data, there are many cooccurrences between the “borrow” event and “sell” event (coming from financial news articles in NYT). In contrast, for KnowSemLM, since 6 Related Work Our work is built upon the previous works for semantic language models (Peng and Roth, 2016; Peng et al., 2017; Chaturvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013"
K19-1051,N13-1104,0,0.0439264,"rvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well"
K19-1051,D13-1178,0,0.0633673,"This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures ("
K19-1051,N18-1204,0,0.0624753,"Missing"
K19-1051,Q14-1029,0,0.0179446,"l inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Mos"
K19-1051,N19-1423,0,0.0303508,"l., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical recurrent networks with at557 Most recently, pre-trained language models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), and XLNET (Yang et al., 2019) have achieved much success for language modeling and generation tasks. Our proposed knowledge infused semantic language model can not be directly applied upon such word-level pre-trained language models. However, as future works, we are interested in exploring the possibility of pre-training a semantic language model with frame and entity abstractions on a large corpus with event causality knowledge, and fine-tune it on application tasks. tention to encode sentences and produced a strong baseline. Lee and Goldwasser (2019) considered"
K19-1051,D12-1062,1,0.83852,"for different relations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utilized in KnowSemLM; as shown in Sec. 4.2 that such knowledge can be both mined from corpus and written down declaratively. 7 Conclusion This paper proposes KnowSemLM, a knowledge infused semantic LM. It utilizes both local contex"
K19-1051,W17-2712,0,0.0282387,"; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical recurrent networks with at557 Most recently, pre-trained language models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), and XLNET (Yang et al., 2019) have achieved much success for language modeling and generation tasks. Our proposed knowledge infused sema"
K19-1051,P17-2097,0,0.0226617,", 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical recurrent networks with at557 Most recently, pre-trained language models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), and XLNET (Yang et al., 2019) have achieved much success for language modeling and generation tasks. Our proposed knowledge infused semantic language model can not be directly applied upon such word-level pre-trained language models. However, as future works, we are interested in exploring the possibility of pre-training a semantic language model with frame and entity abstractions on a large corpus with event causalit"
K19-1051,E14-1006,0,0.024143,"bers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a mo"
K19-1051,D13-1185,0,0.0284253,"al., 2017; Chaturvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et"
K19-1051,P08-1090,0,0.772952,"ory. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (1987-2007); we count with frame and entity level abstractions (see Section 2.1 for details). 1 Related resources refer to https://cogcomp. seas.upenn.edu/page/publication_view/886. 550 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 550–562 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computa"
K19-1051,P07-2044,0,0.0218986,"ion tasks. tention to encode sentences and produced a strong baseline. Lee and Goldwasser (2019) considered the problem of learning relation aware event embeddings for commonsense inference, which can account for different relations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utilized in KnowSemLM; as sho"
K19-1051,P17-1162,0,0.136413,"y, we utilize within-document entity co-reference (Peng et al., 2015a) to produce coreference chains and get the anaphoricity information. To obtain all annotations, we use the Illinois NLP tools (Khashabi et al., 2018).5 Further, we obtain event representations from text with frame, entity and sentiment level abstractions by following procedures described in Peng et al. (2017). pkn (ex ⇒ ey |e1 , e2 , · · · , et ) vec exp(evec x Wa ht ) exp(ey Wb ht ) =P . vec 0vec W h ) b t e∈Vx ,e0 ∈Vy exp(e Wa ht ) exp(e 3 The proposed computational framework of KnowSemLM is similar to DynoNet proposed in He et al. (2017). Compared to DynoNet, the knowledge base utilized here operates on event level representations rather than on tokens. 4 https://catalog.ldc.upenn.edu/ LDC2008T19 5 http://cogcomp.org/page/software/ 553 4.2 Knowledge Mining Method Granroth-Wilding and Clark (2016) Wang et al. (2017) KnowSemLM w/o knowledge KnowSemLM w/o transit. & fine-tuning KnowSemLM w/o fine-tuning KnowSemLM Statistical Way: Part of the human knowledge can be mined from text itself. Since discourse connectives are important for relating different text spans, we carefully select discourse connectives which can indicate a “ca"
K19-1051,P06-1095,0,0.0196545,"tune it on application tasks. tention to encode sentences and produced a strong baseline. Lee and Goldwasser (2019) considered the problem of learning relation aware event embeddings for commonsense inference, which can account for different relations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utiliz"
K19-1051,C16-1266,0,0.0251385,"chank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 201"
K19-1051,W11-1913,0,0.0257621,"r et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical recurrent networks with at557 Most recently, pre-trained language models such as BERT (Devlin e"
K19-1051,C16-1007,0,0.0184772,"ations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utilized in KnowSemLM; as shown in Sec. 4.2 that such knowledge can be both mined from corpus and written down declaratively. 7 Conclusion This paper proposes KnowSemLM, a knowledge infused semantic LM. It utilizes both local context (i.e., what has been de"
K19-1051,E12-1034,0,0.0691872,"nt (coming from financial news articles in NYT). In contrast, for KnowSemLM, since 6 Related Work Our work is built upon the previous works for semantic language models (Peng and Roth, 2016; Peng et al., 2017; Chaturvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some p"
K19-1051,P08-1030,0,0.0484867,"and entity level, and can be obtained either statistically from text or stated declaratively. The proposed method, KnowSemLM1 , infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks. 1 Introduction Natural language understanding requires a coherent understanding of a series of events or actions in a story. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared"
K19-1051,W14-1606,0,0.024923,"rge amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et"
K19-1051,D17-1195,0,0.129833,"t causality knowledge: pk (et+1 |et−k , et−k+1 , · · · , et , KBEC ). Here, knowledge in KBEC is generated manually from event templates specified in Sec. 4.2. Moreover, index k decides how far back we consider the preceding event sequence. We then add this set of conditional probabilities as additional features in a base model (re-implementation of the linear model proposed in Modi et al. (2017), namely “Base∗ ”) to train a classifier to predict the right referent. Results: The accuracy results are shown in Table 3. We compare with the original base model as well as the EntityNLM proposed in Ji et al. (2017) as baselines. Our re-implemented base model (“Re-base”) does not perform as good as the original model. However, with the help of additional features from FES-RNNLM, we outperform the base model. More importantly, with additional features from KnowSemLM, we achieve the best performance and beat the EntityNLM system. This demonstrates the importance of the manually added event causality knowledge, and the ability of KnowSemLM to successfully capture it. 556 5.4 Analysis of KnowSemLM we have the knowledge “PER[*]-borrow.01ARG[*](*) ⇒ PER[old]-return.01-ARG[old](*)”, meaning that something borro"
K19-1051,P16-1002,0,0.0293568,"ion Model The base semantic LM produces a distribution over events from the language model vocabulary, which represents local context, while the knowledge selection model generates a set of outcome events with a probability distribution, which represents global context of event causality knowledge. The sequence generation model then combines the local and global context for generating future events. Therefore, we model the conditional probability of event et+1 given context p(et+1 |Context) = p(et+1 |e1 , e2 , · · · , et , KBEC ). This overall distribution is computed via a copying mechanism (Jia and Liang, 2016), i.e., we either generate the next event (ei ) from the language model vocabulary (V) or copy from the outcome event set (ey ) based on the following probabilities: ( p(et+1 = ei ∈ V|Context) = (1 − λ)plm (ei ) p(et+1 = ey ∈ Vy |Context) = λpkn (ey ). Figure 2: Overview of the computational workflow for the proposed KnowSemLM. There are two key components: 1) a knowledge selection model, which activates the use of knowledge based on probabilistically matching causal event and produce a distribution over outcome events via attention; 2) a sequence generation model, which takes input from both"
K19-1051,1985.tmi-1.17,0,0.376056,"“PER[new]-want.01-buy.01-ARG[new](NEU), PER[old]-have.04-borrow.01-ARG[new](NEU)”. For FES-RNNLM, the system predicts the next event as “PER[old]-sell.01-ARG[new](NEU)” since in training data, there are many cooccurrences between the “borrow” event and “sell” event (coming from financial news articles in NYT). In contrast, for KnowSemLM, since 6 Related Work Our work is built upon the previous works for semantic language models (Peng and Roth, 2016; Peng et al., 2017; Chaturvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and D"
K19-1051,N16-1098,0,0.193903,"tional features. “Base∗ w/ FES-RNNLM” is the ablation study where no event causality knowledge is used. Even though “Base∗ ” model performs not as good as the original base model, we achieve the best performance with added KnowSemLM features. 47.9 49.3 49.6 Table 4: Results for perplexity and narrative cloze test. Both studies are conducted on the NYT holdout data. “FES-RNNLM” represents the semantic LM without the use of knowledge. The numbers show that KnowSemLM has lower perplexity and higher recall on narrative cloze test, which demonstrates the contribution of the infused knowledge. from Mostafazadeh et al. (2016) as the original reported result. KnowSemLM outperforms both baselines and the base model without the use of knowledge, i.e., FES-LM. The best performance achieved by KnowSemLM uses single most informative feature, with the feature being the conditional probability depending on only the nearest preceding event and event causality knowledge). 5.3 121.8 120.7 120.4 NYT InScript Match/Event 0.13 0.82 Activation/Event 0.03 0.28 λ 0.36 0.46 Table 5: Statistics for the use of event causality knowledge in KnowSemLM. We gather the statistics for both NYT and InScript Corpus. “Match/Event” represents a"
K19-1051,P19-1413,0,0.05336,"ls such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), and XLNET (Yang et al., 2019) have achieved much success for language modeling and generation tasks. Our proposed knowledge infused semantic language model can not be directly applied upon such word-level pre-trained language models. However, as future works, we are interested in exploring the possibility of pre-training a semantic language model with frame and entity abstractions on a large corpus with event causality knowledge, and fine-tune it on application tasks. tention to encode sentences and produced a strong baseline. Lee and Goldwasser (2019) considered the problem of learning relation aware event embeddings for commonsense inference, which can account for different relations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was fo"
K19-1051,W17-0906,0,0.0217546,"on which ending is more probable. Here, we test two different inference methods: a single most informative feature (where we go with the decision made by the pair of features which have the highest ratio) or majority voting based on the decision made jointly by all feature pairs. Results: The accuracy results are shown in Table 2. We compare KnowSemLM with Seq2Seq baselines (Sutskever et al., 2014) and Seq2Seq with attention mechanism (Bahdanau et al., 2014). We also include the DSSM system Application for Story Prediction Task Description and Setting: We use the benchmark ROCStories dataset (Mostafazadeh et al., 2017), and follow the test setting in Peng et al. (2017). For each instance, we are given a four-sentence story and the system needs to predict the correct fifth sentence from two choices; with the incorrect ending being semantically unreasonable, or un-related. Instead of treating the task as a supervised binary classification problem with a development set to tune, we evaluate KnowSemLM in an unsupervised fashion where 8 Our event representation is abstracted on a higher level. Thus, we process the original NYT documents, where event chains come from, for abstraction purposes; and then match it t"
K19-1051,W14-1618,0,0.012119,"vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013) propose a simple singlelayer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector logbilinear models, vLBL and ivLBL, and Levy and Goldberg (2014) proposed explicit word embeddings based on a PPMI metric. Additionally, researcher have been attempting to infuse knowledge into the language modeling process (Ahn et al., 2016; Yang et al., 2016; Ji et al., 2017; He et al., 2017; Clark et al., 2018). Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network, as well as by contracts HR0011-15-C-0113 and HR0011-18-2-0052 with the US Defense Advanced Research P"
K19-1051,P13-1008,0,0.0196519,"tatistically from text or stated declaratively. The proposed method, KnowSemLM1 , infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks. 1 Introduction Natural language understanding requires a coherent understanding of a series of events or actions in a story. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document"
K19-1051,N18-1139,0,0.045319,"Missing"
K19-1051,E14-1024,0,0.404581,"we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (1987-2007); we count with frame and entity level abstractions (see Section 2.1 for details). 1 Related resources refer to https://cogcomp. seas.upenn.edu/page/publication_view/886. 550 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 550–562 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics next eve"
K19-1051,P15-1019,0,0.111183,"Missing"
K19-1051,P16-1027,0,0.0174266,"ork is built upon the previous works for semantic language models (Peng and Roth, 2016; Peng et al., 2017; Chaturvedi et al., 2017). This line of work is in general inspired by script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to learn scripts via construction of knowledge bases from text. More recently, researchers focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011;"
K19-1051,N16-1034,0,0.0174006,"laratively. The proposed method, KnowSemLM1 , infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks. 1 Introduction Natural language understanding requires a coherent understanding of a series of events or actions in a story. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (19"
K19-1051,D16-1085,0,0.0242398,"sed method, KnowSemLM1 , infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks. 1 Introduction Natural language understanding requires a coherent understanding of a series of events or actions in a story. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (1987-2007); we count with fram"
K19-1051,D17-1108,1,0.873754,"yond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utilized in KnowSemLM; as shown in Sec. 4.2 that such knowledge can be both mined from corpus and written down declaratively. 7 Conclusion This paper proposes KnowSemLM, a knowledge infused semantic LM. It utilizes both local context (i.e., what has been described in text) an"
K19-1051,N18-1077,1,0.917098,"Missing"
K19-1051,P11-1082,0,0.0112198,"otta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical recurrent networks with at557 Most recently, pre-trained language models such as BERT (Devlin et al., 2019), GPT (Ra"
K19-1051,K15-1002,1,0.820278,"rameter to choose between events from LM vocabulary V and events from event causality knowledge base KBEC . 4 Construction of KnowSemLM 4.1 Dataset and Preprocessing Dataset: We use the New York Times (NYT) Corpus4 (from year 1987 to 2007) as the training corpus. It contains over 1.8M documents in total. Preprocessing: We preprocess all training documents with Semantic Role Labeling and Partof-Speech tagging. We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within-document entity co-reference (Peng et al., 2015a) to produce coreference chains and get the anaphoricity information. To obtain all annotations, we use the Illinois NLP tools (Khashabi et al., 2018).5 Further, we obtain event representations from text with frame, entity and sentiment level abstractions by following procedures described in Peng et al. (2017). pkn (ex ⇒ ey |e1 , e2 , · · · , et ) vec exp(evec x Wa ht ) exp(ey Wb ht ) =P . vec 0vec W h ) b t e∈Vx ,e0 ∈Vy exp(e Wa ht ) exp(e 3 The proposed computational framework of KnowSemLM is similar to DynoNet proposed in He et al. (2017). Compared to DynoNet, the knowledge base utilized h"
K19-1051,K17-1019,1,0.225655,"d in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (1987-2007); we count with frame and entity level abstractions (see Section 2.1 for details). 1 Related resources refer to https://cogcomp. seas.upenn.edu/page/publication_view/886. 550 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 550–562 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics next event. In this way, the proposed KnowSemLM has the"
K19-1051,D15-1195,0,0.28231,"Missing"
K19-1051,N15-1082,1,0.935307,"rameter to choose between events from LM vocabulary V and events from event causality knowledge base KBEC . 4 Construction of KnowSemLM 4.1 Dataset and Preprocessing Dataset: We use the New York Times (NYT) Corpus4 (from year 1987 to 2007) as the training corpus. It contains over 1.8M documents in total. Preprocessing: We preprocess all training documents with Semantic Role Labeling and Partof-Speech tagging. We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within-document entity co-reference (Peng et al., 2015a) to produce coreference chains and get the anaphoricity information. To obtain all annotations, we use the Illinois NLP tools (Khashabi et al., 2018).5 Further, we obtain event representations from text with frame, entity and sentiment level abstractions by following procedures described in Peng et al. (2017). pkn (ex ⇒ ey |e1 , e2 , · · · , et ) vec exp(evec x Wa ht ) exp(ey Wb ht ) =P . vec 0vec W h ) b t e∈Vx ,e0 ∈Vy exp(e Wa ht ) exp(e 3 The proposed computational framework of KnowSemLM is similar to DynoNet proposed in He et al. (2017). Compared to DynoNet, the knowledge base utilized h"
K19-1051,K15-2012,1,0.814617,"base given the context of e1 , e2 , · · · , et as Here, λ is a learned scaling parameter to choose between events from LM vocabulary V and events from event causality knowledge base KBEC . 4 Construction of KnowSemLM 4.1 Dataset and Preprocessing Dataset: We use the New York Times (NYT) Corpus4 (from year 1987 to 2007) as the training corpus. It contains over 1.8M documents in total. Preprocessing: We preprocess all training documents with Semantic Role Labeling and Partof-Speech tagging. We also implement the explicit discourse connective identification module of a shallow discourse parser (Song et al., 2015). Additionally, we utilize within-document entity co-reference (Peng et al., 2015a) to produce coreference chains and get the anaphoricity information. To obtain all annotations, we use the Illinois NLP tools (Khashabi et al., 2018).5 Further, we obtain event representations from text with frame, entity and sentiment level abstractions by following procedures described in Peng et al. (2017). pkn (ex ⇒ ey |e1 , e2 , · · · , et ) vec exp(evec x Wa ht ) exp(ey Wb ht ) =P . vec 0vec W h ) b t e∈Vx ,e0 ∈Vy exp(e Wa ht ) exp(e 3 The proposed computational framework of KnowSemLM is similar to DynoNet"
K19-1051,P16-1028,1,0.885804,"t events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New York Times corpus (1987-2007); we count with frame and entity level abstractions (see Section 2.1 for details). 1 Related resources refer to https://cogcomp. seas.upenn.edu/page/publication_view/886. 550 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 550–562 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics next event. In this way, the propose"
K19-1051,D16-1038,1,0.844477,"text or stated declaratively. The proposed method, KnowSemLM1 , infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks. 1 Introduction Natural language understanding requires a coherent understanding of a series of events or actions in a story. In story comprehension, we need to understand not only what events have appeared in text, but also what is likely to happen next. While event extraction has been well studied (Ji and Grishman, 2008; Huang and Riloff, 2012; Li et al., 2013; Peng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016), the task of predicting future events (Radinsky et al., 2012; Radinsky and Horvitz, 2013) has received less attention. One perspective is to utilize the co-occurrence information between past and future events learned from a large corpus, which has been studied in script learning works (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014, 2016a; Peng and Roth, 2016; Peng et al., 2017). However, only considering co-occurrence information is not 2 The events “check in” and “be cleared” only co-occur twice in a same document in the 20-year New"
K19-1051,N15-1001,0,0.0190232,"; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Rudinger et al., 2015; Pichotta and Mooney, 2016a,b). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015; Inoue et al., 2016). In particular, Ferraro and Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence. Several works have employed neural embeddings (Modi and Titov, 2014a,b; Frermann et al., 2014; Titov and Khoddam, 2015). Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). Several recent works focus on narrative/story telling (Rishes et al., 2013), as well as studying event structures (Brown et al., 2017). Most recently, Mostafazadeh et al. (2016, 2017) proposed story cloze test as a standard way to test a system’s ability to model semantics. They released ROCStories dataset, and organized a shared task for LSDSem’17; which yields many interesting works on this task. Cai et al. (2017) developed a model that uses hierarchical"
K19-1051,C08-3012,0,0.0441695,"oduced a strong baseline. Lee and Goldwasser (2019) considered the problem of learning relation aware event embeddings for commonsense inference, which can account for different relations between events, beyond simple event similarity. We differ from them because the basic semantic unit we model is event level abstractions instead of word tokens. The definition of event causality knowledge in this work includes temporal ordering relationships. Much progress has been made in identifying and modeling such relations. In early works (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), the problem was formulated as a classification problem for determining the pair-wise event temporal relations; while recent works (Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017, 2018) took advantage of utilizing structural constraints such as transitive properties of temporal relationships via ILP to achieve better results. Comparatively, the concept of event causality knowledge here is broader and more flexible. Any event causality relation gained from human experience could be represented and utilized in KnowSemLM; as shown in Sec. 4.2 that such knowledge can be both mined fr"
K19-1051,D17-1006,0,0.0297904,"Missing"
K19-1060,P07-1036,1,0.840104,"n for Computational Linguistics 2 Related Work 2015), word sense disambiguation (Hovy and Hovy, 2012), temporal relation extraction (Ning et al., 2018), dependency parsing (Flannery et al., 2012), and named entity recognition (Jie et al., 2019). In particular, Jie et al. (2019) study a similar problem with a few key differences: since they remove entity surfaces randomly, the dataset is too easy; and they do not use constraints on their output. We compare against their results in our experiments. Our proposed method is most closely aligned with the Constraint Driven Learning (CoDL) framework (Chang et al., 2007), in which an iterative algorithm reminiscent of self-training is guided by constraints that are applied at each iteration. The supervision paradigm in this paper, partial supervision, falls broadly under the category of semisupervision (Chapelle et al., 2009), and is closely related to weak supervision (Hern´andez-Gonz´alez et al., 2016)1 and incidental supervision (Roth, 2017), in the sense that data is constructed through some noisy process. However, all of the most related work shares a key difference from ours: reliance on a small amount of fully annotated data in addition to the noisy da"
K19-1060,W18-3402,0,0.0845622,"m ours: reliance on a small amount of fully annotated data in addition to the noisy data. Fernandes and Brefeld (2011) introduces a transductive version of structured perceptron for partially annotated sequences. However, their definition of partial annotation is labels removed at random, so examples from all classes are still available if not contiguous. Fidelity Weighted Learning (Dehghani et al., 2017) uses a teacher/student model, in which the teacher has access to (a small amount) of high quality data, and uses this to guide the student, which has access to (a large amount) of weak data. Hedderich and Klakow (2018), following Goldberger and Ben-Reuven (2017), add a noise adaptation layer on top of an LSTM, which learns how to correct noisy labels, given a small amount of training data. We compare against this model in our experiments. In the world of weak supervision, Snorkel (Ratner et al., 2017; Fries et al., 2017), is a system that combines automatic labeling functions with data integration and noise reduction methods to rapidly build large datasets. They rely on high recall and consequent redundancy of the labeling functions. We argue that in certain realistic cases, high-recall candidate identifica"
K19-1060,P18-4003,0,0.035942,"um name tokens Entity ratio Num unique name tokens Annotator 1 Prec/Rec/F1 Annotator 2 Prec/Rec/F1 Combined Prec/Rec/F1 Table 3: Experimenting with different entity ratios. Scores reported are average F1 across all languages. Gold b value refers to using the gold annotated data to calculate the optimal entity ratio. This table shows that exact knowledge of the entity ratio is not required for CBL to succeed. Table 4: Bengali Data Statistics. The P/R/F1 scores are computed for the non-speaker annotator with respect to the gold training data. all gold labels from the train split, romanized it7 (Hermjakob et al., 2018), and presented it to two nonBengali speaking annotators using the TALEN interface (Mayhew and Roth, 2018). The instructions were to move quickly and annotate names only when there is high confidence (e.g. when you can also identify the English version of the name). They spent about 5 total hours annotating, without using Google Translate. This sort of non-speaker annotation is possible because the text contains many ‘easy’ entities – foreign names – which are noticeably distinct from native Bengali words. For example, consider the following: However, the main difference has to do with the foc"
K19-1060,D17-1269,1,0.91847,"Missing"
K19-1060,W12-1905,0,0.022432,"ity Recognition (NER) system. In this setting, all (or most) identified entities are correct, but not all entities have been identified, and crucially, there are no reliable examples of the negative class. The sentence shown in Figure 1 shows examples of both a gold and a partially annotated sentence. Such partially annotated data is relatively easy to obtain: for 645 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 645–655 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 Related Work 2015), word sense disambiguation (Hovy and Hovy, 2012), temporal relation extraction (Ning et al., 2018), dependency parsing (Flannery et al., 2012), and named entity recognition (Jie et al., 2019). In particular, Jie et al. (2019) study a similar problem with a few key differences: since they remove entity surfaces randomly, the dataset is too easy; and they do not use constraints on their output. We compare against their results in our experiments. Our proposed method is most closely aligned with the Constraint Driven Learning (CoDL) framework (Chang et al., 2007), in which an iterative algorithm reminiscent of self-training is guided by constr"
K19-1060,S18-2018,1,0.839399,"or most) identified entities are correct, but not all entities have been identified, and crucially, there are no reliable examples of the negative class. The sentence shown in Figure 1 shows examples of both a gold and a partially annotated sentence. Such partially annotated data is relatively easy to obtain: for 645 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 645–655 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 Related Work 2015), word sense disambiguation (Hovy and Hovy, 2012), temporal relation extraction (Ning et al., 2018), dependency parsing (Flannery et al., 2012), and named entity recognition (Jie et al., 2019). In particular, Jie et al. (2019) study a similar problem with a few key differences: since they remove entity surfaces randomly, the dataset is too easy; and they do not use constraints on their output. We compare against their results in our experiments. Our proposed method is most closely aligned with the Constraint Driven Learning (CoDL) framework (Chang et al., 2007), in which an iterative algorithm reminiscent of self-training is guided by constraints that are applied at each iteration. The supe"
K19-1060,N19-1079,0,0.407158,"ally, there are no reliable examples of the negative class. The sentence shown in Figure 1 shows examples of both a gold and a partially annotated sentence. Such partially annotated data is relatively easy to obtain: for 645 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 645–655 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 Related Work 2015), word sense disambiguation (Hovy and Hovy, 2012), temporal relation extraction (Ning et al., 2018), dependency parsing (Flannery et al., 2012), and named entity recognition (Jie et al., 2019). In particular, Jie et al. (2019) study a similar problem with a few key differences: since they remove entity surfaces randomly, the dataset is too easy; and they do not use constraints on their output. We compare against their results in our experiments. Our proposed method is most closely aligned with the Constraint Driven Learning (CoDL) framework (Chang et al., 2007), in which an iterative algorithm reminiscent of self-training is guided by constraints that are applied at each iteration. The supervision paradigm in this paper, partial supervision, falls broadly under the category of semi"
K19-1060,U08-1016,0,0.0484851,"obtain, but a full labeling of all other classes is prohibitively expensive. Named entity classification as an instance of PU learning was introduced in Grave (2014), which uses constrained optimization with constraints similar to ours. However, they only address the problem of named entity classification, in which mentions are given, and the goal is to assign a type to a named-entity (like ‘location’, ‘person’, etc.) as opposed to our goal of identifying and typing named entities. Although the task is slightly different, there has been work on building ‘silver standard’ data from Wikipedia (Nothman et al., 2008, 2013; Pan et al., 2017), using hyperlink annotations as the seed set and propagating throughout the document. Partial annotation in various forms has also been studied in the contexts of POS-tagging (Mori et al., 3 Constrained Binary Learning Our method assigns instance weights to all negative elements (tokens tagged as O), so that false negatives have low weights, and all other instances have high weights. We calculate weights according to the confidence predictions of a classifier trained iteratively over the partially annotated data. We refer to our method as Constrained Binary Learning ("
K19-1060,P17-1178,0,0.0320927,"of all other classes is prohibitively expensive. Named entity classification as an instance of PU learning was introduced in Grave (2014), which uses constrained optimization with constraints similar to ours. However, they only address the problem of named entity classification, in which mentions are given, and the goal is to assign a type to a named-entity (like ‘location’, ‘person’, etc.) as opposed to our goal of identifying and typing named entities. Although the task is slightly different, there has been work on building ‘silver standard’ data from Wikipedia (Nothman et al., 2008, 2013; Pan et al., 2017), using hyperlink annotations as the seed set and propagating throughout the document. Partial annotation in various forms has also been studied in the contexts of POS-tagging (Mori et al., 3 Constrained Binary Learning Our method assigns instance weights to all negative elements (tokens tagged as O), so that false negatives have low weights, and all other instances have high weights. We calculate weights according to the confidence predictions of a classifier trained iteratively over the partially annotated data. We refer to our method as Constrained Binary Learning (CBL).2 We will first desc"
K19-1060,N16-1030,0,0.0910623,"the distance of the ith token to the nearest named entity in P . Finally, we combine the two weighting schemes as: For example, consider phase 1 of Constrained Binary Learning, in which the labelset is collapsed to two labels (L = 2). Assuming that the O label has index 0, then if vi = 0, then Gi = [0.5, 0.5]. If vi = 0.6, then Gi = [0.6, 0.4]. For tokens in P (which have some entity label with high confidence), we always set Gi with 1 in the given label index, and 0 elsewhere. We use pretrained GloVe (Pennington et al., 2014) word vectors for English, and the same pretrained vectors used in Lample et al. (2016) for Dutch, German, and Spanish. The other languages are distributed with monolingual text (Strassel and Tracey, 2016), which we used to train our own skip-n-gram vectors. 4.4 vicombined 4.4.3 We compare against several baselines, including two from prior work. 4.4.4 4.4.1 Raw annotations The simplest baseline is to do nothing to the partially annotated data and train on it as is. (18) Self-training with Marginal CRF Neural Network with Noise Adaptation Following Hedderich and Klakow (2018), we used a neural network with a noise adaptation layer.6 This extra layer attempts to correct noisy exa"
K19-1060,D14-1162,0,0.0797329,"ow that omitting gazetteers impacts performance only slightly. 649 1 − GO i L−1 where di is the distance of the ith token to the nearest named entity in P . Finally, we combine the two weighting schemes as: For example, consider phase 1 of Constrained Binary Learning, in which the labelset is collapsed to two labels (L = 2). Assuming that the O label has index 0, then if vi = 0, then Gi = [0.5, 0.5]. If vi = 0.6, then Gi = [0.6, 0.4]. For tokens in P (which have some entity label with high confidence), we always set Gi with 1 in the given label index, and 0 elsewhere. We use pretrained GloVe (Pennington et al., 2014) word vectors for English, and the same pretrained vectors used in Lample et al. (2016) for Dutch, German, and Spanish. The other languages are distributed with monolingual text (Strassel and Tracey, 2016), which we used to train our own skip-n-gram vectors. 4.4 vicombined 4.4.3 We compare against several baselines, including two from prior work. 4.4.4 4.4.1 Raw annotations The simplest baseline is to do nothing to the partially annotated data and train on it as is. (18) Self-training with Marginal CRF Neural Network with Noise Adaptation Following Hedderich and Klakow (2018), we used a neural"
K19-1060,W09-1119,1,0.61742,"showing number of tags and tokens in Train and Test. The tag counts represent individual spans, not tokens. That is, “[Barack Obama]PER ” counts as one tag, not two. The b column shows the entity ratio as a percentage. s∈D 4.3 However, this formulation assumes that all possible sequences are equally likely. To address this, Jie et al. (2019) introduced a way to weigh sequences. NER Models In principle, CBL can use any NER method that can be trained with instance weights. We experiment with both non-neural and neural models. 4.3.1 L=− For our non-neural system, we use a version of Cogcomp NER (Ratinov and Roth, 2009; Khashabi et al., 2018) modified to use Weighted Averaged Perceptron. This operates on a weighted training set Dw = {(xi , yi , vi )}N i=1 , where N is the number of training examples, and vi ≥ 0 is the weight on the ith training example. In this non-neural system, a training example is a word with context encoded in the features. We change only the update rule, where the learning rate α is multiplied by the weight: log X q(y|x(s) )Pθ (y|x(s) ) (15) (s) y∈C(yp ) It’s easy to see that this formulation is a generalization of the standard CRF if q(.) = 1 for the gold sequence y, and 0 for all ot"
K19-1060,P16-1101,0,0.0868196,"in the label on token xi , and therefore the labeling should not update the model at training time. Conversely, if vi = 1, then this label is to be trusted entirely. If vi = 0, we set the soft labeling weights over xi to be uniform, which is as good as no information. Since vi is defined as confidence in the O label, the soft labeling weight for O increases proportionally to vi . Any remaining probability mass is distributed evenly among the other labels. To be precise, for tokens in N , we calculate values for Gi as follows: Neural Model A common neural model for NER is the BiLSTM-CRF model (Ma and Hovy, 2016). However, because the Conditional Random Field (CRF) layer calculates loss at the sentence level, we need a different method to incorporate token weights. We use a variant of the CRF that allows partial annotations by marginalizing over all possible sequences (Tsuboi et al., 2008). GO i = max(1/L, vi ) 4 Gnon-O = i Separate experiments show that omitting gazetteers impacts performance only slightly. 649 1 − GO i L−1 where di is the distance of the ith token to the nearest named entity in P . Finally, we combine the two weighting schemes as: For example, consider phase 1 of Constrained Binary"
K19-1060,L16-1521,0,0.166364,"Missing"
K19-1060,P18-4014,1,0.859869,"Prec/Rec/F1 Table 3: Experimenting with different entity ratios. Scores reported are average F1 across all languages. Gold b value refers to using the gold annotated data to calculate the optimal entity ratio. This table shows that exact knowledge of the entity ratio is not required for CBL to succeed. Table 4: Bengali Data Statistics. The P/R/F1 scores are computed for the non-speaker annotator with respect to the gold training data. all gold labels from the train split, romanized it7 (Hermjakob et al., 2018), and presented it to two nonBengali speaking annotators using the TALEN interface (Mayhew and Roth, 2018). The instructions were to move quickly and annotate names only when there is high confidence (e.g. when you can also identify the English version of the name). They spent about 5 total hours annotating, without using Google Translate. This sort of non-speaker annotation is possible because the text contains many ‘easy’ entities – foreign names – which are noticeably distinct from native Bengali words. For example, consider the following: However, the main difference has to do with the focus of each algorithm. Recall the discussion in Section 3 regarding the two possible approaches of 1) find"
K19-1060,K16-1022,1,0.925597,"Missing"
K19-1060,C08-1113,0,0.245093,"defined as confidence in the O label, the soft labeling weight for O increases proportionally to vi . Any remaining probability mass is distributed evenly among the other labels. To be precise, for tokens in N , we calculate values for Gi as follows: Neural Model A common neural model for NER is the BiLSTM-CRF model (Ma and Hovy, 2016). However, because the Conditional Random Field (CRF) layer calculates loss at the sentence level, we need a different method to incorporate token weights. We use a variant of the CRF that allows partial annotations by marginalizing over all possible sequences (Tsuboi et al., 2008). GO i = max(1/L, vi ) 4 Gnon-O = i Separate experiments show that omitting gazetteers impacts performance only slightly. 649 1 − GO i L−1 where di is the distance of the ith token to the nearest named entity in P . Finally, we combine the two weighting schemes as: For example, consider phase 1 of Constrained Binary Learning, in which the labelset is collapsed to two labels (L = 2). Assuming that the O label has index 0, then if vi = 0, then Gi = [0.5, 0.5]. If vi = 0.6, then Gi = [0.6, 0.4]. For tokens in P (which have some entity label with high confidence), we always set Gi with 1 in the gi"
K19-1060,N16-1029,0,0.0701724,"Missing"
K19-1060,W03-0419,0,\N,Missing
K19-1065,P11-1049,0,0.0386787,"last token in Xi , which is further fed into a linear layer followed by a softmax layer to generate the probability: exp(Wy hM i ) M 1≤i≤N exp(Wy hi ) Pi = P make predictions for a new instance during testing. We provide more details in Appendix A and refer readers to Wang and Poon (2018) for how to apply DPL as a tool in a downstream task such as relation extraction. 2.2 Silver Standard Evidence Generation Given correct answer options, we use a distant supervision method to generate the silver standard evidence sentences. Inspired by Integer Linear Programming models (ILP) for summarization (Berg-Kirkpatrick et al., 2011; Boudin et al., 2015), we model evidence sentence extraction as a maximum coverage problem and define the value of a selected sentence set as the sum of the weights for the unique words it contains. Formally, let vi denote the weight of word i, vi = 1 if word i appears in the correct answer option, vi = 0.1 if it appears in the question but not in the correct answer option, and vi = 0 otherwise.1 We use binary variables ci and sj to indicate the presence of word i and sentence j in the selected sentence set, respectively. Occi,j is a binary variable indicating the occurrence of word i in sent"
K19-1065,D15-1060,0,0.0471132,"Missing"
K19-1065,D15-1220,0,0.0230168,"ther fed into a linear layer followed by a softmax layer to generate the probability: exp(Wy hM i ) M 1≤i≤N exp(Wy hi ) Pi = P make predictions for a new instance during testing. We provide more details in Appendix A and refer readers to Wang and Poon (2018) for how to apply DPL as a tool in a downstream task such as relation extraction. 2.2 Silver Standard Evidence Generation Given correct answer options, we use a distant supervision method to generate the silver standard evidence sentences. Inspired by Integer Linear Programming models (ILP) for summarization (Berg-Kirkpatrick et al., 2011; Boudin et al., 2015), we model evidence sentence extraction as a maximum coverage problem and define the value of a selected sentence set as the sum of the weights for the unique words it contains. Formally, let vi denote the weight of word i, vi = 1 if word i appears in the correct answer option, vi = 0.1 if it appears in the question but not in the correct answer option, and vi = 0 otherwise.1 We use binary variables ci and sj to indicate the presence of word i and sentence j in the selected sentence set, respectively. Occi,j is a binary variable indicating the occurrence of word i in sentence j, lj denotes the"
K19-1065,N18-4017,0,0.0360985,"Missing"
K19-1065,P17-1171,0,0.0660781,"Missing"
K19-1065,P17-1020,0,0.128428,"Missing"
K19-1065,D17-1070,0,0.0766524,"Missing"
K19-1065,C16-1278,0,0.0478116,"Missing"
K19-1065,D18-1546,0,0.0467381,"Missing"
K19-1065,N18-2007,0,0.0657117,"Missing"
K19-1065,N18-1023,1,0.907106,"ion. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve comparable or better performance than the same reader that considers the full context. The comparison between ground truth evidence sentences and automatically selected sentences indicates that there is still room for improvement. 2.1 Evidence Sentence Extractor We use a multi-layer multi-head transformer (Vaswani et al., 2017) to extract evidence sentences. Let Ww and Wp be the word (subword) and position embeddings, respectively. Let M denote the total number of layers in the transformer. T"
K19-1065,Q18-1023,0,0.0579117,"Missing"
K19-1065,W18-5516,0,0.0784622,"Missing"
K19-1065,D17-1082,0,0.0686899,"ican history. ??? : As knowledge increased, Harvard and other colleges began to teach many new subjects. Question: Which of the following statements is true according to the passage? Options: A. in the early years, everyone can go to colleges. B. in 1782, Harvard began to teach German. C. in the early years, different colleges majored in different fields. D. more and more courses were taught in college with the improvement of knowledge. Evidence Sentence Extractor ?? , ?? , ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidenc"
K19-1065,D17-1214,0,0.0459874,"Missing"
K19-1065,P18-1161,0,0.0650009,", ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve"
K19-1065,P17-1015,0,0.061652,"Missing"
K19-1065,D15-1162,0,0.0199433,"Dataset MultiRC DREAM RACE # of documents Train Dev Test 456 3,869 25,137 83 1,288 1,389 332 1,287 1,407 # of questions Train Dev Test 5,131 6,116 87,866 953 2,040 4,887 3,788 2,041 4,934 Average # of sentences per document Train + Dev + Test 14.5 (Train + Dev) 8.5 17.6 Table 1: Statistics of multiple-choice machine reading comprehension and question answering datasets. 3.2 we set L, the maximum number of silver standard evidence sentences of a question, to 3. For MultiRC, we set L to 5 since many questions have more than 5 ground truth evidence sentences. Implementation Details We use spaCy (Honnibal and Johnson, 2015) for tokenization and named entity tagging. We use the pre-trained transformer (i.e., GPT) released by Radford et al. (2018) with the same preprocessing procedure. When GPT is used as the neural reader, we set training epochs to 4, use eight P40 GPUs for experiments on RACE, and use one GPU for experiments on other datasets. When GPT is used as the evidence sentence extractor, we set batch size 1 per GPU and dropout rate 0.3. We keep other parameters default. Depending on the dataset, training the evidence sentence extractor generally takes several hours. 3.3 Evaluation on MultiRC Since its te"
K19-1065,P18-1160,0,0.133031,"Extractor ?? , ?? , ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show t"
K19-1065,K17-1009,0,0.0612813,"Missing"
K19-1065,N16-1098,0,0.0195499,"rect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for multiple-choice MRC; 2) we s"
K19-1065,P15-1121,0,0.0279158,"o share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate the quality of the extracted e"
K19-1065,speer-havasi-2012-representing,0,0.0301794,"Missing"
K19-1065,S18-1119,0,0.0224214,"he extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for multiple-choice MRC; 2) we show that it may be a prom"
K19-1065,Q19-1014,1,0.892017,"d question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve comparable or better performance than the same reader that considers the full context. The comparison between ground truth evidence sentences and automatically selected sentences indicates that there is still room for improvement. 2.1 Evidence Sentence Extractor We use a multi-layer multi-head transformer (Vaswani et al., 2017) to extract evidence sentences. Let Ww and Wp be the word (subword) and position embeddings, respectively. Let M denote the total number of layers in the transformer. Then, the m-th layer hidden state hm of a token is given"
K19-1065,C18-1069,0,0.0562068,"nappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate the quality of the extracted evidence sentences,"
K19-1065,P18-2124,0,0.0571023,"Missing"
K19-1065,N18-1074,0,0.0433229,"Missing"
K19-1065,D16-1264,0,0.102451,"Missing"
K19-1065,Q19-1016,0,0.0576815,"Missing"
K19-1065,W18-5446,0,0.0505641,"Missing"
K19-1065,D13-1020,0,0.3834,"istic indicators for indirect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for m"
K19-1065,P15-2115,1,0.856476,"excited, like I have some news I have to share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. T"
K19-1065,P15-1024,0,0.0280347,"ve some news I have to share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate"
K19-1065,D18-1215,1,0.938366,"ruth evidence sentences in most multiplechoice MRC tasks, inspired by distant supervision, we first extract silver standard evidence sentences based on the lexical features of a question and its correct answer option (Section 2.2), then we use these noisy labels to train an evidence sentence extractor (Section 2.1). To denoise imperfect labels, we also manually design sentence-level and cross-sentence linguistic indicators such as “adjacent sentences tend to have the same label” and accommodate all the linguistic indicators with a recently proposed deep probabilistic logic learning framework (Wang and Poon, 2018) for indirect supervision (Section 2.3). Method Reference Document ?? : Started in 1636, Harvard University is the oldest of all the colleges and universities in the United States, followed by Yale, Princeton, Columbia... ?? : In the early years, these schools were nearly the same. ?? : Only young men went to college. ?? : All the students studied the same subjects, and everyone learned Latin and Greek………. ?? : In 1782, Harvard started a medical school for young men who wanted to become doctors………. ??? : In 1825, besides Latin and Greek, Harvard began to teach modern languages, such as French"
K19-1065,P18-2118,0,0.339408,"ical Institute at Chicago, Chicago, IL, USA 2 Tencent AI Lab, Bellevue, WA, USA 3 Cornell, Ithaca, NY, USA 4 University of Pennsylvania, Philadelphia, PA, USA {haiwang,mcallester}@ttic.edu, ks985@cornell.edu, {yudian,jianshuchen,dyu}@tencent.com, danroth@seas.upenn.edu tion. Compared to extractive and abstractive MRC tasks (e.g., (Rajpurkar et al., 2016; Koˇcisk`y et al., 2018; Reddy et al., 2019)) where most questions can be answered using spans from the reference documents, the majority of answer options cannot be directly extracted from the given texts. Existing multiple-choice MRC models (Wang et al., 2018b; Radford et al., 2018) take as input the entire reference document and seldom offer any explanation, making interpreting their predictions extremely difficult. It is a natural choice for human readers to use sentences from a given text to explain why they select a certain answer option in reading tests (Bax, 2013). In this paper, as a preliminary attempt, we focus on exacting evidence sentences that entail or support a question-answer pair from the given reference document. For extractive MRC tasks, information retrieval techniques can be very strong baselines to extract sentences that conta"
K19-1065,P18-1042,0,0.0468066,"Missing"
K19-1065,N18-1101,0,0.0799555,"Missing"
K19-1065,D18-1010,1,0.879016,"Missing"
K19-1065,P17-1172,0,0.0699146,"Missing"
K19-1065,C18-1171,0,0.0909856,"Missing"
K19-1065,D15-1075,0,\N,Missing
K19-1065,D17-2011,0,\N,Missing
K19-1065,W17-2604,1,\N,Missing
K19-1065,N19-1423,0,\N,Missing
K19-1065,P18-1157,0,\N,Missing
L16-1645,D13-1184,1,0.677208,"re NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors derived from existing state-of-the-art NLP tools 4085 that serve as reference implementations for those tools’ features, a"
L16-1645,clarke-etal-2012-nlp,1,0.876122,"tion is exposed (if every example has the same representation, the classifier can make no useful prediction; and some features are more generally characteristic of specific focus items than others), and so the application developer has good reason to spend time carefully specifying potentially useful features. 3. E DISON E DISON is a Java library to support feature extraction in Natural Language Processing. It uses the data structures from illinois-core-utilities1 , another Java library from the Cognitive Computation Group2 . Together, these expand on an older version of E DISON described in (Clarke et al., 2012). 3.1. Data Structures The main data structure used by E DISON is called a TextAnnotation. It is used to represent a piece of text, such as a document, and collects all NLP annotations for that text. NLP annotations such as tokens, phrases, sentences, and other text-related constructs are represented in terms of spans of tokens/characters (Constituents) and edges (Relations) linking spans to each other. Each annotation source is represented as an independent View over the original text, that collects the Constituents and Relations generated by that source. Constituents and Relations can be lab"
L16-1645,W02-0109,0,0.162392,"peech-, and chunk-level features that can be easily combined for English text. It uses a very basic data structure to represent the input annotations it requires to generate the feature representation. FEXTOR is implemented in Python and C++ and offers support for multiple languages. It supports feature extraction across sentence boundaries. The workflow is oriented towards file-based interaction – like Fex, FEXTOR uses a purpose-built scripting language to determine feature extraction behavior. There is little explicit support for feature extraction for NLP in learning packages such as NLTK (Loper and Bird, 2002), 4091 in NLP development frameworks like GATE (Cunningham et al., 2002), or in NLP software bundles like Stanford’s CoreNLP (Manning et al., 2014). All these software frameworks provide integrated natural language processing tools, but they do not directly support extraction of arbitrary features by composing the outputs of these processes. Such features are essential to achieve good performance using machine learning models in many NLP tasks. E DISON explicitly supports feature extraction but, in contrast to FEXTOR and Fex, is oriented more towards programmatic interaction (although the pack"
L16-1645,P14-5010,0,0.0126095,"ns it requires to generate the feature representation. FEXTOR is implemented in Python and C++ and offers support for multiple languages. It supports feature extraction across sentence boundaries. The workflow is oriented towards file-based interaction – like Fex, FEXTOR uses a purpose-built scripting language to determine feature extraction behavior. There is little explicit support for feature extraction for NLP in learning packages such as NLTK (Loper and Bird, 2002), 4091 in NLP development frameworks like GATE (Cunningham et al., 2002), or in NLP software bundles like Stanford’s CoreNLP (Manning et al., 2014). All these software frameworks provide integrated natural language processing tools, but they do not directly support extraction of arbitrary features by composing the outputs of these processes. Such features are essential to achieve good performance using machine learning models in many NLP tasks. E DISON explicitly supports feature extraction but, in contrast to FEXTOR and Fex, is oriented more towards programmatic interaction (although the package can be run as an application that reads from and writes to files). It packages a suite of reference implementations of feature extractors used"
L16-1645,K15-1002,1,0.825134,"tive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors derived from existing state-of-the-art NLP tools 4085 that serve as reference impl"
L16-1645,J08-2005,1,0.435174,"n library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors deri"
L16-1645,W09-1119,1,0.68414,"ly fed to a ML system. E DISON is a feature extraction library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feat"
L16-1645,rizzolo-roth-2010-learning,1,0.953117,"information about the way we organized the feature extractor classes and describes the process we used to port the feature extractors of some well-known CogComp NLP tools. 4.1. Organization and Navigation We use descriptive names for feature extractors based on the types of information they use, and the way these pieces of information are combined. There are five key characteristics of the feature extractors we processed: Programmatic integration with learning frameworks Programmatically, E DISON’s feature extractors can be easily integrated into JVM-based learning frameworks such as LBJava (Rizzolo and Roth, 2010), Mallet (McCallum, 2002), Weka (Hall et al., 2009), and Saul (Kordjamshidi et al., 2015). Here we will show programmatic integrations for the CogComp tools (LBJava/Saul) and provide a filebased integration for the other ML tools. We plan to create programmatic interfaces for these tools in the near future. For LBJava, features can be trivially wrapped as Classifier objects and used directly in the LBJava definition file. Figure 4 illustrates the way E DISON feature extractors can be used in the LBJava language, by creating a class that inherits from LBJava’s Classifier interface to wrap the f"
L16-1645,P98-2186,1,0.808399,"rom NLP data structures and either written to file in a generic format, or programmatically fed to a ML system. E DISON is a feature extraction library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wr"
L16-1645,J08-2002,0,0.0103322,"ion to other applications or to generate visual output for an end user. 3.2. Feature Extraction E DISON supports a range of feature types, from the standard combinations provided by Fex (Cumby and Roth, 2003; Cumby and Roth, 2000) and FEXTOR (Broda et al., 2013) – such as collocations of constituents within a specified context window, features combining different levels of annotation, features based on dependency parse paths between constituents – to more specialized features proven useful in more complex NLP tasks like semantic role labeling (e.g. subcategorization frames, or projected path (Toutanova et al., 2008)). These features are extracted from a TextAnnotation data structure populated with the appropriate source annotations: a feature extractor for part-of-speech bigrams will extract features only from a View populated with that information. In a supervised learning setting, the application uses labeled data to extract examples of the data items it wants the learning algorithm to classify. In the context of E DISON, we assume that this labeled data has been used to construct a View that contains a representation of the focus items as Constituents. The application code iterates over these focus it"
L18-1086,P06-4018,0,0.341529,"Missing"
L18-1086,P11-1056,1,0.783998,"Missing"
L18-1086,M98-1001,0,0.489588,"Missing"
L18-1086,clarke-etal-2012-nlp,1,0.65273,"tionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between t"
L18-1086,doddington-etal-2004-automatic,0,0.127526,"Missing"
L18-1086,N06-2015,0,0.256079,"Missing"
L18-1086,P10-2013,0,0.0751131,"Missing"
L18-1086,P14-5010,0,0.0181999,". Here is an example snippet showing how to annotate a sentence with C OG C OMP NLP Y: Pipeline. With all the Annotators generating the same data-structures, the P IPELINE project provides a simple interface to access Annotator components either individually or as a group, with a single function call. Use of P IPELINE is illustrated in Figure 3. A demo of P IPELINE is accessible online at http://nlp.cogcomp.org. One important aspect of our work is the collection of the major NLP annotators. Table 1 contains a summary of components that exist in other well-established NLP libraries. C ORE NLP (Manning et al., 2014) is a popular from ccg_nlpy import remote_pipeline pipeline = remote_pipeline.RemotePipeline() text = &quot;Hello, how are you. I am doing fine&quot; ta = pipeline.doc(text) print(ta.get_pos) # (UH Hello) (, ,) (WRB how) (VBP are) (PRP you)... 4. 1 543 Related Work https://github.com/CogComp/cogcomp-nlpy Task Dataset Measure Setting Result Tokenization POS (Roth and Zelenko, 1998) MASC (Ide et al., 2010) Accuracy – 97 Penn Treebank (Bies et al., 2015) F1 – 96.13 F1 – 91.12 F1 F1 F1 F1 F1 F1 – – – English Spanish Chinese 84.61 88.37 77.21 88.3 85 79.3 NER (Ratinov and Roth, 2009; Redman et al., 2016; Tsa"
L18-1086,W04-2705,0,0.100473,"Missing"
L18-1086,J05-1004,0,0.68556,"yakanok and Roth, 2000) CoNLL 2000 (Sang and Buchholz, 2000) F1 – 93.58 Temporal Normalization (Zhao et al., 2012) TempEval3 (UzZaman et al., 2013) Exact match F1 / Relaxed match F1 Temporal Span Extraction 79.35/ 83.4 F1 70.45 ACE 2005 (Walker et al., 2006) F1 F1 Temporal normalization, given a predicted temporal span Head detection Boundary detection given the head Head detection Boundary detection given the head Gold mention - Coarse Type Gold mention - Fine Type Test-I of Do and Roth (2012) Accuracy – 86.1 (Arivazhagan et al., 2016) F1 – 83.6 (Srikumar and Roth, 2013) F1 – 90.26 PropBank (Palmer et al., 2005a) F1 – 76.22 NomBank (Meyers et al., 2004) F1 – 66.97 Average of F1 score of MUC, B3 Gold mentions 77.05 CoNLL-12 (Pradhan et al., 2012) F1 ACE-05 (Walker et al., 2006) F1 Mention Detection F1 ERE F1 Relation Extraction (Chan and Roth, 2011) Taxonomic Relations (hypernyms, hyponyms, and co-hypernyms) Comma SRL (Arivazhagan et al., 2016) Preposition SRL (Srikumar and Roth, 2013) Verb SRL (Punyakanok et al., 2004) Nominal SRL(Punyakanok et al., 2004) Coreference (Samdani et al., 2014) B3 ACE-04 (Doddington et al., 2004) Wikifier (Tsai and Roth, 2016) F1 F1 F1 TAC-KBP 2016 EDL shared task 89.6 8"
L18-1086,D14-1162,0,0.0787689,"Missing"
L18-1086,W12-4501,0,0.0330716,"Missing"
L18-1086,W00-0721,1,0.380267,"Missing"
L18-1086,C04-1197,1,0.759993,"Missing"
L18-1086,W05-0639,0,0.0601394,"f C OG C OMP NLP core-utilities to extract features to be used by machine learning algorithms. E DISON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = a"
L18-1086,W09-1119,1,0.716643,"Missing"
L18-1086,rizzolo-roth-2010-learning,1,0.923167,"antic analysis, but also word and phrase similarity metrics. It provides essential support for text processing applications, including classes for text cleaning and for reading a number of popular NLP corpora. In addition to describing some key functionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is Te"
L18-1086,P98-2186,1,0.665416,"Missing"
L18-1086,P10-1122,1,0.792389,"SON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = augmentedSrlTa.getView(ViewNames.POS).getConstituents(); System.out.println(list); // (NNP Pierre"
L18-1086,L16-1645,1,0.930972,"trate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between them (with the targets b"
L18-1086,W00-0726,0,0.278326,"Missing"
L18-1086,Q13-1019,1,0.873032,"Missing"
L18-1086,W03-0419,0,0.250227,"Missing"
L18-1086,C16-2031,1,0.811715,"Missing"
L18-1086,S13-2001,0,0.0727449,"Missing"
L18-1086,Q15-1025,1,0.803667,"tic Parse, and Semantic Role Labeling • TAC/ERE Event, Relation, and Named Entity Similarity Utilities. For calculating semantic similarity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level dec"
L18-1086,P14-3006,0,0.0295918,"rity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level decisions (Do et al., 2009): Metric llm = new LLMStringSim(config); String s1 = &quot;Jack bought Alex&apos;s car&quot;; String s2 = &quot;Alex sold his car to"
L18-1086,N12-3008,1,0.753759,"Missing"
N04-1003,P98-1012,0,0.610518,"Missing"
N04-1003,N01-1007,0,0.0632607,"Missing"
N04-1003,W03-0405,0,0.439387,"Missing"
N04-1003,J01-4004,0,0.134707,"Missing"
N04-1003,C98-1012,0,\N,Missing
N04-1003,P02-1014,0,\N,Missing
N06-1011,C00-1056,0,\N,Missing
N06-1011,H05-1011,0,\N,Missing
N06-1011,C04-1122,0,\N,Missing
N06-1011,P97-1017,0,\N,Missing
N06-1011,W99-0613,0,\N,Missing
N06-1011,W99-0612,0,\N,Missing
N09-1034,P07-1083,0,0.224595,"performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target la"
N09-1034,P07-1036,1,0.868408,"r supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature representation of a word pair"
N09-1034,P08-2014,1,0.932369,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,D08-1037,1,0.91608,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,N06-1041,0,0.0335088,"del is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature represent"
N09-1034,P08-1045,0,0.0379901,"Missing"
N09-1034,C00-1056,0,0.0366664,"esource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in"
N09-1034,N06-1011,1,0.682814,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P06-1103,1,0.68478,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P04-1021,0,0.051568,"ing this simple resource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for"
N09-1034,W06-1616,0,0.0203426,"ss data to converge. Training the transliteration model is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser"
N09-1034,P06-1010,0,0.166253,"ethods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target languages and require c"
N09-1034,W06-1630,0,0.155505,"f the temporal information. Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. guages: Russian, Chinese, and Hebrew, and compared our results to previously published results. 5.1 Experimental Settings In our experiments the system is evaluated on its ability to correctly identify the gold transliteration for each source word. We evaluated the system’s performance using two measures adopted in many transliteration works. The first one is Mean Reciprocal Rank (MRR), used in (Tao et al., 2006; Sproat et al., 2006), which is the average of the multiplicative inverse of the rank of the correct answer. Formally, Let n be the number of source NEs. Let GoldRank(i) be the rank the algorithm assigns to the correct transliteration. Then, MRR is defined by: n MRR = 1 1X . n goldRank(i) i=1 Another measure is Accuracy (ACC) used in (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008a), which is the percentage of the top rank candidates being the gold transliteration. In our implementation we used the support vector machine (SVM) learning algorithm with linear kernel as our underlying lea"
N09-1034,P07-1015,0,0.0372154,"e we have in these resources - the romanization table is a noisy mapping covering the character set and is therefore better suited as a feature. Constraints, represented by pervasive, correct character mapping, indicate the sound mapping tendency between source and target languages. For example, certain n-gram phonemic mappings, such as r → l 303 to Chinese transliteration (see Sec. 3.2 for more details). Constraints in boldface apply to all positions, the rest apply only to characters appearing in initial position. These patterns have been used by other systems as features or pseudofeatures (Yoon et al., 2007). However, in our system these language specific ruleof-thumbs are systematically used as constraints to exclude impossible alignments and therefore generate better features for learning. We listed in Table 1 all 20 language specific constraints we used for Chinese. There is a total of 24 constraints for Hebrew and 17 for Russian. The constraints in Table 1 indicate a systematic sound mapping between English and Chinese unigram character mappings. Arranged by manners of articulation each row of the table indicates the sound change tendency among vowels, nasals, approximants (retroflex and glid"
N09-1034,W04-2401,1,\N,Missing
N09-1034,J98-4003,0,\N,Missing
N10-1018,P06-1032,0,0.230314,"to be as similar as possible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native English speakers; we show th"
N10-1018,C08-1022,0,0.710307,"Missing"
N10-1018,W09-2112,0,0.25149,"sible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native English speakers; we show that training on data that con"
N10-1018,I08-1059,0,0.364367,"aining on data with artificial errors is beneficial when compared to utilizing clean data. More importantly, error statistics have not been considered for error correction tasks. Lee and Seneff (2008) examine statistics on article and preposition mistakes in the JLE corpus. While they do not suggest a specific approach, they hypothesize that it might be helpful to incorporate this knowledge into a correction system that targets these two language phenomena. 3.2 Approaches to Detecting Article Mistakes Automated methods for detecting article mistakes generally use a machine learning algorithm. Gamon et al. (2008) use a decision tree model and a 5-gram language model trained on the English Gigaword corpus (LDC2005T12) to correct errors in English article and preposition usage. Han et al. (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. Yi et al. (2008) propose a web count-based system to correct determiner errors. In the above approaches, the classifiers are trained on native data. Therefore the classifiers cannot use the 1 http://www.cambridge.org/elt original article that the writer used as a feature. Han et al. (2006) use the source article at evaluation time and propose a"
N10-1018,P03-2026,0,0.609892,"Missing"
N10-1018,W00-0708,0,0.533452,"Missing"
N10-1018,W10-1004,1,0.589588,"Missing"
N10-1018,W08-1205,0,0.106771,"Missing"
N10-1018,N07-2045,0,0.0264026,"Missing"
N10-1018,J08-2005,1,\N,Missing
N10-1018,I08-2082,0,\N,Missing
N10-1066,D08-1031,1,0.109195,"idden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.6"
N10-1066,P07-1083,0,0.0229497,"operties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning ta"
N10-1066,N09-1034,1,0.852715,"hus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constr"
N10-1066,2008.amta-papers.4,0,0.0547444,"atent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language model. Unfortunately, their implementation is not guaranteed to converge. In CRF-like models with latent variables (McCal436 lum et al., 2005), the decision function marginalizes over the all hidden states when presented with an input example. Unfortunately, the computational cost of applying their framework is prohibitive with constrained latent representations. In contrast, our framework requires only the best hidden representation instead of marginalizing over all possible representations, thus reducing the computational effort. 7 Conclu"
N10-1066,P09-1053,0,0.174568,"ing into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constrained Latent Represent"
N10-1066,C04-1051,0,0.692767,"intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by definin"
N10-1066,P08-2014,1,0.934205,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,D08-1037,1,0.861012,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,H05-1049,0,0.0443099,"a and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and"
N10-1066,D08-1084,0,0.222611,"n et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning in"
N10-1066,J08-2005,1,0.216571,"b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al., 2008). All other words are connected by dependency edges. The intermediate representation is an alignment between the nodes and edges of the graphs. We used three hidden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the feature"
N10-1066,W06-1603,0,0.101349,"e learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show"
N10-1066,W04-3219,0,0.0114581,"ence formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints. While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations. 2 Preliminaries We introduce notation using the Paraphrase Identification task as a running example. This is the bi430 nary classification task of identifying whether one sentence is a paraphrase of another. A paraphrase pair from the MSR Paraphrase corpus (Quirk et al., 2004) is shown in Figure 1. In order to identify that the sentences paraphrase each other , we need to align constituents of these sentences. One possible alignment is shown in the figure, in which the dotted edges correspond to the aligned constituents. An alignment can be specified using binary variables corresponding to every edge between constituents, indicating whether the edge is included in the alignment. Different activations of these variables induce the space of intermediate representations. The notification was first reported Friday by MSNBC. MSNBC.com first reported the CIA request on F"
N10-1066,P09-2015,1,0.892648,"hrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – spec"
N10-1066,U06-1019,0,0.169702,"ngtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 2: Summary of latent variables and feature resources for the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: Dependency labels, NODE-INFO: corresponding node alignment resources, N/A: Hidden variable not used"
N10-1066,P06-1051,0,0.0183547,"hat the similarity in performance between the joint LCLR algorithm and the two stage 4 Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There a"
N10-1066,W07-1401,0,\N,Missing
N10-4005,N06-1046,0,0.042012,"Missing"
N10-4005,P06-2019,0,0.0331472,"Missing"
N10-4005,W06-1616,0,0.0667626,"Missing"
N10-4005,W06-1623,0,0.0298248,"Missing"
N10-4005,W06-1651,0,0.0292179,"Missing"
N10-4005,E06-2026,0,0.0504774,"Missing"
N10-4005,N07-1030,0,0.0596542,"Missing"
N10-4005,D07-1001,0,0.0274425,"Missing"
N10-4005,P08-2012,0,0.0604981,"Missing"
N10-4005,D08-1071,0,0.0647586,"Missing"
N10-4005,D08-1037,1,0.890831,"Missing"
N10-4005,W04-2401,1,\N,Missing
N10-4005,W05-0638,0,\N,Missing
N10-4005,J08-2005,1,\N,Missing
N10-4005,C04-1197,1,\N,Missing
N10-4005,P08-1112,0,\N,Missing
N10-4005,W05-0618,0,\N,Missing
N12-1087,N10-1083,0,0.0705101,"Missing"
N12-1087,J93-2003,0,0.0221784,", information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use EM including text classification (McCallum et al., 1998; Nigam et al., 2000), machine translation (Brown et al., 1993), and parsing (Klein and Manning, 2004). Recently, EM algorithms which incorporate constraints on structured output spaces have been proposed (Chang et al., 2007; Ganchev et al., 2010). Several variations of EM (e.g. hard EM) exist in the literature and choosing a suitable variation is of1. We propose a general framework called Unified Expectation Maximization (UEM) that presents a continuous spectrum of EM algorithms parameterized by a simple temperaturelike tuning parameter. The framework covers both constrained and unconstrained EM algorithms. UEM thus connects EM, hard EM, PR, and CoDL so"
N12-1087,P07-1036,1,0.62282,"Unified EM (UEM), that covers many EM variations including the constrained cases along with a continuum of new ones. UEM allows us to compare and investigate the properties of EM in a systematic way and helps find better alternatives. The contributions of this paper are as follows: We present a general framework containing a graded spectrum of Expectation Maximization (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is"
N12-1087,P06-2019,0,0.032328,"re are a few different styles of expressing EM, following the style of (Neal and Hinton, 1998), we define F (θ, q) = L(θ) − KL(q, Pθ (h|x)), (2) where q is a posterior distribution over H(x) and KL(p1 , p2 ) is the KL divergence between two distributions p1 and p2 . Given this formulation, EM can 689 2.2 Constraints in EM It has become a common practice in the NLP community to use constraints on output variables to guide inference. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata, 2006), and agreement constraints between wordalignment directions (Ganchev et al., 2008) or various parsing models (Koo et al., 2010). In the context of EM, constraints can be imposed on the posterior probabilities, q, to guide the learning procedure (Chang et al., 2007; Ganchev et al., 2010). In this paper, we focus on linear constraints over h (potentially non-linear over x.) This is a very general formulation as it is known that all Boolean constraints can be transformed into sets of linear constraints over binary variables (Roth and Yih, 2007). Assume that we have m linear constraints on output"
N12-1087,P08-1112,0,0.334219,"Expectation Maximization (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervis"
N12-1087,P04-1061,0,0.0173967,"-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use EM including text classification (McCallum et al., 1998; Nigam et al., 2000), machine translation (Brown et al., 1993), and parsing (Klein and Manning, 2004). Recently, EM algorithms which incorporate constraints on structured output spaces have been proposed (Chang et al., 2007; Ganchev et al., 2010). Several variations of EM (e.g. hard EM) exist in the literature and choosing a suitable variation is of1. We propose a general framework called Unified Expectation Maximization (UEM) that presents a continuous spectrum of EM algorithms parameterized by a simple temperaturelike tuning parameter. The framework covers both constrained and unconstrained EM algorithms. UEM thus connects EM, hard EM, PR, and CoDL so that the relation between different alg"
N12-1087,D10-1125,0,0.280133,"ion (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-superv"
N12-1087,P09-1039,0,0.00600942,"ns and their associated values γ. This paper focuses on values of γ between 0 and 1 for the following reasons. First, the Estep (8) is non-convex for γ &lt; 0 and hence computationally expensive; e.g., hard EM (i.e. γ = −∞) requires ILP inference. For γ ≥ 0, (8) is a convex optimization problem which can be solved exactly and efficiently. Second, for γ = 0, the E-step solves P max (10) h∈H(x) q(h) log Pθ (h|x) q s.t. Eq [Uh] ≤ b, q(h) ≥ 0, ∀h ∈ H(x), P h∈H(x) q(h) = 1 , which is an LP-relaxation of hard EM (Eq. (4) and (9)). LP relaxations often provide a decent proxy to ILP (Roth and Yih, 2004; Martins et al., 2009). Third, γ ∈ [0, 1] covers standard EM/PR. KL(q, Pθ (y|x)) + (1 − γ)H(q) — UEM (6) minimizes the former during the E-step, while Standard EM (3) minimizes the latter. The additional term (1 − γ)H(q) is essentially an entropic prior on the posterior distribution q which can be used to regularize the entropy as desired. For γ &lt; 1, the regularization term penalizes the entropy of the posterior thus reducing the probability mass on the tail of the distribution. This is significant, for instance, in unsupervised structured prediction where the tail can carry a substantial amount of probability mass"
N12-1087,P06-1065,0,0.0265801,"Missing"
N12-1087,P00-1056,0,0.168354,"Missing"
N12-1087,J03-1002,0,0.00643672,".94 24.46 23.78 PR CoDL Fr-En 10.71 14.68 8.40 10.09 8.09 8.93 Es-En 22.00 28.13 20.08 23.01 19.70 21.60 UEM 9.21 7.40 6.87 20.83 18.95 18.64 Table 2: AER (Alignment Error Rate) comparisons for French-English (above) and Spanish-English (below) alignment for various data sizes. For French-English setting, tuned γ for all data-sizes is either 0.5 or 0.6. For Spanish-English, tuned γ for all data-sizes is 0.7. threshold, tuned over the development set. Results We compare UEM with EM, PR, and CoDL on the basis of Alignment Error Rate (AER) for different sizes of unlabeled data (See Tab. 2.) See (Och and Ney, 2003) for the definition of AER. UEM consistently outperforms EM, PR, and CoDL with a wide margin. 6 Conclusion We proposed a continuum of EM algorithms parameterized by a single parameter. Our framework naturally incorporates constraints on output variables and generalizes existing constrained and unconstrained EM algorithms like standard and hard EM, PR, and CoDL. We provided an efficient Lagrange relaxation algorithm for inference with constraints in the E-step and empirically showed how important it is to choose the right EM version. Our technique is amenable to be combined with many existing v"
N12-1087,P09-1057,0,0.0215675,"(Spitkovsky et al., 2010) by exploring the space of EM algorithms in a “continuous” way. Furthermore, we also study the relation between quality of model initialization and the value of γ in the case of POS tagging. This is inspired by a general “research wisdom” that hard EM is a better choice than EM with a good initialization point whereas the opposite is true with an “uninformed” initialization. Unsupervised POS Tagging We conduct experiments on unsupervised POS learning experiment with the tagging dictionary assumption. We use a standard subset of Penn Treebank containing 24,115 tokens (Ravi and Knight, 2009) with the tagging dictionary derived from the entire Penn Treebank. We run UEM with a first order (bigram) HMM model5 . We consider initialization points of varying quality and observe the performance for γ ∈ [0, 1]. Different initialization points are constructed as follows. The “posterior uniform” initialization is created by spreading the probability uniformly over all possible tags for each token. Our EM model on 5 (Ravi and Knight, 2009) showed that a first order HMM model performs much better than a second order HMM model on unsupervised POS tagging 694 Relative performance to EM (Gamma="
N12-1087,W09-1110,1,0.833401,"Missing"
N12-1087,W04-2401,1,0.377695,"ommon technique for learning θ, which maximizes a tight lower bound on L(θ). While there are a few different styles of expressing EM, following the style of (Neal and Hinton, 1998), we define F (θ, q) = L(θ) − KL(q, Pθ (h|x)), (2) where q is a posterior distribution over H(x) and KL(p1 , p2 ) is the KL divergence between two distributions p1 and p2 . Given this formulation, EM can 689 2.2 Constraints in EM It has become a common practice in the NLP community to use constraints on output variables to guide inference. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata, 2006), and agreement constraints between wordalignment directions (Ganchev et al., 2008) or various parsing models (Koo et al., 2010). In the context of EM, constraints can be imposed on the posterior probabilities, q, to guide the learning procedure (Chang et al., 2007; Ganchev et al., 2010). In this paper, we focus on linear constraints over h (potentially non-linear over x.) This is a very general formulation as it is known that all Boolean constraints can be transformed into sets of linear constraints ove"
N12-1087,P11-1008,0,0.148874,"s called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use"
N12-1087,D10-1001,0,0.0886964,"or which it is a convex optimization problem, and use a Lagrange relaxation algorithm (Bertsekas, 1999). Our contributions are two fold: • We describe an algorithm for UEM with constraints that is as easy to implement as PR or CoDL. Existing code for constrained EM (PR or CoDL) can be easily extended to run UEM. • We solve the E-step (8) using a Lagrangian dual-based algorithm which performs projected subgradient-ascent on dual variables. Our algorithm covers Lagrange relaxation and dual decomposition techniques (Bertsekas, 1999) which were recently popularized in NLP (Rush and Collins, 2011; Rush et al., 2010; Koo et al., 2010). Not only do we extend the algorithmic framework to a continuum of algorithms, we also allow, unlike the aforementioned works, general inequality constraints over the output variables. Furthermore, we establish new and 692 We combine both the ideas by setting q(h) = G(h, Pθt (·|x), λT U, γ) where G(h, P, v, γ) =      1 P (h) γ e − vh γ 1 P 0 γ h0 P (h ) e − vh γ 0  0    δ(h= arg max P (h0 )e−vh ) γ&gt;0 , γ=0 . h0 ∈H(x) (14) Alg. 2 shows the overall optimization scheme. The dual variables for inequality constraints are restricted to be positive and hence after a gradi"
N12-1087,P04-1062,0,0.15651,"ter, we 3.1 Relationship between UEM and Other EM Algorithms show that UEM naturally includes and generalizes both PR and CoDL. The relation between unconstrained versions of EM 3 Unified Expectation Maximization We now present the Unified Expectation Maximization (UEM) framework which captures a continuum of (constrained and unconstrained) EM algorithms 2 Note that this set is a finite set of discrete variables not to be confused with a polytope. Polytopes are also specified as {z|Az ≤ d} but are over real variables whereas h is discrete. 690 has been mentioned before (Ueda and Nakano, 1998; Smith and Eisner, 2004). We show that the relationship takes novel aspects in the presence of constraints. In order to better understand different UEM variations, we write the UEM E-step (6) explicitly as an optimization problem: 3 The term ‘metric’ is used very loosely. KL(·, ·; γ) does not satisfy the mathematical properties of a metric. Framework Constrained γ = −∞ Hard EM γ=0 Hard EM γ ∈ (0, 1) (NEW) UEMγ γ=1 Standard EM Unconstrained CoDL (Chang et al., 2007) (NEW) EM with Lin. Prog. (NEW) constrained UEMγ PR (Ganchev et al., 2010) γ=∞→1 Deterministic Annealing EM Table 1: Summary of different UEM algorithms. T"
N12-1087,W10-2902,0,0.0809394,"M (6) minimizes the former during the E-step, while Standard EM (3) minimizes the latter. The additional term (1 − γ)H(q) is essentially an entropic prior on the posterior distribution q which can be used to regularize the entropy as desired. For γ &lt; 1, the regularization term penalizes the entropy of the posterior thus reducing the probability mass on the tail of the distribution. This is significant, for instance, in unsupervised structured prediction where the tail can carry a substantial amount of probability mass as the output space is massive. This notion aligns with the observation of (Spitkovsky et al., 2010) who criticize EM for frittering away too much probability mass on unimportant outputs while showing that hard EM does much better in PCFG parsing. In particular, they empirically show that when initialized with a “good” set of parameters obtained by supervised learning, EM drifts away (thus losing accuracy) much farther than hard-EM. 4 interesting connections between existing constrained inference techniques. 4.1 Projected Subgradient Ascent with Lagrangian Dual We provide below a high-level view of our algorithm, omitting the technical derivations due to lack of space. To solve the E-step (8"
N12-1087,H05-1010,0,0.032537,"Missing"
N12-1087,C96-2141,0,0.184257,"Missing"
N12-1087,J94-2001,0,\N,Missing
N12-3008,P05-1022,0,0.050947,"refer to an absolute time point on a universal timeline, making our time intervals absolute as well. Furthermore, we take advantage of the predicted temporal value of each temporal expression from the HeidelTime output. For instance, in the HeidelTime output example above, we extract 1947-02 as the normalized date of February 1947 and then convert it to the interval [1947-02-01 00:00:00, 1947-02-28 23:59:59]. If HeidelTime cannot identify an exact date, month or year, we then resort to our own temporal normalizer, 2 http://cogcomp.cs.illinois.edu/page/software view/POS VP NP We use nlparser (Charniak and Johnson, 2005) which consists of a set of conversion rules, regarding to the document creation time of the input text. An interval endpoint can get infinity value if its temporal boundary cannot be specified. 2.3 Comparison To compare two time intervals (i.e. normalized temporal expressions), we define six temporal relations: before, before-and-overlap, contains, equals, inside, after and after-and-overlap. The temporal relation between two normalized intervals is determined by a set of comparison rules that take the four interval endpoints into consideration. For example, A = [sA , eA ] contains B = [sB ,"
N12-3008,P98-2186,1,0.417853,"time intervals with respect to multiple relations. We believe that with the rapid progress in NLP and IR, more tasks will require temporal information and reasoning, and a system that addresses these three fundamental tasks well will be able to support and facilitate temporal reasoning systems efficiently. 2 The System 2.1 Temporal Expression Extraction We built the temporal expression extraction module on top of the Heideltime system (Str¨otgen and Gertz, 2010) to take advantage of a state-of-the-art temporal extraction system in capturing basic expressions. We use the Illinois POS tagger1 (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime. Below is an example of the HeidelTime output of the example in the previous section: Seventy-five million copies of the rifle have been built since it entered production in <TIMEX3 tid=”t2” type=”DATE” value=”1947-02”>February 1947</TIMEX3> In this example, HeidelTime captures a basic temporal expression: February 1947. However, HeidelTime cannot capture the complex temporal expression since it entered production in February 1947, which expresses a period of time from February 1947 until the document creation t"
N12-3008,S10-1071,0,0.250591,"Missing"
N12-3008,S10-1010,0,\N,Missing
N12-3008,C98-2181,1,\N,Missing
N15-1082,W12-3019,0,0.0227133,"a binary vector of size three. therefore, etc.) if there are any. To avoid sparsity, we only keep the mention roles (only subj or obj; no exact strings are kept). Two triple-pairs are considered different if they have different predicates, different roles, different coreferred argument-pairs, or different discourse connectives. The co-occurrence counts extracted in this form correspond to Type 2 schemas in Table 2. During inference, we match (2) a Type 2 schema for Sgiga (u, v) ≡ S(predu (m = d (m = u, a = av ), cn). u, a = au )|pred v Our method is related, but different from the proposal in Balasubramanian et al. (2012), who suggested to extract triples using an OpenIE system (Mausam et al., 2012). We extracted triples by starting from a mention, then extract the predicate and the other argument. An OpenIE system does not easily provide this ability. Our Gigaword counts are gathered also in a way similar to what has been proposed in Chambers and Jurafsky (2009), but we gather much larger amounts of data. 4.2 Wikipedia Disambiguated Co-occurence One of the problems with blindly extracting triple counts is that we may miss important semantic information. To address this issue, we use the publicly avaiable Illi"
N15-1082,D13-1178,0,0.0414347,"and and the rules were designed manually. 817 With the development of machine learning based models (Connolly et al., 1994; Soon et al., 2001b; Ng and Cardie, 2002a), attention shifted to solving standard coreference resolution problems. However, many hard coreference problems involve pronouns. As Winograd’s schema shows, there is still a need for further investigation in this subarea. World Knowledge Acquisition: Many tasks in NLP (such as Textual Entailment, Question Answering, etc.) require World Knowledge. Although there are many existing works on acquiring them (Schwartz and Gomez, 2009; Balasubramanian et al., 2013; Tandon et al., 2014), there is still no consensus on how to represent, gather and utilize high quality World Knowledge. When it comes to coreference resolution, there are a handful of works which either use web query information or apply alignment to an external knowledge base (Rahman and Ng, 2011b; Kobdani et al., 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012; Zheng et al., 2013). With the introduction of Predicate Schema, our goal is to bring these different approaches together and provide a coherent view. Acknowledgments The authors would like to thank Kai-Wei Chang, Alice Lai, Eri"
N15-1082,P12-1041,0,0.178706,"t, thus providing information on the subject and object preferences of a given predicate. The second specifies two predicates with a semantically shared argument (either subject or object), thus specifies role preferences of one predicate, among roles of the other. We instantiate these schemas by acquiring statistics in an unsupervised way from multiple resources including the Gigaword corpus, Wikipedia, Web Queries and polarity information. A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution (Rahman and Ng, 2011a; Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2012). The common approach has been to inject knowledge as features. However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity. Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework (Roth and Yih, 2004) to make decisions. Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions. Consider the following example: Ex.3 [Jack]e1 threw the bags of [Joh"
N15-1082,D08-1031,1,0.720202,"knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb “rob” is more likely to be the obje"
N15-1082,P09-1068,0,0.0607694,"ounts extracted in this form correspond to Type 2 schemas in Table 2. During inference, we match (2) a Type 2 schema for Sgiga (u, v) ≡ S(predu (m = d (m = u, a = av ), cn). u, a = au )|pred v Our method is related, but different from the proposal in Balasubramanian et al. (2012), who suggested to extract triples using an OpenIE system (Mausam et al., 2012). We extracted triples by starting from a mention, then extract the predicate and the other argument. An OpenIE system does not easily provide this ability. Our Gigaword counts are gathered also in a way similar to what has been proposed in Chambers and Jurafsky (2009), but we gather much larger amounts of data. 4.2 Wikipedia Disambiguated Co-occurence One of the problems with blindly extracting triple counts is that we may miss important semantic information. To address this issue, we use the publicly avaiable Illinois Wikifier (Cheng and Roth, 2013; Ratinov et al., 2011), a system that disambiguates mentions by mapping them into correct Wikipedia pages, to process the Wikipedia data. We then extract from the Wikipedia text all entities, verbs and nouns, and gather co-occurrence statistics with these syntactic variations: 1) immediately after 2) immediatel"
N15-1082,W11-1904,1,0.937367,"erence, then impacts other pronoun decisions in a global decision with re810 spect to all pronouns: pro3 is likely to be different from pro2 , and is likely to refer to e2 . This type of inference can be easily represented as a constraint during inference, but hard to inject as a feature. We then incorporate all constraints into a general coreference system (Chang et al., 2013) utilizing the mention-pair model (Ng and Cardie, 2002b; Bengtson and Roth, 2008; Stoyanov et al., 2010). A classifier learns a pairwise metric between mentions, and during inference, we follow the framework proposed in Chang et al. (2011) using ILP. The main contributions of this paper can be summarized as follows: 1. We propose the Predicate Schemas representation and study two specific schemas that are important for coreference. 2. We show how, in a given context, Predicate Schemas can be automatically compiled into constraints and affect inference. 3. Consequently, we address hard pronoun resolution problems as a standard coreference problem and develop a system1 which shows significant improvement for hard coreference problems while achieving the same state-of-the-art level of performance on standard coreference problems."
N15-1082,D13-1057,1,0.795154,"nto constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb “rob” is more likely to be the object of “arrest” than the object of the verb “rob” is. The knowledge requ"
N15-1082,D13-1184,1,0.6212,"to extract triples using an OpenIE system (Mausam et al., 2012). We extracted triples by starting from a mention, then extract the predicate and the other argument. An OpenIE system does not easily provide this ability. Our Gigaword counts are gathered also in a way similar to what has been proposed in Chambers and Jurafsky (2009), but we gather much larger amounts of data. 4.2 Wikipedia Disambiguated Co-occurence One of the problems with blindly extracting triple counts is that we may miss important semantic information. To address this issue, we use the publicly avaiable Illinois Wikifier (Cheng and Roth, 2013; Ratinov et al., 2011), a system that disambiguates mentions by mapping them into correct Wikipedia pages, to process the Wikipedia data. We then extract from the Wikipedia text all entities, verbs and nouns, and gather co-occurrence statistics with these syntactic variations: 1) immediately after 2) immediately before 3) before 4) after. For each of these variations, we get the probability and count9 of a pair of words (e.g. probability10 /count for “bend” immediately following “limb”) as separate dimensions of the score vector. 9 We use the log(.) of the counts here. Conditional probability"
N15-1082,N07-1030,0,0.0144761,"(u ∈ M, v ∈ P}, which concatenates all the schemas involving u and v. Entries in the score vector are designed so that the larger the value is, the more likely u and v are to be coreferents. We have two ways to use the score values: 1) Augumenting the feature vector φ(u, v) with these scores. 2) Casting the scores as constraints for the coreference resolution ILP in one of the following forms: ( Integer Linear Programming (ILP) based formulations of NLP problems (Roth and Yih, 2004) have been used in a board range of NLP problems and, particularly, in coreference problems (Chang et al., 2011; Denis and Baldridge, 2007). Our formulation is inspired by Chang et al. (2013). Let M be the set of all mentions in a given text snippet, and P the set of all pronouns, such that P ⊂ M. We train a coreference model by learning a pairwise mention scoring function. Specifically, given a mention-pair (u, v) ∈ M (u is the antecedent of v), we learn a left-linking scoring function fu,v = w> φ(u, v), where φ(u, v) is a pairwise feature vector and w is the weight vector. We then follow the Best-Link ap812 if si (u, v) ≥ αi si (w, v) ⇒ yu,v ≥ yw,v , if si (u, v) ≥ si (w, v) + βi ⇒ yu,v ≥ yw,v , (1) where si (.) is the i-th dim"
N15-1082,N10-1115,0,0.0259054,"aint generation and tuning method for coreference resolution with ILP inference. In Section 4, we describe how we acquire the score vectors S(u, v) for the Predicate Schemas in an unsupervised fashion. We now briefly explain the pre-processing step required in order to extract the score vector S(u, v) from a pair of mentions. Define a triple structure tm , predm (m, am ) for any m ∈ M. The subscript m for pred and a, emphasizes that they are extracted as a function of the mention m. The extraction of triples is done by utilizing the dependency parse tree from the Easy-first dependency parser (Goldberg and Elhadad, 2010). We start with a mention m, and extract its related predicate and the other argument based on the dependency parse tree and partof-speech information. To handle multiword predicates and arguments, we use a set of hand-designed rules. We then get the score vector S(u, v) by concatenating all scores of the Predicate Schemas given two triples tu , tv . Thus, we can expand the score representation for each type of Predicate Schemas given in Table 2: 1) For Type 1 schema, S(u, v) ≡ S(predv (m = u, a = av )) 6 2) For Type 2 schema, d (m = S(u, v) ≡ S(predu (m = u, a = au )|pred v v, a = av ), cn)."
N15-1082,P11-1079,0,0.0187864,"s still a need for further investigation in this subarea. World Knowledge Acquisition: Many tasks in NLP (such as Textual Entailment, Question Answering, etc.) require World Knowledge. Although there are many existing works on acquiring them (Schwartz and Gomez, 2009; Balasubramanian et al., 2013; Tandon et al., 2014), there is still no consensus on how to represent, gather and utilize high quality World Knowledge. When it comes to coreference resolution, there are a handful of works which either use web query information or apply alignment to an external knowledge base (Rahman and Ng, 2011b; Kobdani et al., 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012; Zheng et al., 2013). With the introduction of Predicate Schema, our goal is to bring these different approaches together and provide a coherent view. Acknowledgments The authors would like to thank Kai-Wei Chang, Alice Lai, Eric Horn and Stephen Mayhew for comments that helped to improve this work. This work is partly supported by NSF grant #SMA 12-09359 and by DARPA under agreement number FA875013-2-0008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation ther"
N15-1082,D13-1027,0,0.016694,"is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb “rob” is more likely to be the object of “arrest” than the object of the verb “rob” i"
N15-1082,J94-4002,0,0.0648978,"d that small changes in context could completely change coreference decisions. Levesque et al. (2011) proposed to assemble a set of sentences which comply with Winograd’s schema. Specifically, there are pairs of sentences which are identical except for minor differences which lead to different references of the same pronoun in both sentences. These references can be easily solved by humans, but are hard, he claimed, for computer programs. Anaphora Resolution: There has been a lot of work on anaphora resolution in the past two decades. Many of the early rule-based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity. The early designs were easy to understand and the rules were designed manually. 817 With the development of machine learning based models (Connolly et al., 1994; Soon et al., 2001b; Ng and Cardie, 2002a), attention shifted to solving standard coreference resolution problems. However, many hard coreference problems involve pronouns. As Winograd’s schema shows, there is still a need for further investigation in this subarea. World Knowledge Acquisition: Many tasks in NLP (such as Textual Entailment, Question Answering, etc.) require World Knowledge. Although ther"
N15-1082,H05-1004,0,0.0263094,"Missing"
N15-1082,D12-1048,0,0.00516763,"only keep the mention roles (only subj or obj; no exact strings are kept). Two triple-pairs are considered different if they have different predicates, different roles, different coreferred argument-pairs, or different discourse connectives. The co-occurrence counts extracted in this form correspond to Type 2 schemas in Table 2. During inference, we match (2) a Type 2 schema for Sgiga (u, v) ≡ S(predu (m = d (m = u, a = av ), cn). u, a = au )|pred v Our method is related, but different from the proposal in Balasubramanian et al. (2012), who suggested to extract triples using an OpenIE system (Mausam et al., 2012). We extracted triples by starting from a mention, then extract the predicate and the other argument. An OpenIE system does not easily provide this ability. Our Gigaword counts are gathered also in a way similar to what has been proposed in Chambers and Jurafsky (2009), but we gather much larger amounts of data. 4.2 Wikipedia Disambiguated Co-occurence One of the problems with blindly extracting triple counts is that we may miss important semantic information. To address this issue, we use the publicly avaiable Illinois Wikifier (Cheng and Roth, 2013; Ratinov et al., 2011), a system that disam"
N15-1082,C02-1139,0,0.349691,"te Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb"
N15-1082,P02-1014,0,0.421402,"te Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb"
N15-1082,P04-1020,0,0.0407816,"iated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb “rob” is m"
N15-1082,W11-1901,0,0.0141963,"s and their linked entities as mentions (We call this new re-annotated dataset WinoCoref 13 ). Ex.3 in Section 1 is from the Winograd dataset. It originally only specifies he as the pronoun in question, and we added him and his as additional target pronouns. We also use two standard coreference resolution Learning Method BLMP BLMP BLMP+SF BLMP BLMP+SF Inference Method BLL ILP BLL ILP+SC ILP+SC Table 6: Summary of learning and inference methods for all systems. SF stands for schema features while SC represents constraints from schema knowledge. datasets ACE(2004) (NIST, 2004) and OntoNotes5.0 (Pradhan et al., 2011) for evaluation. Statistics of the datasets are provided in Table 5. Baseline Systems: We use the state-of-art Illinois coreference system as our baseline system (Chang et al., 2013). It includes two different versions. One employs Best-Left-Link (BLL) inference method (Ng and Cardie, 2002b), and we name it Illinois14 ; while the other uses ILP with constraints for inference, and we name it IlliCons. Both systems use Best-Link Mention-Pair (BLMP) model for training. On Winograd dataset, we also treat the reported result from Rahman and Ng (2012) as a baseline. Developed Systems: We present thr"
N15-1082,W12-4501,0,0.0528629,"unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that the subject of the verb “rob” is more likely to be the object of “arrest” than th"
N15-1082,P11-1082,0,0.149532,"ifies one predicate with its subject and object, thus providing information on the subject and object preferences of a given predicate. The second specifies two predicates with a semantically shared argument (either subject or object), thus specifies role preferences of one predicate, among roles of the other. We instantiate these schemas by acquiring statistics in an unsupervised way from multiple resources including the Gigaword corpus, Wikipedia, Web Queries and polarity information. A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution (Rahman and Ng, 2011a; Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2012). The common approach has been to inject knowledge as features. However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity. Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework (Roth and Yih, 2004) to make decisions. Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions. Consider the follow"
N15-1082,D12-1071,0,0.600399,"ation on the subject and object preferences of a given predicate. The second specifies two predicates with a semantically shared argument (either subject or object), thus specifies role preferences of one predicate, among roles of the other. We instantiate these schemas by acquiring statistics in an unsupervised way from multiple resources including the Gigaword corpus, Wikipedia, Web Queries and polarity information. A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution (Rahman and Ng, 2011a; Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2012). The common approach has been to inject knowledge as features. However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity. Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework (Roth and Yih, 2004) to make decisions. Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions. Consider the following example: Ex.3 [Jack]e1 threw the bags of [John]e2 into the water si"
N15-1082,D12-1113,1,0.93405,"th its subject and object, thus providing information on the subject and object preferences of a given predicate. The second specifies two predicates with a semantically shared argument (either subject or object), thus specifies role preferences of one predicate, among roles of the other. We instantiate these schemas by acquiring statistics in an unsupervised way from multiple resources including the Gigaword corpus, Wikipedia, Web Queries and polarity information. A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution (Rahman and Ng, 2011a; Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2012). The common approach has been to inject knowledge as features. However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity. Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework (Roth and Yih, 2004) to make decisions. Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions. Consider the following example: Ex.3 [Jack]e"
N15-1082,P11-1138,1,0.690187,"ing an OpenIE system (Mausam et al., 2012). We extracted triples by starting from a mention, then extract the predicate and the other argument. An OpenIE system does not easily provide this ability. Our Gigaword counts are gathered also in a way similar to what has been proposed in Chambers and Jurafsky (2009), but we gather much larger amounts of data. 4.2 Wikipedia Disambiguated Co-occurence One of the problems with blindly extracting triple counts is that we may miss important semantic information. To address this issue, we use the publicly avaiable Illinois Wikifier (Cheng and Roth, 2013; Ratinov et al., 2011), a system that disambiguates mentions by mapping them into correct Wikipedia pages, to process the Wikipedia data. We then extract from the Wikipedia text all entities, verbs and nouns, and gather co-occurrence statistics with these syntactic variations: 1) immediately after 2) immediately before 3) before 4) after. For each of these variations, we get the probability and count9 of a pair of words (e.g. probability10 /count for “bend” immediately following “limb”) as separate dimensions of the score vector. 9 We use the log(.) of the counts here. Conditional probability of “limb” immediately"
N15-1082,W04-2401,1,0.679191,"Gigaword corpus, Wikipedia, Web Queries and polarity information. A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution (Rahman and Ng, 2011a; Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2012). The common approach has been to inject knowledge as features. However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity. Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework (Roth and Yih, 2004) to make decisions. Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions. Consider the following example: Ex.3 [Jack]e1 threw the bags of [John]e2 into the water since [he]pro1 mistakenly asked [him]pro2 to carry [his]pro3 bags. In order to correctly resolve the pronouns in Ex.3, one needs to have the knowledge that “he asks him” indicates that he and him refer to different entities (because they are subject and object of the same predicate; otherwise, himself should be used instead of him). This knowledge, which"
N15-1082,W09-1701,0,0.013196,"signs were easy to understand and the rules were designed manually. 817 With the development of machine learning based models (Connolly et al., 1994; Soon et al., 2001b; Ng and Cardie, 2002a), attention shifted to solving standard coreference resolution problems. However, many hard coreference problems involve pronouns. As Winograd’s schema shows, there is still a need for further investigation in this subarea. World Knowledge Acquisition: Many tasks in NLP (such as Textual Entailment, Question Answering, etc.) require World Knowledge. Although there are many existing works on acquiring them (Schwartz and Gomez, 2009; Balasubramanian et al., 2013; Tandon et al., 2014), there is still no consensus on how to represent, gather and utilize high quality World Knowledge. When it comes to coreference resolution, there are a handful of works which either use web query information or apply alignment to an external knowledge base (Rahman and Ng, 2011b; Kobdani et al., 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012; Zheng et al., 2013). With the introduction of Predicate Schema, our goal is to bring these different approaches together and provide a coherent view. Acknowledgments The authors would like to thank"
N15-1082,J01-4004,0,0.422186,"resentation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 1 Introduction Coreference resolution is one of the most important tasks in Natural Language Processing (NLP). Although there is a plethora of works on this task (Soon et al., 2001a; Ng and Cardie, 2002a; Ng, 2004; Bengtson and Roth, 2008; Pradhan et al., 2012; Kummerfeld and Klein, 2013; Chang et al., 2013), it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language ∗ These authors contributed equally to this work. Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision. Specifically, in Ex.1 one needs to know that “a limb bends” is more likely than “a bird bends”. In Ex.2 one needs to know that t"
N15-1082,P10-2029,0,0.0177088,"me predicate; otherwise, himself should be used instead of him). This knowledge, which can be easily represented as constraints during inference, then impacts other pronoun decisions in a global decision with re810 spect to all pronouns: pro3 is likely to be different from pro2 , and is likely to refer to e2 . This type of inference can be easily represented as a constraint during inference, but hard to inject as a feature. We then incorporate all constraints into a general coreference system (Chang et al., 2013) utilizing the mention-pair model (Ng and Cardie, 2002b; Bengtson and Roth, 2008; Stoyanov et al., 2010). A classifier learns a pairwise metric between mentions, and during inference, we follow the framework proposed in Chang et al. (2011) using ILP. The main contributions of this paper can be summarized as follows: 1. We propose the Predicate Schemas representation and study two specific schemas that are important for coreference. 2. We show how, in a given context, Predicate Schemas can be automatically compiled into constraints and affect inference. 3. Consequently, we address hard pronoun resolution problems as a standard coreference problem and develop a system1 which shows significant impr"
N15-1082,M95-1005,0,0.443666,"Missing"
N15-1082,H05-2018,0,0.0193904,"ension) would give us the score vector Sweb (u, v). 4.4 Polarity of Context Another rich source of information is the polarity of context, which has been previously used for Winograd schema problems (Rahman and Ng, 2012). Here we use a slightly modified version. The polarity scores are used for Type 1 Predicate Schemas and therefore we want to get Spol (u, v) ≡ S(predv (m = u, a = av )). We first extract polarity values for Po(predu ) and Po(predv ) by repeating the following procedures for each of them: • We extract initial polarity information given the predicate (using the data provided by Wilson et al. (2005)). • If the role of the mention is object, we negate its polarity. • If there is a polarity-reversing discourse connective (such as “but”) preceding the predicate, we reverse the polarity. • If there is a negative comparative adverb (such as “less”, “lower”) we reverse the polarity. 11 We query this only when av is an adjective and predv is a to-be verb. # Doc # Train # Test # Mention # Pronoun # Predictions for Pronoun Winograd 1886 1212 674 5658 1886 1348 WinoCoref 1886 1212 674 6404 2595 2118 ACE 375 268 107 23247 3862 13836 OntoNotes 3150 2802 348 175324 58952 37846 Table 5: Statistics of"
N15-1082,H89-1033,0,0.534692,"rming KnowComb system. They show that both Type 1 and Type 2 schema knowledge have higher precision on Category 1 and Category 2 data instances, respectively, compared to that on full data. Type 1 and Type 2 knowledge have similiar performance on full data, but the results show that it is harder to solve instances in category 2 than those in category 1. Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement. 6 Related Work Winograd Schema: Winograd (1972) showed that small changes in context could completely change coreference decisions. Levesque et al. (2011) proposed to assemble a set of sentences which comply with Winograd’s schema. Specifically, there are pairs of sentences which are identical except for minor differences which lead to different references of the same pronoun in both sentences. These references can be easily solved by humans, but are hard, he claimed, for computer programs. Anaphora Resolution: There has been a lot of work on anaphora resolution in the past two decades. Many of the early rule-based systems like Hobbs (1978"
N15-1082,W13-3517,0,0.0272761,"dge Acquisition: Many tasks in NLP (such as Textual Entailment, Question Answering, etc.) require World Knowledge. Although there are many existing works on acquiring them (Schwartz and Gomez, 2009; Balasubramanian et al., 2013; Tandon et al., 2014), there is still no consensus on how to represent, gather and utilize high quality World Knowledge. When it comes to coreference resolution, there are a handful of works which either use web query information or apply alignment to an external knowledge base (Rahman and Ng, 2011b; Kobdani et al., 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012; Zheng et al., 2013). With the introduction of Predicate Schema, our goal is to bring these different approaches together and provide a coherent view. Acknowledgments The authors would like to thank Kai-Wei Chang, Alice Lai, Eric Horn and Stephen Mayhew for comments that helped to improve this work. This work is partly supported by NSF grant #SMA 12-09359 and by DARPA under agreement number FA875013-2-0008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the auth"
N15-1138,P14-1006,0,0.0166552,"er five runs. The results are shown in Table 4. We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or"
N15-1138,P14-1062,0,0.00478261,"We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or set similarity) (Smola et al., 2007; Gretton et"
N15-1138,N13-1090,0,0.446502,"ncepts 23.1 18.1 13.8 10.6 8.4 as the drop in the document size. For example, there are on average 8 concepts in the intersection of two vectors with 500 non-zero concepts when we split each document into 16 parts. When there are fewer overlapping terms between two pieces of texts, it can cause mismatch or biased match and result in less accurate comparison. In this paper, we propose to use unsupervised approaches to improve the representation, along with a corresponding similarity approach between these representations. Our contribution is twofold. First, we incorporate the popular word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) representations into ESA representation, and show that incorporating semantic relatedness between Wikipedia titles can indeed help the similarity measure between short texts. Second, we propose and evaluate three mechanisms for comparing the resulting representations. We verify the superiority of the proposed methods using three different NLP tasks. 2 Sparse Vector Densification In this section, we introduce a way to compute the similarity between two sparse vectors by augmenting the original similarity measure, i.e., cosine similarity. Suppose we have two vectors x ="
N15-1138,W14-1609,0,0.0138721,"re shown in Table 4. We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or set similarity) (Smo"
N15-1138,D14-1162,0,0.117284,"Missing"
N15-1138,W09-1119,1,0.126608,"Missing"
N15-1138,P10-1040,0,0.0883002,"Missing"
N15-1138,P13-1171,0,0.0580633,"Missing"
N16-1072,W08-0336,0,0.0155383,"wi , in Eq. (1). Finally, the title which has the highest relevant score is chosen as the answer to m. 5 Experiments We evaluate the proposed method on the Wikipedia dataset of 12 langugaes and the TAC’15 Entity Linking dataset. For all experiments, we use the Word2Vec implementation in Gensim2 to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014)3 in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250). We use Stanford Word Segmenter (Chang et al., 2008) for tokenizing Chinese, and use the Java builtin BreakIterator for Thai. For all other languages, 2 https://radimrehurek.com/gensim/ https://github.com/mfaruqui/ crosslingual-cca 3 593 LANGUAGE German Spanish French Italian Chinese Hebrew Thai Arabic Turkish Tamil Tagalog Urdu # TOKENS # ALIGN . TITLES 616,347,668 460,984,251 357,553,957 342,038,537 179,637,674 75,076,391 68,991,911 67,954,771 47,712,534 12,665,312 4,925,785 3,802,679 960,624 754,740 1,088,660 836,154 469,982 137,821 72,072 255,935 162,677 50,570 48,725 83,665 Table 2: The number of tokens used in training the skip-gram model"
N16-1072,D13-1184,1,0.866662,"king task including those that relied on the availability of translation from the target language to English. 1 Introduction Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of Wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia or other Knowledge Bases, such as FreeBase. For English text, this problem has been studied extensively (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. In this paper, we develop a general technique which can be applied to all languages in Wikipedia even when no machine translation technology is available for them. The challenges in Wikification are due both to ambiguity and variability in expressing entities and concepts: a given mention in text, e.g., Chicago, may refer to different titles in Wikipedia (Chicago Bulls, the City, Chicago Bears, the band, etc.), and a title can be expressed in the"
N16-1072,D15-1131,0,0.015763,"entations of aligned sentences should be similar. Unlike the CCA-based method which learns monolingual word embeddings first, this model directly learns the cross-lingual embeddings. Luong et al. (2015) propose Bilingual Skip-Gram which extends the monolingual skip-gram model and learns bilingual embeddings using a parallel copora and word alignments. The model jointly considers within language co-occurrence and meaning equivalence across languages. That is, the monolingual objective for each language is also included in their learning objective. Several recent approaches (Gouws et al., 2014; Coulmance et al., 2015; Shi et al., 2015; Soyer et al., 2015) also require a sentence aligned parallel corpus to learn multilingual embeddings. Unlike other approaches, Vuli´c and Moens (2015) propose a model that only requires comparable corpora in two languages to induce cross-lingual vectors. Similar to our proposed approach, this model can also be applied to all languages in Wikipedia if we treat documents across two Wikipedia languages as a comparable corpus. However, the quality and quantity of this comparable corpus for low-resource languages 597 will be low, we believe. We choose the CCA-based model because"
N16-1072,D07-1074,0,0.0402773,"res favorably with the best systems on the TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English. 1 Introduction Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of Wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia or other Knowledge Bases, such as FreeBase. For English text, this problem has been studied extensively (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. In this paper, we develop a general technique which can be applied to all languages in Wikipedia even when no machine translation technology is available for them. The challenges in Wikification are due both to ambiguity and variability in expressing entities and concepts: a given mention in text, e.g., Chicago, may refer to different titles in Wikipedia (Chicago Bulls, the C"
N16-1072,E14-1049,0,0.607865,"), c is a context token within a window of w , vw is the target embedding represents w, vc0 is the embedding of c in context, D is the set of training documents, and D0 contains the sampled token pairs which serve as negative examples. This objective is maximized with 0 ’s. In this model, respect to variables vw ’s and vw tokens in the context are used to predict the target token. The token pairs in the training documents are positive examples, and the randomly sampled pairs are negative examples. 3.2 Multilingual Embeddings After getting monolingual embeddings, we adopt the model proposed in Faruqui and Dyer (2014) to project the embeddings of a foreign language and English to the same space. The requirement of this model is a dictionary which maps the words in English to the words in the foreign language. Note that there is no need to have this mapping for every word. The aligned words are used to learn the projection matrices, and the matrices can later be applied to the embeddings of each word to obtain the enhanced new embeddings. Faruqui and Dyer (2014) obtain this dictionary by picking the most frequent translated word from a parallel corpus. However, there is a limited or no parallel corpus for m"
N16-1072,W15-1521,0,0.0505373,". In contrast, our method only needs Wikipedia documents and the inter-language links. Besides the CCA-based multilingual word embeddings (Faruqui and Dyer, 2014) that we extend in Section 3, several other methods also try to embed words in different languages into the same space. Hermann and Blunsom (2014) use a sentence aligned corpus to learn bilingual word vectors. The intuition behind the model is that representations of aligned sentences should be similar. Unlike the CCA-based method which learns monolingual word embeddings first, this model directly learns the cross-lingual embeddings. Luong et al. (2015) propose Bilingual Skip-Gram which extends the monolingual skip-gram model and learns bilingual embeddings using a parallel copora and word alignments. The model jointly considers within language co-occurrence and meaning equivalence across languages. That is, the monolingual objective for each language is also included in their learning objective. Several recent approaches (Gouws et al., 2014; Coulmance et al., 2015; Shi et al., 2015; Soyer et al., 2015) also require a sentence aligned parallel corpus to learn multilingual embeddings. Unlike other approaches, Vuli´c and Moens (2015) propose a"
N16-1072,I11-1029,0,0.0605205,"vailability of such resources, and therefore can scale also to lowerresource languages, while doing very well also on high-resource languages. Wang et al. (2015) proposed an unsupervised method which matches a knowledge graph with a graph constructed from mentions and the corresponding candidates of the query document. This approach performs well on the Chinese dataset of TAC’13, but falls into the category (1). Moro et al. (2014) proposed another graph-based approach which uses Wikipedia and WordNet in multiple languages as lexical resources. However, they only focus on English Wikification. McNamee et al. (2011) aims at the same crosslingual Wikification setting as we do, where the challenge is in comparing foreign language words with English titles. They treat this problem as a cross-lingual information retrieval problem. That is, given the context words of the target mention in the foreign language, retrieve the most relevant English Wikipedia page. However, their approach requires parallel text to estimate word translation probabilities. In contrast, our method only needs Wikipedia documents and the inter-language links. Besides the CCA-based multilingual word embeddings (Faruqui and Dyer, 2014) t"
N16-1072,Q14-1019,0,0.0465685,"dge Base in the foreign language, whereas the second depends on a good machine translation system. The approach developed in this paper makes significantly simpler assumptions on the availability of such resources, and therefore can scale also to lowerresource languages, while doing very well also on high-resource languages. Wang et al. (2015) proposed an unsupervised method which matches a knowledge graph with a graph constructed from mentions and the corresponding candidates of the query document. This approach performs well on the Chinese dataset of TAC’13, but falls into the category (1). Moro et al. (2014) proposed another graph-based approach which uses Wikipedia and WordNet in multiple languages as lexical resources. However, they only focus on English Wikification. McNamee et al. (2011) aims at the same crosslingual Wikification setting as we do, where the challenge is in comparing foreign language words with English titles. They treat this problem as a cross-lingual information retrieval problem. That is, given the context words of the target mention in the foreign language, retrieve the most relevant English Wikipedia page. However, their approach requires parallel text to estimate word tr"
N16-1072,P11-1138,1,0.897701,"TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English. 1 Introduction Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of Wikification or Entity Linking aims at disambiguating mentions (sub-strings) in text to the corresponding titles (entries) in Wikipedia or other Knowledge Bases, such as FreeBase. For English text, this problem has been studied extensively (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains. In this paper, we develop a general technique which can be applied to all languages in Wikipedia even when no machine translation technology is available for them. The challenges in Wikification are due both to ambiguity and variability in expressing entities and concepts: a given mention in text, e.g., Chicago, may refer to different titles in Wikipedia (Chicago Bulls, the City, Chicago Bears, the band, etc.), and a title"
N16-1072,P15-2093,0,0.012417,"tences should be similar. Unlike the CCA-based method which learns monolingual word embeddings first, this model directly learns the cross-lingual embeddings. Luong et al. (2015) propose Bilingual Skip-Gram which extends the monolingual skip-gram model and learns bilingual embeddings using a parallel copora and word alignments. The model jointly considers within language co-occurrence and meaning equivalence across languages. That is, the monolingual objective for each language is also included in their learning objective. Several recent approaches (Gouws et al., 2014; Coulmance et al., 2015; Shi et al., 2015; Soyer et al., 2015) also require a sentence aligned parallel corpus to learn multilingual embeddings. Unlike other approaches, Vuli´c and Moens (2015) propose a model that only requires comparable corpora in two languages to induce cross-lingual vectors. Similar to our proposed approach, this model can also be applied to all languages in Wikipedia if we treat documents across two Wikipedia languages as a comparable corpus. However, the quality and quantity of this comparable corpus for low-resource languages 597 will be low, we believe. We choose the CCA-based model because we can obtain mul"
N16-1072,P15-2118,0,0.0362016,"Missing"
N16-1072,D14-1167,0,0.0613485,"for all languages in Wikipedia. Section 4 presents the proposed crosslingual wikification model which is based on multilingual embeddings. Evaluations and analyses are presented in Section 5. Section 6 discusses related work. Finally, Section 7 concludes the paper. 3 Multilingual Entity and Word Embeddings In this section, we describe how we generate a vector representation for each word and Wikipedia title in any language. 3.1 Monolingual Embeddings The first step is to train monolingual embeddings for each language separately. We adopt the “Alignment by Wikipedia Anchors” model proposed in Wang et al. (2014). For each language, we take all documents in Wikipedia and replace the hyperlinked text with the corresponding Wikipedia title. For example, consider the following Wikipedia sentence: “It is led by and mainly composed of Sunni Arabs from Iraq and Syria.”, where the three bold faced mentions are linked to some Wikipedia titles. We replace those mentions and the sentence becomes “It is led by and mainly composed of en/Sunni Islam Arabs from en/Iraq and en/Syria.” We then learn the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) on this newly generated text. Since a title appears"
N16-1072,D15-1081,0,0.0517516,"hes: (1) Do entity linking in the foreign language, and then find the corresponding English titles from the resulting foreign language titles; and (2) Translate the query documents to English and do English entity linking. The first approach relies on a large enough Knowledge Base in the foreign language, whereas the second depends on a good machine translation system. The approach developed in this paper makes significantly simpler assumptions on the availability of such resources, and therefore can scale also to lowerresource languages, while doing very well also on high-resource languages. Wang et al. (2015) proposed an unsupervised method which matches a knowledge graph with a graph constructed from mentions and the corresponding candidates of the query document. This approach performs well on the Chinese dataset of TAC’13, but falls into the category (1). Moro et al. (2014) proposed another graph-based approach which uses Wikipedia and WordNet in multiple languages as lexical resources. However, they only focus on English Wikification. McNamee et al. (2011) aims at the same crosslingual Wikification setting as we do, where the challenge is in comparing foreign language words with English titles"
N16-1072,E06-1002,0,\N,Missing
N16-3011,D14-1058,0,0.169096,"Missing"
N16-3011,P14-1026,0,0.171477,"Missing"
N16-3011,D15-1202,1,0.908487,"Missing"
N16-3011,Q15-1001,1,0.881005,"Missing"
N16-3011,D15-1135,0,0.163996,"Missing"
N16-3011,Q15-1042,0,\N,Missing
N18-1023,P99-1042,0,0.522474,"espite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills. 1 Introduction Machine Comprehension of natural language text is a fundamental challenge in AI and it has received significant attention throughout the history of AI (Greene, 1959; McCarthy, 1976; Reiter, 1976; Winograd, 1980). In particular, in natural language processing (NLP) it has been studied under various settings, such as multiplechoice Question-Answering (QA) (Green Jr. et al., 1961), Reading Comprehension (RC) (Hirschman et al., 1999), Recognizing Textual Entailment (RTE) (Dagan et al., 2013) etc. The area has seen rapidly increasing interest, thanks to the existence of sizable datasets and standard benchmarks. CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) to name a few, are some of the datasets that were released recently with the goal of facilitating research in machine comprehension. Despite all the excitement 252 Proceedings of NAACL-HLT 2018, pages 252–262 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics S3: Hearing n"
N18-1023,P13-1035,0,0.0648697,"Missing"
N18-1023,ide-etal-2008-masc,0,0.0213793,"ically collecting paragraphs, composing questions and answer-options through crowdsourcing platform, and manually curating the collected data. We also summarize a pilot study that helped us design this process, and end with a summary of statistics of the collected corpus. 3.1 Sources of documents The paragraphs used in our dataset are extracted from various sources. Here is the complete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in"
N18-1023,ide-suderman-2006-integrating,0,0.093396,"ly curating the collected data. We also summarize a pilot study that helped us design this process, and end with a summary of statistics of the collected corpus. 3.1 Sources of documents The paragraphs used in our dataset are extracted from various sources. Here is the complete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in our challenge require models to use information from multiple sentences of a paragraph. This is ensured through expl"
N18-1023,P16-1223,0,0.115073,"Missing"
N18-1023,D17-1215,0,0.0884839,"Missing"
N18-1023,P17-1147,0,0.118006,"Missing"
N18-1023,D17-1082,0,0.161802,"Missing"
N18-1023,Q15-1025,0,0.0598859,"Missing"
N18-1023,S18-1181,0,0.0231449,"Q (Rec(q)) with Q as the set of all questions. Since by design, each answer-option can be judged independently, we consider another metric, F1a , evaluating binary decisions on all the answer-options in the dataset. We define F1a to be the harmonic mean of Pre(Q) and Rec(Q), with S ˆ A(Q)| Pre(Q) = |A(Q)∩ ; A(Q) = q∈Q A(q); and ˆ |A(Q)| ˆ similar definitions for A(Q) and Rec(Q). 4.1 SurfaceLR (logistic regression baseline). As a simple baseline that makes use of our small training set, we reimplemented and trained a logistic regression model using word-based overlap features. As described in (Merkhofer et al., 2018), this baseline takes into account the lengths of a text, question and each answer candidate, as well as indicator features regarding the (co)occurrences of any words in them. Baselines Human. Human performance provides us with an estimate of the best achievable results on datasets. Using mechanical turk, we ask 4 people (limited to native speakers) to solve our data. We evaluate score of each label by averaging the decision of the individuals. SemanticILP (semi-structured baseline). This state-of-the-art solver, originally proposed for science questions and biology tests, uses a semistructure"
N18-1023,D15-1237,0,0.0582703,"Missing"
N18-1023,P17-2081,0,0.0584787,"Missing"
N18-1023,P15-1121,0,0.0803149,"Missing"
N18-1023,D13-1020,0,0.374141,"plete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in our challenge require models to use information from multiple sentences of a paragraph. This is ensured through explicit validation. We exclude any question that can be answered based on a single sentence from a paragraph. Open-endedness. Our dataset is not restricted to questions whose answer can be found verbatim in a paragraph. Instead, we provide a set of handcrafted answer-options f"
N18-1056,Q16-1031,0,0.0334641,"X1 + λx Sij ||Aei − Af j ||22 (1) 2 Dependency Contexts without a Treebank Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages. To circumvent this issue, we use related languages to train a weak dependency parser. We train a delexicalized parser using treebanks of related languages, where the word form based Bilingual Sparse Coding i,j s.t. Ak > 0 kDki k22 ≤ 1 k ∈ {e, f } where S is a translation matrix, and Ae and Af 3 More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective. 609 are sparse matrices which are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment"
N18-1056,C16-1012,0,0.0216483,"potential to complement existing work on creating cross-lingual ontologies such as BabelNet and the Open Multilingual Wordnet, which are noisy because they are compiled semi-automatically, and have limited language coverage. In general, distributional approaches can help refine ontology construction for any language where sufficient resources are available. It remains to be seen how our approach performs for other language pairs beyond simluated low-resource settings. We anticipate that replacing our delexicalized parser with more sophisticated transfer strategies (Rasooli and Collins, 2017; Aufrant et al., 2016) might be beneficial in such settings.While our delexicalized parsing based approach exhibits robustness, it can benefit from more sophisticated approaches for transfer parsing (Rasooli and Collins, 2017; Aufrant et al., 2016) to improve parser performance. We aim to explore these and other directions in the future. Acknowledgments The authors would like to thank the members of the CLIP lab at the University of Maryland, members of the Cognitive Computation Group at the University of Pennsylvania, and the anonymous reviewers from EMNLP/CoNLL 2017 and NAACL 2018 for their constructive feedback."
N18-1056,P14-2131,0,0.0341054,"ll-defined relation than entailment. Also, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This"
N18-1056,E12-1004,0,0.0621056,"and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010). 4.1 Annotation Setup The annotation task requires annotators to be fluent in both English and the non-English language. To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself. Also, we restrict the task to annotators verified by CrowdFlower to have those language skills. Finally, annotators also 4 http://crowdflower.com pair French-English Russian-English Arabic-English Chinese-English #crowdsourced #pos (= #neg) 2115 2264 2144 2165 763 706 691 806 Table 1: Crowd-sourced dataset statistics."
N18-1056,J10-4006,0,0.0368243,"Af SVD fruit Parsed Corpus 0.8 Ae Xe Figure 1: The B I S PARSE -D EP approach, which learns sparse bilingual embeddings using dependency based contexts. The resulting sparse embeddings, together with an unsupervised entailment scorer, can detect hypernyms across languages (e.g., pomme is a fruit). 3.1 Dependency Based Context Extraction The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph. For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways: • F ULL context (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler). • J OINT context (Chersoni et al., 2016): Parent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler). These two contexts exploit different amounts of syntactic information – J OINT does not require labeled parses, unlike F ULL. The J OINT context combines parent and sibling information, while F ULL keeps them as distinct contexts. Both encode directionality into the con"
N18-1056,W11-2501,0,0.357939,"(Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010). 4.1 Annotation Setup The annotation task requires annotators to be fluent in both English and the non-English language. To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself. Also, we restrict the task to annotators verified by CrowdFlower to have those language skills. Finally, annotators also 4 http://crowdflower.com pair French-English Russian-English Arabic-English Chinese-English #crowdsourced #pos (= #neg) 2115 2264 2144 2165 763 706 691 806 Table 1: Crowd-sourced"
N18-1056,P13-1133,0,0.0696763,"03; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011;"
N18-1056,C92-2082,0,0.315395,"ld even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This scenario is more relevant in a cross-lingual setting, where treebanks might not be available for many languages. 3 Our Approach – B I S PARSE -D EP We p"
N18-1056,J15-4004,0,0.095916,"Missing"
N18-1056,E17-2064,0,0.0375479,"Missing"
N18-1056,2005.mtsummit-papers.11,0,0.0501587,"Missing"
N18-1056,D16-1205,0,0.0465809,"Missing"
N18-1056,P15-1038,0,0.0131658,"ly aligning the monolingual vectors. We compute the translation matrix using word alignments derived from parallel corpora (see corpus statistics in Table ??). While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary. The monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4). Yara Parser was chosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015). The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013). We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives. A co-occurrence matrix is computed over this vocabulary using the context types in §3.1. Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007). The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe , Xf in t"
N18-1056,E14-1049,0,0.0548402,"rades only marginally. In all these cases, it compares favorably with models that have been supplied with all necessary resources, showing promise for low-resource settings. We extensively evaluate B I S PARSE -D EP on a new crowd-sourced cross-lingual dataset, with over 2900 hypernym pairs, spanning four languages from distinct families – French, Russian, Arabic and Chinese – and release the datasets for future evaluations. 2 Related Work Cross-lingual Distributional Semantics Cross-lingual word embeddings have been shown to encode semantics across languages in tasks such as word similarity (Faruqui and Dyer, 2014) and lexicon induction (Vuli´c and Moens, 2015). Our works stands apart in two aspects (1) In contrast to tasks involving similarity and synonymy (symmetric relations), the focus of our work is on detecting asymmetric relations across languages, using cross-lingual embeddings. (2) Unlike most previous work, we use dependency context instead of lexical context to induce crosslingual embeddings, which allows us to abstract away from language specific word order, and (as we show) improves hypernymy detection. More closely related is our prior work (Vyas and Carpuat, 2016) where we used lexical co"
N18-1056,P14-1113,0,0.0910424,"other asymmetric semantic relationships can improve language understanding when translations are not exactly equivalent. One such relationship is cross-lingual hypernymy – identifying that e´ cureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird. The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012, ∗ These authors contributed equally. https://github.com/yogarshi/ bisparse-dep/ 1 2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Pad´o et al., 2009). Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings. At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution. However, this approach cannot capture many phenomena. For instance, the English words cook, leader and supervisor can all be hypernyms of the Frenc"
N18-1056,P05-1014,0,0.638449,"ntifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation d"
N18-1056,P09-2018,0,0.0192746,"ween the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomat"
N18-1056,S12-1012,0,0.0385643,"a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and B"
N18-1056,P14-2050,0,0.0461982,"pus 0.8 Ae Xe Figure 1: The B I S PARSE -D EP approach, which learns sparse bilingual embeddings using dependency based contexts. The resulting sparse embeddings, together with an unsupervised entailment scorer, can detect hypernyms across languages (e.g., pomme is a fruit). 3.1 Dependency Based Context Extraction The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph. For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways: • F ULL context (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler). • J OINT context (Chersoni et al., 2016): Parent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler). These two contexts exploit different amounts of syntactic information – J OINT does not require labeled parses, unlike F ULL. The J OINT context combines parent and sibling information, while F ULL keeps them as distinct contexts. Both encode directionality into the context, either through label"
N18-1056,P98-2127,0,0.0960675,"are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilin"
N18-1056,W15-1521,0,0.0861335,"Missing"
N18-1056,P13-2109,0,0.0303818,"ord alignments derived from parallel corpora (see corpus statistics in Table ??). While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary. The monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4). Yara Parser was chosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015). The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013). We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives. A co-occurrence matrix is computed over this vocabulary using the context types in §3.1. Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007). The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe , Xf in the setup from §3.3 to generate 100 dimensional sparse bilingual vectors. Evaluati"
N18-1056,D11-1006,0,0.0481171,"ct contexts. Both encode directionality into the context, either through label direction or through sibling-parent relations. We use word-context co-occurrences generated using these contexts in a distributional semantic model (DSM) in lieu of window based contexts to generate dependency based embeddings. 3.2 features are turned off, so that the parser is trained on purely non-lexical features (e.g. POS tags). The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011, inter alia) being one popular approach.3 3.3 Given a dependency based co-occurrence matrix described in the previous section(s), we generate B I S PARSE -D EP embeddings using the framework from our prior work (Vyas and Carpuat, 2016), which we henceforth call B I S PARSE. B I S PARSE generates sparse, bilingual word embeddings using a dictionary learning objective with a sparsity inducing l1 penalty. We give a brief overview of this approach, the full details of which can be found in our prior work. For two languages with vocabularies ve and vf , and monolingual dependency embeddings Xe and"
N18-1056,N16-1118,0,0.0438936,"o, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This scenario is more relevant in a cross-ling"
N18-1056,P09-1034,0,0.214013,"Missing"
N18-1056,J07-2002,0,0.182639,"Missing"
N18-1056,Q17-1020,0,0.106759,"Af j ||22 (1) 2 Dependency Contexts without a Treebank Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages. To circumvent this issue, we use related languages to train a weak dependency parser. We train a delexicalized parser using treebanks of related languages, where the word form based Bilingual Sparse Coding i,j s.t. Ak > 0 kDki k22 ≤ 1 k ∈ {e, f } where S is a translation matrix, and Ae and Af 3 More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective. 609 are sparse matrices which are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers"
N18-1056,D16-1234,0,0.11608,"ypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because t"
N18-1056,E14-4008,0,0.34329,"Missing"
N18-1056,W15-4208,0,0.112978,"Missing"
N18-1056,E17-1007,0,0.648684,"precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because the vector spaces of two"
N18-1056,tiedemann-2012-parallel,0,0.0166915,"Missing"
N18-1056,W15-0814,0,0.0178851,"nding when translations are not exactly equivalent. One such relationship is cross-lingual hypernymy – identifying that e´ cureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird. The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012, ∗ These authors contributed equally. https://github.com/yogarshi/ bisparse-dep/ 1 2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Pad´o et al., 2009). Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings. At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution. However, this approach cannot capture many phenomena. For instance, the English words cook, leader and supervisor can all be hypernyms of the French word chef, as the French word does not have a exact translation in Engli"
N18-1056,E17-2065,0,0.193563,"Missing"
N18-1056,P15-2070,0,0.0871147,"Missing"
N18-1056,P15-2118,0,0.0712993,"Missing"
N18-1056,N16-1142,1,0.700744,"ible usages. However, translating chef to cook and then determining hypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approac"
N18-1056,W03-1011,0,0.037717,"ual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bo"
N18-1056,C98-2122,0,\N,Missing
N18-1056,S13-2005,0,\N,Missing
N18-1056,S12-1053,0,\N,Missing
N18-1077,clarke-etal-2012-nlp,1,0.724652,"linois LBJava package (Rizzolo and Roth, 2010) to learn from the training data described above. Since only relations that have sentence distance 0 or 1 are annotated in TBDense, we will have two classifiers, one for same sentence relations, and one for neighboring sentence relations, respectively. 3.3 Corpus As mentioned earlier, the source corpus on which we are going to construct T EM P ROB is comprised of NYT articles from 20 years (1987-2007)6 . It contains more than 1 million documents and we extract events and corresponding features from each document using the Illinois Curator package (Clarke et al., 2012) on Amazon Web Services (AWS) Cloud. In total, we discovered 51K unique verb semantic frames and 80M relations among them in the NYT corpus (15K of the verb frames had more than 20 relations extracted and 9K had more than 100 relations). 3.4 Interesting Statistics We first describe the notations that we are going to use. We denote the set of all verb semantic frames by V . Let Di , i = 1, . . . , N be the i-th document in our corpus, where N is the total number of documents. Let Gi = (Vi , Ei ) be the temporal graph inferred from Di using the approach described above, where Vi ⊆ V is the set o"
N18-1077,D11-1027,1,0.844769,"e when τ is larger. To further justify the quality, we also used 7 Dist=0 P R 65.6 61.3 69.8 44.5 74.6 29.2 81.0 13.9 82.9 5.0 Dist=1 P R 58.5 53.3 60.5 36.9 63.6 18.7 64.8 6.9 76.9 1.2 Table 3: Validating ηb and ηa from T EM P ROB based on the T-Before and T-After examples in TBDense. Performances are decomposed into same sentence examples (Dist=0) and contiguous sentence examples (Dist=1). A larger threshold leads to a higher precision, so ηb and ηa indeed represent a notion of confidence. another dataset that is not in the TempRel domain. Instead, we downloaded the EventCausality dataset8 (Do et al., 2011). For each causally related pair e1 and e2, if EventCausality annotates that e1 causes e2, we changed it to be T-Before; if EventCausality annotates that e1 is caused by e2, we changed it to be T-after. Therefore, based on the assumption that the cause event is T-Before the result event, we converted the EventCausality dataset to be a TempRel dataset and it thus could also be used to evaluate the quality of T EM P ROB. We adopted the same predictor used in Table 3 8 http://cogcomp.org/page/resource_ view/27 Recall the definitions of ηb and ηa in Eq. (1). 846 with τ = 0.5 and in Table 4, we com"
N18-1077,S17-2093,0,0.181254,"Missing"
N18-1077,D12-1062,1,0.869547,"ions are wasBornIn→diedIn→ and graduateFrom→workAt, where → means temporally beThe rest of the paper is organized as follows. Section 2 provides a literature review of TempRels extraction in NLP. Section 3 describes in detail the construction of T EM P ROB. In Sec. 4, we show that T EM P ROB can be used in existing TempRels extraction systems and lead to significant improvement. Finally, we conclude in Sec. 5. 2 Related Work The TempRels between events can be represented by an edge-labeled graph, where the nodes are events, and the edges are labeled with TempRels (Chambers and Jurafsky, 2008; Do et al., 2012; Ning et al., 2017). Given all the nodes, we work on the TempRel extraction task, which is to assign 842 fore. Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year2 , i.e., only when two events happened in"
N18-1077,W06-1623,0,0.288921,"ving local methods for TempRel extraction. In Table 5, we used the original feature set proposed in Sec. 3.2.1 as the baseline, and added the prior distribution obtained from T EM P ROB on top of it. Specifically, we added ηb (see Eq. (1)) and {fr }r∈R , respectively, where {fr }r∈R is the prior distributions of all labels, i.e., fr (vi , vj ) = ∑ C(vi , vj , r) , r ∈ R. ′ r′ ∈R C(vi , vj , r ) As mentioned earlier in Sec. 2, many systems adopt a global inference method via integer linear programming (ILP) (Roth and Yih, 2004) to enforce transitivity constraints over an entire temporal graph (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). In addition to the usage shown in Sec. 4.2.1, the prior distributions from T EM P ROB can also be used to regularize the conventional ILP formulation. Specifically, in each document, let Ir (ij) ∈ {0, 1} be the indicator function of relation r for event i and event j; let xr (ij) ∈ [0, 1] be the corresponding softmax score obtained from the local classifiers (depending on the sentence distance between i and j). Then the ILP objective for global inference is (5) Recall function C is defined in Eq. (2). A"
N18-1077,P14-2082,0,0.781323,"n of this work is thus the construction of such a resource in the form of a probabilistic knowledge base, constructed from a large New York Times (NYT) corpus. We hereafter name our resource TEMporal relation PRObabilistic knowledge Base (T EM P ROB), which can potentially benefit many time-aware tasks. A few example entries of T EM P ROB are shown in Table 1. Second, we show that existing TempRel extraction systems can be improved using T EM P ROB, either in a local method or in a global method (explained later), by a significant margin in performance on the benchmark TimeBank-Dense dataset (Cassidy et al., 2014). Example 2: The original sentences in Example 1. More than 10 people have (e1:died), police said. A car (e2:exploded) on Friday in the middle of a group of men playing volleyball. The first thing I (e3:ask) is that they (e4:help) writing this column. The TempRel extraction task has a strong dependency on prior knowledge, as shown in our earlier examples. However, very limited attention has been paid to generating such a resource and to make use of it; to our knowledge, the T EM P ROB proposed in this work is completely new. We find that the time-sensitive relations proposed in Jiang et al. (2"
N18-1077,W13-1203,0,0.0292017,"f nodes, the follow3.1 Event Extraction Extracting events and the relations between them (e.g., coreference, causality, entailment, and temporal) have long been an active area in the NLP community. Generally speaking, an event is considered to be an action associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Justice”); in the context of TempRels, existing datasets have focused more on predicate verbs rather than nominals4 (Pustejovsky et al., 2003; Graff, 2002; UzZaman et al., 2013). Therefore, we only look at verb semantic frames in this work due to the d"
N18-1077,D08-1073,0,0.934258,"respectively). Exemplar relations are wasBornIn→diedIn→ and graduateFrom→workAt, where → means temporally beThe rest of the paper is organized as follows. Section 2 provides a literature review of TempRels extraction in NLP. Section 3 describes in detail the construction of T EM P ROB. In Sec. 4, we show that T EM P ROB can be used in existing TempRels extraction systems and lead to significant improvement. Finally, we conclude in Sec. 5. 2 Related Work The TempRels between events can be represented by an edge-labeled graph, where the nodes are events, and the edges are labeled with TempRels (Chambers and Jurafsky, 2008; Do et al., 2012; Ning et al., 2017). Given all the nodes, we work on the TempRel extraction task, which is to assign 842 fore. Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year2 , i.e., only when two ev"
N18-1077,C16-1161,0,0.308271,"y et al., 2014). Example 2: The original sentences in Example 1. More than 10 people have (e1:died), police said. A car (e2:exploded) on Friday in the middle of a group of men playing volleyball. The first thing I (e3:ask) is that they (e4:help) writing this column. The TempRel extraction task has a strong dependency on prior knowledge, as shown in our earlier examples. However, very limited attention has been paid to generating such a resource and to make use of it; to our knowledge, the T EM P ROB proposed in this work is completely new. We find that the time-sensitive relations proposed in Jiang et al. (2016) is a close one in literature (although it is still very different). Jiang et al. (2016) worked on the knowledge graph completion task. Based on YAGO2 (Hoffart et al., 2013) and Freebase (Bollacker et al., 2008), it manually selects a small number of relations that are timesensitive (10 relations from YAGO2 and 87 relations from Freebase, respectively). Exemplar relations are wasBornIn→diedIn→ and graduateFrom→workAt, where → means temporally beThe rest of the paper is organized as follows. Section 2 provides a literature review of TempRels extraction in NLP. Section 3 describes in detail the"
N18-1077,Q14-1022,0,0.850962,"08), where the problem was formulated as learning a classification model for determining the label of every edge locally without referring to other edges (i.e., local methods). The predicted temporal graphs by these methods may violate the transitive properties that a temporal graph should possess. For example, given three nodes, e1, e2, and e3, a local method can possibly classify (e1,e2)=before, (e2,e3)=before, and (e1,e3)=after, which is obviously wrong since before is a transitive relation and (e1,e2)=before and (e2,e3)=before dictate that (e1,e3)=before. Recent state-of-the-art methods, (Chambers et al., 2014; Mirza and Tonelli, 2016), circumvented this issue by growing the predicted temporal graph in a multi-step manner, where transitive graph closure is performed on the graph every time a new edge is labeled. This is conceptually solving the structured prediction problem greedily. Another family of methods resorted to Integer Linear Programming (ILP) (Roth and Yih, 2004) to get exact inference to this problem (i.e., global methods), where the entire graph is solved simultaneously and the transitive properties are enforced naturally via ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky"
N18-1077,P07-2044,0,0.336654,"Missing"
N18-1077,S15-2134,0,0.327226,"from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available1 . 1 Introduction Time is an important dimension of knowledge representation. In natural language, temporal information is often expressed as relations between events. Reasoning over these relations can help figuring out when things happened, estimating how long things take, and summarizing the timeline of a series of events. Several recent SemEval workshops are a good showcase of the importance of this topic (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017). One of the challenges in temporal relation extraction is that it requires high-level prior knowledge of the temporal order that events usually follow. In Example 1, we have deleted events from several snippets from CNN, so that we cannot use our prior knowledge of those events. We are also Example 1: Difficulty in understanding TempRels when event content is missing. Note that e1 and e2 have the same tense, and e3 and e4 have the same tense. More than 10 people have (e1:died), police said. A car (e2:exploded) on Friday in the middle of"
N18-1077,W04-3205,0,0.307286,"can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above. Another related work is the VerbOcean (Chklovski and Pantel, 2004), which extracts temporal relations between pairs of verbs using manually designed lexico-syntactic patterns (there are in total 12 such patterns), in contrast to the automatic extraction method proposed in this work. In addition, the only termporal relation considered in VerbOceans is before, while we also consider relations such as after, includes, included, equal, and vague. As expected, the total numbers of verbs and before relations in VerbOcean is about 3K and 4K, respectively, both of which are much smaller than T EM P ROB, which contains 51K verb frames (i.e., disambiguated verbs), 9.2"
N18-1077,P06-1095,0,0.707566,"Missing"
N18-1077,W17-2703,0,0.0128495,"he next, we describe each of these elements. 3 We adopt the commonly used feature set in TempRel extraction (Do et al., 2012; Ning et al., 2017) and here we simply list them for reproducibility. For each pair of nodes, the follow3.1 Event Extraction Extracting events and the relations between them (e.g., coreference, causality, entailment, and temporal) have long been an active area in the NLP community. Generally speaking, an event is considered to be an action associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Justice”); in the context of TempRels, existing dat"
N18-1077,P11-2061,0,0.0549352,"able 5. System 3 is the proposed. Per the McNemar’s test, System 3 is significantly better than System 1 with p<0.0005. We present our results on the test split of TBDense in Table 6, which is an ablation study showing step-by-step improvements in two metrics. In addition to the straightforward precision, recall, and F1 metric, we also compared the F1 of the temporal awareness metric used in TempEval3 (UzZaman et al., 2013). The awareness metric performs graph reduction and closure before evaluation so as to better capture how useful a temporal graph is. Details of this metric can be found in UzZaman and Allen (2011); UzZaman et al. (2013); Ning et al. (2017). In Table 6, the baseline used the original feature set proposed in Sec. 3.2.1 and applied global ILP inference with transitivity constraints. Technically, it is to solve Eq. (6) with λ = 0 (i.e., unregularized) on top of the original system in Table 5. Apart from some implementation details, this baseline is also the same as many existing global methods as Chambers and Jurafsky (2008); Do et al. (2012). System 2, “+Feature: {fr }r∈R ”, 9 https://github.com/nchambers/caevo http://cogcomp.org/page/publication_ view/822 11 There are 731 relations in th"
N18-1077,C16-1007,0,0.383606,"was formulated as learning a classification model for determining the label of every edge locally without referring to other edges (i.e., local methods). The predicted temporal graphs by these methods may violate the transitive properties that a temporal graph should possess. For example, given three nodes, e1, e2, and e3, a local method can possibly classify (e1,e2)=before, (e2,e3)=before, and (e1,e3)=after, which is obviously wrong since before is a transitive relation and (e1,e2)=before and (e2,e3)=before dictate that (e1,e3)=before. Recent state-of-the-art methods, (Chambers et al., 2014; Mirza and Tonelli, 2016), circumvented this issue by growing the predicted temporal graph in a multi-step manner, where transitive graph closure is performed on the graph every time a new edge is labeled. This is conceptually solving the structured prediction problem greedily. Another family of methods resorted to Integer Linear Programming (ILP) (Roth and Yih, 2004) to get exact inference to this problem (i.e., global methods), where the entire graph is solved simultaneously and the transitive properties are enforced naturally via ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller,"
N18-1077,S13-2001,0,0.781977,"tics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available1 . 1 Introduction Time is an important dimension of knowledge representation. In natural language, temporal information is often expressed as relations between events. Reasoning over these relations can help figuring out when things happened, estimating how long things take, and summarizing the timeline of a series of events. Several recent SemEval workshops are a good showcase of the importance of this topic (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017). One of the challenges in temporal relation extraction is that it requires high-level prior knowledge of the temporal order that events usually follow. In Example 1, we have deleted events from several snippets from CNN, so that we cannot use our prior knowledge of those events. We are also Example 1: Difficulty in understanding TempRels when event content is missing. Note that e1 and e2 have the same tense, and e3 and e4 have the same tense. More than 10 people have (e1:died), police said. A car (e2:exploded) on Fr"
N18-1077,W15-0809,0,0.0332417,"n associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Justice”); in the context of TempRels, existing datasets have focused more on predicate verbs rather than nominals4 (Pustejovsky et al., 2003; Graff, 2002; UzZaman et al., 2013). Therefore, we only look at verb semantic frames in this work due to the difficulty of getting TempRel annotation for nominal events, and we will use “verb (semantic frames)” interchangeably with “events” hereafter in this paper. 3.2 TempRel Extraction Given the events extracted in a given article (i.e., given the nodes in a graph),"
N18-1077,S07-1014,0,0.353032,"show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available1 . 1 Introduction Time is an important dimension of knowledge representation. In natural language, temporal information is often expressed as relations between events. Reasoning over these relations can help figuring out when things happened, estimating how long things take, and summarizing the timeline of a series of events. Several recent SemEval workshops are a good showcase of the importance of this topic (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017). One of the challenges in temporal relation extraction is that it requires high-level prior knowledge of the temporal order that events usually follow. In Example 1, we have deleted events from several snippets from CNN, so that we cannot use our prior knowledge of those events. We are also Example 1: Difficulty in understanding TempRels when event content is missing. Note that e1 and e2 have the same tense, and e3 and e4 have the same tense. More than 10 people have (e1:died), police sai"
N18-1077,D17-1108,1,0.58431,"n→diedIn→ and graduateFrom→workAt, where → means temporally beThe rest of the paper is organized as follows. Section 2 provides a literature review of TempRels extraction in NLP. Section 3 describes in detail the construction of T EM P ROB. In Sec. 4, we show that T EM P ROB can be used in existing TempRels extraction systems and lead to significant improvement. Finally, we conclude in Sec. 5. 2 Related Work The TempRels between events can be represented by an edge-labeled graph, where the nodes are events, and the edges are labeled with TempRels (Chambers and Jurafsky, 2008; Do et al., 2012; Ning et al., 2017). Given all the nodes, we work on the TempRel extraction task, which is to assign 842 fore. Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year2 , i.e., only when two events happened in different years can"
N18-1077,C08-3012,0,0.871054,"Missing"
N18-1077,P16-1028,1,0.852022,"may benefit other tasks (Sec. 3.4). In the next, we describe each of these elements. 3 We adopt the commonly used feature set in TempRel extraction (Do et al., 2012; Ning et al., 2017) and here we simply list them for reproducibility. For each pair of nodes, the follow3.1 Event Extraction Extracting events and the relations between them (e.g., coreference, causality, entailment, and temporal) have long been an active area in the NLP community. Generally speaking, an event is considered to be an action associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Just"
N18-1077,D16-1038,1,0.854858,"ks (Sec. 3.4). In the next, we describe each of these elements. 3 We adopt the commonly used feature set in TempRel extraction (Do et al., 2012; Ning et al., 2017) and here we simply list them for reproducibility. For each pair of nodes, the follow3.1 Event Extraction Extracting events and the relations between them (e.g., coreference, causality, entailment, and temporal) have long been an active area in the NLP community. Generally speaking, an event is considered to be an action associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Justice”); in the conte"
N18-1077,rizzolo-roth-2010-learning,1,0.851499,"Missing"
N18-1077,W04-2401,1,0.725605,"test how well the prior distributions from T EM P ROB can be used as features in improving local methods for TempRel extraction. In Table 5, we used the original feature set proposed in Sec. 3.2.1 as the baseline, and added the prior distribution obtained from T EM P ROB on top of it. Specifically, we added ηb (see Eq. (1)) and {fr }r∈R , respectively, where {fr }r∈R is the prior distributions of all labels, i.e., fr (vi , vj ) = ∑ C(vi , vj , r) , r ∈ R. ′ r′ ∈R C(vi , vj , r ) As mentioned earlier in Sec. 2, many systems adopt a global inference method via integer linear programming (ILP) (Roth and Yih, 2004) to enforce transitivity constraints over an entire temporal graph (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). In addition to the usage shown in Sec. 4.2.1, the prior distributions from T EM P ROB can also be used to regularize the conventional ILP formulation. Specifically, in each document, let Ir (ij) ∈ {0, 1} be the indicator function of relation r for event i and event j; let xr (ij) ∈ [0, 1] be the corresponding softmax score obtained from the local classifiers (depending on the sentence distance between i and j). Then"
N18-1077,W15-0812,0,0.0166393,"speaking, an event is considered to be an action associated with corresponding participants involved in this action. In this work, following (Peng and Roth, 2016; Peng et al., 2016; Spiliopoulou et al., 2017) we consider semantic-frame based events, which can be directly detected via off-the-shelf semantic role labeling (SRL) tools. This aligns well with previous works on event detection (Hovy et al., 2013; Peng et al., 2016). Depending on the events of interest, the SRL results are often a superset of events and need to be filtered afterwards (Spiliopoulou et al., 2017). For example, in ERE (Song et al., 2015) and Event Nugget Detection (Mitamura et al., 2015), events are limited to a set of predefined types (such as “Business”, “Conflict”, and “Justice”); in the context of TempRels, existing datasets have focused more on predicate verbs rather than nominals4 (Pustejovsky et al., 2003; Graff, 2002; UzZaman et al., 2013). Therefore, we only look at verb semantic frames in this work due to the difficulty of getting TempRel annotation for nominal events, and we will use “verb (semantic frames)” interchangeably with “events” hereafter in this paper. 3.2 TempRel Extraction Given the events extracted in"
N18-2106,I13-1171,0,0.0966454,"urafsky, 2009) or plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012), or from the perspective of (ii) characters (Wilensky, 1978) or personas in a narrative (Propp, 1968; Bamman et al., 2013, 2014; Valls-Vargas et al., 2014). Elsner (2012) explore the plot structure of novels to distinguish original texts from novels from synthetically altered versions of the same. Some recent approaches have also focused on modeling relationships between literary characters (Chaturvedi, 2016; Iyyer et al., 2016; Chaturvedi et al., 2016), and their social networks (Elson et al., 2010; Agarwal et al., 2013; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). Other research has focused on characterizing narratives in terms of their structure. In particular, seminal formalisms such as plot units (Lehnert, 1981) and Story Grammars (Rumelhart, 1980) have been used to analyze story plots. A significant issue with almost all such frameworks is that they are either largely conceptual, or depend on careful manual annotations of features about narrative plot elements (Elsner, 2012; Elson, 2012; Finlayson and Henry Winston, 2006), which makes them unamenable to comprehensive NLP pre-processing: We p"
N18-2106,P13-1035,0,0.0423058,"Missing"
N18-2106,P14-1035,0,0.10303,"er research has focused on characterizing narratives in terms of their structure. In particular, seminal formalisms such as plot units (Lehnert, 1981) and Story Grammars (Rumelhart, 1980) have been used to analyze story plots. A significant issue with almost all such frameworks is that they are either largely conceptual, or depend on careful manual annotations of features about narrative plot elements (Elsner, 2012; Elson, 2012; Finlayson and Henry Winston, 2006), which makes them unamenable to comprehensive NLP pre-processing: We processed texts of movie summaries using the BookNLP pipeline (Bamman et al., 2014) to get dependency parses, and identify major characters. We also assigned a gender to each character which corresponded to the gender that is most frequently assigned to that character’s mentions across the story using the Stanford Core NLP 674 Number of movies Number of clusters Max number of movies in a cluster Avg number of words in a summary Max number of words in a summary Min number of words in a summary 577 266 7 564 2778 26 alignment score of characters inPthe narrative pair: Schar (si , sj ) = ;c ∈s ,c ∈s xci cj S(ci ,cj ) N ci cj i i j j P subject to P alignment constraints ci xci c"
N18-2106,P09-1068,0,0.0828363,"he narrative. These clusters were then manually pruned to remove errors, and the statistics of the curated dataset are shown in Table 1. In particular, we observe that the average summary is quite long (564 words), which would make human annotations of similarity for such narratives difficult. Related Work The field of computational narratology has focused on algorithmic understanding and generation of narratives (Mani, 2012; Richards et al., 2009). Much previous work has attempted to understand narratives either from the perspective of their (i) sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009) or plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012), or from the perspective of (ii) characters (Wilensky, 1978) or personas in a narrative (Propp, 1968; Bamman et al., 2013, 2014; Valls-Vargas et al., 2014). Elsner (2012) explore the plot structure of novels to distinguish original texts from novels from synthetically altered versions of the same. Some recent approaches have also focused on modeling relationships between literary characters (Chaturvedi, 2016; Iyyer et al., 2016; Chaturvedi et al., 2016), and their social networks (Elson et al., 2010; Agarwal et al."
N18-2106,E12-1065,0,0.193356,"or such narratives difficult. Related Work The field of computational narratology has focused on algorithmic understanding and generation of narratives (Mani, 2012; Richards et al., 2009). Much previous work has attempted to understand narratives either from the perspective of their (i) sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009) or plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012), or from the perspective of (ii) characters (Wilensky, 1978) or personas in a narrative (Propp, 1968; Bamman et al., 2013, 2014; Valls-Vargas et al., 2014). Elsner (2012) explore the plot structure of novels to distinguish original texts from novels from synthetically altered versions of the same. Some recent approaches have also focused on modeling relationships between literary characters (Chaturvedi, 2016; Iyyer et al., 2016; Chaturvedi et al., 2016), and their social networks (Elson et al., 2010; Agarwal et al., 2013; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). Other research has focused on characterizing narratives in terms of their structure. In particular, seminal formalisms such as plot units (Lehnert, 1981) and Story Grammars (Rumelhart,"
N18-2106,D10-1008,0,0.461114,"Missing"
N18-2106,N16-1180,1,0.902966,"Missing"
N18-2106,N15-1185,0,0.0968337,"units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012), or from the perspective of (ii) characters (Wilensky, 1978) or personas in a narrative (Propp, 1968; Bamman et al., 2013, 2014; Valls-Vargas et al., 2014). Elsner (2012) explore the plot structure of novels to distinguish original texts from novels from synthetically altered versions of the same. Some recent approaches have also focused on modeling relationships between literary characters (Chaturvedi, 2016; Iyyer et al., 2016; Chaturvedi et al., 2016), and their social networks (Elson et al., 2010; Agarwal et al., 2013; Krishnan and Eisenstein, 2015; Srivastava et al., 2016). Other research has focused on characterizing narratives in terms of their structure. In particular, seminal formalisms such as plot units (Lehnert, 1981) and Story Grammars (Rumelhart, 1980) have been used to analyze story plots. A significant issue with almost all such frameworks is that they are either largely conceptual, or depend on careful manual annotations of features about narrative plot elements (Elsner, 2012; Elson, 2012; Finlayson and Henry Winston, 2006), which makes them unamenable to comprehensive NLP pre-processing: We processed texts of movie summari"
N18-2106,P14-5010,0,0.00946539,"Missing"
N18-2106,P10-1158,0,0.0940181,"lly pruned to remove errors, and the statistics of the curated dataset are shown in Table 1. In particular, we observe that the average summary is quite long (564 words), which would make human annotations of similarity for such narratives difficult. Related Work The field of computational narratology has focused on algorithmic understanding and generation of narratives (Mani, 2012; Richards et al., 2009). Much previous work has attempted to understand narratives either from the perspective of their (i) sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009) or plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012), or from the perspective of (ii) characters (Wilensky, 1978) or personas in a narrative (Propp, 1968; Bamman et al., 2013, 2014; Valls-Vargas et al., 2014). Elsner (2012) explore the plot structure of novels to distinguish original texts from novels from synthetically altered versions of the same. Some recent approaches have also focused on modeling relationships between literary characters (Chaturvedi, 2016; Iyyer et al., 2016; Chaturvedi et al., 2016), and their social networks (Elson et al., 2010; Agarwal et al., 2013; Krishnan and Eisenstein, 2015; Sr"
N19-1053,J17-3005,0,0.0239768,"a subset of the data with low rater–rater agreement ρ (see Appendix A.2). In certain steps, we use an information retrieval (IR) system2 to generate the best candidates for the task at hand. Feng and Hirst (2011) classified an input into one of the argument schemes. Habernal and Gurevych (2017) provided a large corpus annotated with argument units. Cabrio and Villata (2018) provide a thorough survey the recent work in this direction. A few other works studied other aspects of argumentative structures (Cabrio and Villata, 2012; Khatib et al., 2016; Lippi and Torroni, 2016; Zhang et al., 2017; Stab and Gurevych, 2017). A few recent works use a similar conceptual design that involves a claim, perspectives and evidence.These works are either too small due to the high cost of construction (Aharoni et al., 2014) or too noisy because of the way they are crawled from online resources (Wachsmuth et al., 2017; Hua and Wang, 2017). Our work makes use of both online content and of crowdsourcing, in order to construct a sizable and high-quality dataset. 4 4.1 Step 1: The initial data collection. We start by crawling the content of a few notable debating websites: idebate.com, debatewise.org, procon.org. This yields ∼"
N19-1053,S16-1003,0,0.124324,"Missing"
N19-1053,W15-4631,0,0.0217366,"laim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and"
N19-1053,N18-1074,0,0.0775384,"Missing"
N19-1053,W14-2105,0,0.0482376,"ing phrases that support or oppose a given claim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures f"
N19-1053,C10-1099,1,0.654772,"xhaustive spectrum of ideas with respect to a claim, presenting a small but diverse set of perspectives could be an important step towards addressing the selection bias problem. Moreover, it would be impractical to develop an exhaustive pool of evidence for all perspectives, from a diverse set of credible sources. We are not attempting to do that. We aim at formulating the core NLP problems, and developing a dataset that will facilitate studying these problems from the NLP angle, realizing that using the outcomes of this research in practice requires addressing issues such as trustworthiness (Pasternack and Roth, 2010, 2013) and possibly others. Inherently, our objective requires understanding the relations between perspectives and claims, the nuances in the meaning of various perspectives in the context of claims, and relations between perspectives and evidence. This, we argue, can be done with a diverse enough, but not exhaustive, dataset. And it can be done without attending to the legitimacy and credibility of sources contributing evidence, an important problem but orthogonal to the one studied here. Figure 2: Depiction of a few claims, their perspectives and evidences from P ERSPECTRUM. The supporting"
N19-1053,W14-2508,0,0.447756,"urch, 2005). Stance classification of perspectives: a system is supposed to assess the stances of the perspectives with respect to the given claim (supporting, opposing, etc.) (Hasan and Ng, 2014). Substantiating the perspectives: a system is expected to find valid evidence paragraph(s) in support of each perspective. Conceptually, this is similar to the well-studied problem of textual entailment (Dagan et al., 2013) except that here the en3 Related Work Claim verification. The task of fact verification or fact-checking focuses on the assessment of the truthfulness of a claim, given evidence (Vlachos and Riedel, 2014; Mitra and Gilbert, 2015; Samadi et al., 2016; Wang, 2017; Nakov et al., 2018; Hanselowski et al., 2018; Karimi et al., 2018; Alhindi et al., 2018). These tasks are highly related to the task of textual-entailment that has been extensively studied in the field (Bentivogli et al., 2008; Dagan et al., 2013; Khot et al., 2018). Some recent work study jointly the problem of identifying evidence and verifying that it supports the claim (Yin and Roth, 2018). Our problem structure encompasses the fact verification problem (as verification of perspectives from evidence; Figure 1). Stance classificati"
N19-1053,D15-1050,0,0.0316394,"t or oppose a given claim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; f"
N19-1053,W17-5106,0,0.346926,"gumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and Moens (2009); Khatib et al. (2016); Ajjour et al. (2017) studied elements of arguments and the internal relations between them. 544 Dataset Stance Classification Evidence Verification Human Verified Open Domain P ERSPECTRUM (this work) FEVER (Thorne et al., 2018) (Wachsmuth et al., 2017) LIAR (Wang, 2017) (Vlachos and Riedel, 2014) (Hasan and Ng, 2014) 3 7 3 7 7 3 3 3 3 3 3 7 3 3 7 3 3 3 3 3 3 3 3 7 Table 1: Comparison of P ERSPECTRUM to a few notable datasets in the field. make sure that the workers are responding objectively to the tasks (as opposed to using their personal opinions or preferences). The screen-shots of the annotation interfaces for each step are included in the Appendix (Section A.3). In the steps outlined below, we filter out a subset of the data with low rater–rater agreement ρ (see Appendix A.2). In certain steps, we use an information retrieval (IR) syst"
N19-1053,P17-2067,0,0.476969,"to assess the stances of the perspectives with respect to the given claim (supporting, opposing, etc.) (Hasan and Ng, 2014). Substantiating the perspectives: a system is expected to find valid evidence paragraph(s) in support of each perspective. Conceptually, this is similar to the well-studied problem of textual entailment (Dagan et al., 2013) except that here the en3 Related Work Claim verification. The task of fact verification or fact-checking focuses on the assessment of the truthfulness of a claim, given evidence (Vlachos and Riedel, 2014; Mitra and Gilbert, 2015; Samadi et al., 2016; Wang, 2017; Nakov et al., 2018; Hanselowski et al., 2018; Karimi et al., 2018; Alhindi et al., 2018). These tasks are highly related to the task of textual-entailment that has been extensively studied in the field (Bentivogli et al., 2008; Dagan et al., 2013; Khot et al., 2018). Some recent work study jointly the problem of identifying evidence and verifying that it supports the claim (Yin and Roth, 2018). Our problem structure encompasses the fact verification problem (as verification of perspectives from evidence; Figure 1). Stance classification. Stance classification aims at detecting phrases that s"
N19-1053,E17-2088,0,0.125359,"ntion in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and Moens (2009); Khatib et al. (2016); Ajjour et"
N19-1227,ambati-etal-2010-active,0,0.0440415,"ances to label in the beginning, and stops before a structure is completed. We do not claim that ESPA should always be preferred; instead, it serves as an alternative to conventional, complete annotation schemes that we should keep in mind, because, as we show later, it can be comparable to (and sometimes even better than) complete annotation schemes. ESPA is straightforward to implement even in crowdsourcing; instances to annotate can be selected offline and distributed to crowdsourcers; this can be contrasted with the difficulties of implementing active learning protocols in these settings (Ambati et al., 2010; Laws et al., 2011). We think that ESPA is a good representative for a systematic study of partialness. (a) Complete (b) Partial Figure 2: If we need training data for a graph labeling task (assuming the gold values for the nodes are given) and our annotation budget allows us to annotate, for instance, 10 edges in total, we could (a) completely annotate one graph (and then we run out of budget), or (b) partially annotate two graphs. Our second contribution is the development of an information theoretic formulation to explain the benefit of ESPA (Sec. 2), which we further demonstrate via three"
N19-1227,W05-0620,0,0.290665,"Missing"
N19-1227,P07-1036,1,0.721332,"ph (and then we run out of budget), or (b) partially annotate two graphs. Our second contribution is the development of an information theoretic formulation to explain the benefit of ESPA (Sec. 2), which we further demonstrate via three structured learning tasks in Sec. 4: temporal relation (TempRel) extraction (UzZaman et al., 2013), semantic role classification (SRC),1 and shallow parsing (Tjong Kim Sang and Buchholz, 2000). These tasks are chosen because they each represent a wide spectrum of structures that we will detail later. As a byproduct, we extend constraint-driven learning (CoDL) (Chang et al., 2007) to cope with partially annotated structures (Sec. 3); we call the algorithm Structured Selflearning with Partial ANnotations (SSPAN) to distinguish it from CoDL.2 We believe in the importance of work in this direction. First, partialness is inevitable in practice, either by mistake or by choice, so our theoretical analysis can provide unique insight into understanding partialness. Second, it opens up opportunities for new annotation schemes. Instead of considering partial annotations as a compromise, we can in fact annotate partial data intentionally, allowing us to design favorable guideline"
N19-1227,D15-1076,0,0.020369,"y ask human annotators for partial annotations, the annotation tasks can be more flexible and potentially, cost even less. This is because annotating complex structures typically require certain expertise, and smaller tasks are often easier (Fernandes and Brefeld, 2011). It is very likely that some complex annotation tasks require people to read dozens of pages of annotation guidelines, but once decomposed into smaller subtasks, even laymen can handle them. Annotation schemes driven by crowdsourced questionanswering, known to provide only partial coverage are successful examples of this idea (He et al., 2015; Michael et al., 2017). Therefore, this paper is hopefully interesting to a broad audience. Acknowledgements This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network; Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA); and by the Army Research Laboratory (ARL) and was accomplished under Cooperative Agreement Number W911NF-09-20053 (the ARL Network Science CTA). The"
N19-1227,W12-1905,0,0.0236291,"). By treating them as hidden variables, EM “marginalizes” out the missing labels of U via expectation (i.e., soft EM) or maximization (i.e., hard EM). For structured ML tasks, soft and hard EMs turn into posterior regularization (PR) (Ganchev et al., 2010) and constraintdriven learning (CoDL) (Chang et al., 2007), respectively. Unlike unlabeled data, the partially annotated structures caused by early stopping urge us to gain information not only about p(x), but also from their labeled parts. There have been many existing work along this line (Tsuboi et al., 2008; Fernandes and Brefeld, 2011; Hovy and Hovy, 2012; Lou and Hamprecht, 2012), but in this paper, we decide to extend CoDL to cope with partial annotations due to two reasons. First, CoDL, which itself can be viewed as an extension of self-training to structured learning, is a wrapper algorithm having wide applications. Second, as its name suggests, CoDL learns from U by guidance of constraints, so partial annotations in U are technically easy to be added as extra equality constraints. Algorithm 1 describes our Structured Selflearning with Partial ANnotations (SSPAN) algorithm that learns a model H. The same as CoDL, SSPAN is a wrapper algorit"
N19-1227,kingsbury-palmer-2002-treebank,0,0.603732,"o assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig. 1a, if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday (“met (1)”) or has to happen on Thursday, too (“met (2)”) (Ning et al., 2018a). Similarly, in the semantic frame of the predicate gave (Kingsbury and Palmer, 2002) in Fig. 1b, if the boy is A RG 0 (short for argument 0), then it rules out the possibility of a frog ! FOREHEAD ! LEFT_EYE ! TORSO Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be B EFORE (“met (1)”) or B E I NCLUDED (“met (2)”). (b) The argument roles of a frog and to the girl cannot be A RG 0 anymore. (c) Given the position of the cat’s FOREHEAD and LEFT EYE, a rough estimate of its NECK can be the red solid box rather than the blue dashed box. or to the gi"
N19-1227,D11-1143,0,0.0841459,"Missing"
N19-1227,J93-2004,0,0.0652929,"e the Verb SRL dataset provided by the CoNLL-2005 shared task (Carreras and M`arquez, 2005), where the semantic roles include numbered arguments, e.g., A RG 0 and A RG 1, and argument modifiers, e.g., location (A M -L OC), temporal (A M -T MP), and manner (A M -M NR) (see PropBank (Kingsbury and Palmer, 2002)). The structural constraints for SRC is that each argument can be assigned to exactly one semantic role, and the same role cannot appear twice for a single verb, so SRC is an assignment problem as in Example 2. Specifically, we use the Wall Street Journal (WSJ) part of Penn TreeBank III (Marcus et al., 1993). We randomly select 700 sentences from the Sec. 24 of WSJ, among which 100 sentences as T0 and 600 sentences as U0 . Our Ttest is 5700 sentences (about 40K arguments) from Secs. 00, 01, 23. The budget here is defined as the total num4 The original TimeBank-Dense section contains 36 documents, but in collecting MATRES, one of the documents was filtered out because it contained no TempRels between mainaxis events. ber of the arguments. We adopt the SRL system in CogCompNLP (Khashabi et al., 2018) and uses the sparse averaged perceptron as L EARN and ILP as I NFERENCE. 4.3 Shallow Parsing Shallo"
N19-1227,neubig-mori-2010-word,0,0.0280485,"mprovement of F1 brought by ESPA for each task in Fig. 6. Note that we conjectured earlier in Fig. 4 that the BIO structure is the weakest among the three, which is consistent with the fact that shallow parsing benefits the least from ESPA. 5 Dicussion and Conclusion In this paper, we investigate a less studied, yet important question for structured learning: Given a limited annotation budget (either in time or money), which strategy is better, completely annotating each structure until the budget runs out, or annotating more structures at the cost of leaving some of them partially annotated? Neubig and Mori (2010) investigated this issue specifically in annotating word boundaries and pronunciations for Japanese. Instead of annotating full sentences, they proposed to annotate only some words in a sentence (i.e., partially) that can be chosen heuristically (e.g., skip those that we have seen or those low frequency words). Conceptually, Neubig and Mori (2010) is an active learning work, with the understanding that if the order of annotation is deliberately designed, better learning can be achieved. The current paper addresses the problem from a different angle: Even without active learning, can we still a"
N19-1227,P18-1212,1,0.924519,"ion Many machine learning tasks require structured outputs, and the goal is to assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig. 1a, if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday (“met (1)”) or has to happen on Thursday, too (“met (2)”) (Ning et al., 2018a). Similarly, in the semantic frame of the predicate gave (Kingsbury and Palmer, 2002) in Fig. 1b, if the boy is A RG 0 (short for argument 0), then it rules out the possibility of a frog ! FOREHEAD ! LEFT_EYE ! TORSO Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be B EFORE (“met (1)”) or B E I NCLUDED (“met (2)”). (b) The argument roles of a frog and to the girl cannot be A RG 0 anymore. (c) Given the position of the cat’s FOREHEAD and LEFT EYE, a rough esti"
N19-1227,P18-1122,1,0.941012,"ion Many machine learning tasks require structured outputs, and the goal is to assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig. 1a, if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday (“met (1)”) or has to happen on Thursday, too (“met (2)”) (Ning et al., 2018a). Similarly, in the semantic frame of the predicate gave (Kingsbury and Palmer, 2002) in Fig. 1b, if the boy is A RG 0 (short for argument 0), then it rules out the possibility of a frog ! FOREHEAD ! LEFT_EYE ! TORSO Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be B EFORE (“met (1)”) or B E I NCLUDED (“met (2)”). (b) The argument roles of a frog and to the girl cannot be A RG 0 anymore. (c) Given the position of the cat’s FOREHEAD and LEFT EYE, a rough esti"
N19-1227,S18-2018,1,0.937192,"ion Many machine learning tasks require structured outputs, and the goal is to assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig. 1a, if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday (“met (1)”) or has to happen on Thursday, too (“met (2)”) (Ning et al., 2018a). Similarly, in the semantic frame of the predicate gave (Kingsbury and Palmer, 2002) in Fig. 1b, if the boy is A RG 0 (short for argument 0), then it rules out the possibility of a frog ! FOREHEAD ! LEFT_EYE ! TORSO Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be B EFORE (“met (1)”) or B E I NCLUDED (“met (2)”). (b) The argument roles of a frog and to the girl cannot be A RG 0 anymore. (c) Given the position of the cat’s FOREHEAD and LEFT EYE, a rough esti"
N19-1227,D18-2013,1,0.869537,"ion Many machine learning tasks require structured outputs, and the goal is to assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig. 1a, if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday (“met (1)”) or has to happen on Thursday, too (“met (2)”) (Ning et al., 2018a). Similarly, in the semantic frame of the predicate gave (Kingsbury and Palmer, 2002) in Fig. 1b, if the boy is A RG 0 (short for argument 0), then it rules out the possibility of a frog ! FOREHEAD ! LEFT_EYE ! TORSO Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be B EFORE (“met (1)”) or B E I NCLUDED (“met (2)”). (b) The argument roles of a frog and to the girl cannot be A RG 0 anymore. (c) Given the position of the cat’s FOREHEAD and LEFT EYE, a rough esti"
N19-1227,W00-0726,0,0.766776,"a graph labeling task (assuming the gold values for the nodes are given) and our annotation budget allows us to annotate, for instance, 10 edges in total, we could (a) completely annotate one graph (and then we run out of budget), or (b) partially annotate two graphs. Our second contribution is the development of an information theoretic formulation to explain the benefit of ESPA (Sec. 2), which we further demonstrate via three structured learning tasks in Sec. 4: temporal relation (TempRel) extraction (UzZaman et al., 2013), semantic role classification (SRC),1 and shallow parsing (Tjong Kim Sang and Buchholz, 2000). These tasks are chosen because they each represent a wide spectrum of structures that we will detail later. As a byproduct, we extend constraint-driven learning (CoDL) (Chang et al., 2007) to cope with partially annotated structures (Sec. 3); we call the algorithm Structured Selflearning with Partial ANnotations (SSPAN) to distinguish it from CoDL.2 We believe in the importance of work in this direction. First, partialness is inevitable in practice, either by mistake or by choice, so our theoretical analysis can provide unique insight into understanding partialness. Second, it opens up oppor"
N19-1227,C08-1113,0,0.0394951,"pectation-Maximization (EM) (Dempster et al., 1977). By treating them as hidden variables, EM “marginalizes” out the missing labels of U via expectation (i.e., soft EM) or maximization (i.e., hard EM). For structured ML tasks, soft and hard EMs turn into posterior regularization (PR) (Ganchev et al., 2010) and constraintdriven learning (CoDL) (Chang et al., 2007), respectively. Unlike unlabeled data, the partially annotated structures caused by early stopping urge us to gain information not only about p(x), but also from their labeled parts. There have been many existing work along this line (Tsuboi et al., 2008; Fernandes and Brefeld, 2011; Hovy and Hovy, 2012; Lou and Hamprecht, 2012), but in this paper, we decide to extend CoDL to cope with partial annotations due to two reasons. First, CoDL, which itself can be viewed as an extension of self-training to structured learning, is a wrapper algorithm having wide applications. Second, as its name suggests, CoDL learns from U by guidance of constraints, so partial annotations in U are technically easy to be added as extra equality constraints. Algorithm 1 describes our Structured Selflearning with Partial ANnotations (SSPAN) algorithm that learns a mod"
N19-1227,S13-2001,0,0.0327974,"atic study of partialness. (a) Complete (b) Partial Figure 2: If we need training data for a graph labeling task (assuming the gold values for the nodes are given) and our annotation budget allows us to annotate, for instance, 10 edges in total, we could (a) completely annotate one graph (and then we run out of budget), or (b) partially annotate two graphs. Our second contribution is the development of an information theoretic formulation to explain the benefit of ESPA (Sec. 2), which we further demonstrate via three structured learning tasks in Sec. 4: temporal relation (TempRel) extraction (UzZaman et al., 2013), semantic role classification (SRC),1 and shallow parsing (Tjong Kim Sang and Buchholz, 2000). These tasks are chosen because they each represent a wide spectrum of structures that we will detail later. As a byproduct, we extend constraint-driven learning (CoDL) (Chang et al., 2007) to cope with partially annotated structures (Sec. 3); we call the algorithm Structured Selflearning with Partial ANnotations (SSPAN) to distinguish it from CoDL.2 We believe in the importance of work in this direction. First, partialness is inevitable in practice, either by mistake or by choice, so our theoretical"
N19-1227,P95-1026,0,0.385107,"ach to learning from partial structures. In this study, we assume the existence of a relatively small but complete dataset that can provide a good initialization for learning from a partial dataset, which is very similar to semi-supervised learning (SSL). SSL, in its most standard form, studies the combined usage of a labeled set T = {(xi , yi )}i and an unlabeled set U = {xj }j , where the x’s are instances and y’s are the corresponding labels. SSL gains information about p(x) through U , which may improve the estimation of p(y|x). Specific algorithms range from self-training (Scudder, 1965; Yarowsky, 1995), co-training (Blum and Mitchell, 1998), generative models (Nigam et al., 2000), to transductive SVM (Joachims, 1999) etc., among which one of the most basic algorithms is Expectation-Maximization (EM) (Dempster et al., 1977). By treating them as hidden variables, EM “marginalizes” out the missing labels of U via expectation (i.e., soft EM) or maximization (i.e., hard EM). For structured ML tasks, soft and hard EMs turn into posterior regularization (PR) (Ganchev et al., 2010) and constraintdriven learning (CoDL) (Chang et al., 2007), respectively. Unlike unlabeled data, the partially annotate"
N19-1319,P18-1031,0,0.0235446,"th a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of bespoke, challenging text classification problems that crop up in practical settings (Yu et al., 2018). Obtaining representative labeled examples for classification problems with many labels, like taxonomies, is especially challen"
N19-1319,D14-1181,0,0.0498632,"specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of be"
N19-1319,S18-2031,1,0.938075,"same-label phenomenon holds; that depends on the granularity of the classes. Cross-example analysis is required to determine how neighbors at various distances are distributed among labels in the training data. This should allow us to include barley and peaches as evidence for a class like Agriculture but only barley for Grains. Most existing systems ignore cross-example parallelism and thus miss out on a strong classification signal. We introduce a flexible method for controlled generalization that selects syntactosemantic features from sparse representations constructed by Category Builder (Mahabal et al., 2018). Starting with sparse representations of words and their contexts, a tuning algorithm selects features with the relevant kinds and appropriate amounts of generalization, making use of parallelism among examples. This produces taskspecific dense embeddings for new texts that can be easily incorporated into classifiers. Our simplest model, CBC (Category Builder Classifier), is a feed-forward network that uses only CB embeddings to represent a document. For small amounts of training data, this simple model dramatically outperforms both CNNs and BERT (Devlin et al., 2018). When more data is avail"
N19-1319,N18-1202,0,0.0358907,"iring this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of bespoke, challenging text classification problems that crop up in practical settings (Yu et al., 2018). Obtaining representative labeled examples for classification problems with many labels, like taxonomie"
N19-1319,C18-1180,0,0.0323366,"Missing"
N19-1319,N16-2013,0,0.0170465,"mance with limited data. To that end, we obtain learning curves on four standard text classification datasets (Table 2) based on evaluating predictions on the full test sets. At each sample size, we produce multiple samples and run several text classification methods multiple times, measuring the following: 3159 • Macro-F1 score. Macro-F1 measures support for all classes better than accuracy, especially with imbalanced class distributions. • Recall for the rarest class. Many measures like F1 and accuracy often mask performance on infrequent but high impact classes, such as detecting toxicity (Waseem and Hovy, 2016)) • Degenerate solutions. Complex classifiers with millions of parameters sometimes produce degenerate classifiers when provided very few training examples; as a result, they can skip some output classes entirely. The datasets we chose for evaluation, while all multi-class, form a diverse set in terms of the number of classes and kinds of cohesion among examples in a single class. The former clearly affects training data needs, while the latter informs appropriate generalization. • 20 Newsgroups 20Newsgroups (20NG) contains documents from 20 different newsgroups with about 1000 messages from e"
N19-1319,N18-1109,0,0.0316836,"Missing"
N19-1319,D18-1030,1,0.865224,"Missing"
P06-1103,N06-1011,1,\N,Missing
P06-1103,C00-1056,0,\N,Missing
P06-1103,H05-1011,0,\N,Missing
P06-1103,C04-1122,0,\N,Missing
P06-1103,P97-1017,0,\N,Missing
P06-1103,W99-0613,0,\N,Missing
P06-1103,W99-0612,0,\N,Missing
P06-2009,C96-1058,0,0.045379,"y competitive parsing results. A somewhat similar approach was used in (Nivre and Scholz, 2004) to develop a hybrid bottom-up/topdown approach; there, the edges are also labeled with semantic types, yielding lower accuracy than the works mentioned above. The overall goal of dependency parsing (DP) learning is to infer a tree structure. A common way to do that is to predict with respect to each potential edge (i, j) in the tree, and then choose a global structure that (1) is a tree and that (2) maximizes some score. In the context of DPs, this “edge based factorization method” was proposed by (Eisner, 1996). In other contexts, this is similar to the approach of (Roth and Yih, 2004) in that scoring each edge depends only on the raw data observed and not on the classifications of other edges, and that global considerations can be used to overwrite the local (edge-based) decisions. On the other hand, the key in a pipeline model is that making a decision with respect to the edge (i, j) may gain from taking into account decisions already made with respect to neighboring edges. However, given that these decisions are noisy, there is a need to devise policies for reducing the number of predictions in o"
P06-2009,H05-1049,0,0.0342671,"decisions we made, guided by these, principles lead to a significant improvement in the accuracy of the resulting parse tree. 1.1 Dependency Parsing and Pipeline Models Dependency trees provide a syntactic reresentation that encodes functional relationships between words; it is relatively independent of the grammar theory and can be used to represent the structure of sentences in different languages. Dependency structures are more efficient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al., 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Eisner’s work – O(n3 ) parsing time generative algorithm – embarked the interest in this area. His model, however, seems to be limited when dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003"
P06-2009,W05-0618,0,0.0172521,"rocessing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural language processing. The key problem of this model is that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. We develop a"
P06-2009,J93-2004,0,0.0269474,"ithm considers the pair (a, b), a &lt; b, we call the word a the current focus point. Next we describe several policies for determining the focus point of the algorithm following an action. We note that, with a few exceptions, determining the focus point does not affect the correctness of the algorithm. It is easy to show that for (almost) any focus point chosen, if the correct Policy Start over Stay Step back #Shift 156545 117819 43374 #Left 26351 26351 26351 #Right 27918 27918 27918 Table 1: The number of actions required to build all the trees for the sentences in section 23 of Penn Treebank (Marcus et al., 1993) as a function of the focus point placement policy. The statistics are taken with the correct (gold-standard) actions. It is clear from Table 1 that the policies result 1 Note that (Yamada and Matsumoto, 2003) mention that they move the focus point back after R, but do not state what they do after executing L actions, and why. (Yamada, 2006) indicates that they also move focus point back after L. 67 Algorithm 2 Pseudo Code of the dependency parsing algorithm. getFeatures extracts the features describing the word pair currently considered; getAction determines the appropriate action for the pai"
P06-2009,P05-1012,0,0.0195453,"ng parse tree. 1.1 Dependency Parsing and Pipeline Models Dependency trees provide a syntactic reresentation that encodes functional relationships between words; it is relatively independent of the grammar theory and can be used to represent the structure of sentences in different languages. Dependency structures are more efficient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al., 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Eisner’s work – O(n3 ) parsing time generative algorithm – embarked the interest in this area. His model, however, seems to be limited when dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003), that develop a bottom-up approach and learn the parsing decisions between consecutive words in the sentence."
P06-2009,C04-1010,0,0.331122,"h sentence may be viewed as a pipeline computation as it processes a token and, potentially, makes use of this result when processing the token to the right. 65 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rather than on the overall quality of the parser, and chained to yield the global structure. Clearly, it suffers from the limitations of pipeline processing, such as accumulation of errors, but nevertheless, yields very competitive parsing results. A somewhat similar approach was used in (Nivre and Scholz, 2004) to develop a hybrid bottom-up/topdown approach; there, the edges are also labeled with semantic types, yielding lower accuracy than the works mentioned above. The overall goal of dependency parsing (DP) learning is to infer a tree structure. A common way to do that is to predict with respect to each potential edge (i, j) in the tree, and then choose a global structure that (1) is a tree and that (2) maximizes some score. In the context of DPs, this “edge based factorization method” was proposed by (Eisner, 1996). In other contexts, this is similar to the approach of (Roth and Yih, 2004) in th"
P06-2009,W03-3017,0,0.070011,"Missing"
P06-2009,W05-0639,0,0.354463,"ion in natural language processing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural language processing. The key problem of this model is that it results in error accumulation and suffers from its inability to correct mistakes in p"
P06-2009,W97-0301,0,0.0399474,"n dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003), that develop a bottom-up approach and learn the parsing decisions between consecutive words in the sentence. Local actions are used to generate a dependency tree using a shift-reduce parsing approach (Aho et al., 1986). This is a true pipeline approach, as was done in other successful parsers, e.g. (Ratnaparkhi, 1997), in that the classifiers are trained on individual decisions 2 Efficient Dependency Parsing This section describes our DP algorithm and justifies its advantages as a pipeline model. We pro66 action is selected for the corresponding edge, the algorithm will eventually yield the correct tree (but may require multiple cycles through the sentence). In practice, the actions selected are noisy, and a wasteful focus point policy will result in a large number of actions, and thus in error accumulation. To minimize the number of actions taken, we want to find a good focus point placement policy. After"
P06-2009,W04-2401,1,0.901718,"f Illinois at Urbana-Champaign Urbana, IL 61801 {mchang21, quangdo2, danr}@uiuc.edu Abstract The pipeline model is a standard model of computation in natural language processing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural"
P06-2009,N03-1033,0,0.0163304,"Missing"
P06-2009,W03-3023,0,0.250114,"and Yih, 2004) in that scoring each edge depends only on the raw data observed and not on the classifications of other edges, and that global considerations can be used to overwrite the local (edge-based) decisions. On the other hand, the key in a pipeline model is that making a decision with respect to the edge (i, j) may gain from taking into account decisions already made with respect to neighboring edges. However, given that these decisions are noisy, there is a need to devise policies for reducing the number of predictions in order to make the parser more robust. This is exemplified in (Yamada and Matsumoto, 2003) – a bottom-up approach, that is most related to the work presented here. Their model is a “traditional” pipeline model – a classifier suggests a decision that, once taken, determines the next action to be taken (as well as the input the next action observes). In the rest of this paper, we propose and justify a framework for improving pipeline processing based on the principles mentioned above: (i) make local decisions as reliably as possible, and (ii) reduce the number of decisions made. We use the proposed principles to examine the (Yamada and Matsumoto, 2003) parsing algorithm and show that"
P07-1036,W99-0613,0,0.927384,"ng, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels."
P07-1036,W02-1001,0,0.037614,"× Y → R to assign scores to each possible input/output pair. Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs. The global scoring function is often decomposed as a weighted sum of feature functions, f (x, y) = M X λi fi (x, y) = λ · F (x, y). i=1 This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case). Even when not dictated by the model, the feature functions fi (x, y) used are local to allow inference tractability. Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference. Now, consider a scenario where we have a set of constraints C1 , . . . , CK . We define a constraint C : X × Y → {0, 1} as a function that indicates whether the input/output sequence violates some desired properties. When the constraints are hard, the solution is given by argmax λ · F (x, y),"
P07-1036,P05-1046,0,0.570757,"process. The core of our approach, (1), is described in Section 5. The task is described in Section 3 and the Experimental study in Section 6. It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework. 2 Related Work In the semi-supervised domain there are two main approaches for injecting domain specific knowledge. One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure. For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singe"
P07-1036,N06-1041,0,0.734055,"k allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our forma"
P07-1036,N06-1020,0,0.0154554,"is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model. The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter γ which controls the convex combination of models induced by the labeled and the unlabeled data. Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms. Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006). Similarly to self-training, we use the current model to generate new training examples from the unla284 Input: Cycles: learning cycles T r = {x, y}: labeled training set. U : unlabeled dataset F : set of feature functions. {ρi }: set of penalties. {Ci }: set of constraints. γ: balancing parameter with the supervised model. learn(T r, F ): supervised learning algorithm Top-K-Inference: returns top-K labeled scored by the cost function (1) CODL: 1. Initialize λ0 = learn(T r, F ). 2. λ = λ0 . 3. For Cycles iterations do: 4. T =φ 5. For each x ∈ U 6. {(x, y 1 ), . . . , (x, y K )} = 7. Top-K-Inf"
P07-1036,W02-1028,0,\N,Missing
P07-1036,P05-1073,0,\N,Missing
P07-1036,P05-1044,0,\N,Missing
P08-1117,P05-1022,0,0.0890764,"Missing"
P08-1117,P04-1054,0,0.0535599,"n-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems"
P08-1117,P08-1079,1,0.824531,"to identify specialized phrase types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations be"
P08-1117,P07-1030,1,0.762465,"e types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities;"
P08-1117,P07-2040,0,0.0615941,"unberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to"
P08-1117,J93-2004,0,0.0322102,"represented by commas, there are two main strands of research with similar goals: 1) systems that directly analyze commas, whether labeling them with syntactic information or correcting inappropriate use in text; and 2) systems that extract relations from text, typically by trying to identify paraphrases. The significance of interpreting the role of commas in sentences has already been identified by (van Delden and Gomez, 2002; Bayraktar et al., 1998) and others. A review of the first line of research is given in (Say and Akman, 1997). In (Bayraktar et al., 1998) the WSJ PennTreebank corpus (Marcus et al., 1993) is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created. However, they do not study the extraction of entailed relations as a function of the comma’s interpretation. Furthermore, the syntactic patterns they identify are unlexicalized and would not support the level of semantic relations that we show in this paper. Finally, theirs is a manual process completely dependent on syntactic patterns. While our comma resolution system uses syntactic parse information as its main source of features, the approach we have developed focuses on the"
P08-1117,P06-1015,0,0.0143877,"o relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of senten"
P08-1117,P06-1102,0,0.0129114,"tem has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general ap"
P08-1117,W04-2401,1,0.775126,"eloping comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited ac"
P08-1117,P06-2094,0,0.0190797,", it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and li"
P08-1117,W04-3206,0,0.0131183,", so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of sentences from WSJ (not among"
P08-2014,P06-1103,1,0.958269,"have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998). Unlike other approaches,our approach is based on minimizing the distance between the feature distribution of a comprehensive referenc"
P08-2014,P97-1017,0,0.114719,"ent in many linguistic applications such as machine translation and information retrieval, which require identifying out-of-vocabulary words. In our settings, we have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP application"
P08-2014,W06-1630,0,0.146807,"ntifying out-of-vocabulary words. In our settings, we have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998). Unlike other approaches,our approach is based on minimizing the distanc"
P09-2015,P08-1004,0,0.0936339,"Missing"
P09-2015,P04-1054,0,0.045224,"Missing"
P09-2015,P06-1114,0,0.0569635,"Missing"
P09-2015,W07-1428,0,0.0498594,"Missing"
P09-2015,W04-2401,1,0.870622,"Missing"
P09-2015,P06-2094,0,0.137886,"Missing"
P09-2015,J08-2005,1,\N,Missing
P09-2015,W07-1401,0,\N,Missing
P10-1101,W04-2412,0,0.0220128,"Missing"
P10-1101,W09-1112,1,0.90699,"mental evidence suggests that they do: 21-month-olds mistakenly interpreted word order in sentences such as “The girl and the boy kradded” as conveying agent-patient roles (Gertner and Fisher, 2006). Previous computational experiments with a system for automatic semantic role labeling (BabySRL: (Connor et al., 2008)) showed that it is possible to learn to assign basic semantic roles based on the shallow sentence representations proposed by the structure-mapping view. Furthermore, these simple structural features were robust to drastic reductions in the integrity of the semantic-role feedback (Connor et al., 2009). These experiments showed that representations of sentence structure as simple as ‘first of two nouns’ are useful, but the experiments relied on perfect A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argu"
P10-1101,D08-1036,0,0.0124236,"g stage is to give the learner a representation permitting it to generalize over word forms. The exact parse we are after is a distributional and context-sensitive clustering of words based on sequential processing. We chose an HMM based parser for this since, in essence the HMM yields an unsupervised POS classifier, but without names for states. An HMM trained with expectation maximization (EM) is analogous to a simple process of predicting the next word in a stream and correcting connections accordingly for each sentence. 2 We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. 3 We also include a small third class for punctuation, which is discarded. 4 TO,IN,EX,POS,WDT,PDT,WRB,MD,CC,DT,RP,UH 1 We used parts of the Bloom (Bloom, 1970; Bloom, 1973), Brent (Brent and Siskind, 2001), Brown (Brown, 1973), Clark (Clark, 1978), Cornell, MacWhinney (MacWhinney, 2000), Post (Demetras et al., 1986) and Providence (Demuth et al., 2006) collections. 991 1989), and requires far fewer resources than the full tagging dictionary that is often used to intelligently initialize an unsupervised POS classifier (e.g."
P10-1101,J08-2005,1,0.933705,"of-speech tagger in this way, we can ask how the simple structural features that we propose children start with stand up to reductions in parsing accuracy. In doing so, we move to a parser derived from a particular theoretical account of how the human learner might classify words, and link them into a system for sentence comprehension. 2 stand sentences at the level of who did what to whom. The architecture of our system is similar to a previous approach to modeling early language acquisition (Connor et al., 2009), which is itself based on the standard architecture of a full SRL system (e.g. (Punyakanok et al., 2008)). This basic approach follows a multi-stage pipeline, with each stage feeding in to the next. The stages are: (1) Parsing the sentence, (2) Identifying potential predicates and arguments based on the parse, (3) Classifying role labels for each potential argument relative to a predicate, (4) Applying constraints to find the best labeling of arguments for a sentence. In this work we attempt to limit the knowledge available at each stage to the automatic output of the previous stage, constrained by knowledge that we argue is available to children in the early stages of language learning. In the"
P10-1101,P07-1094,0,0.0437573,"ly providing information on one specific class. 4 Parser Evaluation 5.2 EM VB EM+Funct VB+Funct Variation of Information 5 4.8 4.6 4.4 4.2 4 3.8 3.6 3.4 3.2 100 1000 10000 100000 Training Sentences 1e+06 Figure 1: Unsupervised Part of Speech results, matching states to gold POS labels. All systems use 80 states, and comparison is to gold labeled CDS text, which makes up a subset of the HMM training data. Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meil˘a, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). Smaller numbers are better, indicating less information lost in moving from the HMM states to the gold POS tags. Note that incorporating function word preclustering allows both EM and VB algorithms to achieve the same performance with an order of magnitude fewer sentences. We first evaluate these parsers (the first stage of our SRL system) on unsupervised POS tagging. Figure 1 shows the performance of the four systems using Variation of Information to measure match between gold states and unsupervised parsers as we vary the amount of text they receive. Each point on the graph represents the"
P10-1101,P09-1057,0,0.0157599,"evaluate final performance. 3 We also include a small third class for punctuation, which is discarded. 4 TO,IN,EX,POS,WDT,PDT,WRB,MD,CC,DT,RP,UH 1 We used parts of the Bloom (Bloom, 1970; Bloom, 1973), Brent (Brent and Siskind, 2001), Brown (Brown, 1973), Clark (Clark, 1978), Cornell, MacWhinney (MacWhinney, 2000), Post (Demetras et al., 1986) and Providence (Demuth et al., 2006) collections. 991 1989), and requires far fewer resources than the full tagging dictionary that is often used to intelligently initialize an unsupervised POS classifier (e.g. (Brill, 1997; Toutanova and Johnson, 2007; Ravi and Knight, 2009)). Because the function and content word preclustering preceded parameter estimation, it can be combined with either EM or VB learning. Although this initial split forces sparsity on the emission matrix and allows more uniform sized clusters, Dirichlet priors may still help, if word clusters within the function or content word subsets vary in size and frequency. The third parser was an 80 state HMM trained with EM estimation, with 30 states pre-allocated to function words; the fourth parser was the same except that it was trained with VB EM. states than tags) is a many to one greedy mapping of"
P10-1101,N06-1041,0,0.0128433,"1984). We use a small set of known nouns to transform unlabeled word clusters into candidate arguments for the SRL: HMM states that are dominated by known names for animate or inanimate objects are assumed to be argument states. Given text parsed by the HMM parser and a list of known nouns, the argument identifier proceeds in multiple steps as illustrated in figure 2. The first stage identifies as argument states those states that appear at least half the time in the training data with known nouns. This use of a seed list and distributional clustering is similar to Prototype Driven Learning (Haghighi and Klein, 2006), except we are only providing information on one specific class. 4 Parser Evaluation 5.2 EM VB EM+Funct VB+Funct Variation of Information 5 4.8 4.6 4.4 4.2 4 3.8 3.6 3.4 3.2 100 1000 10000 100000 Training Sentences 1e+06 Figure 1: Unsupervised Part of Speech results, matching states to gold POS labels. All systems use 80 states, and comparison is to gold labeled CDS text, which makes up a subset of the HMM training data. Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meil˘a, 2002), and first used for Unsupervised P"
P10-1101,D07-1031,0,0.208472,"partial knowledge of syntax to guide sen989 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 989–998, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics knowledge of arguments and predicates as a start to classification. Perfect built-in parsing finesses two problems facing the human learner. The first problem involves classifying words by part-of-speech. Proposed solutions to this problem in the NLP and human language acquisition literatures focus on distributional learning as a key data source (e.g., (Mintz, 2003; Johnson, 2007)). Importantly, infants are good at learning distributional patterns (Gomez and Gerken, 1999; Saffran et al., 1996). Here we use a fairly standard Hidden Markov Model (HMM) to generate clusters of words that occur in similar distributional contexts in a corpus of input sentences. The second problem facing the learner is more contentious: Having identified clusters of distributionally-similar words, how do children figure out what role these clusters of words should play in a sentence interpretation system? Some clusters contain nouns, which are candidate arguments; others contain verbs, which"
P10-1101,J93-2004,0,0.0456908,"e clustering of words into grammatical categories. The phonological difference between content and function words is particularly striking (Shi et al., 1998). Even newborns can categorically distinguish content and function words, based on the phonological difference between the two classes (Shi et al., 1999). Human learners may treat content and function words as distinct classes from the start. To implement this division into function and content words3 , we start with a list of function word POS tags4 and then find words that appear predominantly with these POS tags, using tagged WSJ data (Marcus et al., 1993). We allocated a fixed number of states for these function words, and left the rest of the states for the rest of the words. This amounts to initializing the emission matrix for the HMM with a block structure; words from one class cannot be emitted by states allocated to the other class. This trick has been used before in speech recognition work (Rabiner, Unsupervised Parsing As a first step of processing, we feed the learner large amounts of unlabeled text and expect it to learn some structure over this data that will facilitate future processing. The source of this text is child directed spe"
P10-1101,kingsbury-palmer-2002-treebank,0,\N,Missing
P10-1101,N10-1066,1,\N,Missing
P10-1101,W08-2121,0,\N,Missing
P10-1101,J08-2001,0,\N,Missing
P10-1101,W02-1001,0,\N,Missing
P10-1101,W08-2111,1,\N,Missing
P10-1101,P09-1056,0,\N,Missing
P10-1101,W11-0604,0,\N,Missing
P10-1101,J92-4003,0,\N,Missing
P10-1101,2008.amta-papers.4,0,\N,Missing
P10-1101,W09-1201,0,\N,Missing
P10-1101,P04-1061,0,\N,Missing
P10-1101,P02-1031,0,\N,Missing
P10-1101,J05-1004,0,\N,Missing
P10-1101,D07-1033,0,\N,Missing
P10-1122,W04-3205,0,0.015887,"tactic and dependency parsers, and coreference resolvers; and the use of special-purpose ad-hoc modules designed to address specific entailment phenomena the researchers had identified, such as the need for numeric reasoning. However, it is not possible to objectively assess the role these capabilities play in each system’s performance from the system outputs alone. 2.3 The Need for Detailed Evaluation An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. The observed contribution was in most cases limited or non-existent. It is premature, however, to conclude that these resources have little potential impact on RTE system performance: most RTE researchers agree that the real contribution of individual resources is difficult to assess. As the example in figure 1 illustrates, most RTE examples require a number of phenomena to be correctly resolved in order to reliably determine the correct label (the Interaction problem); a perfect coreference resolver might as a result yield little"
P10-1122,P08-1118,0,0.297436,"Missing"
P10-1122,W07-1401,0,0.101081,"ask of Recognizing Textual Entailment (RTE), as formulated by (Dagan et al., 2006), requires automated systems to identify when a human reader would judge that given one span of text (the Text) and some unspecified (but restricted) world knowledge, a second span of text (the Hy1200 The purchase of LexCorp by BMI for $2Bn prompted widespread sell-offs by traders as they sought to minimize exposure. Hyp 1: BMI acquired another company. Hyp 2: BMI bought LexCorp for $3.4Bn. Text: Rank 1 2 3 4 5 6 7 8 9 9 9 - Figure 1: Some representative RTE examples. pothesis) is true. The task was extended in (Giampiccolo et al., 2007) to include the additional requirement that systems identify when the Hypothesis contradicts the Text. In the example shown in figure 1, this means recognizing that the Text entails Hypothesis 1, while Hypothesis 2 contradicts the Text. This operational definition of Textual Entailment avoids commitment to any specific knowledge representation, inference method, or learning approach, thus encouraging application of a wide range of techniques to the problem. 2.1 An Illustrative Example The simple RTE examples in figure 1 (most RTE examples have much longer Texts) illustrate some typical inferen"
P10-1122,P06-1114,0,0.0227164,"nd what tasks must be solved in order to understand text sufficiently well to reliably reason with it. This is an appropriate time to consider a systematic process for identifying semantic analysis tasks relevant to natural language understanding, and for assessing their potential impact on NLU system performance. Research on Recognizing Textual Entailment (RTE), largely motivated by a “grand challenge” now in its sixth year, has already begun to address some of the problems identified above. Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al., 2009), (Mirkin et al., 2009). The RTE challenge examples are drawn from multiple domains, providing a relatively task-neutral setting in which to evaluate contributions of different component solutions, and RTE researchers have already made incremental progress by identifying sub-problems of entailment, and developing ad-hoc solutions for them. In this paper we challenge the NLP community to contribute to a joint, long-term effort to identify, formalize, and solve textual inference problems motivated by the Recognizing Textual Entailment setting, in the f"
P10-1122,N06-2015,0,0.0362314,"and promote adoption of successful solutions to sub-problems. An annotation-side solution also maintains the desirable agnosticism of the RTE problem formulation, by not imposing the requirement on system developers of generating an explanation for each answer. Of course, if examples were also annotated with explanations in a consistent format, this could form the basis of a new evaluation of the kind essayed in the pilot study in (Giampiccolo et al., 2007). 3 Annotation Proposal and Pilot Study As part of our challenge to the NLP community, we propose a distributed OntoNotes-style approach (Hovy et al., 2006) to this annotation effort: distributed, because it should be undertaken by a diverse range of researchers with interests in different semantic phenomena; and similar to the OntoNotes annotation effort because it should not presuppose a fixed, closed ontology of entailment phenomena, but rather, iteratively hypothesize and refine such an ontology using interannotator agreement as a guiding principle. Such an effort would require a steady output of RTE examples to form the underpinning of these annotations; and in order to get sufficient data to represent less common, but nonetheless important,"
P10-1122,W09-3714,0,0.0218432,"xamples require a number of phenomena to be correctly resolved in order to reliably determine the correct label (the Interaction problem); a perfect coreference resolver might as a result yield little improvement on the standard RTE evaluation, even though coreference resolution is clearly required by human readers in a significant percentage of RTE examples. Various efforts have been made by individual research teams to address specific capabilities that are intuitively required for good RTE performance, such as (de Marneffe et al., 2008), and the formal treatment of entailment phenomena in (MacCartney and Manning, 2009) depends on and formalizes a divide-and-conquer approach to entailment resolution. But the phenomena-specific capabilities described in these approaches are far from complete, and many are not yet invented. To devote real effort to identify and develop such capabilities, researchers must be confident that the resources (and the will!) exist to create and evaluate their solutions, and that the resource can be shown to be relevant to a sufficiently large subset of the NLP community. While there is widespread belief that there are many relevant entailment phenomena, though each individually may b"
P10-1122,P09-1089,0,0.00817868,"l to reliably reason with it. This is an appropriate time to consider a systematic process for identifying semantic analysis tasks relevant to natural language understanding, and for assessing their potential impact on NLU system performance. Research on Recognizing Textual Entailment (RTE), largely motivated by a “grand challenge” now in its sixth year, has already begun to address some of the problems identified above. Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al., 2009), (Mirkin et al., 2009). The RTE challenge examples are drawn from multiple domains, providing a relatively task-neutral setting in which to evaluate contributions of different component solutions, and RTE researchers have already made incremental progress by identifying sub-problems of entailment, and developing ad-hoc solutions for them. In this paper we challenge the NLP community to contribute to a joint, long-term effort to identify, formalize, and solve textual inference problems motivated by the Recognizing Textual Entailment setting, in the following ways: (a) Making the Recognizing Textual Entailment settin"
P10-1122,P09-1034,0,0.0157182,"text sufficiently well to reliably reason with it. This is an appropriate time to consider a systematic process for identifying semantic analysis tasks relevant to natural language understanding, and for assessing their potential impact on NLU system performance. Research on Recognizing Textual Entailment (RTE), largely motivated by a “grand challenge” now in its sixth year, has already begun to address some of the problems identified above. Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al., 2009), (Mirkin et al., 2009). The RTE challenge examples are drawn from multiple domains, providing a relatively task-neutral setting in which to evaluate contributions of different component solutions, and RTE researchers have already made incremental progress by identifying sub-problems of entailment, and developing ad-hoc solutions for them. In this paper we challenge the NLP community to contribute to a joint, long-term effort to identify, formalize, and solve textual inference problems motivated by the Recognizing Textual Entailment setting, in the following ways: (a) Making the Recognizing Te"
P11-1056,C10-1018,1,0.768046,"icant improvement in RE performance. 1 Introduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “. . . the Seattle zoo . . . ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too"
P11-1056,W06-0204,0,0.042783,"QhasN ull do p = structure inference on (mi , mj ) using patterns if p ∈ S r = relation prediction for (mi , mj ) using REs R=R∪r else if Lmi 6= null ∧ Lmj 6= null r = relation prediction for (mi , mj ) using REbase R=R∪r done Output: R Figure 2: RE using predicted mentions and patterns. Abbreviations: Lm : predicted entity label for mention m using the mention entity typing (MET) classifier described in Section 4; PM ET : prediction probability according to the MET classifier; t: used for thresholding. There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009). However, these works operate along the first dimension, that of using patterns to mine for relation type examples. In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automati"
P11-1056,N07-1015,0,0.221404,"RE performance. 1 Introduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “. . . the Seattle zoo . . . ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we bui"
P11-1056,P09-1114,0,0.0131823,"oduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “. . . the Seattle zoo . . . ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we build on the obs"
P11-1056,P04-3022,0,0.0116229,"e dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did 553 In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2 . ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaware of any previous computational approaches that recognize these structures automatically in text, as we do, and use it in the context of RE (or any other problem). In (Qian et al., 2008), the authors"
P11-1056,W09-1119,1,0.264122,"mi , mj and P+1,+1 of mi , mj hw of mi , mj and P−2,−1 of mi , mj hw of mi , mj and P−1,+1 of mi , mj hw of mi , mj and P+1,+2 of mi , mj any base phrase chunk between mi , mj Table 3: Additional RE features. root and this path can be represented as a simple bit string. As part of our features, we use the cluster bit string representation of wk and lw. Contextual We extract the word C−1,−1 immediately before mi , the word C+1,+1 immediately after mi , and their associated POS tags P . NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009). This tagger annotates proper nouns with the tags PER (person), ORG (organization), LOC (location), or MISC (miscellaneous). If the lw of mi coincides (actual token offset) with the lw of any NE annotated by the NE tagger, we extract the NE tag as a feature. Syntactic parse We parse the sentences using the syntactic parser of (Klein and Manning, 2003). We extract the label of the parse tree constituent (if it exists) that exactly covers the mention, and also labels of all constituents that covers the mention. 4.2 Extracting Candidate Mentions From a sentence, we gather the following as candid"
P11-1056,P05-1053,0,0.93097,"tion extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “. . . the Seattle zoo . . . ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we build on the observation that there"
P11-1056,Y07-1043,0,\N,Missing
P11-1056,J92-4003,0,\N,Missing
P11-1093,P96-1041,0,0.0381487,"ior(p) = P C(p) q∈Conf Set C(q) , = log(prior(p)) + X + log(P (f |p)) (3) f ∈F (S,p) NB weights and its free coefficient are also summarized in Table 2. 2.4 SumLM For candidate p, SumLM (Bergsma et al., 2009)6 produces a score by summing over the logs of all feature counts: X g(S, p) = log(C(f )) f ∈F (S,p) = X log(P (f |p)C(p)) f ∈F (S,p) (2) = |F (S, p)|C(p) + X log(P (f |p)) f ∈F (S,p) where C(p) and C(q) denote the number of times preposition p and q, respectively, occurred in the training data. We implement a count-based LM with Jelinek-Mercer linear interpolation as a smoothing method5 (Chen and Goodman, 1996), where each n-gram length, from 1 to n, is associated with an interpolation smoothing weight λ. Weights are optimized on a held-out set of ESL sentences. Win2 and Win3 features correspond to 4-gram LMs and Win4 to 5-gram LMs. Language models are trained with SRILM (Stolcke, 2002). 4 LBJ can be downloaded from http://cogcomp.cs. illinois.edu. 5 Unlike other LM methods, this approach allows us to train LMs on very large data sets. Although we found that backoff LMs may perform slightly better, they still maintain the same hierarchy in the order of algorithm performance. 927 where C(f ) denotes"
P11-1093,W07-1604,0,0.0341386,"r even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outperform a maximum entropy classifier (Gamon, 2010). However, the language model was trained on the Gigaword corpus, 17 · 109 words (Linguistic Data Consortium, 2003), a corpus several orders of magnitude larger than the corpus used to train the classifier. Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10, 000 times smaller (Chodorow et al., 2007)1 . In this work, we compare four popular learning methods applied to the problem of correcting preposition and article errors and evaluate on a common ESL data set. We compare two probabilistic approaches – Na¨ıve Bayes and language modeling; a discriminative algorithm Averaged Perceptron; and a count-based method SumLM (Bergsma et al., 2009), which, as we show, is very similar to Na¨ıve Bayes, but with a different free coefficient. We train our models on data from several sources, varying training sizes and feature sets, and show that there are significant differences in the performance of t"
P11-1093,C10-2031,0,0.0774458,"rs exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of"
P11-1093,C08-1022,0,0.536614,"Missing"
P11-1093,I08-1059,0,0.422305,"ge of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confus"
P11-1093,N10-1019,0,0.607863,", models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of a confusable word in text is represen"
P11-1093,han-etal-2010-using,0,0.235479,"or ESL and native data. Percentage of test n-gram features that occurred in training. Native refers to data from Wikipedia and NYT. B09 refers to statistics from Bergsma et al. (2009). Roth, 2010a). For instance, a Chinese learner of English might say “congratulations to this achievement” instead of “congratulations on this achievement”, while a Russian speaker might say “congratulations with this achievement”. A system performs much better when it makes use of knowledge about typical errors. When trained on annotated ESL data instead of native data, systems improve both precision and recall (Han et al., 2010; Gamon, 2010). Annotated data include both the writer’s preposition and the intended (correct) one, and thus the knowledge about typical errors is made available to the system. Another way to adapt a model to the first language is to generate in native training data artificial errors mimicking the typical errors of the non-native writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). Henceforth, we refer to this method, proposed within the discriminative framework AP, as AP-adapted. To determine typical mistakes, error statistics are collected on a small set of annotated ESL senten"
P11-1093,J08-2005,1,0.623978,"e the regularized version of AP in Learning Based Java4 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with a generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training. It also has been shown to produce stateof-the-art results on many natural language applications (Punyakanok et al., 2008). 2.3 Na¨ıve Bayes NB is another linear model, which is often hard to beat using more sophisticated approaches. NB architecture is also particularly well-suited for adapting the model to the first language of the writer (Section 4). Weights in NB are determined, similarly to LM, by the feature counts and the prior probability of each candidate p (Eq. (2)). For each candidate p, NB computes the joint probability of p and the feature space F , assuming that the features are conditionally independent given p: Y P (f |p)} g(S, p) = log{prior(p) · f ∈F (S,p) 2.2 Language Modeling Given a feature f"
P11-1093,W10-1004,1,0.85896,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,D10-1094,1,0.560607,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,N10-1018,1,0.858911,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,C08-1109,0,0.700232,"rors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, ther"
P11-1093,P10-2065,0,0.514208,"larities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of a confusable word in tex"
P11-1093,P03-2026,0,\N,Missing
P11-1138,E06-1002,0,0.88156,"re 1). We denote the output matching as an N -tuple Γ = (t1 , . . . , tN ) where ti is the output disambiguation for mention mi . 1 The data sets are available http://cogcomp.cs.illinois.edu/Data for download at 2.1 Local and Global Disambiguation A local D2W approach disambiguates each mention mi separately. Specifically, let φ(mi , tj ) be a score function reflecting the likelihood that the candidate title tj ∈ W is the correct disambiguation for mi ∈ M . A local approach solves the following optimization problem: Γ∗local = arg max Γ φ(mi , ti ) (1) i=1 Local D2W approaches, exemplified by (Bunescu and Pasca, 2006) and (Mihalcea and Csomai, 2007), utilize φ functions that assign higher scores to titles with content similar to that of the input document. We expect, all else being equal, that the correct disambiguations will form a “coherent” set of related concepts. Global approaches define a coherence function ψ, and attempt to solve the following disambiguation problem: N X φ(mi , ti ) + ψ(Γ)] Γ = arg max[ ∗ Γ (2) i=1 The global optimization problem in Eq. 2 is NPhard, and approximations are required (Cucerzan, 2007). The common approach is to utilize the Wikipedia link graph to obtain an estimate pair"
P11-1138,D07-1074,0,0.399825,"g friends in <Chicago>,” we output http://en.wikipedia.org/wiki/Chicago – the Wikipedia page for the city of Chicago, Illinois, and not (for example) the page for the 2002 film of the same name. Local D2W approaches disambiguate each mention in a document separately, utilizing clues such as the textual similarity between the document and each candidate disambiguation’s Wikipedia page. Recent work on D2W has tended to focus on more sophisticated global approaches to the problem, in which all mentions in a document are disambiguated simultaneously to arrive at a coherent set of disambiguations (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009). For example, if a mention of “Michael Jordan” refers to the computer scientist rather than the basketball player, then we would expect a mention of “Monte Carlo” in the same document to refer to the statistical technique rather than the location. Global approaches utilize the Wikipedia link graph to estimate coherence. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1375–1384, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Document text with mentions m1 = Taiwan"
P11-1138,C10-1150,0,0.111927,"Results In this section, we evaluate and analyze G LOW’s performance on the D2W task. We begin by evaluating the mention detection component (Step 1 of the algorithm). The second column of Table 2 shows how many of the “non-null” mentions and corresponding titles we could successfully identify (e.g. out of 747 mentions in the MSNBC data set, only 530 appeared in our anchor-title index). Missing entities were primarily due to especially rare surface forms, or sometimes due to idiosyncratic capitalization in the corpus. Improving the number of identified mentions substantially is non-trivial; (Zhou et al., 2010) managed to successfully identify only 59 more entities than we do in the MSNBC data set, using a much more powerful detection method based on search engine query logs. We generate disambiguation candidates for a 5 We evaluate the mention identification stage in Section 6. Features P (t|m) Naive Reweighted All above NER Unambiguous Predictions All features Data sets ACE MSNBC AQUAINT 94.05 81.91 93.19 P (t|m)+Local 95.67 84.04 94.38 96.21 85.10 95.57 95.67 84.68 95.40 P (t|m)+Global 96.21 84.04 94.04 94.59 84.46 95.40 96.75 88.51 95.91 P (t|m)+Local+Global 97.83 87.02 94.38 Wiki 85.88 92.76 93"
P11-1149,P09-1010,0,0.0651001,"interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different appro"
P11-1149,P07-1036,1,0.198445,"omingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as"
P11-1149,N09-1034,1,0.825093,"and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Re"
P11-1149,N03-1004,0,0.0122379,"iew, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to"
P11-1149,W10-2903,1,0.859398,"on top, preferably by a large margin to allow generalization.The structured learning algorithm can directly use the top ranking predictions of the model (line 8 in Alg. 1) as training data. In this case the underlying algorithm is a structural SVM with squared-hinge loss, using hamming distance as the distance function. We use the cuttingplane method to efficiently optimize the learning process’ objective function. 4 Model Semantic parsing as formulated in Eq. 1 is an inference procedure selecting the top ranked output logical formula. We follow the inference approach in (Roth and Yih, 2007; Clarke et al., 2010) and formalize this process as an Integer Linear Program (ILP). Due to space consideration we provide a brief description, and refer the reader to that paper for more details. Combined The two approaches defined above capture different views of the data, a natural question is then - can these two measures be combined to provide a more powerful estimation? We suggest a third approach which combines the first two approaches. It first uses the score produced by the latter approach 2 Without normalization longer sentences would have more to filter out unlikely candidates, and then ranks the influe"
P11-1149,W99-0613,0,0.0778258,"on: training the model using examples selected according to the model’s parameters (i.e., the top ranking structures) may not generalize much further beyond the existing model, as the training examples will simply reinforce the existing model. The statistics used for confidence estimation are different than those used by the model to create the output structures, and can therefore capture additional information unobserved by the prediction model. This assumption is based on the well established idea of multi-view learning, applied successfully to many NL applications (Blum and Mitchell, 1998; Collins and Singer, 1999). According to this idea if two models use different views of the data, each of them can enhance the learning process of the other. The success of our learning procedure hinges on finding good confidence measures, whose confidence prediction correlates well with the true quality of the prediction. The ability of unsupervised confidence estimation to provide high quality confidence predictions can be explained by the observation that prominent prediction patterns are more likely to be correct. If a non-random model produces a prediction pattern multiple times it is likely to be an indication of"
P11-1149,N04-4028,0,0.0109676,"riterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic pars"
P11-1149,W05-0602,0,0.271332,"result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et"
P11-1149,P06-1115,0,0.721507,"ged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learnin"
P11-1149,P09-1011,0,0.277238,"tracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011)"
P11-1149,P08-2055,0,0.0122683,"tion model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensat"
P11-1149,N06-1020,0,0.0106108,"direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works"
P11-1149,P06-2080,0,0.125091,"pproximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et a"
P11-1149,D09-1001,0,0.44694,"ervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al.,"
P11-1149,P07-1052,1,0.831409,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,P07-1078,1,0.786226,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,W10-2909,1,0.808796,"Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensate for the lack of training data we use a self-training protocol, driven by unsupervised confidence estimation. We demonstrate empirically that our approach results in a high preforming semantic parser and show that confidence"
P11-1149,P07-1076,0,0.0342205,"les for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised lea"
P11-1149,W00-1317,0,0.0141848,"result obtained in any of the learning algorithm iterations (Best), the result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefor"
P11-1149,P11-1145,0,0.0349398,"y, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A"
P11-1149,P10-1098,0,0.159121,"rs. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation"
P11-1149,J07-1003,0,0.00798533,"timate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to ret"
P11-1149,N06-1056,0,0.367679,"result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using"
P11-1149,P07-1121,0,0.586562,"vert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time consuming task which requi"
P11-1149,P95-1026,0,0.3364,"al step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored"
P11-1149,W06-1604,0,0.0219323,"es its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process"
P11-1149,D07-1071,0,0.903992,"reasons, as the ability to convert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time co"
P11-1149,P09-1110,0,0.0810929,"nce criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional ste"
P12-1088,N10-1083,0,0.0315726,"Missing"
P12-1088,P07-1036,1,0.931812,"i)[fk (si, h, t)] (13) i 3.2 Approximate Learning Computation of the denominator terms of Equation 10 (and the second term of Equation 11) can be done 839 pΘ (t, h|si ; κ) ∝ ef(si ,h,t)·Θ × κ(si , h, t) pΘ (t, h|si ) ∝ ef(si ,h,t)·Θ Recall that s is an event span, t is a specfic realization of the event template, and h is the hidden mention information for the event span. 4 Discussion: Preferences v.s. Constraints Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al., 2007) (and similarly, posterior regularization (Ganchev et al., 2010), which we will discuss later at Section 6). Specifically, in CoDL, the following cost function is involved in its EM-like inference procedure: X d(y, Yc ) (14) arg max Θ · f(x, y) − ρ y c where Yc defines the set of y’s that all satisfy a certain constraint c, and d defines a distance function from y to that set. The parameter ρ controls the degree of the penalty when constraints are violated. There are some important distinctions between structured preference modeling (PM) and CoDL. CoDL primarily concerns constraints, which pen"
P12-1088,N10-1066,1,0.371947,"design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al., 2009), where good neighborhoods can be identified. However, it is less intuitive what constitutes a good neighborhood in this task. The neighborhood assumption of CE is relaxed in another latent structure approach (Chang et al., 2010a; Chang et al., 2010b) that focuses on semisupervised learning with indirect supervisions, inspired by the CoDL model described above. The locally normalized logistic regression (Berg843 Kirkpatrick et al., 2010) is another recently proposed framework for unsupervised structured prediction. Their model can be regarded as a generative model whose component multinomial is replaced with a miniature logistic regression where a rich set of local features can be incorporated. Empirically the model is effective in various unsupervised structured prediction tasks, and outperforms the globally normali"
P12-1088,N06-1041,0,0.0152495,"popular topic in natural language processing. 4 For each event, we only performed 1 run with all the initial feature weights set to zeros. Event Attack Meet Die Transport Random 14.26 26.65 19.17 15.78 Unsup 26.19 14.08 9.09 10.14 PM 32.89 45.28 44.44 49.73 semi-CRF 46.92 58.18 48.57 52.34 Table 3: Event extraction performance with automatic mention identifier and typer. We report F1 percentage scores for preference modeling (PM) as well as two baseline approaches. We also report performance of the supervised approach trained with the semi-CRF model for comparison. Prototype driven learning (Haghighi and Klein, 2006) tackled the sequence labeling problem in a primarily unsupervised setting. In their work, a Markov random fields model was used, where some local constraints are specified via their prototype list. Constraint-driven learning (CoDL) (Chang et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comp"
P12-1088,P06-1059,0,0.00539906,"itional probability of observing the template realization for the observed event span s: X L(Θ) = log PΘ (ti |si ) i = X i P f(si ,h,ti )·Θ e log P h f(s ,h,t)·Θ i t,h e (2) This function is not convex due to the summation over the hidden variable h. To optimize it, we take its partial derivative with respect to θj : X ∂L(Θ) = EpΘ (h|si ,ti ) [fj (si , h, ti )] ∂θj i X − EpΘ (t,h|si ) [fj (si , h, t)] (3) i which requires computation of expectations terms under two different distributions. Such statistics can be collected efficiently with a forward-backward style algorithm in polynomial time (Okanohara et al., 2006). We will discuss the time complexity for our case in the next section. Given its partial derivatives in Equation 3, one could optimize the objective function of Equation 2 with stochastic gradient ascent (LeCun et al., 1998) or L-BFGS (Liu and Nocedal, 1989). We choose to use L-BFGS for all our experiments in this paper. Inference involves computing the most probable template realization t for a given event span: X arg max PΘ (t|s) = arg max PΘ (t, h|s) (4) t t h where the possible hidden assignments h need to be marginalized out. In this task, a particular realization t already uniquely defi"
P12-1088,N09-1024,0,0.0132905,", 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al., 2009), where good neighborhoods can be identified. However, it is less intuitive what constitutes a good neighborhood in this task. The neighborhood assumption of CE is relaxed in another latent structure approach (Chang et al., 2010a; Chang et al., 2010b) that focuses on semisupervised learning with indirect supervisions, inspired by the CoDL model described above. The locally normalized logistic regression (Berg843 Kirkpatrick et al., 2010) is another recently proposed framework for unsupervised structured prediction. Their model can be regarded as a generative model whose component multinomial i"
P12-1088,W09-1119,1,0.171038,"Consider the following text span that describes an “Attack” event: . . . North Korea’s military may have fired a laser at a U.S. helicopter in March, a U.S. official said Tuesday, as the communist state ditched its last legal obligation to keep itself free of nuclear weapons . . . A partial event template for the “Attack” event is shown on the left of Figure 1. Each row shows an Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Str¨otgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al., 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal. Instead of obtaining the above ideal annotation, one might observe the following noisy and ambiguous annotation for the given event span: . . . [[North Korea’s]G PE|L OC military]O RG may have fired a laser at [a [U.S.]G PE|L OC helicopter]V EH in [March]T ME , [a [U.S.]G PE|L OC official]P ER said [Tuesday]T ME , as [the communist state]O RG|FAC|L OC ditched its last legal obligation to keep [itself ]O RG free of [nuclea"
P12-1088,P11-1138,1,0.165136,": . . . North Korea’s military may have fired a laser at a U.S. helicopter in March, a U.S. official said Tuesday, as the communist state ditched its last legal obligation to keep itself free of nuclear weapons . . . A partial event template for the “Attack” event is shown on the left of Figure 1. Each row shows an Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Str¨otgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al., 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal. Instead of obtaining the above ideal annotation, one might observe the following noisy and ambiguous annotation for the given event span: . . . [[North Korea’s]G PE|L OC military]O RG may have fired a laser at [a [U.S.]G PE|L OC helicopter]V EH in [March]T ME , [a [U.S.]G PE|L OC official]P ER said [Tuesday]T ME , as [the communist state]O RG|FAC|L OC ditched its last legal obligation to keep [itself ]O RG free of [nuclear weapons]W EA . . . Our task is to design a model to effectivel"
P12-1088,N12-1087,1,0.801652,"pecified via their prototype list. Constraint-driven learning (CoDL) (Chang et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comparison to our model. Unlike CoDL, in the PR framework constraints are relaxed to expectation constraints, in order to allow tractable dynamic programming. See also Samdani et al. (2012) for more discussions. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar i"
P12-1088,P05-1044,0,0.0423737,"ng et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comparison to our model. Unlike CoDL, in the PR framework constraints are relaxed to expectation constraints, in order to allow tractable dynamic programming. See also Samdani et al. (2012) for more discussions. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon e"
P12-1088,S10-1071,0,0.0244013,"Missing"
P13-1089,D12-1102,1,0.511983,"gin-based Decomposed Amortized Inference Gourab Kundu∗ and Vivek Srikumar∗ and Dan Roth University of Illinois, Urbana-Champaign Urbana, IL. 61801 {kundu2, vsrikum2, danr}@illinois.edu Abstract some structures like sequences or parse trees, specialized and tractable dynamic programming algorithms have proven to be very effective. However, as the structures under consideration become increasingly complex, the computational problem of predicting structures can become very expensive, and in the worst case, intractable. In this paper, we focus on an inference technique called amortized inference (Srikumar et al., 2012), where previous solutions to inference problems are used to speed up new instances. The main observation that leads to amortized inference is that, very often, for different examples of the same size, the structures that maximize the score are identical. If we can efficiently identify that two inference problems have the same solution, then we can re-use previously computed structures for newer examples, thus giving us a speedup. This paper has two contributions. First, we describe a novel algorithm for amortized inference called margin-based amortization. This algorithm is on an examination"
P13-1089,D11-1003,0,0.153732,"ence can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances and the number of unique observed part-of-speech structures in the Gigaword corpus. Note that the"
P13-1089,J08-2002,0,0.0174154,"for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inference variables are obtained from a classifier trained on the PropBank corpus. Constraints encode structural and linguistic knowledge about the problem. For details about the formulations of the inference problem, please see (Punyakanok et al., 2008). Recall from Sectio"
P13-1089,P06-2019,0,0.0254204,"oal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inferenc"
P13-1089,J02-3001,0,0.00542721,"across these two sets of entities to obtain two independent inference problems. Tasks We report the performance of inference on two NLP tasks: semantic role labeling and the task of extracting entities and relations from text. In both cases, we used an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inf"
P13-1089,W10-2924,0,0.150342,"typically occurs much more frequently than the others. Figure 1 illustrates this observation in the context of part-of-speech tagging. If we can efficiently characterize and identify inference instances that have the same solution, we can take advantage of previously performed computation without paying the high computational cost of inference. We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) provides a convenient analytical tool for representing structured prediction problems. The general setting con"
P13-1089,D10-1125,0,0.0992846,"algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of"
P13-1089,J08-2005,1,0.924523,"3 Association for Computational Linguistics data. Furthermore, among the observed structures, a small subset typically occurs much more frequently than the others. Figure 1 illustrates this observation in the context of part-of-speech tagging. If we can efficiently characterize and identify inference instances that have the same solution, we can take advantage of previously performed computation without paying the high computational cost of inference. We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) prov"
P13-1089,N12-1008,0,0.0318112,"mance metrics – the percentage decrease in the number of ILP calls, and the percentage decrease in the wall-clock inference time. These are comparable to the speedup and clock speedup defined in (Srikumar et al., 2012). For measuring time, since other aspects of prediction (like feature extraction) are the same across all settings, we only measure the time taken for inference and ignore other aspects. For both 2 Discussion Lagrangian Relaxation in the literature In the literature, in applications of the Lagrangian relaxation technique (such as (Rush and Collins, 2011; Chang and Collins, 2011; Reichart and Barzilay, 2012) and others), the relaxed problems are solved using specialized algorithms. However, in both the relaxations considered in this paper, even the relaxed problems cannot be solved without an ILP solver, and yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. After T iterations, we call the ILP solver and solve the"
P13-1089,W06-1616,0,0.0276182,"nd the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observati"
P13-1089,W04-2401,1,0.726344,"with a score. The goal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivati"
P13-1089,P11-1008,0,0.0951259,"the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances and the number of unique observed part-of-speech structures in the Gigaw"
P13-1089,D10-1001,0,0.0243174,"or special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances"
P13-2082,D11-1131,0,0.0384776,"Missing"
P13-2082,N10-1066,1,0.841026,"ooney, 2008), describes robotic soccer events. The dataset was collected for the purpose of constructing semantic parsers from ambiguous supervision and consists of both “noisy” and gold labeled data. The noisy dataset Combined Model In order to consider both types of information we augment our decision model with the new variables, resulting in the following objective function (Eq. 2). P P Fw (x) = arg maxα,β c∈x s∈D αcs ·w1 T Φ1 (x, c, s)+ P P P T i j c,d∈x s,t∈D i,j βcsi ,dtj · w2 Φ2 (x, c, s , d, t ) + P P T T c∈x γc · w3 Φ3 (x, c) + c,d∈x δcd · w4 Φ4 (x, c, d) (2) 2 Details omitted, see (Chang et al., 2010) for more details. For example, a unary relation symbol for “He plays”, and a binary for “He plays with a ball”. 3 464 System Training Procedure D OM -I NIT P RED -A RGS C OMBINEDRL C OMBINEDRI+S w1 : Noisy probabilistic model, described below. Only w3 , w4 Trained over the Situ. dataset. w1 , w2 , w3 , w4 :learned from Robocup gold w3 , w4 : learned from the Situ. dataset, w1 uses the D OM -I NIT Robocup model. w3 , w4 : Initially learned over the Situ. dataset, updated jointly with w1 , w2 over Robocup gold C OMBINEDRL+S System P RED -A RGS D OM -I NIT C OMBINEDRI+S ¨ (B ORSCHINGER ET AL .,"
P13-2082,W10-2903,1,0.931959,"that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c). The overall inference problem (Eq. 1) is as follows: P P Fw (x) = arg maxα,β c∈x s∈D αcs · wT Φ1 (x, c, s) P P + c,d∈x s,t∈D βcs,dt · wT Φ2 (x, c, s, d, t) (1) pass(pink1, pink11) We restrict the possible assignments to the decision variables, forcing the resulting output formula to be syntactically legal, for example by restricting active β-variables to be type consistent, and forcing the resulting functional composition to be acyclic and fully connected (we refer the reader to (Clarke et al., 2010) for more details). We take advantage of the flexible ILP framework and encode these restrictions as global constraints. In order to overcome this difficulty, we suggest a flexible model that is able to leverage the supervision provided in one domain to learn an abstract intermediate layer, and show empirically that it learns a robust model, improving results significantly in a second domain. 2 Domain-Dependent Model Semantic Interpretation Model Our model consists of both domain-dependent (mapping between text and a closed set of symbols) and domain independent (abstract predicateargument str"
P13-2082,J02-3001,0,0.0402084,"w variables to semantic interpretation : This information is used to assist the overall learning process. We assume that these labels correspond to a binding to some logical symbol, and encode it as a constraint forcing the relations between the two models. Moreover, since learning this layer is a by-product of the learning process (as it does not use any labeled data) forcing the connection between the decisions is the mechanism that drives learning this model. Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. Unlike SRL, which aims to identify linguistic structures alone, in our framework these structures capture both natural-language and domain-language considerations. ∀c ∈ x (γc → αc,s1 ∨ αc,s2 ∨ ... ∨ αc,sn ) ∀c ∈ x, ∀d ∈ x (δc,d → βc,s1 ,dt1 ∨βc,s2 ,dt1 ∨...∨βc,sn ,dtn ) (where n is the length of x). 2.3 Learning the Combined Model The supervision to the learning process is given via data consisting of pairs of sentences and (domain specific) semantic interpretation. G"
P13-2082,P11-1149,1,0.851705,"ar objective, which uses a vector w, mapping features to weights and a feature function Φ which maps the output decision to a feature vector. The output interpretation y is described using a subset of first order logic, consisting of typed constants (e.g., robotic soccer player), functions capturing relations between entities, and their properties (e.g., pass(x, y), where pass is a function symbol and x, y are typed arguments). We use data taken from two grounded domains, describing robotic soccer events and household situations. We begin by formulating the domain-specific process. We follow (Goldwasser et al., 2011; Clarke et al., 2010) and formalize semantic inference as an Integer Linear Program (ILP). Due to space consideration, we provide a brief description (see (Clarke et al., 2010) for more details). We then proceed to augment this model with domain-independent information, and connect the two models by constraining the ILP model. Features We use two types of feature, first-order Φ1 and second-order Φ2 . Φ1 depends on lexical information: each mapping of a lexical item c to a domain symbol s generates a feature. In addition each combination of a lexical item c and an symbol type generates a featu"
P13-2082,C10-2062,0,0.0239819,"Missing"
P13-2082,D10-1119,0,0.023594,"vel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains. 1 Introduction Natural Language (NL) understanding can be intuitively understood as a general capacity, mapping words to entities and their relationships. However, current work on automated NL understanding (typically referenced as semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Chen and Mooney, 2008; Kwiatkowski et al., 2010; B¨orschinger et al., 2011)) is restricted to a given output domain1 (or task) consisting of a closed set of meaning representation symbols, describing domains such as robotic soccer, database queries and flight ordering systems. In this work, we take a first step towards constructing a semantic interpreter that can leverage information from multiple tasks. This is not a straight forward objective – the domain specific nature of semantic interpretation, as described in the current literature, does not allow for an easy move between domains. For example, a system trained for the task of unders"
P13-2082,P07-1121,0,0.0297055,"nt and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains. 1 Introduction Natural Language (NL) understanding can be intuitively understood as a general capacity, mapping words to entities and their relationships. However, current work on automated NL understanding (typically referenced as semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Chen and Mooney, 2008; Kwiatkowski et al., 2010; B¨orschinger et al., 2011)) is restricted to a given output domain1 (or task) consisting of a closed set of meaning representation symbols, describing domains such as robotic soccer, database queries and flight ordering systems. In this work, we take a first step towards constructing a semantic interpreter that can leverage information from multiple tasks. This is not a straight forward objective – the domain specific nature of semantic interpretation, as described in the current literature, does not allow for an easy move between domains. For"
P14-6004,C10-1032,0,\N,Missing
P14-6004,E06-1002,0,\N,Missing
P14-6004,C10-1150,0,\N,Missing
P14-6004,D12-1010,0,\N,Missing
P14-6004,W11-2213,0,\N,Missing
P14-6004,C10-1145,0,\N,Missing
P14-6004,D11-1074,0,\N,Missing
P14-6004,mcnamee-etal-2010-evaluation,0,\N,Missing
P14-6004,D09-1025,0,\N,Missing
P14-6004,D12-1011,0,\N,Missing
P14-6004,D12-1082,0,\N,Missing
P14-6004,P11-1138,1,\N,Missing
P14-6004,P12-1086,0,\N,Missing
P14-6004,P13-2006,0,\N,Missing
P14-6004,P11-1095,0,\N,Missing
P14-6004,D13-1041,0,\N,Missing
P14-6004,I11-1029,0,\N,Missing
P14-6004,C12-1028,1,\N,Missing
P14-6004,D13-1184,1,\N,Missing
P14-6004,P13-1128,0,\N,Missing
P14-6004,D07-1074,0,\N,Missing
P14-6004,N10-1072,0,\N,Missing
P14-6004,W12-1014,0,\N,Missing
P14-6004,D11-1011,0,\N,Missing
P14-6004,P13-1107,1,\N,Missing
P14-6004,D11-1071,1,\N,Missing
P14-6004,I11-1113,0,\N,Missing
P14-6004,D12-1113,1,\N,Missing
P16-1028,E12-1034,0,0.475106,"Missing"
P16-1028,P98-1013,0,0.454528,"Two Models for SemLM In this section, we describe how we capture sequential semantic information consisted of semantic frames and discourse markers as semantic units (i.e. the vocabulary). 3.1 Frame-Chain SemLM Semantic Frames and Discourse Markers Semantic Frames A semantic frame is composed of a predicate and its corresponding argument participants. Here we require the predicate to be disambiguated to a specific sense, and we need a certain level of abstraction of arguments so that we can assign abstract labels. The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs. They both have a limited set of frames (in the scale of thousands) and each frame can be uniquely represented by its predicate sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. We use these frames as part of our vocabulary for SemLMs. Formally, we use the notation f to represent a frame. Also, we denote fa , f#Arg when referring to an argument role label (Arg) inside a frame (f). Discourse Markers We use discourse markers (connectives) to model discourse relationships between fram"
P16-1028,kingsbury-palmer-2002-treebank,0,0.631291,"ication to coreference resolution and shallow discourse parsing tasks. Following the traditional evaluation standard of language models, we first use perplexity as our metric. We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.e. randomly removing a token from a sequence and test the system’s ability to recover it. We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al., 2006) ). By comparing the results on these test sets, we show that we do not incur noticeable degradation when building SemLMs using preprocessing tools. Moreover, we show that SemLMs improves the performance of co-reference resolu1 Some works may utilize a certain probabilistic framework, but they mainly focus on generating high-quality frames by filtering. 291 Table 1: Comparison of vocabularies between frame-chain (FC) and entity-centered (EC) SemLMs. “F-Sen” stands for frames with predicate sense information while “F-Arg” stands f"
P16-1028,D13-1178,0,0.152248,"able for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning r"
P16-1028,Q14-1029,0,0.0625121,"2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications"
P16-1028,N13-1090,0,0.0875388,"antic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good coverage of frame semantics. For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others yield embeddings for semantic frames as they are neural language models. In our empirical study, we evaluate both the quality of all SemLMs and their application to coreference resolution and shallow discourse parsing tasks. Following the traditional evaluation standard of language models, we first use perplexity as our metric. We also follow the script learning literat"
P16-1028,D08-1031,1,0.705724,"pc and 1/pc as features. To get the value of pc , we follow the definitions in Sec. 4, and we only use the entity-centered SemLM here as its vocabulary covers frames with argument labels. For the neural language model implementations (CBOW, SG and LB), we also include frame embeddings as additional features. We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al., 2012). We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008). For CoNLL12 data, we follow the train and test document split from CoNLL-2012 Shared Task. We report CoNLL AVG for results (average of MUC, B3 , and CEAFe metrics), using the v7.0 scorer provided by the CoNLL-2012 Shared Task. Co-reference resolution results with entitycentered SemLM features are shown in Table 5. Tri-grams with conditional probability features improve the performance by a small margin, while the log-bilinear model achieves a 0.4-0.5 F1 points improvement. By employing log-bilinear model embeddings, we further improve the numbers and we outperform the best reported results o"
P16-1028,D08-1073,0,0.685757,"— but — arrest.01#obj” (here “obj” indicates the argument label for “Kevin” and “him” respectively). While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). Howe"
P16-1028,W14-1606,0,0.132754,"ney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks ("
P16-1028,P08-1090,0,0.935434,"— but — arrest.01#obj” (here “obj” indicates the argument label for “Kevin” and “him” respectively). While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). Howe"
P16-1028,P09-1068,0,0.829478,"data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our wor"
P16-1028,1985.tmi-1.17,0,0.65668,"determine the discourse markers between these frames. Thus, each unique frame contains both the disambiguated predicate and the argument label of the mention. In Ex.3, the resulting sequence is “rob.01#obj — but — arrest.01#obj” (here “obj” indicates the argument label for “Kevin” and “him” respectively). While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b"
P16-1028,D13-1185,0,0.172284,"els can be induced at high quality, and that they are suitable for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as t"
P16-1028,P15-1019,0,0.308443,"Missing"
P16-1028,N13-1104,0,0.280563,"Missing"
P16-1028,N07-1011,0,0.0211278,"). √ We also add p2c , pc and 1/pc as features. To get the value of pc , we follow the definitions in Sec. 4, and we only use the entity-centered SemLM here as its vocabulary covers frames with argument labels. For the neural language model implementations (CBOW, SG and LB), we also include frame embeddings as additional features. We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al., 2012). We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008). For CoNLL12 data, we follow the train and test document split from CoNLL-2012 Shared Task. We report CoNLL AVG for results (average of MUC, B3 , and CEAFe metrics), using the v7.0 scorer provided by the CoNLL-2012 Shared Task. Co-reference resolution results with entitycentered SemLM features are shown in Table 5. Tri-grams with conditional probability features improve the performance by a small margin, while the log-bilinear model achieves a 0.4-0.5 F1 points improvement. By employing log-bilinear model embeddings, we further improve the numbers and we outperform t"
P16-1028,E14-1006,0,0.226732,"Missing"
P16-1028,K15-1002,1,0.815875,", 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good coverage of frame semantics. For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others yield embeddings for semantic frames as"
P16-1028,N15-1082,1,0.852808,", 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good coverage of frame semantics. For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others yield embeddings for semantic frames as"
P16-1028,E14-1024,0,0.217037,"el for “Kevin” and “him” respectively). While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks. 2 Related Work Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script"
P16-1028,N06-2015,0,0.293249,"lowing the traditional evaluation standard of language models, we first use perplexity as our metric. We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.e. randomly removing a token from a sequence and test the system’s ability to recover it. We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al., 2006) ). By comparing the results on these test sets, we show that we do not incur noticeable degradation when building SemLMs using preprocessing tools. Moreover, we show that SemLMs improves the performance of co-reference resolu1 Some works may utilize a certain probabilistic framework, but they mainly focus on generating high-quality frames by filtering. 291 Table 1: Comparison of vocabularies between frame-chain (FC) and entity-centered (EC) SemLMs. “F-Sen” stands for frames with predicate sense information while “F-Arg” stands for frames with argument role label information; “Conn” means disc"
P16-1028,W12-4501,0,0.121142,"Missing"
P16-1028,W11-1913,0,0.101764,"b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good coverage of frame semantics. For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others"
P16-1028,prasad-etal-2008-penn,0,0.0272483,"te sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. We use these frames as part of our vocabulary for SemLMs. Formally, we use the notation f to represent a frame. Also, we denote fa , f#Arg when referring to an argument role label (Arg) inside a frame (f). Discourse Markers We use discourse markers (connectives) to model discourse relationships between frames. There is only a limited number of unique discourse markers, such as and, but, however, etc. We get the full list from the Penn Discourse Treebank (Prasad et al., 2008) and include them as part of our vocabulary for SemLMs. Formally, we use dis to denote the discourse marker. Note that discourse relationships can exist without an explicit discourse marker, which is also a challenge for discourse parsing. Since we cannot reliably identify implicit discourse relationships, we only consider explicit ones here. More importantly, discourse markers are associated with ar3.3 Entity-Centered SemLM We generate semantic sequences according to co-reference chains for entity-centered SemLM. From co-reference resolution, we can get a sequence like [m1 , m2 , m3 , . . .],"
P16-1028,C04-1197,1,0.796932,"pping to FrameNet frames via VerbNet senses (Schuler, 2005), thus achieving a higher level of abstraction. The mapping file4 defines deterministic mappings. However, the mapping is not complete and there are remaining PropBank frames. Thus, the generated vocabulary for SemLMs contains both PropBank and FrameNet frames. For example, “place” and p(wt |c(wt ), θ). k Y Dataset and Preprocessing Dataset We use the New York Times Corpus2 (from year 1987 to 2007) for training. It contains a bit more than 1.8M documents in total. Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015). Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains. To obtain all annotations, we employ the Illinois NLP tools3 . p(c ∈ c(wt )|wt , θ). k Y Y Building SemLMs from Scratch Log-bilinear Model LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token. However, LB associates a token with 2 https://catalog.ldc.upenn.ed"
P16-1028,P11-1082,0,0.0514087,"014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good coverage of frame semantics. For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others yield embeddings for"
P16-1028,P98-2186,1,0.22783,", 2005), thus achieving a higher level of abstraction. The mapping file4 defines deterministic mappings. However, the mapping is not complete and there are remaining PropBank frames. Thus, the generated vocabulary for SemLMs contains both PropBank and FrameNet frames. For example, “place” and p(wt |c(wt ), θ). k Y Dataset and Preprocessing Dataset We use the New York Times Corpus2 (from year 1987 to 2007) for training. It contains a bit more than 1.8M documents in total. Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015). Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains. To obtain all annotations, we employ the Illinois NLP tools3 . p(c ∈ c(wt )|wt , θ). k Y Y Building SemLMs from Scratch Log-bilinear Model LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token. However, LB associates a token with 2 https://catalog.ldc.upenn.edu/LDC2008T19 http://cogcomp.cs.illinois.edu/page/so"
P16-1028,D15-1195,0,0.441714,"Missing"
P16-1028,K15-2012,1,0.870059,"not complete and there are remaining PropBank frames. Thus, the generated vocabulary for SemLMs contains both PropBank and FrameNet frames. For example, “place” and p(wt |c(wt ), θ). k Y Dataset and Preprocessing Dataset We use the New York Times Corpus2 (from year 1987 to 2007) for training. It contains a bit more than 1.8M documents in total. Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015). Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains. To obtain all annotations, we employ the Illinois NLP tools3 . p(c ∈ c(wt )|wt , θ). k Y Y Building SemLMs from Scratch Log-bilinear Model LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token. However, LB associates a token with 2 https://catalog.ldc.upenn.edu/LDC2008T19 http://cogcomp.cs.illinois.edu/page/software/ 4 http://verbs.colorado.edu/verb-index/fn/vn-fn.xml 3 293 Table 2: Statistics on SemLM vocabularies and sequences. “"
P16-1028,N15-1001,0,0.125357,"ts to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks. Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, si"
P16-1028,D07-1010,0,0.0388637,"how that SemLMs improves the performance of co-reference resolu1 Some works may utilize a certain probabilistic framework, but they mainly focus on generating high-quality frames by filtering. 291 Table 1: Comparison of vocabularies between frame-chain (FC) and entity-centered (EC) SemLMs. “F-Sen” stands for frames with predicate sense information while “F-Arg” stands for frames with argument role label information; “Conn” means discourse marker and “Per” means period. “Seq/Doc” represents the number of sequence per document. FC EC 3 F-Sen YES YES F-Arg NO YES Conn YES YES Per YES NO guments (Wellner and Pustejovsky, 2007) in text (usually two sentences/clauses, sometimes one). We only add a discourse marker in the semantic sequence when its corresponding arguments contain semantic frames which belong to the same semantic sequence. We call them frame-related discourse markers. Details on generating semantic frames and discourse markers to form semantic sequences are discussed in Sec. 5. Seq/Doc Single Multiple 3.2 For frame-chain SemLM, we model all semantic frames and discourse markers in a document. We form the semantic sequence by first including all semantic frames in the order they appear in the text: [f1"
P16-1028,H89-1033,0,0.530864,"obbed by Robert, but the police mistakenly arrested him. In this case, “him” should refer to “Kevin” as the discourse marker “but” reverses the meaning, illustrating that it is necessary to take discourse markers into account when modeling semantics. Introduction Natural language understanding often necessitates deep semantic knowledge. This knowledge needs to be captured at multiple levels, from words to phrases, to sentences, to larger units of discourse. At each level, capturing meaning frequently requires context sensitive abstraction and disambiguation, as shown in the following example (Winograd, 1972): Ex.1 [Kevin] was robbed by [Robert]. [He] was arrested by the police. Ex.2 [Kevin] was robbed by [Robert]. [He] was rescued by the police. In both cases, one needs to resolve the pronoun “he” to either “Robert” or “Kevin”. To make In this paper we propose that these aspects of semantic knowledge can be modeled as a Semantic Language Model (SemLM). Just like the “standard” syntactic language models (LM), we define a basic vocabulary, a finite representation language, and a prediction task, which allows us to model the distribution over the occurrence of elements in the vocabulary as a functio"
P16-1028,P15-1137,0,0.0710941,"etric. 5 Rudinger et al. (2015) is similar to our entity-centered SemLM without discourse information. So, in Table 4, we 296 Table 5: Co-reference resolution results with entity-centered SemLM features. “EC” stands for the entity-centered SemLM. “TRI” is the trigram model while “LB” is the log-bilinear model. “pc ” means conditional probability features and “em” represents frame embedding features. “w/o DIS” indicates the ablation study by removing all discourse makers for SemLMs. We conduct the experiments by adding SemLM features into the base system. We outperform the state-of-art system (Wiseman et al., 2015), which reports the best results on CoNLL12 dataset. The improvement achieved by “EC LB (pc +em)” over the base system is statistically significant. Wiseman et al. (2015) Base (Peng et al., 2015a) Base+EC-TRI (pc ) Base+EC-TRI w/o DIS Base+EC-LB (pc ) Base+EC-LB (pc + em) Base+EC-LB w/o DIS ACE04 —– 71.20 71.31 71.08 71.71 71.79 71.12 appears before m2 , we first extract the corresponding semantic frame and the argument role label of each mention. We do this by following the procedures in Sec. 5. Thus, we can get a pair of semantic frames with argument information (fa1 , fa2 ). We may also get"
P16-1028,K15-2001,0,0.01289,"uccessfully to the two challenging tasks of co-reference resolution and shallow discourse parsing, exhibiting improvements over state-ofthe-art systems. In future work, we plan to apply SemLMs to other semantic related NLP tasks e.g. machine translation and question answering. qc = p(dis|f1 , f2 ). and, similar to what we do for co-reference resolu√ tion, we add qc , qc2 , qc , 1/qc as conditional probability features, which can be computed following the definitions in Sec. 4. We also include frame embeddings as additional features. We only use frame-chain SemLMs here. We evaluate on CoNLL16 (Xue et al., 2015) test and blind sets, following the train and development document split from the Shared Task, and report F1 using the official shared task scorer. Table 6 shows the results for shallow discourse parsing with SemLM features. Tri-gram with conditional probability features improve the performance for both explicit and implicit connective sense classifiers. Log-bilinear model with conditional probability features achieves even better results, and frame embeddings further improve the numbers. SemLMs improve relatively more on explicit connectives than on implicit ones. We also show an ablation stu"
P16-1028,C98-1013,0,\N,Missing
P16-1028,C98-2181,1,\N,Missing
P16-1157,W13-3520,0,0.0130875,"ector representations in a unified framework, and conduct experiments to compare the performance of existing 1668 models in a unbiased manner. We chose existing cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al.,"
P16-1157,P13-1133,0,0.0310622,"Missing"
P16-1157,W06-2920,0,0.0208404,"ce-side embeddings as lexical features. These embeddings can be replaced by target-side embeddings at test time. All models are trained for 5000 iterations with fixed word embeddings during training. Since our goal is to determine the utility of word embeddings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in Guo et al. (2015). We use the universal dependency treebank (McDonald et al., 2013) version2.0 for our evaluation. For Chinese, we use the treebank released as part of the CoNLL-X shared task (Buchholz and Marsi, 2006). We first evaluate how useful the word embeddings are in cross-lingual model transfer of dependency parsers (Table 6). On an average, BiCCA does better than other models. BiSkip is a close second, with an average performance gap of less than 1 point. BiSkip outperforms BiCVM on German and French (over 2 point improvement), owing to word alignment information BiSkip’s model uses during training. It is not surprising that English-Chinese transfer scores are low, due to the significant difference in syntactic structure of the 10 github.com/jiangfeng1124/ acl15-clnndep l1 l2 BiSkip BiCVM BiCCA Bi"
P16-1157,D15-1131,0,0.0393742,"ng cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al., 2015) and crosslingual Auto-encoder (Chandar et al., 2014) are similar to BiCVM in this respect. Multi-view CCA (Rastogi et al., 2015) and deep CCA (Lu et al.,"
P16-1157,N13-1073,1,0.749116,"gs of size 200. We provide all models with parallel corpora, irrespective of their requirements. Whenever possible, we also report statistical significance of our results. 3 www.statmt.org/europarl/v7/{de, sv}-en.tgz 4 www.statmt.org/wmt15/ translation-task.html Parameter Selection BiSkip. All models were trained using a window size of 10 (tuned over {5, 10, 20}), and 30 negative samples (tuned over {10, 20, 30}). The cross-lingual weight was set to 4 (tuned over {1, 2, 4, 8}). The word alignments for training the model (available at github. com/lmthang/bivec) were generated using fast_align (Dyer et al., 2013). The number of training iterations was set to 5 (no tuning) and we set α = 1 and β = 1 (no tuning). BiCVM. We use the tool (available at github. com/karlmoritz/bicvm) released by Hermann and Blunsom (2014) to train all embeddings. We P train an additive model (that is, f (~x) = g(~x) = i xi ) with hinge loss margin set to 200 (no tuning), batch size of 50 (tuned over 50, 100, 1000) and noise parameter of 10 (tuned over {10, 20, 30}). All models are trained for 100 iterations (no tuning). BiCCA. First, monolingual word vectors are trained using the skip-gram model5 with negative sampling (Miko"
P16-1157,E14-1049,1,0.924174,"ity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while"
P16-1157,N15-1157,0,0.0143038,", 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different"
P16-1157,P15-1119,0,0.498815,"els often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is"
P16-1157,J15-4004,0,0.129398,"Missing"
P16-1157,C12-1089,0,0.159635,"r different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014;"
P16-1157,2005.mtsummit-papers.11,0,0.0208035,"sh-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document clas"
P16-1157,N15-1028,0,0.060572,"do-bilingual document and P (t |s) ∝ exp(tT s). Note that t, s ∈ W ∪ V . Although BiVCD is designed to use comparable corpus, we provide it with parallel data in our experiments (to ensure comparability), and treat two aligned sentences as comparable. 3 Data We train cross-lingual embeddings for 4 language pairs: English-German (en-de), English-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh"
P16-1157,W15-1521,0,0.671634,", inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different models can be viewed as instances of a more general framework for inducing"
P16-1157,N15-1058,0,0.0609469,"Missing"
P16-1157,D15-1036,0,0.0547762,"Missing"
P16-1157,P15-1165,0,0.0457057,"Missing"
P16-1157,N12-1052,0,0.0186424,"Missing"
P16-1157,I05-3027,0,0.050151,"er selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document classification • Cross-lingual syntactic dependency parsing The first two tasks intrinsically measure how much can monolingual and cross-lingual similarity benefit from cross-lingual training. The last two tasks measure the ability of cross"
P16-1157,D15-1243,1,0.603503,"e respective English side of each language is shown in column marked Mono. In all cases (except BiCCA on ensv), the bilingually trained vectors achieve better scores than the mono-lingually trained vectors. Overall, across all language pairs, BiCVM is the best performing model in terms of Spearman’s correlation, but its improvement over BiSkip and BiVCD is often insignificant. It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned corpus only, which are easier to obtain than parallel data with word alignments required by BiSkip. Q VEC. Tsvetkov et al. (2015) proposed an intrinsic evaluation metric for estimating the quality of English word vectors. The score produced by Q VEC measures how well a given set of word vectors is able to quantify linguistic properties 6 We implemented the code for performing the merging as we could not find a tool provided by the authors. en-de en-fr en-sv en-zh 0.29 0.30 0.28 0.28 0.34 0.35 0.32 0.34 0.37 0.39 0.34 0.39 0.30 0.31 0.27 0.30 0.32 0.36 0.32 0.31 avg. 0.29 0.34 0.37 0.30 0.33 Table 2: Word similarity score measured in Spearman’s correlation ratio for English on SimLex-999. The best score for each language"
P16-1157,N13-1011,0,0.0375489,"Missing"
P16-1157,D13-1168,0,0.0341321,"Missing"
P16-1157,P15-2118,0,0.408453,"Missing"
P16-1157,D13-1141,0,0.630951,"ing intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015"
P16-1157,D15-1245,0,\N,Missing
P16-1208,P01-1005,0,0.060046,"translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data a"
P16-1208,N10-1019,0,0.462467,"following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation: role of learn"
P16-1208,P11-1092,0,0.11127,"= arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation: role of learner data in training the"
P16-1208,D12-1052,0,0.821128,"sets are also from the student population studying at the same University but annotated separately. We report results on the CoNLL-2014 test. The annotation includes specifying the relevant correction as well as the information about each error type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors are related to grammar but also include mechanical, collocation, and other errors. An F-based scorer, named M2, was used to score the systems (Dahlmeier and Ng, 2012). The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall. Two types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train th"
P16-1208,W13-1703,0,0.365025,"3 presents error analysis. In Section 4, we develop classifier and MT systems that make use of the strengths of each framework. Section 5 shows how to combine the two approaches. Section 6 concludes. 2 Related Work We first introduce the CoNLL-2014 shared task and briefly describe the state-of-the-art GEC systems in the competition and beyond. Next, an overview of the two leading methods is presented. 2.1 CoNLL-2014 shared task and approaches CoNLL-2014 training data (henceforth CoNLLtrain) is a corpus of learner essays (1.2M words) written by students at the National University of Singapore (Dahlmeier et al., 2013), corrected and error-tagged. The CoNLL-2013 test set was included in CoNLL-2014 and is used as development. Both the development and the test sets are also from the student population studying at the same University but annotated separately. We report results on the CoNLL-2014 test. The annotation includes specifying the relevant correction as well as the information about each error type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors"
P16-1208,W11-2838,0,0.0275862,"Missing"
P16-1208,W12-2006,0,0.0326593,"Missing"
P16-1208,P11-2071,0,0.0134095,"incorrect adjectival form, an error that is typically not modeled with standard classifiers. 3.2 Generalizability Because MT systems extract error/correction pairs from phrase-translation tables, they can only identify erroneous surface forms observed in training and propose corrections that occurred with the corresponding surface forms. Crucially, in a standard MT scenario, any resulting translation consists of “matches” mined from the translation tables, so a standard MT model lacks lexical abstractions that might help generalize, thus out-of-vocabulary words is a well-known problem in MT (Daume and Jagarlamudi, 2011). While more advanced MT models can abstract by adding higher-level Error type Orthog./punc. (Mec) Article (ArtOrDet) Preposition (Prep) Noun number (Nn) Verb tense (Vt) Subj.-verb agr. (SVA) Verb form (Vform) Word form (Wform) AMU (MT) P R F0.5 61.6 16.3 39.6 38.0 10.9 25.4 54.9 10.4 29.5 49.6 43.2 48.2 30.2 9.3 20.8 48.3 14.9 33.3 40.5 16.8 31.8 59.0 36.6 52.6 CUUI (Classif.) P R F0.5 53.3 8.7 26.4 31.8 47.9 34.0 31.7 8.8 20.9 42.5 46.2 43.2 61.1 5.4 19.9 57.7 57.7 57.7 69.2 15.1 40.3 60.0 13.5 35.6 Table 6: Performance of MT and classifier systems from CoNLL-2014 on common errors. features"
P16-1208,W01-0502,1,0.719233,"Missing"
P16-1208,C08-1022,0,0.29075,"Missing"
P16-1208,W14-1702,0,0.436542,"grammar and usage mistakes that are not addressed by standard proofing tools. Recently, there has been a spike in research on grammatical error correction (GEC), correcting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the"
P16-1208,W12-2012,0,0.078999,"tems in this work. Comparison to existing state-of-the-art. System Performance P R F0.5 MT is trained on CoNLL-train MT 43.34 11.81 28.25 Spelling+MT 49.86 16.36 35.37 Article+MT 45.11 13.99 31.22 Verb agr.+MT 46.36 14.63 32.33 Art.+Verb agr.+Spell+MT 52.07 20.89 40.10 MT is trained on Lang-8 MT 66.15 15.11 39.48 Spelling+MT 65.87 16.94 41.75 Article+MT 63.81 17.70 41.95 Verb. agr.+MT 66.09 18.01 43.08 Art.+Verb agr.+Spell+MT 64.13 22.15 46.51 Table 13: Pipelines: select classifiers and MT. compile a list of patterns using CoNLL training data. We also use an off-the-shelf speller (Flor, 2012; Flor and Futagi, 2012). Results are shown in Table 12. Performance improves by almost 5 and 7 points for the native-trained system and for the best configuration of classifiers with supervision. Both systems also outperform the top CoNLL system, by 1 and 6 points, respectively. The result of 43.11 by the best classifier configuration substantially outperforms the existing state-of-the-art: a combination of two MT systems and two classifier systems, and MT with re-ranking (Susanto et al., 2014; Mizumoto and Matsumoto, 2016). 5 Combining MT and Classifier Systems Since MT and classifiers differ with respect to the ty"
P16-1208,I08-1059,0,0.383036,"ear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions"
P16-1208,han-etal-2010-using,0,0.0573728,"ce of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors in"
P16-1208,W09-0408,0,0.0627986,"Missing"
P16-1208,P13-2121,0,0.0132949,"able 6. 4.1 Machine Translation Systems A key advantage of the MT framework is that, unlike with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input. We build two MT systems that differ only in the use of parallel data: the CoNLL2014 training data and Lang-8. Our MT systems are trained using Moses (Koehn et al., 2007) and follow the standard approach (Junczys-Dowmunt and Grundkiewicz, 2014; Susanto et al., 2014). Both systems use two 5-gram language models – English Wikipedia and the corrected side of CoNLL-train – trained with KenLM (Heafield et al., 2013). Table 9 reports the performance of the systems. As shown, performance increases by more than 11 points when a larger parallel corpus is used. The best MT system outperforms the top CoNLL system by 2 points. 4.2 Classifiers We now present several classifier systems, exploring the two important properties of the classification framework – the ability to train without super2210 Parallel data CoNLL-train Lang-8 CoNLL-2014 top 1 Performance P R F0.5 43.34 11.81 28.25 66.15 15.11 39.48 39.71 30.10 37.33 System Classifiers (learner) Classifiers (native) MT CoNLL-2014 top 1 CoNLL-2014 top 2 CoNLL-20"
P16-1208,P03-2026,0,0.117514,"h by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizabili"
P16-1208,W14-1703,0,0.518218,"orrecting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al.,"
P16-1208,N03-1017,0,0.0303842,"rs of the translation model are estimated from a parallel corpus, i.e. the set of foreign sentences and their corresponding translations into the target language. In error correction, the task is cast as translating from erroneous learner writing into corrected well-formed English. The MT approach relies on the availability of a parallel corpus for learning the translation model. In case of error correction, a set of learner sentences and their corrections functions as a parallel corpus. State-of-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon (Koehn et al., 2003). The resulting lexicon consists of a list of pairs (seqf , seqe ) where seqf is a sequence of one or more foreign words, seqe is a predicted translation. Each pair comes with an associated score. At decoding time, all phrases from sentence f are collected with their corresponding translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = a"
P16-1208,P07-2045,0,0.0255458,"ponents and show how to exploit the strengths of each framework in combination. Table 8 summarizes the data used. Results are reported with respect to all errors in the test data. This is different from performance for individual errors in Table 6. 4.1 Machine Translation Systems A key advantage of the MT framework is that, unlike with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input. We build two MT systems that differ only in the use of parallel data: the CoNLL2014 training data and Lang-8. Our MT systems are trained using Moses (Koehn et al., 2007) and follow the standard approach (Junczys-Dowmunt and Grundkiewicz, 2014; Susanto et al., 2014). Both systems use two 5-gram language models – English Wikipedia and the corrected side of CoNLL-train – trained with KenLM (Heafield et al., 2013). Table 9 reports the performance of the systems. As shown, performance increases by more than 11 points when a larger parallel corpus is used. The best MT system outperforms the top CoNLL system by 2 points. 4.2 Classifiers We now present several classifier systems, exploring the two important properties of the classification framework – the ability to"
P16-1208,de-marneffe-etal-2006-generating,0,0.0450643,"Missing"
P16-1208,N16-1133,0,0.423966,"hermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al., 2014; Mizumoto and Matsumoto, 2016). The purpose of this work is to gain a better understanding of the values offered by each method and to facilitate progress on the task, building on the advantages of each approach. Through better understanding of the methods, we exploit the strengths of each technique and, building on existing architecture, develop superior systems within 2205 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2205–2215, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics each framework. Further combination of these systems yields e"
P16-1208,I11-1017,0,0.355369,"wo types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners’ Corpus and the Lang-8 corpus (Mizumoto et al., 2011), respectively. Many teams also used native English datasets. The most common ones are the Web1T corpus (Brants and Franz, 2006), the CommonCrawl dataset, which is similar to Web1T, and the English Wikipedia. Several teams used off-the-shelf spellcheckers. In addition, Susanto et al. (2014) made an attempt at combining MT and classifiers. They used CoNLL-train and Lang-8 as non-native data and English Wikipedia as native data. We believe that the reason this study did not yield significant improvements (Table 1) is that individual strengths of each framework have not been fully exploited. Furt"
P16-1208,W13-3601,0,0.130237,"Missing"
P16-1208,W14-1701,0,0.698888,"or type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors are related to grammar but also include mechanical, collocation, and other errors. An F-based scorer, named M2, was used to score the systems (Dahlmeier and Ng, 2012). The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall. Two types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners’ Corpus and the Lang-8 corpus (Mizumoto et al., 2011), respectively. Many teams also used native English datasets. The most common ones are the Web1T corpus (Brants and Franz, 200"
P16-1208,P02-1038,0,0.0245171,"f-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon (Koehn et al., 2003). The resulting lexicon consists of a list of pairs (seqf , seqe ) where seqf is a sequence of one or more foreign words, seqe is a predicted translation. Each pair comes with an associated score. At decoding time, all phrases from sentence f are collected with their corresponding translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice an"
P16-1208,P11-1094,0,0.0198862,"or type, a confusion set is specified and includes a list of confusable words. For some errors, confusion sets are constructed using a closed list (e.g. prepositions). For other error types, NLP tools are required. To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems an"
P16-1208,rizzolo-roth-2010-learning,1,0.266301,"32.15 17.96 27.76 38.41 23.05 33.89 43.34 11.81 28.25 39.71 30.10 37.33 41.78 24.88 36.79 41.62 21.40 35.01 Table 10: Classifier systems trained with and without supervision. Learner data refers to CoNLL-train. Native data refers to Web1T. The MT system uses CoNLL-train for parallel data. yakanok and Roth, 2001), a syntactic parser (Klein and Manning, 2003) and a dependency converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares th"
P16-1208,D10-1094,1,0.642674,"is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact. (2) Generalizibility refers to t"
P16-1208,N10-1018,1,0.666684,"is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact. (2) Generalizibility refers to t"
P16-1208,P11-1093,1,0.752827,"lassifiers. Training without supervision is possible in the classification framework, as follows. For a given mistake type, e.g. preposition, a classifier is trained on native data that is assumed to be correct; the classifier uses context words around each preposition as features. The resulting model is then applied to learner prepositions and will predict the most likely preposition in a given context. If the preposition predicted by the classifier is different from what the author used in text, this preposition is flagged as a mistake. We refer the reader to Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) for a description of training classifiers with and without supervision for error correction tasks. Below, we address two questions related to the use of supervision: • Training with supervision: When training using learner data, how does a classifier-based system compare against an MT system? • Training without supervision: How well can we do by building a classifier system with native data only, compared to MT and classifier-based systems that use supervision? Our classifier system is based on the implementation framework of the second CoNLL-2014 system (Rozovskaya et al., 2014) and consists"
P16-1208,D13-1074,1,0.826188,"other error types, NLP tools are required. To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and c"
P16-1208,Q14-1033,1,0.922797,"cy converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train. The first classifier system performs comparably to the MT system (27.76 vs. 28.25), however, the native-trained classifier system outperforms both, and does not use any annotated data. The native-trained classifier system would place fourth in CoNLL-2014. 4.2.2 Flexibility We now explore another advantage of the classifier-based a"
P16-1208,W11-2843,1,0.882666,"to CoNLL-train. Native data refers to Web1T. The MT system uses CoNLL-train for parallel data. yakanok and Roth, 2001), a syntactic parser (Klein and Manning, 2003) and a dependency converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train. The first classifier system performs comparably to the MT system (27.76 vs. 28.25), however, the native-trained classifier system outperforms both, and"
P16-1208,D14-1102,0,0.267908,"as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al., 2014; Mizumoto and Matsumoto, 2016)."
P16-1208,P10-2065,0,0.0875681,"orrection maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation:"
P16-1208,W14-1704,1,\N,Missing
P16-2063,J92-4003,0,0.0764577,"Missing"
P16-2063,W06-2920,0,0.0120663,"fact, result in almost the same vectors as produced by the original truncation method. 5 Conclusion Experiments on Dependency Parsing We also incorporate word embedding results into a downstream task, dependency parsing, to evaluate whether the truncated embedding results are still good features compared to the original features. We follow the setup of (Guo et al., 2015) in a monolingual setting3 . We train the parser with 5,000 iterations using different truncation settings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task (Buchholz and Marsi, 2006). We follow (Guo et al., 2015) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions. The results shown in Table 3 for dependency parsing are consistent with word similarity and paraphrasing. First, we see that binarization for CBOW and skipgram is again better than the truncation approach. Second, for truncation results, more bits leads to better results. With 8-bits, we can again obtain results similar to those"
P16-2063,E14-1049,0,0.0711899,"Missing"
P16-2063,P15-1144,0,0.0341449,"lov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets. These works indicate that, for some tasks, we do not need all the information encoded in “standard” word embeddings. Nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks. 3 for each value. In practice, we first scale all"
P16-2063,D14-1012,0,0.255319,"tational Linguistics, pages 387–392, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets. These works indicate that,"
P16-2063,P15-1119,0,0.0127636,"tend these experiments and study the impact of the representation size on more advanced tasks. Trunc. (8 b.) 0.997 0.992 1.001 0.999 ing method to converge. Auxiliary update vectors achieve very similar results to the original vectors, and, in fact, result in almost the same vectors as produced by the original truncation method. 5 Conclusion Experiments on Dependency Parsing We also incorporate word embedding results into a downstream task, dependency parsing, to evaluate whether the truncated embedding results are still good features compared to the original features. We follow the setup of (Guo et al., 2015) in a monolingual setting3 . We train the parser with 5,000 iterations using different truncation settings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task (Buchholz and Marsi, 2006). We follow (Guo et al., 2015) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions. The results shown in Table 3 for dependency parsing are consistent with word sim"
P16-2063,Q15-1016,0,0.0323312,"memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. 1 Introduction There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Levy et al., 2015). Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications that make use of billions of tokens and multiple languages, size issues impose significan"
P16-2063,N13-1090,0,0.421697,"er, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used. The results we present are quite surprising. We show that it is possible to reduce the memory consumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide two ways to train existing algorithms (Mikolov et al., 2013a; Mikolov et al., 2013b) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16. The truncated word"
P16-2063,W09-1119,1,0.501275,"Meeting of the Association for Computational Linguistics, pages 387–392, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datase"
P16-2063,P10-1040,0,0.0687955,"ion on word embeddings. We present a systematic evaluation of word embeddings with limited memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. 1 Introduction There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Levy et al., 2015). Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications"
P16-2063,Q15-1025,0,\N,Missing
P18-1122,S15-2136,0,0.0701952,"Missing"
P18-1122,bethard-etal-2012-annotating,0,0.0661422,"f anchorable events (i.e., the relation annotation step); finally, we can move to another axis and repeat the two steps above. Note that ruling out cross-axis relations is only a strategy we adopt in this paper to separate well-defined relations from ill-defined relations. We do not claim that cross-axis relations are unimportant; instead, as shown in Fig. 2, we think that cross-axis relations are a different semantic phenomenon that requires additional investigation. 1320 2.3 Comparisons with Existing Work There have been other proposals of temporal structure modelings (Bramsen et al., 2006; Bethard et al., 2012), but in general, the semantic phenomena handled in our work are very different and complementary to them. (Bramsen et al., 2006) introduces “temporal segments” (a fragment of text that does not exhibit abrupt changes) in the medical domain. Similarly, their temporal segments can also be considered as a special temporal structure modeling. But a key difference is that (Bramsen et al., 2006) only annotates inter-segment relations, ignoring intra-segment ones. Since those segments are usually large chunks of text, the semantics handled in (Bramsen et al., 2006) is in a very coarse granularity (a"
P18-1122,S17-2093,0,0.2391,"Missing"
P18-1122,W06-1623,0,0.651001,"annotate every pair of anchorable events (i.e., the relation annotation step); finally, we can move to another axis and repeat the two steps above. Note that ruling out cross-axis relations is only a strategy we adopt in this paper to separate well-defined relations from ill-defined relations. We do not claim that cross-axis relations are unimportant; instead, as shown in Fig. 2, we think that cross-axis relations are a different semantic phenomenon that requires additional investigation. 1320 2.3 Comparisons with Existing Work There have been other proposals of temporal structure modelings (Bramsen et al., 2006; Bethard et al., 2012), but in general, the semantic phenomena handled in our work are very different and complementary to them. (Bramsen et al., 2006) introduces “temporal segments” (a fragment of text that does not exhibit abrupt changes) in the medical domain. Similarly, their temporal segments can also be considered as a special temporal structure modeling. But a key difference is that (Bramsen et al., 2006) only annotates inter-segment relations, ignoring intra-segment ones. Since those segments are usually large chunks of text, the semantics handled in (Bramsen et al., 2006) is in a ver"
P18-1122,P14-2082,0,0.671089,"ataset, when compared with system performance in the literature (Sec. 6). The paper’s results are very encouraging and hopefully, this work would significantly benefit research in this area. 2 Temporal Structure of Events Given a set of events, one important question in designing the TempRel annotation task is: which pairs of events should have a relation? The answer to it depends on the modeling of the overall temporal structure of events. 2.1 Motivation TimeBank (Pustejovsky et al., 2003b) laid the foundation for many later TempRel corpora, e.g., (Bethard et al., 2007; UzZaman et al., 2013; Cassidy et al., 2014).2 In TimeBank, the annotators were allowed to label TempRels between any pairs of events. This setup models the overall structure of events using a general graph, which made annotators inadvertently overlook some pairs, resulting in low IAAs and many false negatives. Example 4: Dense Annotation Scheme. Serbian police (e7:tried) to (e8:eliminate) the proindependence Kosovo Liberation Army and (e1:restore) order. At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region. Given 4 N ON -G ENERIC events above, the dense scheme presents 6 pairs t"
P18-1122,Q14-1022,0,0.789337,"rt-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.1 1 Introduction Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b). Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007), (e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans. Note that only relevant events are highlighted here. Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order. At least 51 people were (e"
P18-1122,D12-1062,1,0.9424,"standing task that requires an understanding of the entire document or external knowledge. Interested readers are referred to Appendix B for a detailed analysis of the difference between Anchorable (onto the main axis) and Actual on a subset of RED. 3 are 13 relations between two intervals (for readers who are not familiar with it, please see Fig. 4 in the appendix). To reduce the burden of annotators, existing schemes often resort to a reduced set of the 13 relations. For instance, Verhagen et al. (2007) merged all the overlap relations into a single relation, overlap. Bethard et al. (2007); Do et al. (2012); O’Gorman et al. (2016) all adopted this strategy. In Cassidy et al. (2014), they further split overlap into includes, included and equal. Let [t1start , t1end ] and [t2start , t2end ] be the time intervals of two events (with the implicit assumption that tstart  tend ). Instead of reducing the relations between two intervals, we try to explicitly compare the time points (see Fig. 3). In this way, the label set is simply before, after and equal,7 while the expressivity remains the same. This interval splitting technique has also been used in (Raghavan et al., 2012). Interval Splitting All ex"
P18-1122,D15-1189,0,0.0319681,"he deep-seated causes that have resulted in these problems. Example 6: Intentions may not always be fulfilled. A passerby called the police to (e11:report) the body. A passerby called the police to (e12:report) the body. Unfortunately, the line was busy. I asked everyone to (e13:tell) the truth. 2.3.3 Differences from Factuality Event modality have been discussed in many existing event annotation schemes, e.g., Event Nugget (Mitamura et al., 2015), Rich ERE (Song et al., 2015), and RED. Generally, an event is classified as Actual or Non-Actual, a.k.a. factuality (Saur´ı and Pustejovsky, 2009; Lee et al., 2015). The main-axis events defined in this paper seem to be very similar to Actual events, but with several important differences: First, future events are Non-Actual because they indeed have not happened, but they may be on the main axis. Second, events that are not on the main axis can also be Actual events, e.g., intentions that are fulfilled, or opinions that are true. Third, as demonstrated by Examples 5-6, identifying anchorability as defined in Table 1 is relatively easy, but judging if an event actually happened is often a high-level understanding task that requires an understanding of the"
P18-1122,E17-1108,0,0.188757,"ment in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.1 1 Introduction Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b). Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007), (e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans. Note that only relevant events are highlighted here. Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order. At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region. Example 2: Service i"
P18-1122,S15-2134,0,0.258367,"expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.1 1 Introduction Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b). Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007), (e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans. Note that only relevant events are highlighted here. Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order. At least 51 people were (e2:killed) in clashes b"
P18-1122,W15-0809,0,0.0273345,"ing for I NTENTIONS and O PINIONS. Example 5: Opinion events may not always be true. He is ostracized by the West for (e9:sponsoring) terrorism. We need to (e10:resolve) the deep-seated causes that have resulted in these problems. Example 6: Intentions may not always be fulfilled. A passerby called the police to (e11:report) the body. A passerby called the police to (e12:report) the body. Unfortunately, the line was busy. I asked everyone to (e13:tell) the truth. 2.3.3 Differences from Factuality Event modality have been discussed in many existing event annotation schemes, e.g., Event Nugget (Mitamura et al., 2015), Rich ERE (Song et al., 2015), and RED. Generally, an event is classified as Actual or Non-Actual, a.k.a. factuality (Saur´ı and Pustejovsky, 2009; Lee et al., 2015). The main-axis events defined in this paper seem to be very similar to Actual events, but with several important differences: First, future events are Non-Actual because they indeed have not happened, but they may be on the main axis. Second, events that are not on the main axis can also be Actual events, e.g., intentions that are fulfilled, or opinions that are true. Third, as demonstrated by Examples 5-6, identifying anchorabil"
P18-1122,W16-1007,0,0.18689,"Missing"
P18-1122,D17-1108,1,0.832238,"onal 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.1 1 Introduction Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b). Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007), (e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans. Note that only relevant events are highlighted here. Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order. At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region. Example 2: Service industries (e3:showe"
P18-1122,N18-1077,1,0.773945,"Missing"
P18-1122,S18-2018,1,0.88186,"Missing"
P18-1122,W16-5706,0,0.309363,"Missing"
P18-1122,P12-2014,0,0.0715499,"Missing"
P18-1122,P16-1207,0,0.380391,"rers were originally expected to be hardest e4:hit but actually e3:showed gains, so e4:hit is before e3:showed; however, one can also argue that the two areas had showed gains but had not been hit, so e4:hit is after e3:showed. Again, e5:rebuilding is a hypothetical event: “we will act if rebuilding is true”. Readers do not know for sure if “he is already rebuilding weapons but we have no evidence”, or “he will be building weapons in the future”, so annotators may disagree on the relation between e5:rebuilding and e6:responded. Despite, importantly, minimizing missing annota2 EventTimeCorpus (Reimers et al., 2016) is based on TimeBank, but aims at anchoring events onto explicit time expressions in each document rather than annotating TempRels between events, which can be a good complementary to other TempRel datasets. 3 For example, lions eat meat is G ENERIC. 1319 Event Type I NTENTION, O PINION H YPOTHESIS, G ENERIC N EGATION S TATIC, R ECURRENT tions, the current dense scheme forces annotators to label many such ill-defined pairs, resulting in low IAA. 2.2 Multi-Axis Modeling Arguably, an ideal annotator may figure out the above ambiguity by him/herself and mark them as vague, but it is not a feasib"
P18-1122,W15-0812,0,0.0341553,"Example 5: Opinion events may not always be true. He is ostracized by the West for (e9:sponsoring) terrorism. We need to (e10:resolve) the deep-seated causes that have resulted in these problems. Example 6: Intentions may not always be fulfilled. A passerby called the police to (e11:report) the body. A passerby called the police to (e12:report) the body. Unfortunately, the line was busy. I asked everyone to (e13:tell) the truth. 2.3.3 Differences from Factuality Event modality have been discussed in many existing event annotation schemes, e.g., Event Nugget (Mitamura et al., 2015), Rich ERE (Song et al., 2015), and RED. Generally, an event is classified as Actual or Non-Actual, a.k.a. factuality (Saur´ı and Pustejovsky, 2009; Lee et al., 2015). The main-axis events defined in this paper seem to be very similar to Actual events, but with several important differences: First, future events are Non-Actual because they indeed have not happened, but they may be on the main axis. Second, events that are not on the main axis can also be Actual events, e.g., intentions that are fulfilled, or opinions that are true. Third, as demonstrated by Examples 5-6, identifying anchorability as defined in Table 1 is r"
P18-1122,Q14-1012,0,0.0720828,"Missing"
P18-1122,S13-2001,0,0.710846,"TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.1 1 Introduction Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b). Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007), (e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans. Note that only relevant events are highlighted here. Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order. At"
P18-1122,S07-1014,0,0.541985,"as defined in Table 1 is relatively easy, but judging if an event actually happened is often a high-level understanding task that requires an understanding of the entire document or external knowledge. Interested readers are referred to Appendix B for a detailed analysis of the difference between Anchorable (onto the main axis) and Actual on a subset of RED. 3 are 13 relations between two intervals (for readers who are not familiar with it, please see Fig. 4 in the appendix). To reduce the burden of annotators, existing schemes often resort to a reduced set of the 13 relations. For instance, Verhagen et al. (2007) merged all the overlap relations into a single relation, overlap. Bethard et al. (2007); Do et al. (2012); O’Gorman et al. (2016) all adopted this strategy. In Cassidy et al. (2014), they further split overlap into includes, included and equal. Let [t1start , t1end ] and [t2start , t2end ] be the time intervals of two events (with the implicit assumption that tstart  tend ). Instead of reducing the relations between two intervals, we try to explicitly compare the time points (see Fig. 3). In this way, the label set is simply before, after and equal,7 while the expressivity remains the same."
P18-1180,K17-2001,0,0.0233174,"fficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work. The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b). They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Sch¨utze, 2016; Cotterell et al., 2016). We use this as our baseline. ing derivational transformations (Gladkova et al., 2016). However, we use more expressive, nonlinear functions to model derivational transformations and report positive results. Gupta et al. (2017) then learn a linear transformation per orthographic rule to solve a word analogy task. Our distributional model learns a function per derivational transformation, not per orthographic rule, which allows it to generalize to unseen orthography. Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into the inference algorithm at little additional cost. Roth and Yih (2004, 2007) propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program. Our model"
P18-1180,W16-2002,0,0.0126537,"ords in bold mark the correct derivations. “Hindrance” and “vacancy” are the expected derived words for the last two rows. 2013; Rastogi et al., 2016; Hulden et al., 2014). It is a structurally similar task to ours, but does not have the same difficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work. The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b). They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Sch¨utze, 2016; Cotterell et al., 2016). We use this as our baseline. ing derivational transformations (Gladkova et al., 2016). However, we use more expressive, nonlinear functions to model derivational transformations and report positive results. Gupta et al. (2017) then learn a linear transformation per orthographic rule to solve a word analogy task. Our distributional model learns a function per derivational transformation, not per orthographic rule, which allows it to generalize to unseen orthography. Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into th"
P18-1180,D17-1074,0,0.505249,"Missing"
P18-1180,N13-1138,0,0.125653,"Missing"
P18-1180,N16-1077,0,0.0183911,"ional cost. Roth and Yih (2004, 2007) propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program. Our models are implemented in Python using the DyNet deep learning library (Neubig et al., 2017). The code is freely available for download.4 Beam or hypothesis rescoring to incorporate an expensive or non-decomposable signal into search has a history in machine translation (Huang and Chiang, 2007). In inflectional morphology, Nicolai et al. (2015) use this idea to rerank hypotheses using orthographic features and Faruqui et al. (2016) use a character-level language model. Our approach is similar to Faruqui et al. (2016) in that we use statistics from a raw corpus, but at the token level. There have been several attempts to use distributional information in morphological generation and analysis. Soricut and Och (2015) collect pairs of words related by any morphological change in an unsupervised manner, then select a vector offset which best explains their observations. There has been subsequent work exploring the vector offset method, finding it unsuccessful in captur9 Implementation Details Sequence Model The sequence-to-s"
P18-1180,N16-2002,0,0.0254033,"rived words for the last two rows. 2013; Rastogi et al., 2016; Hulden et al., 2014). It is a structurally similar task to ours, but does not have the same difficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work. The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b). They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Sch¨utze, 2016; Cotterell et al., 2016). We use this as our baseline. ing derivational transformations (Gladkova et al., 2016). However, we use more expressive, nonlinear functions to model derivational transformations and report positive results. Gupta et al. (2017) then learn a linear transformation per orthographic rule to solve a word analogy task. Our distributional model learns a function per derivational transformation, not per orthographic rule, which allows it to generalize to unseen orthography. Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into the inference algorithm at little additional cost. Roth and Yih (2004, 2007) propose a ge"
P18-1180,E14-1060,0,0.0217419,"NT S UBJECT N OMINAL AGENT N OMINAL approval bankruptcy irreparably connectivity strolls emigre ubiquity hinderer vacance approvation bankruption irretrievably connection stroller emigrator ubiquit hinderer vacance approval bankruptcy irretrievably connection stroller emigrator ubiquit hinderer vacance approval bankruptcy irretrievably connection stroller emigrant ubiquity hinderer vacance Table 6: Sample output from a selection of models. The words in bold mark the correct derivations. “Hindrance” and “vacancy” are the expected derived words for the last two rows. 2013; Rastogi et al., 2016; Hulden et al., 2014). It is a structurally similar task to ours, but does not have the same difficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work. The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b). They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Sch¨utze, 2016; Cotterell et al., 2016). We use this as our baseline. ing derivational transformations (Gladkova et al., 2016). However, we use more expressive, nonlinear functions to mod"
P18-1180,P16-2090,0,0.026192,"Missing"
P18-1180,D15-1166,0,0.0107567,"tterell et al., 2017b; Bahdanau et al., 2014). The input to the bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) is the sequence #, x1 , x2 , . . . xm , #, t, where # is a special symbol to denote the start and end of a word, and the encoding of the derivational transformation t is concatenated to the input characters. The model is trained to minimize the cross entropy of the training data. We refer to our reimplementation of this model as S EQ. For a more detailed treatment of neural sequenceto-sequence models with attention, we direct the reader to Luong et al. (2015). 3.2 The goal of decoding is to find the most probable ˆ conditioned on some observation x and structure y transformation t. That is, the problem is to solve Dictionary Constraint The suffix ambiguity problem poses challenges for models which rely exclusively on input characters for information. As previously demonstrated, words derived via the same transformation may take different suffixes, and it is hard to select among them based on character information alone. Here, we describe a process for restricting our inference procedure to only generate known English words, which we call a diction"
P18-1180,W13-3512,0,0.143204,"Missing"
P18-1180,W04-2705,0,0.0713595,"Missing"
P18-1180,N15-1093,0,0.0369151,"onstraint which can be directly incorporated into the inference algorithm at little additional cost. Roth and Yih (2004, 2007) propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program. Our models are implemented in Python using the DyNet deep learning library (Neubig et al., 2017). The code is freely available for download.4 Beam or hypothesis rescoring to incorporate an expensive or non-decomposable signal into search has a history in machine translation (Huang and Chiang, 2007). In inflectional morphology, Nicolai et al. (2015) use this idea to rerank hypotheses using orthographic features and Faruqui et al. (2016) use a character-level language model. Our approach is similar to Faruqui et al. (2016) in that we use statistics from a raw corpus, but at the token level. There have been several attempts to use distributional information in morphological generation and analysis. Soricut and Och (2015) collect pairs of words related by any morphological change in an unsupervised manner, then select a vector offset which best explains their observations. There has been subsequent work exploring the vector offset method, f"
P18-1180,N16-1076,0,0.0176773,"AL A DVERB R ESULT AGENT S UBJECT N OMINAL AGENT N OMINAL approval bankruptcy irreparably connectivity strolls emigre ubiquity hinderer vacance approvation bankruption irretrievably connection stroller emigrator ubiquit hinderer vacance approval bankruptcy irretrievably connection stroller emigrator ubiquit hinderer vacance approval bankruptcy irretrievably connection stroller emigrant ubiquity hinderer vacance Table 6: Sample output from a selection of models. The words in bold mark the correct derivations. “Hindrance” and “vacancy” are the expected derived words for the last two rows. 2013; Rastogi et al., 2016; Hulden et al., 2014). It is a structurally similar task to ours, but does not have the same difficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work. The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b). They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Sch¨utze, 2016; Cotterell et al., 2016). We use this as our baseline. ing derivational transformations (Gladkova et al., 2016). However, we use more expressive, nonl"
P18-1180,W04-2401,1,0.504962,"ransformations (Gladkova et al., 2016). However, we use more expressive, nonlinear functions to model derivational transformations and report positive results. Gupta et al. (2017) then learn a linear transformation per orthographic rule to solve a word analogy task. Our distributional model learns a function per derivational transformation, not per orthographic rule, which allows it to generalize to unseen orthography. Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into the inference algorithm at little additional cost. Roth and Yih (2004, 2007) propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program. Our models are implemented in Python using the DyNet deep learning library (Neubig et al., 2017). The code is freely available for download.4 Beam or hypothesis rescoring to incorporate an expensive or non-decomposable signal into search has a history in machine translation (Huang and Chiang, 2007). In inflectional morphology, Nicolai et al. (2015) use this idea to rerank hypotheses using orthographic features and Faruqui et al. (2016) use a ch"
P18-1180,Q15-1026,0,0.0457882,"Missing"
P18-1180,N15-1186,0,0.0588167,"Missing"
P18-1212,P08-2045,0,0.0649768,"f implicit markers called AltLex. A classifier was then applied to identify causality. Dunietz et al. (2017) used the concept of construction grammar to tag causally related clauses or phrases. Do et al. (2011) considered global statistics over a large corpora, the cause-effect association (CEA) scores, and combined it with discourse relations using ILP to identify causal relations. These work only focused on the causality task and did not address the temporal aspect. However, as illustrated by Examples 1-2, temporal and causal relations are closely related, as assumed by many existing works (Bethard and Martin, 2008; Rink et al., 2010). Here we argue that being able to capture both aspects in a joint framework provides a more complete understanding of events in natural language documents. Researchers have started paying attention to this direction recently. For example, Mostafazadeh et al. (2016b) proposed an annotation framework, CaTeRs, which captured both temporal and causal aspects of event relations in common sense stories. CATENA (Mirza and Tonelli, 2016) extended the multi-sieve framework of CAEVO to extracting both temporal and causal relations and exploited their interaction through post-editing"
P18-1212,W06-1623,0,0.645368,"tage of these early methods is that the resulting graph may break the symmetric and transitive constraints. There are conceptually two ways to enforce such graph constraints (i.e., global reasoning). CAEVO (Chambers et al., 2014) grows the temporal graph in a multi-sieve manner, where predictions are added sieve-by-sieve. A graph closure operation had to be performed after each sieve to enforce constraints. This is solving the global inference problem greedily. A second way is to perform exact inference via ILP and the symmetry and transitivity requirements can be enforced as ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). We adopt the ILP approach in the temporal component of this work for two reasons. First, as we show later, it is straightforward to build a joint framework with both temporal and causal relations as an extension of it. Second, the relation between a pair of events is often determined by the relations among other events. In Ex 3, if a system is unaware of (e5, e6)=simultaneously when trying to make a decision for (e5, e7), it is likely to predict that e5 is before e72 ; but, in fact, (e5, e7)=after given"
P18-1212,P14-2082,0,0.825593,"e temporal annotations. This dataset allows us to show statistically significant improvements on both relations via the proposed joint framework. This paper also presents an empirical result of improving the temporal extraction component. Specifically, we incorporate explicit time expressions present in the text and high-precision knowledge-based rules into the ILP objective. These sources of information have been successfully adopted by existing methods (Chambers et al., 2014; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), Ex 3: Global considerations are needed when making local decisi"
P18-1212,S13-2012,0,0.13654,"d improves for both relations. 4.1 Temporal Performance on TB-Dense Multiple datasets with temporal annotations are available thanks to the TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The dataset we used here to demonstrate our improved temporal component was TB-Dense, which was annotated on top of 36 documents out of the classic TimeBank dataset (Pustejovsky et al., 2003). The main purpose of TB-Dense was to alleviate the known issue of sparse annotations in the evaluation dataset provided with TE3 (UzZaman et al., 2013), as pointed out in many previous work (Chambers, 2013; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017). Annotators of TB-Dense were forced to look at each pair of events or timexes within the same sentence or contiguous sentences, so that much fewer links were missed. Since causal link annotation is not available on TB-Dense, we only show our improvement in terms of temporal performance on TB-Dense. The standard train/dev/test split of TBDense was used and parameters were tuned to optimize the F1 performance on dev. Gold events and time expressions were also used as in existing systems. The contributions of each proposed informat"
P18-1212,Q14-1022,0,0.372443,"ork is the development of such a jointly annotated dataset which we did by augmenting the EventCausality dataset (Do et al., 2011) with dense temporal annotations. This dataset allows us to show statistically significant improvements on both relations via the proposed joint framework. This paper also presents an empirical result of improving the temporal extraction component. Specifically, we incorporate explicit time expressions present in the text and high-precision knowledge-based rules into the ILP objective. These sources of information have been successfully adopted by existing methods (Chambers et al., 2014; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; C"
P18-1212,P16-1135,0,0.0790352,"natural language text was relatively sparse. Many causal extraction work in other domains assumes the existence of ground truth timestamps (e.g., (Sun et al., 2007; G¨uler et al., 2016)), but gold timestamps rarely exist in natural language text. In NLP, people have focused on causal relation identification using lexical features or discourse relations. For 2 Consider the case that “The FAA e5:announced. . . it e7:said it would. . . ”. Even humans may predict that e5 is before e7. 2279 example, based on a set of explicit causal discourse markers (e.g., “because”, “due to”, and “as a result”), Hidey and McKeown (2016) built parallel Wikipedia articles and constructed an open set of implicit markers called AltLex. A classifier was then applied to identify causality. Dunietz et al. (2017) used the concept of construction grammar to tag causally related clauses or phrases. Do et al. (2011) considered global statistics over a large corpora, the cause-effect association (CEA) scores, and combined it with discourse relations using ILP to identify causal relations. These work only focused on the causality task and did not address the temporal aspect. However, as illustrated by Examples 1-2, temporal and causal re"
P18-1212,D08-1073,0,0.792441,"thods is that the resulting graph may break the symmetric and transitive constraints. There are conceptually two ways to enforce such graph constraints (i.e., global reasoning). CAEVO (Chambers et al., 2014) grows the temporal graph in a multi-sieve manner, where predictions are added sieve-by-sieve. A graph closure operation had to be performed after each sieve to enforce constraints. This is solving the global inference problem greedily. A second way is to perform exact inference via ILP and the symmetry and transitivity requirements can be enforced as ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). We adopt the ILP approach in the temporal component of this work for two reasons. First, as we show later, it is straightforward to build a joint framework with both temporal and causal relations as an extension of it. Second, the relation between a pair of events is often determined by the relations among other events. In Ex 3, if a system is unaware of (e5, e6)=simultaneously when trying to make a decision for (e5, e7), it is likely to predict that e5 is before e72 ; but, in fact, (e5, e7)=after given the existence of e6. Using g"
P18-1212,P06-1095,0,0.67145,"hambers et al., 2014; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), Ex 3: Global considerations are needed when making local decisions. The FAA on Friday (e5:announced) it will close 149 regional airport control towers because of forced spending cuts. Before Friday’s (e6:announcement), it (e7:said) it would consider keeping a tower open if the airport convinces the agency it is in the ”national interest” to do so. which formulated the problem as that of learning a classification model for determining the label of each edge locally (i.e., local methods). A disadvantage of these earl"
P18-1212,P07-2044,0,0.590983,"4; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), Ex 3: Global considerations are needed when making local decisions. The FAA on Friday (e5:announced) it will close 149 regional airport control towers because of forced spending cuts. Before Friday’s (e6:announcement), it (e7:said) it would consider keeping a tower open if the airport convinces the agency it is in the ”national interest” to do so. which formulated the problem as that of learning a classification model for determining the label of each edge locally (i.e., local methods). A disadvantage of these early methods is that the r"
P18-1212,C14-1198,0,0.0323757,"ion compared to System 8 is due to the lack of structured learning, and the low precision compared to System 7 is propagated from the baseline (System 1), which was tuned to maximize its F1 score. However, the effectiveness of the proposed information sources is already justified in Systems 1-5. 4.2 Joint Performance on Our New Dataset 4.2.1 Data Preparation TB-Dense only has temporal relation annotations, so in the evaluations above, we only evaluated our temporal performance. One existing dataset with both temporal and causal annotations available is the Causal-TimeBank dataset (Causal-TB) (Mirza and Tonelli, 2014). However, Causal-TB is sparse in temporal annotations and is even sparser in causal annotations: In Table 3, we can see that with four times more documents, Causal-TB still has fewer temporal relations (denoted as T-Links therein), compared to TB-Dense; as for causal relations (C-Links), it has less than two causal relations in each document on average. Note that the T-Link sparsity of Causal-TB originates from TimeBank, which is known to have missing links (Cassidy et al., 2014; Ning et al., 2017). The CLink sparsity was a design choice of Causal-TB in which C-Links were annotated based on o"
P18-1212,C16-1007,0,0.674161,"of such a jointly annotated dataset which we did by augmenting the EventCausality dataset (Do et al., 2011) with dense temporal annotations. This dataset allows us to show statistically significant improvements on both relations via the proposed joint framework. This paper also presents an empirical result of improving the temporal extraction component. Specifically, we incorporate explicit time expressions present in the text and high-precision knowledge-based rules into the ILP objective. These sources of information have been successfully adopted by existing methods (Chambers et al., 2014; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; Chambers et al., 2007; Beth"
P18-1212,D11-1027,1,0.902314,"and the physical nature of causality and build connections between temporal and causal relations, making CCM a natural choice for this problem. As far as we know, very limited work has been done in joint extraction of both relations. Formulating the joint problem in the CCM framework is novel and thus the first contribution of this work. A key obstacle in jointly studying temporal and causal relations lies in the absence of jointly annotated data. The second contribution of this work is the development of such a jointly annotated dataset which we did by augmenting the EventCausality dataset (Do et al., 2011) with dense temporal annotations. This dataset allows us to show statistically significant improvements on both relations via the proposed joint framework. This paper also presents an empirical result of improving the temporal extraction component. Specifically, we incorporate explicit time expressions present in the text and high-precision knowledge-based rules into the ILP objective. These sources of information have been successfully adopted by existing methods (Chambers et al., 2014; Mirza and Tonelli, 2016), but were never used within a global ILP-based inference method. Results on TimeBa"
P18-1212,D12-1062,1,0.920855,"tric and transitive constraints. There are conceptually two ways to enforce such graph constraints (i.e., global reasoning). CAEVO (Chambers et al., 2014) grows the temporal graph in a multi-sieve manner, where predictions are added sieve-by-sieve. A graph closure operation had to be performed after each sieve to enforce constraints. This is solving the global inference problem greedily. A second way is to perform exact inference via ILP and the symmetry and transitivity requirements can be enforced as ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). We adopt the ILP approach in the temporal component of this work for two reasons. First, as we show later, it is straightforward to build a joint framework with both temporal and causal relations as an extension of it. Second, the relation between a pair of events is often determined by the relations among other events. In Ex 3, if a system is unaware of (e5, e6)=simultaneously when trying to make a decision for (e5, e7), it is likely to predict that e5 is before e72 ; but, in fact, (e5, e7)=after given the existence of e6. Using global considerations is thus beneficial i"
P18-1212,Q17-1009,0,0.0553475,"al., 2016)), but gold timestamps rarely exist in natural language text. In NLP, people have focused on causal relation identification using lexical features or discourse relations. For 2 Consider the case that “The FAA e5:announced. . . it e7:said it would. . . ”. Even humans may predict that e5 is before e7. 2279 example, based on a set of explicit causal discourse markers (e.g., “because”, “due to”, and “as a result”), Hidey and McKeown (2016) built parallel Wikipedia articles and constructed an open set of implicit markers called AltLex. A classifier was then applied to identify causality. Dunietz et al. (2017) used the concept of construction grammar to tag causally related clauses or phrases. Do et al. (2011) considered global statistics over a large corpora, the cause-effect association (CEA) scores, and combined it with discourse relations using ILP to identify causal relations. These work only focused on the causality task and did not address the temporal aspect. However, as illustrated by Examples 1-2, temporal and causal relations are closely related, as assumed by many existing works (Bethard and Martin, 2008; Rink et al., 2010). Here we argue that being able to capture both aspects in a joi"
P18-1212,N16-1098,0,0.102694,"iation (CEA) scores, and combined it with discourse relations using ILP to identify causal relations. These work only focused on the causality task and did not address the temporal aspect. However, as illustrated by Examples 1-2, temporal and causal relations are closely related, as assumed by many existing works (Bethard and Martin, 2008; Rink et al., 2010). Here we argue that being able to capture both aspects in a joint framework provides a more complete understanding of events in natural language documents. Researchers have started paying attention to this direction recently. For example, Mostafazadeh et al. (2016b) proposed an annotation framework, CaTeRs, which captured both temporal and causal aspects of event relations in common sense stories. CATENA (Mirza and Tonelli, 2016) extended the multi-sieve framework of CAEVO to extracting both temporal and causal relations and exploited their interaction through post-editing temporal relations based on causal predictions. In this paper, we push this idea forward and tackle the problem in a joint and more principled way, as shown next. 3 Temporal and Causal Reasoning In this section, we explain the proposed joint inference framework, Temporal and Causal R"
P18-1212,W16-1007,0,0.285984,"iation (CEA) scores, and combined it with discourse relations using ILP to identify causal relations. These work only focused on the causality task and did not address the temporal aspect. However, as illustrated by Examples 1-2, temporal and causal relations are closely related, as assumed by many existing works (Bethard and Martin, 2008; Rink et al., 2010). Here we argue that being able to capture both aspects in a joint framework provides a more complete understanding of events in natural language documents. Researchers have started paying attention to this direction recently. For example, Mostafazadeh et al. (2016b) proposed an annotation framework, CaTeRs, which captured both temporal and causal aspects of event relations in common sense stories. CATENA (Mirza and Tonelli, 2016) extended the multi-sieve framework of CAEVO to extracting both temporal and causal relations and exploited their interaction through post-editing temporal relations based on causal predictions. In this paper, we push this idea forward and tackle the problem in a joint and more principled way, as shown next. 3 Temporal and Causal Reasoning In this section, we explain the proposed joint inference framework, Temporal and Causal R"
P18-1212,D17-1108,1,0.50557,"ve constraints. There are conceptually two ways to enforce such graph constraints (i.e., global reasoning). CAEVO (Chambers et al., 2014) grows the temporal graph in a multi-sieve manner, where predictions are added sieve-by-sieve. A graph closure operation had to be performed after each sieve to enforce constraints. This is solving the global inference problem greedily. A second way is to perform exact inference via ILP and the symmetry and transitivity requirements can be enforced as ILP constraints (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017). We adopt the ILP approach in the temporal component of this work for two reasons. First, as we show later, it is straightforward to build a joint framework with both temporal and causal relations as an extension of it. Second, the relation between a pair of events is often determined by the relations among other events. In Ex 3, if a system is unaware of (e5, e6)=simultaneously when trying to make a decision for (e5, e7), it is likely to predict that e5 is before e72 ; but, in fact, (e5, e7)=after given the existence of e6. Using global considerations is thus beneficial in this context not o"
P18-1212,N18-1077,1,0.902867,"r3 yi,j + yj,k − yi,k ≤ 1 (transitivity) r3 ∈Trans(r1 ,r2 ) wrT ϕ(i) see {i 7→ r} = ∑ , i ∈ EE, r ∈ RT , T r ′ ∈RT wr ′ ϕ(i) where {wr } is the learned weight vector for relation r ∈ RT and ϕ(i) is the feature vector for pair i ∈ EE. Given a pair of ordered events, we need sc (·) to estimate the scores of them being “causes” or where r¯ is the reverse relation of r (i.e., ¯b = a, ¯i = ii, s¯ = s, and v¯ = v), and Trans(r1 , r2 ) is defined in Table 1. As for the transitivity constraints, 3 https://catalog.ldc.upenn.edu/ LDC2008T19, which is disjoint to the test set used here. Please refer to (Ning et al., 2018a) for more analysis on using this corpus to acquire prior knowledge that aids temporal relation classification. 2282 r1 r2 if both yi,j and yj,k are 1, then the constraint rer3 quires at least one of yi,k , r3 ∈ Trans(r1 , r2 ) to be 1, which means the relation between i and k has to be chosen from Trans(r1 , r2 ), which is exactly what Y1 is intended to do. The rules in Y2 is written as Y2 : yjr = I{rule(j)=r} , ∀j ∈ J (rule) (linguistic rules) where rule(j) and J (rule) have been defined in Eq. (2). Converting the T T constraints, i.e., Y0 , into constraints is as straightforward as Y2 , so"
P18-1212,P18-1122,1,0.868391,"r3 yi,j + yj,k − yi,k ≤ 1 (transitivity) r3 ∈Trans(r1 ,r2 ) wrT ϕ(i) see {i 7→ r} = ∑ , i ∈ EE, r ∈ RT , T r ′ ∈RT wr ′ ϕ(i) where {wr } is the learned weight vector for relation r ∈ RT and ϕ(i) is the feature vector for pair i ∈ EE. Given a pair of ordered events, we need sc (·) to estimate the scores of them being “causes” or where r¯ is the reverse relation of r (i.e., ¯b = a, ¯i = ii, s¯ = s, and v¯ = v), and Trans(r1 , r2 ) is defined in Table 1. As for the transitivity constraints, 3 https://catalog.ldc.upenn.edu/ LDC2008T19, which is disjoint to the test set used here. Please refer to (Ning et al., 2018a) for more analysis on using this corpus to acquire prior knowledge that aids temporal relation classification. 2282 r1 r2 if both yi,j and yj,k are 1, then the constraint rer3 quires at least one of yi,k , r3 ∈ Trans(r1 , r2 ) to be 1, which means the relation between i and k has to be chosen from Trans(r1 , r2 ), which is exactly what Y1 is intended to do. The rules in Y2 is written as Y2 : yjr = I{rule(j)=r} , ∀j ∈ J (rule) (linguistic rules) where rule(j) and J (rule) have been defined in Eq. (2). Converting the T T constraints, i.e., Y0 , into constraints is as straightforward as Y2 , so"
P18-1212,W04-2401,1,0.598086,"ution of this work is proposing a joint framework for Temporal and Causal Reasoning (TCR), inspired by these examples. Assuming the availability of a temporal extraction system and a causal extraction system, the proposed joint framework combines these two using a constrained conditional model (CCM) (Chang et al., 2012) framework, with an integer linear pro2278 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2278–2288 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics gramming (ILP) objective (Roth and Yih, 2004) that enforces declarative constraints during the inference phase. Specifically, these constraints include: (1) A cause must temporally precede its effect. (2) Symmetry constraints, i.e., when a pair of events, (A, B), has a temporal relation r (e.g., before), then (B, A) must have the reverse relation of r (e.g., after). (3) Transitivity constraints, i.e., the relation between (A, C) must be temporally consistent with the relation derived from (A, B) and (B, C). These constraints originate from the one-dimensional nature of time and the physical nature of causality and build connections betwe"
P18-1212,S13-2001,0,0.262138,"lobal and done via ILP. All systems are significantly different to its preceding one with p<0.05 (McNemar’s test). Experiments In this section, we first show on TimeBank-Dense (TB-Dense) (Cassidy et al., 2014), that the proposed framework improves temporal relation identification. We then explain how our new dataset with both temporal and causal relations was collected, based on which the proposed method improves for both relations. 4.1 Temporal Performance on TB-Dense Multiple datasets with temporal annotations are available thanks to the TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The dataset we used here to demonstrate our improved temporal component was TB-Dense, which was annotated on top of 36 documents out of the classic TimeBank dataset (Pustejovsky et al., 2003). The main purpose of TB-Dense was to alleviate the known issue of sparse annotations in the evaluation dataset provided with TE3 (UzZaman et al., 2013), as pointed out in many previous work (Chambers, 2013; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017). Annotators of TB-Dense were forced to look at each pair of events or timexes within the same sentence or contiguous sentences, so that"
P18-1212,S07-1014,0,0.563864,"ich the inference has to be global and done via ILP. All systems are significantly different to its preceding one with p<0.05 (McNemar’s test). Experiments In this section, we first show on TimeBank-Dense (TB-Dense) (Cassidy et al., 2014), that the proposed framework improves temporal relation identification. We then explain how our new dataset with both temporal and causal relations was collected, based on which the proposed method improves for both relations. 4.1 Temporal Performance on TB-Dense Multiple datasets with temporal annotations are available thanks to the TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013). The dataset we used here to demonstrate our improved temporal component was TB-Dense, which was annotated on top of 36 documents out of the classic TimeBank dataset (Pustejovsky et al., 2003). The main purpose of TB-Dense was to alleviate the known issue of sparse annotations in the evaluation dataset provided with TE3 (UzZaman et al., 2013), as pointed out in many previous work (Chambers, 2013; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017). Annotators of TB-Dense were forced to look at each pair of events or timexes within the same sentence or"
P18-1212,C08-3012,0,0.699289,"sed within a global ILP-based inference method. Results on TimeBank-Dense (Cassidy et al., 2014), a benchmark dataset with temporal relations only, show that these modifications can also be helpful within ILP-based methods. 2 Related Work Temporal and causal relations can both be represented by directed acyclic graphs, where the nodes are events and the edges are labeled with either before, after, etc. (in temporal graphs), or causes and caused by (in causal graphs). Existing work on temporal relation extraction was initiated by (Mani et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008), Ex 3: Global considerations are needed when making local decisions. The FAA on Friday (e5:announced) it will close 149 regional airport control towers because of forced spending cuts. Before Friday’s (e6:announcement), it (e7:said) it would consider keeping a tower open if the airport convinces the agency it is in the ”national interest” to do so. which formulated the problem as that of learning a classification model for determining the label of each edge locally (i.e., local methods). A disadvantage of these early methods is that the resulting graph may break the symmetric and transitive c"
P18-2086,D15-1075,0,0.0577839,"al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation of considering alignment positions in TE. The same color in"
P18-2086,D16-1244,0,0.159967,"Missing"
P18-2086,W05-1209,0,0.0304846,"turned into better QA performance (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation"
P18-2086,N18-1202,0,0.0469423,"etween A and B. This challenge is expected to be handled by rules. Ambiguity: The pair #3 looks like having a similar challenge with the pair #2. We guess the annotators treat “· · · a vertebral column or backbone” and “ · · · the backbone (or vertebral column)” as the same convention, which may be debatable. Complex discourse relation: The premise in Table 3: D E I S T E vs. baselines on SNLI. D E I S T E S CI TAIL has exactly the same system layout and hyperparameters as the one reported on S CI TAIL in Table 2; D E I S T Etune : tune hyperparameters on SNLI dev. State-of-the-art refers to (Peters et al., 2018). Ensemble results are not considered. 3 Experiments Dataset. S CI TAIL3 (Khot et al., 2018) is for textual entailment in binary classification: entailment or neutral. Accuracy is reported. Training setup. All words are initialized by 300D Word2Vec (Mikolov et al., 2013) embeddings, and are fine-tuned during training. The whole system is trained by AdaGrad (Duchi et al., 2011). Other hyperparameter values include: learning rate 0.01, dm =50 for position embeddings M, hidden size 300, batch size 50, filter width 3. Baselines. (i) Decomposable Attention Model (Decomp-Att) (Parikh et al., 2016):"
P18-2086,P17-1152,0,0.139231,"question answering problem. All hypotheses H were obtained by rewriting (question, correct answer) pairs; all premises P are relevant web sentences collected by an Information retrieval (IR) method; then (P , H) pairs are annotated via crowdsourcing. Table 1 shows examples. By this construction, a substantial performance gain on S CI TAIL can be turned into better QA performance (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th An"
P18-2086,N16-1170,0,0.0813247,"Missing"
P18-2086,D16-1053,0,0.0742271,"Missing"
P18-2086,E17-1066,1,0.895944,"Missing"
P18-2086,Q16-1019,1,0.882957,"Missing"
P18-2086,N18-2017,0,0.0202319,"en et al., 2017): Enhance LSTM by encoding syntax and semantics from parsing information. (iii) Ngram Overlap: An overlap baseline, considering unigrams, oneskip bigrams and one-skip trigrams. (iv) DGEM (Khot et al., 2018): A decomposed graph entailment model, the current state-of-the-art. (v) AttentiveConvNet (Yin and Sch¨utze, 2017a): Our top-performing textual entailment system on SNLI dataset, equipped with RNN-style attention mechanism in convolution.4 In addition, to check if S CI TAIL can be easily resolved by features from only premises or hypotheses (like the problem of SNLI shown by Gururangan et al. (2018)), we put a vanilla CNN (convolution&max-pooling) over merely hypothesis or premise to derive the pair label. 3 4 Please refer to (Khot et al., 2018) for more details. https://github.com/yinwenpeng/Attentive Convolution 543 # 1 2 3 4 5 6 (Premise P , Hypothesis H) Pair G/P Challenge (P ) Front – The boundary between two different air masses. language 1/0 (H) In weather terms, the boundary between two air masses is called front. conventions (P ) . . . the notochord forms the backbone (or vertebral column). language 1/0 (H) Backbone is another name for the vertebral column. conventions (P ) · ·"
P18-2086,marelli-etal-2014-sick,0,0.0331684,"ce (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation of considering alignment pos"
P18-4014,W16-4011,0,0.0531146,"Missing"
P18-4014,N16-1029,0,0.0573891,"fer it over the alternative. TALEN is available at: github.com/CogComp/talen. 1 Introduction Named entity recognition (NER), the task of finding and classifying named entities in text, has been well-studied in English, and a select few other languages, resulting in a wealth of resources, particularly annotated training data. But for most languages, no training data exists, and annotators who speak the language can be hard or impossible to find. This low-resource scenario calls for new methods for gathering training data. Several works address this with automatic techniques (Tsai et al., 2016; Zhang et al., 2016; Mayhew et al., 2017), but often a good starting point is to elicit manual annotations from annotators who do not speak the target language. Language annotation strategies and software have historically assumed that annotators speak the language in question. Although there has been 2 Main Features In this section, we describe the main features individually in detail. Named Entity-Specific Interface The interface is designed specifically for Named Entity (NE) annotation, where entities are rela1 We use ‘non-speaker’ to denote a person who does not speak or understand the language, in contrast"
P18-4014,kamholz-etal-2014-panlex,0,0.0171593,"ture of TALEN is in-place lexicon integration, which replaces words in the annotation screen with their translation from the lexicon. For example, in Figure 1, the English word doctor is displayed in line (translations are marked by italics). As before, this is built on the notion that annotation is easiest when text looks organic, with translations seamlessly integrated. Users can click on a token and add a definition, which is immediately saved to disk. The next time the annotator encounters this word, the definition will be displayed. If available, a bilingual lexicon (perhaps from PanLex (Kamholz et al., 2014)), can kickstart the process. A side effect of the definition addition feature is a new or updated lexicon, which can be shared with other users, or used for other tasks. Entity Propagation In a low-resource scenario, it can be difficult to discover and notice names because all words look unfamiliar. To ease this burden, and to save on clicks, the interface propagates all annotation decisions to every matching surface in the document. For example, if ’iteyop’eya (Ethiopia) shows up many times in a document, then a single click will 81 Figure 2: Sentence-based annotation screen, with sentences"
P18-4014,W09-1325,0,0.0341394,"s, although to our knowledge none are designed specifically for non-speaker annotations. Many of the following tools are powerful and flexible, but would require significant refactoring to accommodate non-speakers. The most prominent and popular is brat: rapid annotation tool (brat) (Stenetorp et al., 2012), a web-based general purpose annotation tool capable of a handling a wide variety of annotation tasks, including span annotations, and relations between spans. brat is open source, reliable, and available to download and run. There are a number of tools with similar functionality. Sapient (Liakata et al., 2009) is a webbased system for annotating sentences in scientific documents. WebAnno (de Castilho et al., 2016) uses the frontend visualization from brat with a new backend designed to make large-scale project management easier. EasyTree (Little and Tratz, 2016) is a simple web-based tool for annotating dependency trees. Callisto2 is a desktop tool for rich linguistic annotation. In practice, we found that the size of k, which is the number of sentences retrieved per seed term, affects the overall corpus diversity. If k is large relative to the desired number of sentences, then the annotation is fa"
P18-4014,L16-1371,0,0.0195423,"rapid annotation tool (brat) (Stenetorp et al., 2012), a web-based general purpose annotation tool capable of a handling a wide variety of annotation tasks, including span annotations, and relations between spans. brat is open source, reliable, and available to download and run. There are a number of tools with similar functionality. Sapient (Liakata et al., 2009) is a webbased system for annotating sentences in scientific documents. WebAnno (de Castilho et al., 2016) uses the frontend visualization from brat with a new backend designed to make large-scale project management easier. EasyTree (Little and Tratz, 2016) is a simple web-based tool for annotating dependency trees. Callisto2 is a desktop tool for rich linguistic annotation. In practice, we found that the size of k, which is the number of sentences retrieved per seed term, affects the overall corpus diversity. If k is large relative to the desired number of sentences, then the annotation is fast (because entity propagation can annotate all sentences with one click), but the method produces a smaller number of unique entities. However, if k is small, annotation may be slower, but return more diverse entities. In practice, we use a default value o"
P18-4014,D17-1269,1,0.862332,"rnative. TALEN is available at: github.com/CogComp/talen. 1 Introduction Named entity recognition (NER), the task of finding and classifying named entities in text, has been well-studied in English, and a select few other languages, resulting in a wealth of resources, particularly annotated training data. But for most languages, no training data exists, and annotators who speak the language can be hard or impossible to find. This low-resource scenario calls for new methods for gathering training data. Several works address this with automatic techniques (Tsai et al., 2016; Zhang et al., 2016; Mayhew et al., 2017), but often a good starting point is to elicit manual annotations from annotators who do not speak the target language. Language annotation strategies and software have historically assumed that annotators speak the language in question. Although there has been 2 Main Features In this section, we describe the main features individually in detail. Named Entity-Specific Interface The interface is designed specifically for Named Entity (NE) annotation, where entities are rela1 We use ‘non-speaker’ to denote a person who does not speak or understand the language, in contrast with ‘nonnative speake"
P18-4014,W16-5808,0,0.337975,"annotator can see as much as possible on any given screen. This makes it easy for an annotator to make document-level decisions, for example, if an unusual-looking phrase appears several times. To add an annotation, as shown in Figure 1, the annotator clicks (and drags) on a word (or phrase), and a popover appears with buttons corresponding to label choices as defined in the configuration file. Most words are not names, so a default label of non-name (O) is assigned to all untouched tokens, keeping the number of clicks to a minimum. In contrast, a part-ofspeech (POS) annotation system, SAWT (Samih et al., 2016), for example, is designed so that every token requires a decision and a click. Lexicon Integration The main difficulty for non-speakers is that they do not understand the text they are annotating. An important feature of TALEN is in-place lexicon integration, which replaces words in the annotation screen with their translation from the lexicon. For example, in Figure 1, the English word doctor is displayed in line (translations are marked by italics). As before, this is built on the notion that annotation is easiest when text looks organic, with translations seamlessly integrated. Users can c"
P18-4014,D08-1027,0,0.176646,"Missing"
P18-4014,E12-2021,0,0.259284,"Missing"
P18-4014,K16-1022,1,0.843749,"users strongly prefer it over the alternative. TALEN is available at: github.com/CogComp/talen. 1 Introduction Named entity recognition (NER), the task of finding and classifying named entities in text, has been well-studied in English, and a select few other languages, resulting in a wealth of resources, particularly annotated training data. But for most languages, no training data exists, and annotators who speak the language can be hard or impossible to find. This low-resource scenario calls for new methods for gathering training data. Several works address this with automatic techniques (Tsai et al., 2016; Zhang et al., 2016; Mayhew et al., 2017), but often a good starting point is to elicit manual annotations from annotators who do not speak the target language. Language annotation strategies and software have historically assumed that annotators speak the language in question. Although there has been 2 Main Features In this section, we describe the main features individually in detail. Named Entity-Specific Interface The interface is designed specifically for Named Entity (NE) annotation, where entities are rela1 We use ‘non-speaker’ to denote a person who does not speak or understand the la"
P18-5008,I11-1095,0,0.071247,"Missing"
P18-5008,P12-1086,0,0.0400622,"Missing"
P18-5008,P15-1026,0,0.046284,"Missing"
P18-5008,E06-1002,0,0.0205863,"ge processing and information extraction. We will try to provide a concise road-map of recent approaches, perspectives, and results, as well as point to some of our EL resources that are available to the research community. We live in a golden age of information, where we have access to vast amount of data in various forms: text, video and audio. Over the last few years, one of the key task that has been studied in support of natural language understanding and information extraction from text, is the task of Entity Linking (previously studied as Wikification). Entity Linking (henceforth, EL) (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et a"
P18-5008,N16-1150,0,0.0303098,"Missing"
P18-5008,D13-1184,1,0.896846,"Missing"
P18-5008,Q15-1036,0,0.03164,"Missing"
P18-5008,I11-1029,0,0.0695698,"Missing"
P18-5008,W12-3016,0,0.0546427,"Missing"
P18-5008,D12-1082,0,0.0679824,"Missing"
P18-5008,P17-2085,1,0.897868,"Missing"
P18-5008,N09-2051,0,0.0412,"Missing"
P18-5008,P04-1018,0,0.0519451,"Missing"
P18-5008,P17-1178,1,0.839536,"ems and motivate neural EL. 2.3 2.4 Coffee break: [30 minutes] 2.5 Language Universal Methods for Cross-lingual EDL [30 mins] We will then present some recent advances at developing low-cost approaches to perform crosslingual EL for 282 Wikipedia languages, such as deriving silver-standard annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links (Pan et al., 2017a). We will also introduce some recent extensions along this line of work, including extending the number of entity types from five to thousands, and its impact on other NLP applications such as Machine Translation. Neural Methods for EDL [30 mins] Various shared tasks such as TAC-KBP, ACE and CONLL, along with corpora like OntoNotes and ERE have provided the community substantial amount of annotations for both entity mention extraction (1,500+ documents) and entity linking (5,000+ query entities). Therefore supervised models have become popular again for each step of EDL. Among all of the sup"
P18-5008,N15-1026,0,0.0433028,"Missing"
P18-5008,P11-1138,1,0.757906,"We will try to provide a concise road-map of recent approaches, perspectives, and results, as well as point to some of our EL resources that are available to the research community. We live in a golden age of information, where we have access to vast amount of data in various forms: text, video and audio. Over the last few years, one of the key task that has been studied in support of natural language understanding and information extraction from text, is the task of Entity Linking (previously studied as Wikification). Entity Linking (henceforth, EL) (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research"
P18-5008,D12-1110,0,0.0356304,"Missing"
P18-5008,P14-6004,1,0.871881,"sca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page i"
P18-5008,D13-1170,0,0.00580383,"Missing"
P18-5008,spitkovsky-chang-2012-cross,0,0.0838424,"Missing"
P18-5008,P14-2046,1,0.818026,"works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/"
P18-5008,D12-1011,0,0.587029,"Missing"
P18-5008,D17-1007,0,0.0311032,"ACL 2014. domain linking, has to model each domain separately. We will discuss a multi-KB entity linking framework that employs one general-knowledge KB and a large set of domain-specific KBs as linking targets that extends the work from (Cucerzan, 2007, 2014a), as well as a supervised model with a large and diverse set of features to detect when a domain-specific KB matches a document targeted for entity analysis (Gao and Cucerzan, 2017). 2.7 New Tasks, Trends and Open Questions [15 mins] Here, we will address some of the new settings: multi-lingual EL for search engines (Pappu et al., 2017; Tan et al., 2017). We will discuss some open questions such as improving the title candidate generation process for situations where the corresponding titles only exist in the English Wikipedia and also investigate the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking (Tsai and Roth, 2018). We will also discuss EL for noisy data like social media (Meij et al., 2012; Guo et al., 2013). Finally, we will discuss the possibilities of extending the ideas taught in this EL tutorial to other multi-lingual IE tasks. 2.8 • Name: Dan"
P18-5008,P16-1213,0,0.0517392,"equent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/taskspec pilot.pdf 22 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstr"
P18-5008,N16-1072,1,0.909349,"e coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/taskspec pilot.pdf 22 Proceedings of the 56th Annual Meeting of the Association for Computational Li"
P18-5008,D15-1081,1,0.908012,"Missing"
P18-5008,D14-1167,0,0.260672,"Missing"
P18-5008,C16-1127,0,0.0210845,"Missing"
P18-5008,K16-1025,0,0.0344507,"Missing"
P19-1040,N19-1053,1,0.875952,"Missing"
P19-1040,N13-1132,0,0.054772,"et al., 2018). However, this pipeline is local in that it applies to a given claim. The missing step here is to assess the trustworthiness of the sources producing the claims and evidence. This is a global step that, in principle, accounts for all claims made by a source and all sources making a claim. Previous work has studied how to estimate the trustworthiness or credibility of information sources for fact-finding (Vydiswaran et al.; Pasternack and Roth, 2013), truth discovery (Dong et al.; Pochampally et al., 2014; Dong et al., 2015; Li et al., 2016) and crowdsourcing (Sabou et al., 2012; Hovy et al., 2013; Gao et al., 2015). Usually, given a list of conflicting facts, e.g. “source s asserts claim c”, or “annotator x labels data item t by label y”, we detect the true claims or correct labels for the data item by resolving conflicts, and then compute the trustworthiness of sources. However, many sources do not directly assert claims, but rather generate articles as evidence, expecting readers to infer claims from this evidence. In practice, given a claim of interest, people may search for related articles from multiple sources and collect evidence for the claim; they can then determine the verac"
P19-1040,P14-1095,0,0.114532,". However, many sources do not directly assert claims, but rather generate articles as evidence, expecting readers to infer claims from this evidence. In practice, given a claim of interest, people may search for related articles from multiple sources and collect evidence for the claim; they can then determine the veracity of the claim by deciding whether the evidence found supports or refutes the claim. However, most existing work that attempted to study trustworthiness of sources assumed that sources make assertions directly. Even when intermediate text was accounted for (Vydiswaran et al.; Nakashole and Mitchell, 2014), it was assumed that clean evidence and clear connections between evidence and conflicting claims are provided, disregarding the fact that NLP systems attempting to support these tasks are noisy. This paper considers two situations when evalThe information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, an"
P19-1040,D18-1010,1,0.911386,"Missing"
P19-1040,D16-1244,0,0.0874226,"Missing"
P19-1040,D14-1162,0,0.0828048,"φw (e, m, c) evaluates the reliability of an entailment result given by the entailment model. As we described in Section 2.3, φw (e, m, c) is a sigmoid function of a linear combination of feature values, and we include following features: Entailment Score. For each prediction of the given entailment model, the model will predict a label, i.e. entailment, contradiction or neutral as well as a score to support its conclusion. Text Similarity. This feature is computed by the cosine similarity between numerical representations of the evidence and the claim. In this work, we use tf-idf and Glove (Pennington et al., 2014) to represent sentences respectively. To represent a sentence, we use the pre-trained Glove 1 with a simple method proposed in (Arora et al., 2017). Entity Similarity. We identify entities for each pair of evidence and claim, and compute the overlap of entities by jaccard similarity and entity similarity by NESim (Do et al., 2009) as two features. bs,e,ym e ws,e we,m ηi N (8) where φ = φw (e, m, c) for abbreviation. Means while, since Hs ∼ RsR+Q , we model it by mins imizing their KL divergence. Therefore, we also minimize: X Hs Hs ]= Hs log EHs [log Ps Ps s (9) X Hs (Rs + Qs ) = Hs log Rs s"
P19-1040,W18-5523,0,0.0226992,"Missing"
P19-1040,N18-1202,0,0.0697172,"Missing"
P19-1040,N18-1074,0,0.113273,"Missing"
P19-1040,P17-2067,0,0.0830313,"Missing"
P19-1388,P16-1231,0,0.0158348,", we get broader coverage but also fewer occurrences. When reducing the context size, we get a sparser resource, but better attribution accuracy. More sophisticated collection methods are possible (e.g. 3975 Filter/Type none 5 100 1000 measuring parse-tree distances), but are left for future work. Running the Entire Process We created the D O Q resource using the Flume framework (Chambers et al., 2010), to quickly processes billions of English webpages in parallel. First, we identified and normalized measurements (Sec. 3.1). Then, these sentences were parsed for POS tags and dependency trees (Andor et al., 2016) and the relevant objects gathered by identifying co-occurences (within sentence or distance threshold). The following step aggregated all of the objects with the same object-headmeasurement tuple, creating a distribution of numbers (Sec. 3.2). 3.3 De-noising Distance Based Co-Occurrences When aggregating co-occurrences, we also record the token distance between the measurement and the object. This can be a good indication of the degree of relatedness of a word to its surroundings. We used two context distances in our experiments: (1) co-occurrence within the same sentence, (2) cooccurrence wi"
P19-1388,P16-1055,0,0.0304919,"r Semantic Error Correction (Dahlmeier and Ng, 2011), it makes more grounded suggestions, with realistic estimates of different measurements. 3974 sparse. Speed (in km/h.) 400 3.1 300 200 100 0 slowest_car slow_car car fast_car fastest_car Figure 2: Car modifiers: Speed of cars, sidelong by different modifiers, which shift the cars speed distribution. Interesting to point out the high distribution of “slowest car” phrase (See the bias discussion in Sec. 6). Our work overlaps with a number of approaches to ground textual objects by: achieving a commonsense understanding of numeric expressions (Chaganty and Liang, 2016), grounding adjectives into RGB colors (Winn and Muresan, 2018), grounding events duration (Pan et al., 2006; Gusev et al., 2011) and measurements’ intensity within a given context (Narisawa et al., 2013). Finally, our resource collection is in the line of work that uses counting across very large amounts of data (such as n-grams from books) to produce big resources (Lin et al., 2012; Goldberg and Orwant, 2013), which have had a significant impact on NLP Research. 3 Distribution over Quantities: Method We propose a process for automatically extracting co-occurrences of objects and measurements"
P19-1388,D18-1202,0,0.180404,"Missing"
P19-1388,D11-1010,0,0.0180874,"ies, i.e. ‘freezing’ &lt; ‘cold’. In contrast, we infer magnitudes as well, which make us robust to comparisons between different polarities of the same cluster (e.g. ‘hot’ vs. ‘cold’). Spithourakis and Riedel (2018) propose several methods to represent numbers in language models (LMs) instead of using an out-of-vocabulary token, giving the LM more expressive ability to produce numbers. Spithourakis et al. (2016) showed that conditioning on numerical values in the LM can improve the consistency of the modeling for clinical reports. When using it along with a scorer for Semantic Error Correction (Dahlmeier and Ng, 2011), it makes more grounded suggestions, with realistic estimates of different measurements. 3974 sparse. Speed (in km/h.) 400 3.1 300 200 100 0 slowest_car slow_car car fast_car fastest_car Figure 2: Car modifiers: Speed of cars, sidelong by different modifiers, which shift the cars speed distribution. Interesting to point out the high distribution of “slowest car” phrase (See the bias discussion in Sec. 6). Our work overlaps with a number of approaches to ground textual objects by: achieving a commonsense understanding of numeric expressions (Chaganty and Liang, 2016), grounding adjectives into"
P19-1388,S13-1035,0,0.0239925,"car” phrase (See the bias discussion in Sec. 6). Our work overlaps with a number of approaches to ground textual objects by: achieving a commonsense understanding of numeric expressions (Chaganty and Liang, 2016), grounding adjectives into RGB colors (Winn and Muresan, 2018), grounding events duration (Pan et al., 2006; Gusev et al., 2011) and measurements’ intensity within a given context (Narisawa et al., 2013). Finally, our resource collection is in the line of work that uses counting across very large amounts of data (such as n-grams from books) to produce big resources (Lin et al., 2012; Goldberg and Orwant, 2013), which have had a significant impact on NLP Research. 3 Distribution over Quantities: Method We propose a process for automatically extracting co-occurrences of objects and measurements from a large text corpus. Examples of the resulting output are the mass distributions of animals in Figure 1a, typical meal hours in Figure 1b and the car modifiers in Figure 2. We first use a rule-based method for detecting and normalizing measurement mentions (Sec. 3.1). We then aggregate the detected measurements and objects that occurred in the nearby context (Sec. 3.2) and describe some simple heuristics"
P19-1388,W11-0116,0,0.612984,"ng commonsense knowledge from natural language text has been the subject of a lot of recent work. These approaches focus on facilitating comparisons between quantitative attributes of nouns (Bagherinezhad et al., 2016; Forbes and 3973 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973–3983 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Choi, 2017; Yang et al., 2018), intensity of adjectives (De Melo and Bansal, 2013; Cocos et al., 2018) and coarse classification of events duration and relative order (Gusev et al., 2011; Ning et al., 2018). However, they do not have complete coverage even for comparable objects, as a result of how they are acquired, and lack the ability to assign a numerical value to objects and events (“How hot is it in New York?”), which is often useful for reasoning, text generation, and other tasks. In this work, we propose a method for acquiring distributions over ten dimensions: TIME, CUR RENCY , LENGTH , AREA , VOLUME , MASS , TEM PERATURE , DURATION , SPEED , and VOLTAGE . We do this for nouns (e.g. elephant, airplane, NBA game), adjectives (e.g. cold, hot, lukewarm) and verbs (e.g."
P19-1388,C92-2082,0,0.246143,"ability density of the collected distribution at different values. resulting in new, cleaner, evaluation datasets. Overall, we make the following contributions: 1. A new method for collecting expressive quantitative information about objects. 2. A large resource of distributions over quantitative attributes of nouns, adjectives, and verbs. 3. Strong results on existing datasets for noun and adjective comparison, refining and improving an existing dataset, and creating a new dataset for evaluating noun comparisons. 2 Related Work There has been a lot of work trying to use Hearststyle patterns (Hearst, 1992) to extract relations between objects in large corpora (Tandon et al., 2014; Shivade et al., 2016). For example, from the sentence: “Melons are bigger than apples” they extract the relation: ‘Melons’ &gt; ‘apples’. These methods suffer from reporting bias and low coverage, since the precise patterns need to be found to make these inferences. Our method, which relies on co-occurring objects, is robust to this issue. Pattern-based methods were also used in the context of OpenIE, e.g., to extract event duration information (Gusev et al., 2011; Kozareva and Hovy, 2011), but were found to be highly br"
P19-1388,D13-1169,0,0.0745728,"Missing"
P19-1388,P12-3029,0,0.0822395,"Missing"
P19-1388,S18-2031,1,0.751568,"Missing"
P19-1388,P13-1038,0,0.029455,"low_car car fast_car fastest_car Figure 2: Car modifiers: Speed of cars, sidelong by different modifiers, which shift the cars speed distribution. Interesting to point out the high distribution of “slowest car” phrase (See the bias discussion in Sec. 6). Our work overlaps with a number of approaches to ground textual objects by: achieving a commonsense understanding of numeric expressions (Chaganty and Liang, 2016), grounding adjectives into RGB colors (Winn and Muresan, 2018), grounding events duration (Pan et al., 2006; Gusev et al., 2011) and measurements’ intensity within a given context (Narisawa et al., 2013). Finally, our resource collection is in the line of work that uses counting across very large amounts of data (such as n-grams from books) to produce big resources (Lin et al., 2012; Goldberg and Orwant, 2013), which have had a significant impact on NLP Research. 3 Distribution over Quantities: Method We propose a process for automatically extracting co-occurrences of objects and measurements from a large text corpus. Examples of the resulting output are the mass distributions of animals in Figure 1a, typical meal hours in Figure 1b and the car modifiers in Figure 2. We first use a rule-based"
P19-1388,Q13-1023,0,0.0585677,"Missing"
P19-1388,N18-1077,1,0.791062,"edge from natural language text has been the subject of a lot of recent work. These approaches focus on facilitating comparisons between quantitative attributes of nouns (Bagherinezhad et al., 2016; Forbes and 3973 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973–3983 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Choi, 2017; Yang et al., 2018), intensity of adjectives (De Melo and Bansal, 2013; Cocos et al., 2018) and coarse classification of events duration and relative order (Gusev et al., 2011; Ning et al., 2018). However, they do not have complete coverage even for comparable objects, as a result of how they are acquired, and lack the ability to assign a numerical value to objects and events (“How hot is it in New York?”), which is often useful for reasoning, text generation, and other tasks. In this work, we propose a method for acquiring distributions over ten dimensions: TIME, CUR RENCY , LENGTH , AREA , VOLUME , MASS , TEM PERATURE , DURATION , SPEED , and VOLTAGE . We do this for nouns (e.g. elephant, airplane, NBA game), adjectives (e.g. cold, hot, lukewarm) and verbs (e.g. eating, walking, run"
P19-1388,P17-1025,0,0.200414,"16). For example, from the sentence: “Melons are bigger than apples” they extract the relation: ‘Melons’ &gt; ‘apples’. These methods suffer from reporting bias and low coverage, since the precise patterns need to be found to make these inferences. Our method, which relies on co-occurring objects, is robust to this issue. Pattern-based methods were also used in the context of OpenIE, e.g., to extract event duration information (Gusev et al., 2011; Kozareva and Hovy, 2011), but were found to be highly brittle due to the dependence on finding specific pre-defined patterns. There is a line of work (Forbes and Choi, 2017; Yang et al., 2018) to determine the quantitative relation between two nouns on a specific scale. For adjectives (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Shivade et al., 2015; Cocos et al., 2018), comparisons were made only for relative intensities, i.e. ‘freezing’ &lt; ‘cold’. In contrast, we infer magnitudes as well, which make us robust to comparisons between different polarities of the same cluster (e.g. ‘hot’ vs. ‘cold’). Spithourakis and Riedel (2018) propose several methods to represent numbers in language models (LMs) instead of using an out-of-vocabulary token, giving the L"
P19-1388,pan-etal-2006-annotated,0,0.134671,"Missing"
P19-1388,N15-1051,0,0.0459696,"Missing"
P19-1388,W16-2903,0,0.0421501,"Missing"
P19-1388,D16-1101,0,0.0412863,"tive relation between two nouns on a specific scale. For adjectives (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Shivade et al., 2015; Cocos et al., 2018), comparisons were made only for relative intensities, i.e. ‘freezing’ &lt; ‘cold’. In contrast, we infer magnitudes as well, which make us robust to comparisons between different polarities of the same cluster (e.g. ‘hot’ vs. ‘cold’). Spithourakis and Riedel (2018) propose several methods to represent numbers in language models (LMs) instead of using an out-of-vocabulary token, giving the LM more expressive ability to produce numbers. Spithourakis et al. (2016) showed that conditioning on numerical values in the LM can improve the consistency of the modeling for clinical reports. When using it along with a scorer for Semantic Error Correction (Dahlmeier and Ng, 2011), it makes more grounded suggestions, with realistic estimates of different measurements. 3974 sparse. Speed (in km/h.) 400 3.1 300 200 100 0 slowest_car slow_car car fast_car fastest_car Figure 2: Car modifiers: Speed of cars, sidelong by different modifiers, which shift the cars speed distribution. Interesting to point out the high distribution of “slowest car” phrase (See the bias dis"
P19-1388,P18-1196,0,0.122564,"Missing"
P19-1388,L16-1424,0,0.0156223,"LENGTH and WEIGHT dimensions for fair comparison to previous work.) The Relative Size Dataset Bagherinezhad et al. (2016) created a dataset of 486 object pairs between 41 physical objects. The dataset (which we refer to as R ELATIVE) focuses solely on the physical size dimension. Nevertheless, we use it as another evaluation of D O Q. 4.2 Scalar Adjectives Several test sets have been created to evaluate the intensity of adjectives. The dataset created by De Melo and Bansal (2013) uses adjective clusters based on the ‘dumbbell’ structure of adjectives in WordNet e.g. “cold &lt; frigid &lt; frozen”. Wilkinson and Oates (2016) created another testset, by defining a total order between adjectives in the same cluster, spanning the entire scale range. For example, in the SIZE domain, the full cluster is: “minuscule &lt; tiny &lt; small &lt; big &lt; large &lt; huge &lt; enormous &lt; gigantic”. A total of 60 adjectives were collected across 12 clusters. Since our method only handles measurable objects, we manually removed all of the nonmeasurable clusters (e.g., “known &lt; famous &lt; legendary” was removed) and evaluated on the rest. In this process we found that the new dataset by Cocos et al. (2018) contains only a small number of measurabl"
P19-1388,P18-2125,0,0.0228711,"re grounded suggestions, with realistic estimates of different measurements. 3974 sparse. Speed (in km/h.) 400 3.1 300 200 100 0 slowest_car slow_car car fast_car fastest_car Figure 2: Car modifiers: Speed of cars, sidelong by different modifiers, which shift the cars speed distribution. Interesting to point out the high distribution of “slowest car” phrase (See the bias discussion in Sec. 6). Our work overlaps with a number of approaches to ground textual objects by: achieving a commonsense understanding of numeric expressions (Chaganty and Liang, 2016), grounding adjectives into RGB colors (Winn and Muresan, 2018), grounding events duration (Pan et al., 2006; Gusev et al., 2011) and measurements’ intensity within a given context (Narisawa et al., 2013). Finally, our resource collection is in the line of work that uses counting across very large amounts of data (such as n-grams from books) to produce big resources (Lin et al., 2012; Goldberg and Orwant, 2013), which have had a significant impact on NLP Research. 3 Distribution over Quantities: Method We propose a process for automatically extracting co-occurrences of objects and measurements from a large text corpus. Examples of the resulting output are"
P19-1388,P18-2102,0,0.389614,"ons. et al., 2013) and, more generally, in order to support reasoning about events described in natural language and converse with people naturally. Acquiring commonsense knowledge from natural language text has been the subject of a lot of recent work. These approaches focus on facilitating comparisons between quantitative attributes of nouns (Bagherinezhad et al., 2016; Forbes and 3973 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973–3983 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Choi, 2017; Yang et al., 2018), intensity of adjectives (De Melo and Bansal, 2013; Cocos et al., 2018) and coarse classification of events duration and relative order (Gusev et al., 2011; Ning et al., 2018). However, they do not have complete coverage even for comparable objects, as a result of how they are acquired, and lack the ability to assign a numerical value to objects and events (“How hot is it in New York?”), which is often useful for reasoning, text generation, and other tasks. In this work, we propose a method for acquiring distributions over ten dimensions: TIME, CUR RENCY , LENGTH , AREA , VOLUME , MASS , TEM"
P19-3022,D14-1083,0,0.0542943,"Missing"
P19-3022,P17-2032,0,0.0288719,"y similar technologies. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and return"
P19-3022,C18-1176,0,0.229818,"Missing"
P19-3022,N19-4014,0,0.0408059,"Missing"
P19-3022,N18-5005,0,0.202546,"es. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments"
P19-3022,D18-1402,0,0.040554,"es. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments"
P19-3022,N18-1074,0,0.0357591,"Missing"
P19-3022,N19-1053,1,0.928078,"sity of the perspectives presented in them or whether they are supported by evidence. While it might be impractical to show an exhaustive spectrum of views with respect to a claim, cherry-picking a small but diverse set of perspectives could be a tangible step towards addressing the limitations of the current systems. Inherently this objective requires the understanding of the relations between each perspective and claim, as well as the nuance in semantic meaning between perspectives under the context of the claim. This work presents a demo for the task of substantiated perspective discovery (Chen et al., 2019). Our system receives a claim and it is expected to present a diverse set of wellcorroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. A typical output of the system is shown in Figure 3. The input to the system is a claim: Social media (like facebook or twitter) have had very positive effects in our life style. There is no single, best way to respond to the claim, but rather there are many valid responses that form a spectrum of perspectives, each with a stance relati"
P19-3022,W17-5106,0,0.163366,"provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments retrieved from the web. This work takes the effort one step further by employing language understanding techniques. There is a rich line of work on using Wikipedia as source for argument mining or to assess the veracity of a claim (Thorne et al., 2018). For instance, FAKTA is a system that extracts relevant documents from Wikipedia, among other sources, to predict the factuality of an input claim (Nadeem et al., 2019"
P19-3022,N19-1423,0,0.00809579,"o ensure both quality and efficiency. The retrieval systems extract candidates (perspectives or evidence paragraphs) which are later evaluated by carefully designed classifiers. C4: Extraction of Supporting Evidence. This classifier decides whether a given document lends enough evidence for a given perspective to a claim. In training the classifiers for each of the tasks, 130 Figure 2: Overview of the system structure: given a query to the system, it extracts candidates from its internal knowledge imal set of perspectives with the DBSCAN clustering algorithm (Ester et al., 1996). we use BERT (Devlin et al., 2019) and we follow the same steps described in Chen et al. (2019). 2.3 Candidate Retrieval Algorithm 1: Minimal Perspective Extraction Input: claim c. Output: perspectives, their stances & evidence. Pˆ ←IR(c) // candidate perspectives P = {} foreach p ∈ Pˆ do // perspective relevance if C1(c, p) &gt; T 1 and abs(C2(c, p)) &gt; T 2 then e ← C2(c, p) ˆ ←IR(c, p) // candidate evidence E E = {} ˆ do foreach e ∈ E // evidence verification if C4(c, p, e) &gt; T 4 then E ← E ∪ {e}. end end P ← P ∪ {(p, s, E)}. end end P ← /* minimal perspectives after clustering with DBSCAN on the equivalence scores between any p"
P19-3022,P18-1023,0,0.025386,"ng.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments retrieved from the web. T"
P98-2186,W98-0717,1,0.761584,"k-propagation algorithm on the training stage. The input nodes of the network usually correspond to the tags of the words surrounding the word being tagged. The performance of the algorithms is comparable to that of HMM methods. In this paper, we address the POS problem with no unknown words (the closed world assumption) from the standpoint of SNOW. T h a t is, we represent a POS tagger as a network of linear separators and use Winnow for learning weights of the network. The S N O W approach has been successfully applied to other problems of natural language processing(Golding and Roth, 1998; Krymolowski and Roth, 1998; Roth, 1998). However, this problem offers additional challenges to the S N O W architecture and algorithms. First, we are trying to learn a multi-class predictor, where the number of classes is unusually large(about 50) for such learning problems. Second, evaluating hypothesis in testing is done in a presence of attribute noise. The reason is that input features of the network are computed with respect to parts of speech of words, which are initially assigned from a lexicon. We address the first problem by restricting the parts of speech a tag for a word is selected from. Second problem is a"
P98-2186,C94-1027,0,0.156018,"s to m a n y possible P O S tagging of the sentence one of which, determiner, noun, modal-verb, verb, respectively, is correct. The problem has numerous application in information retrieval, machine translation, speech recognition, and appears to be an important intermediate stage in m a n y natural language understanding related inferences. In recent years, a number of approaches have been tried for solving the problem. The most notable methods are based on Hidden Markov Models(HMM)(Kupiec, 1992; Schiitze, 1995), transformation rules(Brill, 1995; Brill, 1997), and multi-layer neural networks(Schmid, 1994). HMM taggers use manually tagged training data to compute statistics on features. For example, they can estimate lexical probabilities Prob(wordlta9) and contextual probabilities P r o b ( t a g l p r e v i o u s n tags). On the testing stage, the taggers conduct a search in the space of POS tags to arrive at the most probable POS labeling with respect to the computed statistics. T h a t is, given a sentence, the taggers assign in the sentence a sequence of tags that maximize the product of lexical and contextual probabilities over all words in the sentence. Transformation based learning(TBL)"
P98-2186,E95-1020,0,0.0523062,"Missing"
P98-2186,J95-4004,0,\N,Missing
Q13-1019,J09-2001,0,0.0591225,"Missing"
Q13-1019,N10-1066,1,0.849328,"if we had a weight vector w, we could predict the full structure using inference as follows: y = arg max wT Φ(x, y) (4) y0 We propose an iterative learning algorithm to learn this weight vector. In the following discussion, for a labeled example (x, y∗ ), we refer to the missing part of its structure as h(y∗ ). That is, h(y∗ ) is the assignment to the arguments of the relation and their types. We use the notation r(y) to denote the relation label specified by a structure y. Our learning algorithm is closely related to recently developed latent variable based frameworks (Yu and Joachims, 2009; Chang et al., 2010a; Chang et al., 2010b), where the supervision provides only partial annotation. We begin by defining two additional inference procedures: 1. Latent Inference: Given a weight vector w and a partially labeled example (x, y∗ ), we can ‘complete’ the rest of the structure by inferring the highest scoring assignment to the missing parts. In the algorithm, we call this procedure LatentInf (w, x, y∗ ), which solves the following maximization problem: ˆ = arg maxy wT Φ(x, y), y s.t. (5) ∗ r(y) = r(y ). 2. Loss augmented inference: This is a variant of the the standard loss augmented inference for str"
Q13-1019,clarke-etal-2012-nlp,1,0.845595,"Missing"
Q13-1019,D09-1047,0,0.285663,"ition Project (Litkowski and Hargraves, 2005) and the related SemEval 2007 shared task of word sense disambiguation of prepositions (Litkowski and Hargraves, 2007). The Preposition Project identifies preposition senses based on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while"
Q13-1019,N10-1138,0,0.0391634,"Missing"
Q13-1019,J02-3001,0,0.0916152,"roduce a new inventory of preposition relations that covers the 34 prepositions that formed the basis of the SemEval 2007 task of preposition sense disambiguation. 2. We model preposition relations, arguments and their types jointly and propose a learning algorithm that learns to predict all three using training data that annotates only relation labels. 3. We show that jointly predicting relations with 232 word sense not only improves the relation predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary wo"
Q13-1019,N10-1115,0,0.0161165,"n Monday books on Shakespeare Table 1: List of preposition relations ning example, the relation label is Cause. We represent the predicted relation label by r. Arguments The relation label crucially depends on correctly identifying the arguments of the preposition, which are death and pneumonia in our running example. While a parser can identify the arguments of a preposition, simply relying on the parser may impose an upper limit on the accuracy of relation prediction. We build an oracle experiment to highlight this limitation. Table 2 shows the recall of the easy-first dependency parser of (Goldberg and Elhadad, 2010) on Section 23 of the Penn Treebank for identifying the governor and object of prepositions. We define heuristics that generate a candidate governors and objects for a preposition. For the governor, this set includes the previous verb or noun and for the object, it includes only the next noun. The row labeled Best(Parser, Heuristics) shows the performance of an oracle predictor which selects the true governor/object if present among the parser’s prediction and the heuristics. We see that, even for the in-domain case, if we are able to re-rank the candidates, we could achieve a big improvement"
Q13-1019,C10-2052,0,0.793611,"on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while disambiguating the sense of a preposition does indeed reveal nuances of its meaning, it leads to a proliferation of labels to be predicted. Most importantly, sense labels do not transfer to other prepositions that expre"
Q13-1019,P98-2127,0,0.0631926,"35 Figure 1 shows the hypernym hierarchy for the word pneumonia. In this case, synsets in the hypernym hierarchy, like pathological state or physical condition, would also include ailments like flu. pneumonia =&gt; respiratory disease =&gt; disease =&gt; illness =&gt; ill health =&gt; pathological state =&gt; physical condition =&gt; condition =&gt; state =&gt; attribute =&gt; abstraction =&gt; entity Figure 1: Hypernym hierarchy for the word pneumonia We define a semantic type to be a cluster of words. In addition to WordNet hypernyms, we also cluster verbs, nouns and adjectives using the dependencybased word similarity of (Lin, 1998) and treat cluster membership as types. These are described in detail in Section 5.1. Relation prediction involves not only identifying the arguments, but also selecting the right semantic type for them, which together, help predicting the relation label. Given an argument candidate and a collection of possible types (given by WordNet or the similarity based clusters), we need to select one of the types. For example, in the WordNet case, we need to pick one of the hypernyms in the hypernym hierarchy. Thus, for the governor and object, we have a set of type labels, comprised of one element for"
Q13-1019,W04-2705,0,0.0696821,"gnificant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focuses on semantic relations expressed by transitive prepositions1 . We can define the two prediction tasks for prepositions as follows: identifying the relation label for a preposition, and predicting the arguments of the relation. Prepositions can mark arguments (both core and adjunct) for verbal and nominal predicates. In addition, they can also trigger relations that are not part of other predicates. For example, in sentence"
Q13-1019,J09-2002,0,0.384977,"Missing"
Q13-1019,J05-1004,0,0.0849467,"ion predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focuses on semantic relations expressed by transitive prepositions1 . We can define the two prediction tasks for prepositions as follows: identifying the relation label for a preposition, and predicting the arguments of the relation. Prepositions can mark arguments (both core and adjunct) for verbal and nominal predicates. In addition, they can also trigger relations that are not part of other pre"
Q13-1019,J08-2005,1,0.224743,"that covers the 34 prepositions that formed the basis of the SemEval 2007 task of preposition sense disambiguation. 2. We model preposition relations, arguments and their types jointly and propose a learning algorithm that learns to predict all three using training data that annotates only relation labels. 3. We show that jointly predicting relations with 232 word sense not only improves the relation predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focu"
Q13-1019,W04-2401,1,0.739506,"vious iteration’s weight vector influence the learning. 4.4 Joint inference between preposition senses and relations By defining preposition relations as disjoint sets of preposition senses, we effectively have a hierarchical relationship between senses and relations. This suggests that joint inference can be employed between sense and relation predictions with a validity constraint connecting the two. The idea of employing inference to combine independently trained predictors to obtain a coherent output structure has been used for various NLP tasks in recent years, starting with the work of (Roth and Yih, 2004; Roth and Yih, 2007). We use the features defined by (Hovy et al., 2010), which we write as φs (x, s) for a given input x and sense label s, and train a separate preposition sense model on the SemEval data with features φs (x, s) using the structural SVM algorithm. Thus, we have two weight vectors – the one for predicting preposition relations described earlier, and the preposition sense weight vector. At prediction time, for a given input, we find the highest scoring joint assignment to the relation, arguments and types and the sense, subject to the constraint that the sense and the relation"
Q13-1019,D11-1012,1,0.761915,"the relations to aid natural language understanding and makes the prediction task harder than it should be: using the standard word sense classification approach, we need to train a separate classifier for each word because the labels are defined per-preposition. In other words, we cannot share features across the different prepositions. This motivates the need to combine such senses of prepositions into the same class label. In this direction, (O’Hara and Wiebe, 2009) describes an inventory of preposition relations obtained using Penn Treebank function tags and frame elements from FrameNet. (Srikumar and Roth, 2011) merged preposition senses of seven prepositions into relation labels. (Litkowski, 2012) also suggests collapsing the definitions of prepositions into a smaller set of semantic classes. To aid better generalization and to reduce the label complexity, we follow this line of work to define a set of relation labels which abstract word senses across prepositions2 . 3 Preposition-triggered Relations This section describes the inventory of preposition relations introduced in this paper, and then identifies the components of the preposition relation extraction problem. 3.1 Preposition Relation Invent"
Q13-1019,N09-3017,0,0.521283,"es preposition senses based on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while disambiguating the sense of a preposition does indeed reveal nuances of its meaning, it leads to a proliferation of labels to be predicted. Most importantly, sense labels do not transfer to other"
Q13-1019,S07-1051,0,0.703348,"Missing"
Q13-1019,S07-1005,0,\N,Missing
Q13-1019,J13-3006,0,\N,Missing
Q13-1019,P08-1037,0,\N,Missing
Q13-1019,C98-2122,0,\N,Missing
Q14-1033,W13-3608,0,0.097315,"Missing"
Q14-1033,N13-1055,0,0.026784,"OS and syntactic knowledge, training on learner data is advantageous. Finally, for verb form errors, there is an advantage when training on a lot of native data, although the difference is not as substantial as for noun errors. This suggests that unlike agreement mistakes that are better addressed using syntax, form errors, similarly to nouns, benefit from training on a lot of data with n-gram features. To summarize, choice of the training data is an important consideration for building a robust system. Researchers compared native- and learnertrained models for prepositions (Han et al., 2010; Cahill et al., 2013), while the analysis in this work addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and"
Q14-1033,P96-1041,0,0.0649767,"1 29.40 32.41 34.40 33.53 36.42 08.46 12.16 Table 6: Comparison of learning models. Web1T corpus. Modules that are part of the Illinois submission are marked with an asterisk. Source ED INF ING S ED 0.99675 0.00177 0.00124 0.00054 Candidates INF ING 0.00192 0.00103 0.99630 0.00168 0.00447 0.99407 0.00544 0.00132 S 0.00030 0.00025 0.00022 0.99269 Table 7: Priors confusion matrix used for adapting NB. Each entry shows Prob(candidate|source), where source corresponds to the verb form chosen by the author. with SRILM (Stolcke, 2002) using Jelinek-Mercer linear interpolation as a smoothing method (Chen and Goodman, 1996). On the CoNLL test data, NB outperforms LM on all errors; on the FCE corpus, NB is superior on all errors, except preposition errors, where LM outperforms NB only very slightly. We attribute this to the fact that the preposition problem has more labels; when there is a big confusion set, more features have default smooth weights, so there is no advantage to running NB. We found that with fewer classes (6 rather than 12 prepositions), NB outperforms LM. It is also possible that when we have a lot of labels, the theoretical difference between the algorithms disappears. Note that NB can be impro"
Q14-1033,C12-1038,0,0.0612245,"Missing"
Q14-1033,P11-1092,0,0.337864,"error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and learner data in training, see Gamon (2010) and Dahlmeier and Ng (2011). Error Art. Prep. Noun Agr. Form All Illinois submission Model F1 AP-infl. 33.50 NB-adapt. 07.10 NB 42.60 NB 26.14 NB 14.50 31.43 This work Model F1 AP-infl. 33.50 LM 12.09 NB 42.60 AP-infl. 27.93 NB-adapt. 18.35 31.75 Table 15: Results on CoNLL of the Illinois system (without post-processing) and this work. NB and LM models are trained on Web1T; AP models are trained on NUCLE. Modules different from the Illinois submission are in bold. different from the 12.14 in Table 8); the agreement classifier is trained on the learner data using AP with rich features and error inflation; the form classi"
Q14-1033,D12-1052,0,0.247026,"t additional experiments that further analyze each dimension. While a direct comparison with other systems is not always possible due to other differences between the systems, we believe that these results are still useful. Table 5 lists systems used for comparion. It is important to note that the dimensions are not independent. For instance, there is a correlation between algorithm choice and training data. 7 The tool and more detail about it can be found at http://cogcomp.cs.illinois.edu/page/publication view/743 Results are reported on the test data using F1 computed with the CoNLL scorer (Dahlmeier and Ng, 2012). Error-specific results are generated based on the output of individual modules. Note that these are not directly comparable to error-specific results in the CoNLL overview paper: the latter are approximate as the organizers did not have the error type information for corrections in the output. The complete system includes the union of corrections made by each of these modules, where the corrections are applied in order. Ordering overlapping candidates8 might potentially affect the final output, when modules correctly identify an error but propose different corrections, but this does not happ"
Q14-1033,W13-1703,0,0.203789,"Missing"
Q14-1033,W11-2838,0,0.084967,"describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance. 1 Dan Roth Department of Computer Science University of Illinois Urbana, IL 61801 danr@illinois.edu Introduction The field of text correction has seen an increased interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated"
Q14-1033,W12-2006,0,0.0843321,"are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance. 1 Dan Roth Department of Computer Science University of Illinois Urbana, IL 61801 danr@illinois.edu Introduction The field of text correction has seen an increased interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated in the task developed a wide a"
Q14-1033,W01-0502,1,0.803217,"Missing"
Q14-1033,W09-2112,0,0.150223,"be noted that although inflation also decreases precision it is still helpful. In fact, because of the low error rates, performance on the CoNLL dataset with natural errors is very poor, often resulting in F1 being equal to 0 due to no errors being detected. Inflation vs. Sampling To demonstrate the impact of error inflation, we compare it against sampling, an approach used by other teams – e.g. HIT – that improves recall by removing correct examples in training. The HIT article model is similar to the 9 The idea of using artificial errors goes back to Izumi et al. (2003) and was also used in Foster and Andersen (2009). The approach discussed here refers to the adaptation method in Rozovskaya and Roth (2010b) that generates artificial errors using the distribution of naturally-occurring errors. Error Art. Prep. Agr. Form Model AP (natural errors) AP (infl. const. 0.9)* AP (natural errors) AP (infl. const. 0.7) AP (natural errors) AP (infl. const. 0.8) AP (natural errors) AP (infl. const. 0.9) F1 CoNLL 07.06 24.61 0.0 07.37 0.0 17.06 0.0 10.53 Error FCE 27.65 30.96 14.69 34.77 08.05 31.03 01.56 09.43 Art. Agr. trained on learner data with word n-gram features and the source feature. Inflation constant shows"
Q14-1033,N10-1019,0,0.103835,"rk addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and learner data in training, see Gamon (2010) and Dahlmeier and Ng (2011). Error Art. Prep. Noun Agr. Form All Illinois submission Model F1 AP-infl. 33.50 NB-adapt. 07.10 NB 42.60 NB 26.14 NB 14.50 31.43 This work Model F1 AP-infl. 33.50 LM 12.09 NB 42.60 AP-infl. 27.93 NB-adapt. 18.35 31.75 Table 15: Results on CoNLL of the Illinois system (without post-processing) and this work. NB and LM models are trained on Web1T; AP models are trained on NUCLE. Modules different from the Illinois submission are in bold. different from the 12.14 in Table 8); the agreement classifier is trained on the learner data using AP with rich features and erro"
Q14-1033,han-etal-2010-using,0,0.0708563,"ver, when we add POS and syntactic knowledge, training on learner data is advantageous. Finally, for verb form errors, there is an advantage when training on a lot of native data, although the difference is not as substantial as for noun errors. This suggests that unlike agreement mistakes that are better addressed using syntax, form errors, similarly to nouns, benefit from training on a lot of data with n-gram features. To summarize, choice of the training data is an important consideration for building a robust system. Researchers compared native- and learnertrained models for prepositions (Han et al., 2010; Cahill et al., 2013), while the analysis in this work addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that direc"
Q14-1033,P03-2026,0,0.193888,"recall and, consequently, F1. It should be noted that although inflation also decreases precision it is still helpful. In fact, because of the low error rates, performance on the CoNLL dataset with natural errors is very poor, often resulting in F1 being equal to 0 due to no errors being detected. Inflation vs. Sampling To demonstrate the impact of error inflation, we compare it against sampling, an approach used by other teams – e.g. HIT – that improves recall by removing correct examples in training. The HIT article model is similar to the 9 The idea of using artificial errors goes back to Izumi et al. (2003) and was also used in Foster and Andersen (2009). The approach discussed here refers to the adaptation method in Rozovskaya and Roth (2010b) that generates artificial errors using the distribution of naturally-occurring errors. Error Art. Prep. Agr. Form Model AP (natural errors) AP (infl. const. 0.9)* AP (natural errors) AP (infl. const. 0.7) AP (natural errors) AP (infl. const. 0.8) AP (natural errors) AP (infl. const. 0.9) F1 CoNLL 07.06 24.61 0.0 07.37 0.0 17.06 0.0 10.53 Error FCE 27.65 30.96 14.69 34.77 08.05 31.03 01.56 09.43 Art. Agr. trained on learner data with word n-gram features a"
Q14-1033,W13-3603,0,0.045134,"Missing"
Q14-1033,P08-1021,0,0.0530787,"n on learner data with rich features. The word n-gram and POS agreement features are the same as those in the article module. Syntactic features encode properties of the subject of the verb and are presented in Rozovskaya et al. (2014b, Table 7) and Appendix Table A.18; these are based on the syntactic parser (Klein and Manning, 2003) and the dependency converter (Marneffe et al., 2006). Table 11 shows that adding rich features is helpful. Notably, adding deeper syntactic knowledge to the agreement module is useful, although parse features are likely to contain more noise.11 Foster (2007) and Lee and Seneff (2008) observe a degrade in performance on syntactic parsers due to grammatical noise that also includes agreement errors. For articles, we chose to add syntactic knowledge from shallow parse as it is likely to be sufficient for articles and more accurate than full-parse features. Candidate Identification for errors on open-class 10 Feature engineering will also be relevant when training on a native corpus that has linguistic annotation. 11 Parse features have also been found useful in preposition error correction (Tetreault et al., 2010). words is rarely discussed but is a crucial step: it is not p"
Q14-1033,P11-2089,1,0.920942,"Missing"
Q14-1033,de-marneffe-etal-2006-generating,0,0.130055,"Missing"
Q14-1033,W13-3601,0,0.196437,"Missing"
Q14-1033,W14-1701,0,0.138751,"rners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated in the task developed a wide array of approaches that include discriminative classifiers, language models, statistical machine-translation systems, and rule-based modules. Many of the systems also made use of linguistic resources such as additional annotated learner corpora, and defined highlevel features that take into account syntactic and semantic knowledge. Even though the systems incorporated similar resources, the scores varied widely. The top system, from the University"
Q14-1033,rizzolo-roth-2010-learning,1,0.845713,"although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first language backgrounds, most of the preposition errors are replacements, i.e., where the 6 421 Determiner Errors The variants “a” and “an” are collapsed to one class. “Hence, the environmental factors also *contributes/ contribute to variou"
Q14-1033,W10-1004,1,0.944277,"rst language (Gass and Selinker, 1992; Ionin et al., 2008). There are different ways to adapt a model that depend on the type of training data (learner or native) and the algorithm choice. The key application of adaptation is for models trained on native English data, because the learned models do not know anything about the errors learners make. With adaptation, models trained on native data can use the author’s word (the source word) as a feature and thus propose a correction based on what the author originally wrote. This is crucial, as the source word is an important piece of information (Rozovskaya and Roth, 2010b). Below, several adaptation techniques are summarized and evaluated. The Illinois system makes use of adaptation in the article model via the inflation method and adapts its NB preposition classifier trained on Web1T with the priors method. Adapting NB The priors method (Rozovskaya and Roth, 2011, Sec. 4) is an adaptation technique for a NB model trained on native English data; it is based on changing the distribution of priors over the correction candidates. Candidate prior is a special parameter in NB; when NB is trained on native data, candidate priors correspond to the relative frequenci"
Q14-1033,N10-1018,1,0.931993,"rst language (Gass and Selinker, 1992; Ionin et al., 2008). There are different ways to adapt a model that depend on the type of training data (learner or native) and the algorithm choice. The key application of adaptation is for models trained on native English data, because the learned models do not know anything about the errors learners make. With adaptation, models trained on native data can use the author’s word (the source word) as a feature and thus propose a correction based on what the author originally wrote. This is crucial, as the source word is an important piece of information (Rozovskaya and Roth, 2010b). Below, several adaptation techniques are summarized and evaluated. The Illinois system makes use of adaptation in the article model via the inflation method and adapts its NB preposition classifier trained on Web1T with the priors method. Adapting NB The priors method (Rozovskaya and Roth, 2011, Sec. 4) is an adaptation technique for a NB model trained on native English data; it is based on changing the distribution of priors over the correction candidates. Candidate prior is a special parameter in NB; when NB is trained on native data, candidate priors correspond to the relative frequenci"
Q14-1033,P11-1093,1,0.86309,"f the Web 1T 5-gram corpus (henceforth Web1T, (Brants and Franz, 2006)). NARA employs a statistical machine translation model for two error types; two systems have rule-based components for selected errors. Based on the analysis of the Illinois system, we identify the following, inter-dependent, dimensions that will be examined in this work: 1. Learning algorithm: Most of the teams, including Illinois, built statistical models. We show that the choice of the learning algorithm is very important and affects the performance of the system. 2. Adaptation to learner errors: Previous studies, e.g. (Rozovskaya and Roth, 2011) showed that adaptation, i.e. developing models that utilize knowledge about error patterns of the non-native writers, is extremely important. We summarize adaptation techniques proposed earlier and examine their impact on the performance of the system. 3. Linguistic knowledge: It is essential to use some linguistic knowledge when developing error correction modules, e.g., to identify which type of verb System Error Art. Illinois (Rozovskaya et al., 2013) Prep. Noun/Agr./Form NTHU (Kao et al., 2013) All Art./Prep./Noun HIT (Xiang et al., 2013) Agr./Form Art./Prep. NARA (Yoshimoto et al., 2013)"
Q14-1033,D13-1074,1,0.883728,"ate-of-the-art error correction system. In this paper, we identify key principles for building a robust grammatical error correction system and show their importance in the context of the shared task. We do this by analyzing the Illinois system and evaluating it along several dimensions: choice Illinois CoNLL-2013 system, ranked at the top. For a description of the Illinois-Columbia submission, we refer the reader to Rozovskaya et al. (2014a). 2 The state-of-the-art performance of the Illinois system discussed here is with respect to individual components for different errors. Improvements in Rozovskaya and Roth (2013) over the Illinois system that are due to joint learning and inference are orthogonal, and the analysis in this paper still applies there. 3 F1 might not be the ideal metric for this task but this was the one chosen in the evaluation. See more in Sec. 6. 419 Transactions of the Association for Computational Linguistics, 2 (2014) 419–434. Action Editor: Alexander Koller. c Submitted 10/2013; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. of learning algorithm; choice of training data (native or annotated learner data); model adaptation to the mistakes made by"
Q14-1033,W11-2843,1,0.861924,"s an option for a post-processing step where corrections that always result in a false positive in training are ignored but this option is not used here. 4.1 The majority of determiner errors involve articles, although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first language backgrounds, most of"
Q14-1033,W12-2032,1,0.902717,"m. The Illinois system has an option for a post-processing step where corrections that always result in a false positive in training are ignored but this option is not used here. 4.1 The majority of determiner errors involve articles, although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first lan"
Q14-1033,E14-1038,1,0.793163,"red 25.01 and the median result was 8.48 points.3 These results suggest that there is not enough understanding of what works best and what elements are essential for building a state-of-the-art error correction system. In this paper, we identify key principles for building a robust grammatical error correction system and show their importance in the context of the shared task. We do this by analyzing the Illinois system and evaluating it along several dimensions: choice Illinois CoNLL-2013 system, ranked at the top. For a description of the Illinois-Columbia submission, we refer the reader to Rozovskaya et al. (2014a). 2 The state-of-the-art performance of the Illinois system discussed here is with respect to individual components for different errors. Improvements in Rozovskaya and Roth (2013) over the Illinois system that are due to joint learning and inference are orthogonal, and the analysis in this paper still applies there. 3 F1 might not be the ideal metric for this task but this was the one chosen in the evaluation. See more in Sec. 6. 419 Transactions of the Association for Computational Linguistics, 2 (2014) 419–434. Action Editor: Alexander Koller. c Submitted 10/2013; Revised 6/2014; Publishe"
Q14-1033,P10-2065,0,0.252235,"Missing"
Q14-1033,W13-3616,0,0.036017,"Missing"
Q14-1033,W13-3605,0,0.0248993,"Missing"
Q14-1033,P11-1019,0,0.107752,"ions made by each of these modules, where the corrections are applied in order. Ordering overlapping candidates8 might potentially affect the final output, when modules correctly identify an error but propose different corrections, but this does not happen in practice. Modules that are part of the Illinois submission are marked with an asterisk in all tables. To demonstrate that our findings are not specific to CoNLL, we also show results on the FCE dataset. It is produced by learners from seventeen first language backgrounds and contains 500,000 words from the Cambridge Learner Corpus (CLC) (Yannakoudakis et al., 2011). We split the corpus into two equal parts – training and test. The statistics are shown in Appendix Tables A.16 and A.17. 5.1 Dim. 1: Learning Algorithm Rozovskaya and Roth (2011, Sec. 3) discuss the relations between the amount of training data, learning algorithms, and the resulting performance. They show that on training sets of similar sizes, discriminative classifiers outperform other machine learning methods on this task. Following these results, the Illinois article module that is trained on the NUCLE corpus uses the discriminative approach AP. Most of the other teams that train on the"
Q14-1033,W14-1704,1,\N,Missing
Q14-1033,W13-3602,1,\N,Missing
Q14-1033,W13-3604,0,\N,Missing
Q15-1001,D08-1031,1,0.475108,"he number of adults and children with HIV/AIDS” and the object is the mention “39.4 million”. Hence, we conclude that the subject can be a candidate for the unit of “39.4 million”. For the purpose of entailment, we keep the entire set of possible word chunks, which are linked by the SRL to our quantity mention, as candidate units. Since most units are found in positions adjacent to the numeric mention, we optimize on runtime by applying the SRL and coreference resolver only when the segmented chunk does not have adequate information to infer the unit. We use the Illinois Coreference Resolver (Bengtson and Roth, 2008; Chang et al., 2013) and the Illinois SRL (Punyakanok et al., 2008), for coreference and semantic role labelling, respectively. 5 Quantity Entailment In this section we describe our approach to quantitative reasoning from natural language text. We first formulate the task of Quantity Entailment, and then describe our reasoning framework. 5 1. entails: there exists a quantity in T which entails h. 2. contradicts: no quantity in T entails h, but there is a quantity in T which contradicts h. 3. no relation: there exists no quantity in T, which is comparable with h. The need to identify sub-probl"
Q15-1001,D13-1057,1,0.565339,"hildren with HIV/AIDS” and the object is the mention “39.4 million”. Hence, we conclude that the subject can be a candidate for the unit of “39.4 million”. For the purpose of entailment, we keep the entire set of possible word chunks, which are linked by the SRL to our quantity mention, as candidate units. Since most units are found in positions adjacent to the numeric mention, we optimize on runtime by applying the SRL and coreference resolver only when the segmented chunk does not have adequate information to infer the unit. We use the Illinois Coreference Resolver (Bengtson and Roth, 2008; Chang et al., 2013) and the Illinois SRL (Punyakanok et al., 2008), for coreference and semantic role labelling, respectively. 5 Quantity Entailment In this section we describe our approach to quantitative reasoning from natural language text. We first formulate the task of Quantity Entailment, and then describe our reasoning framework. 5 1. entails: there exists a quantity in T which entails h. 2. contradicts: no quantity in T entails h, but there is a quantity in T which contradicts h. 3. no relation: there exists no quantity in T, which is comparable with h. The need to identify sub-problems of textual infere"
Q15-1001,W02-1001,0,0.0333808,"t: Set of Quantity-value triples extracted from T 1: Q ← ∅ 2: S ← Segmentation( T ) 3: for all segment s ∈ S do 4: q ← Standardization( s ) 5: if unit of q not inferred then 6: q ← InferUnitFromSemantics( q, s, T ) 7: end if 8: Q ← Q ∪ {q} 9: end for 10: return Q We model the segmentation step as a sequence segmentation task because quantities often appear as segments of contiguous text. We adapt and compare two approaches that were found successful in previous sequential segmentation work in NLP: 1. A Semi-CRF model (Sarawagi and Cohen, 2004), trained using a structured Perceptron algorithm (Collins, 2002), with Parameter Averaging (Freund and Schapire, 1998). 2. A bank of classifiers approach (Punyakanok and Roth, 2001) that we retrain with a new set of features. The same feature set was used for both approaches. Despite the additional expressive power of CRFs, we found that the bank of classifiers (which is followed by a simple and tractable inference step) performs better for our task, and also requires significantly less computation time. 4.1 Features For each token xi in the input sequence we extract the following features: 1. Word class features: xi appears in a list of known scientific u"
Q15-1001,P08-1118,0,0.0749014,"Missing"
Q15-1001,D12-1062,1,0.161265,"Missing"
Q15-1001,P14-1026,0,0.305065,"Missing"
Q15-1001,W04-0902,0,0.663113,"Missing"
Q15-1001,C08-1066,0,0.127982,"Missing"
Q15-1001,J08-2005,1,0.242205,"he mention “39.4 million”. Hence, we conclude that the subject can be a candidate for the unit of “39.4 million”. For the purpose of entailment, we keep the entire set of possible word chunks, which are linked by the SRL to our quantity mention, as candidate units. Since most units are found in positions adjacent to the numeric mention, we optimize on runtime by applying the SRL and coreference resolver only when the segmented chunk does not have adequate information to infer the unit. We use the Illinois Coreference Resolver (Bengtson and Roth, 2008; Chang et al., 2013) and the Illinois SRL (Punyakanok et al., 2008), for coreference and semantic role labelling, respectively. 5 Quantity Entailment In this section we describe our approach to quantitative reasoning from natural language text. We first formulate the task of Quantity Entailment, and then describe our reasoning framework. 5 1. entails: there exists a quantity in T which entails h. 2. contradicts: no quantity in T entails h, but there is a quantity in T which contradicts h. 3. no relation: there exists no quantity in T, which is comparable with h. The need to identify sub-problems of textual inference, in the context of the RTE task, has been m"
Q15-1001,P98-2186,1,0.195681,"g bounds : We convert values which have a bound to a range of values. Scalar implicature is taken into consideration here. Consider the sentence “John bought 10 books.”, although it can be interpreted that buying 5 books is a corollary of buying 10, in this case, we make the assumption that 5 books were not purchased. See section 5.2 for a discussion on the subject. 2. Character-based: xi contains a digit, is all digits, has a suffix (st,nd,rd,th). We use the following rules, where v is the value extracted before using bound information. 3. Part of speech tags: we use the Illinois POS Tagger (Roth and Zelenko, 1998). 4. Most of the features were generated from a window of [−3, 3] around the current word. Additional features were generated from these by conjoining them with offset values from the current word. 4.2 Mapping Text Segments into QVR We develop a rule-based standardization step, that is informed, as needed, by deeper NL processing, including semantic role labeling (SRL, (Palmer et al., 2010)) and Co-reference resolution. Some key steps of this procedure are as follows: 4 e.g., • ≤ v → (−∞, v], similarly for ≥, <, >. • = v → {v} • ≈ v → [v − c.v, v + c.v], we use c = 0.2. 4.3 Extraction of Units"
Q15-1001,P10-1122,1,0.756581,"ence and semantic role labelling, respectively. 5 Quantity Entailment In this section we describe our approach to quantitative reasoning from natural language text. We first formulate the task of Quantity Entailment, and then describe our reasoning framework. 5 1. entails: there exists a quantity in T which entails h. 2. contradicts: no quantity in T entails h, but there is a quantity in T which contradicts h. 3. no relation: there exists no quantity in T, which is comparable with h. The need to identify sub-problems of textual inference, in the context of the RTE task, has been motivated by (Sammons et al., 2010). Quantity Entailment can be considered as one such step. Since we envision that our QE module will be one module in an RTE system, we expect that the RTE system will provide it with some control information. For example, it is often important to know whether the quantity is mentioned in an upward or downward monotonic context. Since we are evaluating our QE approach in isolation, we will always assume upward monotonicity, which is a lot more common. Monotonicity has been modeled with some success in entailment systems (Maccartney and Manning, 2008), thus providing a clear and intuitive framew"
Q15-1001,H05-1088,0,0.0163757,"Missing"
Q15-1001,C98-2181,1,\N,Missing
Q16-1011,E09-1005,0,0.157871,"mention it is a candidate for. • CollectiveInf (Zheng et al., 2015) In this method, the initial score for each concept is calculated by a modified PageRank algorithm, in which the entropy of relations are used as the edges’ weights. The final score of a concept candidate is further adjusted by the matching between neighboring concepts in the KB and the concept candidates around the mention. That is, if a neighbor concept in the KBs also appears in the context of the mention, the score of the concept candidate is increased according to the initial score of the matched neighbor concept. • Ppr (Agirre and Soroa, 2009) The Personalized PageRank algorithm implemented in the UKB package4 . This method first inserts the context mentions into the graph as nodes, and links them with directed edges to the corresponding concept candidates. The PageRank algorithm is then applied by concentrating the initial probability mass uniformly over the mention nodes. We take a window of 30 mentions as the context. Note that in order to have a fair comparison of the disambiguation ability, we use the proposed candidate generation method in Section 4 to produce the confusion set for each 4 http://ixa2.si.ehu.es/ukb/ Approach T"
Q16-1011,P98-1013,0,0.0601044,"conceptconcept pairs. These, as we show, approximate well the features of mention-concept pairs at test time. If (1) is satisfied, that is, there are multiple KBs and enough entries in them that can be aligned, then the proposed method can be applied. However, the performance of this method highly depends on (2), the quality of features and how well the indirect supervision examples approximate the text at prediction time. An application which fits this setting is the verb sense disambiguation problem, where there are multiple sense inventories (e.g., VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005)) and many of the senses are aligned by different resources (e.g., 152 UBY (Gurevych et al., 2012) and Unified Verb Index5 ). There are corpora which have annotations from one or more these verb sense inventories available, such as OntoNotes (Pradhan et al., 2007) and MASC6 . However, unlike the biomedical ontologies which have many common attributes and relatively uniform structure, different verb sense inventories vary in format and content: some resources have descriptions or example sentences of the senses, but others only have the names of semantic role"
Q16-1011,E06-1002,0,0.185851,"oth and Yih, 2004; Chang et al., 2012) to enforce a coherent global mapping of all mentions in a given document to their corresponding concepts. The proposed system, CCMIS (CCM with Indirect Supervision), performs significantly better than the best unsupervised baseline and is competitive with a directly supervised model we use to assess the quality of the automatically generated indirect supervision. 2 Related Work In the news domain, many researchers have studied ways to train a model to disambiguate concepts by directly using hyperlinks in Wikipedia documents as supervision. Earlier works (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007) focus on local features which compare context words with the content of candidate Wikipedia pages. Later, several works (Cucerzan, 2007; Milne and Witten, 2008; Han and Zhao, 2009; Ferragina and Scaiella, 2010; Ratinov et al., 2011) explore global features, trying to capture coherence among concepts that appear in close proximity in the text. Shen et al. (2012) and Dredze et al. (2010) train their model on a small manually created data set to handle documents in different domains. Cheng and Roth (2013) use relations between entities as constraints to support global"
Q16-1011,D13-1184,1,0.938389,"Tse Tsai and Dan Roth University of Illinois at Urbana-Champaign 201 N. Goodwin, Urbana, Illinois, 61801 {ctsai12, danr}@illinois.edu Abstract a popular method for contextually disambiguating them and can be used also for focused knowledge acquisition. It has been shown a valuable component for several natural language processing and information extraction tasks across different domains. In the news domain, the task is often called Wikification or Entity Linking and has been studied extensively recently (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). Wikipedia is widely used as the target KB due to its broad coverage and detailed information of concepts. While Wikipedia is an excellent general purpose encyclopedic resource, when the text is domain specific, it may not be the single ideal resource; the text could be better “covered” by multiple ontological or encyclopedic resources. We consider the problem of disambiguating concept mentions appearing in documents and grounding them in multiple knowledge bases, where each knowledge base addresses some aspects of the domain. This problem poses a few additional challenges beyond those addres"
Q16-1011,E14-1008,0,0.0468664,") use relations between entities as constraints to support global inference with ranker scores, and show substantial improvement on several datasets. The main difference between our method and these Wikification approaches is that we train a ranking model by constructing indirect supervision signals from multiple KBs without using any annotated documents. Concept Grounding and Word Sense Disambiguation (WSD) are closely related tasks as they both address the lexical ambiguity of language. Recently, several works try to relate the two by incorporating the lexical resources used in these tasks. Cholakov et al. (2014) disambiguate verbs to the senses in WordNet by creating semantic patterns from multiple lexical KBs, i.e., Wikipedia, Wikitionary, WordNet, FrameNet, and VerbNet, and also for each verb mention in the text. Moro et al. (2014) propose a graphbased approach which uses Wikipedia and WordNet as lexical resources. Their unified approach can achieve state of the art results on 6 Wikification and WSD datasets. The observation from these two papers are consistent with our conclusion that using multiple KBs jointly can improve individual tasks. Matuschek and Gurevych (2014) try to align different lexi"
Q16-1011,D07-1074,0,0.905361,"unding to Multiple Knowledge Bases via Indirect Supervision Chen-Tse Tsai and Dan Roth University of Illinois at Urbana-Champaign 201 N. Goodwin, Urbana, Illinois, 61801 {ctsai12, danr}@illinois.edu Abstract a popular method for contextually disambiguating them and can be used also for focused knowledge acquisition. It has been shown a valuable component for several natural language processing and information extraction tasks across different domains. In the news domain, the task is often called Wikification or Entity Linking and has been studied extensively recently (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). Wikipedia is widely used as the target KB due to its broad coverage and detailed information of concepts. While Wikipedia is an excellent general purpose encyclopedic resource, when the text is domain specific, it may not be the single ideal resource; the text could be better “covered” by multiple ontological or encyclopedic resources. We consider the problem of disambiguating concept mentions appearing in documents and grounding them in multiple knowledge bases, where each knowledge base addresses some aspects of the do"
Q16-1011,C10-1032,0,0.0796377,"In the news domain, many researchers have studied ways to train a model to disambiguate concepts by directly using hyperlinks in Wikipedia documents as supervision. Earlier works (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007) focus on local features which compare context words with the content of candidate Wikipedia pages. Later, several works (Cucerzan, 2007; Milne and Witten, 2008; Han and Zhao, 2009; Ferragina and Scaiella, 2010; Ratinov et al., 2011) explore global features, trying to capture coherence among concepts that appear in close proximity in the text. Shen et al. (2012) and Dredze et al. (2010) train their model on a small manually created data set to handle documents in different domains. Cheng and Roth (2013) use relations between entities as constraints to support global inference with ranker scores, and show substantial improvement on several datasets. The main difference between our method and these Wikification approaches is that we train a ranking model by constructing indirect supervision signals from multiple KBs without using any annotated documents. Concept Grounding and Word Sense Disambiguation (WSD) are closely related tasks as they both address the lexical ambiguity o"
Q16-1011,E12-1059,0,0.0167633,"that is, there are multiple KBs and enough entries in them that can be aligned, then the proposed method can be applied. However, the performance of this method highly depends on (2), the quality of features and how well the indirect supervision examples approximate the text at prediction time. An application which fits this setting is the verb sense disambiguation problem, where there are multiple sense inventories (e.g., VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005)) and many of the senses are aligned by different resources (e.g., 152 UBY (Gurevych et al., 2012) and Unified Verb Index5 ). There are corpora which have annotations from one or more these verb sense inventories available, such as OntoNotes (Pradhan et al., 2007) and MASC6 . However, unlike the biomedical ontologies which have many common attributes and relatively uniform structure, different verb sense inventories vary in format and content: some resources have descriptions or example sentences of the senses, but others only have the names of semantic roles; some have relations between senses but some do not. Therefore, the question of what features would be useful in this case could be"
Q16-1011,C14-1025,0,0.0126201,"cal resources used in these tasks. Cholakov et al. (2014) disambiguate verbs to the senses in WordNet by creating semantic patterns from multiple lexical KBs, i.e., Wikipedia, Wikitionary, WordNet, FrameNet, and VerbNet, and also for each verb mention in the text. Moro et al. (2014) propose a graphbased approach which uses Wikipedia and WordNet as lexical resources. Their unified approach can achieve state of the art results on 6 Wikification and WSD datasets. The observation from these two papers are consistent with our conclusion that using multiple KBs jointly can improve individual tasks. Matuschek and Gurevych (2014) try to align different lexical resources (WordNet, Wikitionary, and Wikipedia in different languages). This approach is related to our construction of indirect supervision, and it would be interesting to see if the alignments could improve the quality of the indirect supervision and thus the quality of the disambiguation. In the biomedical domain, the extensively studied word sense disambiguation problem (Weeber et al., 2001) focuses on disambiguating mentions to UMLS (Unified Medical Language System) Metathesaurus (Bodenreider, 2004). The main difference from our problem is that the WSD prob"
Q16-1011,P08-3009,0,0.0265171,"improve the quality of the indirect supervision and thus the quality of the disambiguation. In the biomedical domain, the extensively studied word sense disambiguation problem (Weeber et al., 2001) focuses on disambiguating mentions to UMLS (Unified Medical Language System) Metathesaurus (Bodenreider, 2004). The main difference from our problem is that the WSD problem only addresses a small number of terms and the candidate concepts for each ambiguous mention are provided as part of the input. Researchers have developed various unsupervised methods that 143 make use of information in the KB. McInnes (2008) compared the context words of the ambiguous mention to a profile built from UMLS concepts. Viewing the KB as a graph and adding context information into the graph, Agirre et al. (2010) compared the original PageRank algorithm with a personalized version. Jimeno-Yepes and Aronson (2010) automatically built training examples for each sense by retrieving documents from a large corpus. This approach is infeasible for our problem because we have a large amount of candidate concepts. The popular system MetaMap (Aronson and Lang, 2010) disambiguates mentions to semantic categories in UMLS using jour"
Q16-1011,Q14-1019,0,0.0272193,"we train a ranking model by constructing indirect supervision signals from multiple KBs without using any annotated documents. Concept Grounding and Word Sense Disambiguation (WSD) are closely related tasks as they both address the lexical ambiguity of language. Recently, several works try to relate the two by incorporating the lexical resources used in these tasks. Cholakov et al. (2014) disambiguate verbs to the senses in WordNet by creating semantic patterns from multiple lexical KBs, i.e., Wikipedia, Wikitionary, WordNet, FrameNet, and VerbNet, and also for each verb mention in the text. Moro et al. (2014) propose a graphbased approach which uses Wikipedia and WordNet as lexical resources. Their unified approach can achieve state of the art results on 6 Wikification and WSD datasets. The observation from these two papers are consistent with our conclusion that using multiple KBs jointly can improve individual tasks. Matuschek and Gurevych (2014) try to align different lexical resources (WordNet, Wikitionary, and Wikipedia in different languages). This approach is related to our construction of indirect supervision, and it would be interesting to see if the alignments could improve the quality o"
Q16-1011,J05-1004,0,0.00782527,"show, approximate well the features of mention-concept pairs at test time. If (1) is satisfied, that is, there are multiple KBs and enough entries in them that can be aligned, then the proposed method can be applied. However, the performance of this method highly depends on (2), the quality of features and how well the indirect supervision examples approximate the text at prediction time. An application which fits this setting is the verb sense disambiguation problem, where there are multiple sense inventories (e.g., VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005)) and many of the senses are aligned by different resources (e.g., 152 UBY (Gurevych et al., 2012) and Unified Verb Index5 ). There are corpora which have annotations from one or more these verb sense inventories available, such as OntoNotes (Pradhan et al., 2007) and MASC6 . However, unlike the biomedical ontologies which have many common attributes and relatively uniform structure, different verb sense inventories vary in format and content: some resources have descriptions or example sentences of the senses, but others only have the names of semantic roles; some have relations between sense"
Q16-1011,P11-1138,1,0.949089,"rect Supervision Chen-Tse Tsai and Dan Roth University of Illinois at Urbana-Champaign 201 N. Goodwin, Urbana, Illinois, 61801 {ctsai12, danr}@illinois.edu Abstract a popular method for contextually disambiguating them and can be used also for focused knowledge acquisition. It has been shown a valuable component for several natural language processing and information extraction tasks across different domains. In the news domain, the task is often called Wikification or Entity Linking and has been studied extensively recently (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). Wikipedia is widely used as the target KB due to its broad coverage and detailed information of concepts. While Wikipedia is an excellent general purpose encyclopedic resource, when the text is domain specific, it may not be the single ideal resource; the text could be better “covered” by multiple ontological or encyclopedic resources. We consider the problem of disambiguating concept mentions appearing in documents and grounding them in multiple knowledge bases, where each knowledge base addresses some aspects of the domain. This problem poses a few additional challen"
Q16-1011,W04-2401,1,0.84036,"s. and should therefore be grounded to human concepts in the NCBI Taxonomy, we can easily rule out all the candidate genes from other species, which are not mentioned in the document; we can develop these constraints since genes in the Entrez Gene KB have NCBI Taxonomy IDs as species attributes. If we do not use the NCBI Taxonomy as a knowledge source to ground concepts but rather only focus on disambiguating gene names in a document, we may lose this valuable information. Our final model combines this kind of prior knowledge with our ranker scores using a Constrained Conditional Model (CCM) (Roth and Yih, 2004; Chang et al., 2012) to enforce a coherent global mapping of all mentions in a given document to their corresponding concepts. The proposed system, CCMIS (CCM with Indirect Supervision), performs significantly better than the best unsupervised baseline and is competitive with a directly supervised model we use to assess the quality of the automatically generated indirect supervision. 2 Related Work In the news domain, many researchers have studied ways to train a model to disambiguate concepts by directly using hyperlinks in Wikipedia documents as supervision. Earlier works (Bunescu and Pasca"
Q16-1011,tanenblatt-etal-2010-conceptmapper,0,0.0225642,"organ et al., 2008; Lu and Wilbur, 2010; Mao et al., 2013) and chemical document indexing (Krallinger et al., 2013). These tasks are closer to the problem of automatic indexing of biomedical literature, however, all these studies focus on a single KB or even a subset of it. In our experiments we make use of the CRAFT dataset that has been studied extensively; however, most of these studies focus on mention extraction rather than disambiguating mentions. Funk et al. (2014) comprehensively compared three dictionarybased systems: MetaMap, NCBO Annotator (Jonquet et al., 2009), and ConceptMapper (Tanenblatt et al., 2010) and shows that the latter has the best performance. However, it only applies various string matching strategies on the surface string of the mention and the concept names in KBs, and does not attempt any disambiguation based on the context of the mentions. 3 Task Definition and Model Overview We formalize the problem as follows. We are given a document d with a set of mentions M = Figure 2: Algorithmic components of our system. {m1 , . . . , mn }, and l KBs, k1 , . . . , kl . Each KB kj is a graph (Tj , Rj ), where a concept t ∈ Tj represents a node and a relation r ∈ Rj between two concepts"
Q16-1011,C98-1013,0,\N,Missing
Q18-1012,P07-1036,1,0.702135,"ion of each declarative rule usually requires reasoning across multiple sentences. Further, we do not require an explicit grounding of words or phrases to logical variables. 2.3 Background Knowledge in Learning Approaches to incorporate knowledge in learning started with Explanation Based Learning (EBL) (DeJong, 1993; DeJong, 2014). EBL uses domain knowledge based on observable predicates, whereas we learn to map text to predicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and Yih, 2004; Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et 161 al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness, or lack of, the approach (Khashabi et al., 2016; Jia and Liang, 2017). We make use of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation Here, we introduce our representation of domain knowledge. We organize the knowledge hier"
Q18-1012,D14-1082,0,0.0183917,"Missing"
Q18-1012,W10-2903,1,0.72263,"uces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Similarly, we learn to select declarative rules from supervision that only includes the final operation (and not which rule generated it). However, in contrast to the semantic parsing work, in our case the selection of each declarative rule usually requires reasoning across multiple sentences. Further, we do not require an explicit grounding of words or phrases to logical variables. 2.3 Background K"
Q18-1012,D14-1139,0,0.0125945,"s or phrases to logical variables. 2.3 Background Knowledge in Learning Approaches to incorporate knowledge in learning started with Explanation Based Learning (EBL) (DeJong, 1993; DeJong, 2014). EBL uses domain knowledge based on observable predicates, whereas we learn to map text to predicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and Yih, 2004; Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et 161 al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness, or lack of, the approach (Khashabi et al., 2016; Jia and Liang, 2017). We make use of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation Here, we introduce our representation of domain knowledge. We organize the knowledge hierarchically in two levels – concepts and declarative rules. A math concept is a phenomenon which needs to be understood to apply reasoning over qu"
Q18-1012,D17-1083,0,0.0328639,"Missing"
Q18-1012,D14-1058,0,0.566747,"background knowledge in learning. 2.1 Automatic Word Problem Solving There has been a growing interest in automatically solving math word problems, with various systems focusing on particular types of problems. These can be broadly categorized into two types: arithmetic and algebra. Arithmetic Word Problems Arithmetic problems involve combining numbers with basic operations (addition, subtraction, multiplication and division), and are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) and this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction. Some of these approaches also try to incorporate some form of declarative or domain knowledge. Hosseini et al. (2014) incorporates the transfer phenomenon by classifying verbs; Mitra and Baral (2016) maps problems to a set of formulas. Both require extensive annotations for intermediate steps (verb classification for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Baral (2016), etc). In contrast, our method can handle a more general class of problems, while training only req"
Q18-1012,D17-1084,0,0.372036,"d here by incorporating common concepts required for arithmetic problems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a kn"
Q18-1012,D17-1215,0,0.0223093,"able predicates, whereas we learn to map text to predicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and Yih, 2004; Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et 161 al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness, or lack of, the approach (Khashabi et al., 2016; Jia and Liang, 2017). We make use of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation Here, we introduce our representation of domain knowledge. We organize the knowledge hierarchically in two levels – concepts and declarative rules. A math concept is a phenomenon which needs to be understood to apply reasoning over quantities. Examples of concepts include part-whole relations, dimensional analysis, etc. Under each concept, there are a few declarative rules, which dictate which operation is needed in a particular context. An example of a declarati"
Q18-1012,Q15-1042,0,0.239273,"s of problems, while training only requires problemequation pairs coupled with rate component annotations. Roy and Roth (2017) focuses only on using dimensional analysis knowledge, and handles the same class of problems as we do. In contrast, our method provides a framework for including any form of declarative knowledge, exemplified here by incorporating common concepts required for arithmetic problems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more co"
Q18-1012,N16-1136,1,0.881498,"Missing"
Q18-1012,P14-1026,0,0.474814,"017) focuses only on using dimensional analysis knowledge, and handles the same class of problems as we do. In contrast, our method provides a framework for including any form of declarative knowledge, exemplified here by incorporating common concepts required for arithmetic problems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect superv"
Q18-1012,P11-1060,0,0.0300876,"approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Similarly, we learn to select declarative rules from supervision that only includes the final operation (and not which rule generated it). However, in contrast to the semantic parsing work, in our case the selection of each declarative rule usually requires reasoning across multiple sentences. Further, we do not require an explicit grounding of words or phrases to logical variables. 2.3 Background Knowledge in Learning"
Q18-1012,P17-1015,0,0.294614,"Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Similarly, we learn to select declarative rules from supervision that only includ"
Q18-1012,P17-1195,0,0.0223316,"cting (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Similarly, we learn to select declarative rules from supervision that only includes the final operation (and not which rule generated it). However, in contrast to the seman"
Q18-1012,P16-1202,0,0.60339,"rning. 2.1 Automatic Word Problem Solving There has been a growing interest in automatically solving math word problems, with various systems focusing on particular types of problems. These can be broadly categorized into two types: arithmetic and algebra. Arithmetic Word Problems Arithmetic problems involve combining numbers with basic operations (addition, subtraction, multiplication and division), and are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) and this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction. Some of these approaches also try to incorporate some form of declarative or domain knowledge. Hosseini et al. (2014) incorporates the transfer phenomenon by classifying verbs; Mitra and Baral (2016) maps problems to a set of formulas. Both require extensive annotations for intermediate steps (verb classification for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Baral (2016), etc). In contrast, our method can handle a more general class of problems, while training only requires problemequation pairs"
Q18-1012,D10-1120,0,0.0185236,"Missing"
Q18-1012,D14-1162,0,0.0835043,"Missing"
Q18-1012,W04-2401,1,0.370798,"our case the selection of each declarative rule usually requires reasoning across multiple sentences. Further, we do not require an explicit grounding of words or phrases to logical variables. 2.3 Background Knowledge in Learning Approaches to incorporate knowledge in learning started with Explanation Based Learning (EBL) (DeJong, 1993; DeJong, 2014). EBL uses domain knowledge based on observable predicates, whereas we learn to map text to predicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and Yih, 2004; Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et 161 al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness, or lack of, the approach (Khashabi et al., 2016; Jia and Liang, 2017). We make use of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation Here, we introduce our representation of domain knowledge. We organiz"
Q18-1012,D15-1202,1,0.733529,"ajor strands of research - automatic word problem solving, semantic parsing, and approaches incorporating background knowledge in learning. 2.1 Automatic Word Problem Solving There has been a growing interest in automatically solving math word problems, with various systems focusing on particular types of problems. These can be broadly categorized into two types: arithmetic and algebra. Arithmetic Word Problems Arithmetic problems involve combining numbers with basic operations (addition, subtraction, multiplication and division), and are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) and this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction. Some of these approaches also try to incorporate some form of declarative or domain knowledge. Hosseini et al. (2014) incorporates the transfer phenomenon by classifying verbs; Mitra and Baral (2016) maps problems to a set of formulas. Both require extensive annotations for intermediate steps (verb classification for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Bar"
Q18-1012,D16-1117,1,0.808943,"m the works of Hosseini et al. (2014); Roy and Roth (2015), and found that the math concepts and declarative rules introduced in this paper cover all their problems. A major challenge in applying these concepts and rules to algebra word problems is the use of variables in constructing equations. Variables are often implicitly described, and it is difficult to extract units, dependent verbs, associated subjects and objects for the variables. However, we need these extractions in order to apply our declarative rules to combine variables. There has been some work to extract meaning of variables (Roy et al., 2016) in algebra word problems; an extension of this can possibly support the application of rules in algebra word problems. We leave this exploration to future work. Higher standard word problems often require the application of math formulas like ones related to area, interest, probability, etc. Extending our approach to handle such problems will involve encoding math formulas in terms of concepts and rules, as well as adding concept specific features to the learned predictors. The declarative rules under the Explicit Math category currently handles simple cases; this set needs to be augmented to"
Q18-1012,D15-1135,0,0.178826,"ed with rate component annotations. Roy and Roth (2017) focuses only on using dimensional analysis knowledge, and handles the same class of problems as we do. In contrast, our method provides a framework for including any form of declarative knowledge, exemplified here by incorporating common concepts required for arithmetic problems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also re"
Q18-1012,P06-1072,0,0.0104691,"multiple sentences. Further, we do not require an explicit grounding of words or phrases to logical variables. 2.3 Background Knowledge in Learning Approaches to incorporate knowledge in learning started with Explanation Based Learning (EBL) (DeJong, 1993; DeJong, 2014). EBL uses domain knowledge based on observable predicates, whereas we learn to map text to predicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and Yih, 2004; Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et 161 al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness, or lack of, the approach (Khashabi et al., 2016; Jia and Liang, 2017). We make use of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation Here, we introduce our representation of domain knowledge. We organize the knowledge hierarchically in two levels – concepts and declarative rules. A math"
Q18-1012,D16-1029,0,0.291852,"e knowledge, exemplified here by incorporating common concepts required for arithmetic problems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the"
Q18-1012,D15-1096,0,0.444647,"Missing"
Q19-1001,E14-3013,0,0.171247,"aining from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form of artificial errors or by changing the model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign"
Q19-1001,P18-1127,0,0.174583,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,N10-1019,0,0.160887,"focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng,"
Q19-1001,P18-1059,0,0.0560671,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,I08-1059,0,0.052464,"15; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al.,"
Q19-1001,N18-2020,0,0.117356,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,P11-1092,0,0.0206157,"nder, and person and agree with the grammatical subject. Other categories for verbs are aspect, tense, and voice. These are typically expressed through morphemes corresponding to functional words in English (shall, will, was, have, had, been, etc.). made by non-native speakers are not random (Montrul and Slabakova, 2002; Ionin et al., 2008), using the potentially erroneous word and the correction provides the models with knowledge about learner error patterns. For this reason, models trained on error-annotated data often outperform models trained on larger amounts of native data (Gamon, 2010; Dahlmeier and Ng, 2011). But this approach requires large amounts of annotated learner data (Gamon, 2010). The minimal supervision approach (Rozovskaya and Roth, 2014; Rozovskaya et al., 2017) incorporates the best of both modes: training on native texts to facilitate the possibility of training from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form o"
Q19-1001,D12-1052,0,0.0221461,"d creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting comple"
Q19-1001,W12-2006,0,0.0997316,"Missing"
Q19-1001,W10-1802,0,0.0737954,"Missing"
Q19-1001,W11-2838,0,0.0745788,"Missing"
Q19-1001,P13-2121,0,0.0295061,"Missing"
Q19-1001,C08-1022,0,0.097737,"Missing"
Q19-1001,C08-2008,0,0.0746635,"Missing"
Q19-1001,dickinson-ledbetter-2012-annotating,0,0.0263887,"dress the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task:"
Q19-1001,P12-2076,0,0.0615788,"rammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction syste"
Q19-1001,N15-1060,0,0.0133458,"of one to five sentences). Mizumoto et al. (2011) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Kore"
Q19-1001,P07-2045,0,0.00859509,"Missing"
Q19-1001,W17-3204,0,0.0132583,"orpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse features is an improvement of 3 to 4 points over the baseline system (they also rely on much large tuning sets, as required for sparse features). The baseline system is essentially the same as that of Susanto et al. (2014). Because our MT result is so much lower than the classification system, we do not expect that adding sparse and dense features will close that gap. approach (Rozovskaya et al., 2017) to simulate lea"
Q19-1001,I13-1199,0,0.0159925,"ve been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana"
Q19-1001,P17-1070,0,0.0154963,"performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tok"
Q19-1001,D16-1161,0,0.155647,"elice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), b"
Q19-1001,N18-2046,0,0.092722,"nd Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tokens. Approaches to Text Correction There are currently two well-studied paradigms that achieve competitive results on the task in English–MT and machine learning classification. In the classification approach, error-specific classifiers are built. Given a confusion set, for example {a, the, zero article} for articles, each occurrence of a confusabl"
Q19-1001,I11-1017,0,0.710635,"t and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for o"
Q19-1001,N18-1055,0,0.0645008,"fication methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tokens. Approaches to Text Correction There are currently two well-studied paradigms that achieve competitive results on the task in English–MT and machine learning classification. In the classification approach, error-specific classifiers are built. Given a confusion set, for example {a, the, zero article} for"
Q19-1001,W14-3605,1,0.942416,"ry well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morpholog"
Q19-1001,P15-2097,0,0.252455,"Missing"
Q19-1001,E17-2037,0,0.144591,"Missing"
Q19-1001,W14-1701,0,0.180795,"Missing"
Q19-1001,W15-3204,1,0.93391,"ned types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, wit"
Q19-1001,W13-3601,0,0.103359,"Missing"
Q19-1001,W10-1004,1,0.79392,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P03-1021,0,0.0313127,".0087 Inst. .0002 .0031 0045 .0008 .9511 .0017 Loc. .0007 .0027 .0060 .0031 .0014 .0980 Table 9: Confusion matrix for noun case errors based on the training and development data from the RULEC-GEC corpus. The left column shows the correct case. Each row shows the author’s case choices for that label and P rob(source|label). of RULEC-GEC. We use two 4-gram language models—one is trained on the Yandex corpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse fea"
Q19-1001,D10-1094,1,0.822469,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P02-1040,0,0.10899,"045 .0008 .9511 .0017 Loc. .0007 .0027 .0060 .0031 .0014 .0980 Table 9: Confusion matrix for noun case errors based on the training and development data from the RULEC-GEC corpus. The left column shows the correct case. Each row shows the author’s case choices for that label and P rob(source|label). of RULEC-GEC. We use two 4-gram language models—one is trained on the Yandex corpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse features is an improvement of 3 to 4 poi"
Q19-1001,N10-1018,1,0.713855,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P11-1093,1,0.895451,"notating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly"
Q19-1001,W16-6509,0,0.0305538,"at addresses errors in morphology, syntax, and word usage, and takes into account linguistic properties of the Russian language, by emphasizing those that are most commonly misused. The common phenomena were identified through a pilot annotation, and with the help of sample errors that had been collected with the Russian National Corpus in the process of developing a similar annotation of Russian learner texts. The sample errors were made available to us by the authors (Klyachko et al., 2013). This study resulted in an annotated corpus, available for online search at http:// web-corpora.net/ (Rakhilina et al., 2016). Corpus and Annotation We annotated data from the Russian Learner Corpus of Academic Writing (RULEC, 560K words) (Alsufieva et al., 2012), which consists of essays and papers written in a university setting in the United States by students learning Russian as a foreign language and heritage speakers (those who grew up in the United States but had exposure to Russian at home). This closely mirrors the datasets used for English grammar correction. The corpus contains data from 15 foreign language learners and 13 heritage speakers. RULEC is freely available for research use.5 3.1 Russian Grammat"
Q19-1001,Q14-1033,1,0.835863,"ed through morphemes corresponding to functional words in English (shall, will, was, have, had, been, etc.). made by non-native speakers are not random (Montrul and Slabakova, 2002; Ionin et al., 2008), using the potentially erroneous word and the correction provides the models with knowledge about learner error patterns. For this reason, models trained on error-annotated data often outperform models trained on larger amounts of native data (Gamon, 2010; Dahlmeier and Ng, 2011). But this approach requires large amounts of annotated learner data (Gamon, 2010). The minimal supervision approach (Rozovskaya and Roth, 2014; Rozovskaya et al., 2017) incorporates the best of both modes: training on native texts to facilitate the possibility of training from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form of artificial errors or by changing the model priors. In this work, we use the artificial errors approach; it has been studied extensively for E"
Q19-1001,P16-1208,1,0.878385,"al annotated learner datasets became available, models were also trained on annotated learner data. More recently, the statistical machine translation (MT) methods, including neural MT, have gained considerable popularity thanks to the availability of large annotated corpora of learner writing (e.g., Yuan and Briscoe, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach th"
Q19-1001,J17-4002,1,0.944323,"es extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology. 1 Introduction This paper addresses the task of correcting errors in text. Most of the research in the area of grammar error correction (GEC) focused on correcting mistakes made by English language learners. One standard approach to dealing with these errors, which proved highly successful in text correction competitions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014; Rozovskaya et al., 2017), makes use of a machine1 Transactions of the Association for Computational Linguistics, vol. 7, pp. 1–17, 2019. Action Editor: Jianfeng Gao. Submission batch: 4/2018; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. that are specific to these languages. (4) We demonstrate that the classification framework with minimal supervision is particularly useful for morphologically rich languages; they can benefit from large amounts of native data, due to a large variability of word forms, and small amounts of annotation"
Q19-1001,W18-3706,0,0.0406869,"is indicates that the MT system suffers more than classifiers, when the amount of supervision is particularly small, while the morphological complexity of the language is higher. Considering Arabic and Chinese, where the training data is also limited, the results are also much lower than in English. In Arabic, where the supervised learner data includes 43K words, the best reported F-score is 27.32 (Rozovskaya et al., 2015).8 In Chinese, the supervised dataset size is about 50K sentences, and the highest reported scores are 26.93 for detection (Rao et al., 2017) and 17.23 for error correction (Rao et al., 2018), respectively. These results confirm that the approaches that rely on large amounts of supervision do not carry over to low-resource 6.1 Error Analysis To understand the challenges of grammar correction in a morphologically rich language such as Russian, we perform error analysis of the MT system and the classification system that uses minimal supervision. The nature of grammar correction is such that multiple different corrections are often acceptable (Ng et al., 2014). Furthermore, annotators often disagree on what constitutes a mistake, and some gold errors missed by a system may be consid"
Q19-1001,Q16-1013,0,0.02778,"tem that uses CoNLL data obtains an F0.5 score of 36.26 (Rozovskaya and Roth, 2016). In contrast, the classification system for Russian obtains a much lower score of 21.0. This may be due to a larger variety of grammatical phenomena in Russian, lower error rates, and a high proportion of spelling errors (especially among heritage speakers), which we currently do not specifically target. Note also that the CoNLL-2014 results are based on two gold references for each sentence, while we evaluate with respect to one, and having more reference annotations improves performance (Bryant and Ng, 2015; Sakaguchi et al., 2016; Choshen and Abend, 2018b).7 It should also be noted that the gap between the MT system and the classification system when both are trained with limited supervision is larger for Russian (10.6 vs. 20.5) than for English (28.25 vs. 36.26). This indicates that the MT system suffers more than classifiers, when the amount of supervision is particularly small, while the morphological complexity of the language is higher. Considering Arabic and Chinese, where the training data is also limited, the results are also much lower than in English. In Arabic, where the supervised learner data includes 43K"
Q19-1001,I17-4001,0,0.0857053,"Missing"
Q19-1001,P10-2065,0,0.0303007,"has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Cholla"
Q19-1001,W17-1408,0,0.0278391,"attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman"
Q19-1001,vincze-etal-2014-automatic,0,0.259444,"Missing"
Q19-1001,P11-1019,0,0.0865019,"Missing"
Q19-1001,D14-1102,0,0.403566,"mon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Ju"
Q19-1001,N16-1042,0,0.0802466,"rill, 2001). In this approach, classifiers are trained for a particular mistake type: for example, preposition, article, or noun number (Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010c,b; Dahlmeier and Ng, 2012). Originally, classifiers were trained on native English data. As several annotated learner datasets became available, models were also trained on annotated learner data. More recently, the statistical machine translation (MT) methods, including neural MT, have gained considerable popularity thanks to the availability of large annotated corpora of learner writing (e.g., Yuan and Briscoe, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 20"
Q19-1001,P12-2039,0,0.169966,"Missing"
Q19-1001,W08-1205,0,0.11017,"Missing"
Q19-1001,W14-3304,0,\N,Missing
Q19-1001,W17-5037,0,\N,Missing
rizzolo-roth-2010-learning,W09-1119,1,\N,Missing
rizzolo-roth-2010-learning,W02-1001,0,\N,Missing
rizzolo-roth-2010-learning,J08-2005,1,\N,Missing
rizzolo-roth-2010-learning,N06-1046,0,\N,Missing
rizzolo-roth-2010-learning,D08-1031,1,\N,Missing
rizzolo-roth-2010-learning,N07-1030,0,\N,Missing
rizzolo-roth-2010-learning,P09-1039,0,\N,Missing
rizzolo-roth-2010-learning,P06-4018,0,\N,Missing
rizzolo-roth-2010-learning,W05-0618,0,\N,Missing
rizzolo-roth-2010-learning,W02-0109,0,\N,Missing
S12-1010,P05-1022,0,0.0242475,"n window size, such as VP, PP, etc. (3)ParserBigram: the bi-gram of the nonterminal label of the parents of both the verb and the particle. For example, from this partial tree (VP (VB get)(PP (IN through)(NP (DT the)(NN day))), the parent label for the verb get is VP and the parent node label for the particle through is PP. Thus, this feature value is VP-PP. Our feature extractor is implemented in Java through a publicly available NLP library4 via the tool called Curator (Clarke et al., 2012). The shallow parser is publicly available (Punyakanok and Roth, 2001)5 and the parser we use is from (Charniak and Johnson, 2005). 3.1 Data Preparation and Annotation All sentences in our dataset are extracted from BNC (XML Edition), a balanced synchronic corpus containing 100 million words collected from various sources of British English. We first construct a list of phrasal verbs for the six verbs that we are interested in from two resources, WN3.0 (Fellbaum, 1998) and DIRECT6 . Since these targeted verbs are also commonly used in English Light Verb Constructions (LVCs), we filter out LVCs in our list using a publicly available LVC corpus (Tu and Roth, 2011). The result list consists of a total of 245 phrasal verbs."
S12-1010,clarke-etal-2012-nlp,1,0.857214,"Missing"
S12-1010,W06-1207,0,0.0618864,"Missing"
S12-1010,P03-1065,0,0.169352,"ts. Adam was not giving anything and he was 1 http://cogcomp.cs.illinois.edu/page/resources/PVC Data This paper is targeting to build an automatic learner which can recognize a true phrasal verb from its orthographically identical construction with a verb and a prepositional phrase. Similar to other types of MultiWord Expressions (MWEs) (Sag et al., 2002), the syntactic complexity and semantic idiosyncrasies of phrasal verbs pose many particular challenges in empirical Natural Language Processing (NLP). Even though a few of previous works have explored this identification problem empirically (Li et al., 2003; Kim and Baldwin, 2009) and theoretically (Jackendoff, 2002), we argue in this paper that this context sensitive identification problem is not so easy as conceivably shown before, especially when it is used to handle those more compositional phrasal verbs which are empirically used either way in the corpus as a true phrasal verb or a simplex verb with a preposition combination. In addition, there is still a lack of adequate resources or benchmark datasets to identify and treat phrasal 65 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 65–69, c Montr´eal, Canada, Ju"
S12-1010,W11-0807,1,0.674755,"Missing"
S12-1010,W03-1808,0,0.126615,"igations reveal a wide range of English phrasal verbs that are syntactically uniform, but diverge largely in semantics, argument structure and lexical status. The complexity and idiosyncrasies of English phrasal verbs also pose a special challenge to computational linguistics and attract considerable amount of interest and investigation for their extraction, disambiguation as well as identification. Recent computational research on English phrasal verbs have been focused on increasing the coverage and scalability of phrasal verbs by either extracting unlisted phrasal verbs from large corpora (Villavicencio, 2003; Villavicencio, 2006), or constructing productive lexical rules to generate new cases (Villanvicencio and Copestake, 2003). Some other researchers follow the semantic regularities of the particles associated with these phrasal verbs and concentrate on disambiguation of phrasal 2 verb semantics, such as the investigation of the most common particle up by (Cook and Stevenson, 2006). Research on token identification of phrasal verbs is much less compared to the extraction. (Li et al., 2003) describes a regular expression based simple system. Regular expression based method requires human constru"
S18-2018,S17-2093,0,0.158451,"Missing"
S18-2018,W06-1623,0,0.568852,"ally focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annota"
S18-2018,P14-2082,0,0.755482,"a-Champaign, Urbana, IL 61801, USA 3 Department of Computer Science, University of Pennsylvania, Philadelphia, PA 19104, USA {qning2,zyu19,cfan10}@illinois.edu, danroth@seas.upenn.edu Abstract and labeling them is time-consuming. Perhaps more importantly, the other reason is that #edges is quadratic in #nodes. If labeling an edge takes 30 seconds (already an optimistic estimation), a typical document with 50 nodes would take more than 10 hours to annotate. Even if existing annotation schemes make a compromise by only annotating edges whose nodes are from a same sentence or adjacent sentences (Cassidy et al., 2014), it still takes more than 2 hours to fully annotate a typical document. Consequently, the only fully annotated dataset, TB-Dense (Cassidy et al., 2014), contains only 36 documents, which is rather small compared with datasets for other NLP tasks. Annotating temporal relations (TempRel) between events described in natural language is known to be labor intensive, partly because the total number of TempRels is quadratic in the number of events. As a result, only a small number of documents are typically annotated, limiting the coverage of various lexical/semantic phenomena. In order to improve e"
S18-2018,D08-1073,0,0.174087,"pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annotated in this paper. The first system on TB3 Joint Learning on F and P In this work, we study two learning p"
S18-2018,S13-2012,0,0.023911,"lyzing the results in Sec. 4. Recent advances in TempRel data annotation (Ning et al., 2018c) can be used in the future to collect both F and P more easily. Existing Datasets and Methods TimeBank (Pustejovsky et al., 2003) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g"
S18-2018,S13-2002,0,0.0602348,"et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annotated in this paper. The first system on TB3 Joint Learning on F and P In this work, we study two learning paradigms that make use of both F and P. In the first, we simply treat those edges that are annotated in P as edges in F so that the learning"
S18-2018,Q14-1022,0,0.432073,"wever, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences This work is a case study in exploring various usages of P in the TempRel extraction task. We empirically show that P is indeed useful within a (constrained) boots"
S18-2018,D17-1108,1,0.722383,"work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences This work is a case study in exploring various usages of P in the TempRel extraction task. We empirically show that P is indeed useful within a (constrained) bootstrapping type of learning approach. This cas"
S18-2018,P07-2044,0,0.0222025,"ient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annotated in this paper. The"
S18-2018,P18-1212,1,0.6715,"ure of the problem being investigated here. At this point, TB-Dense is the only fully annotated dataset that can be adopted in this study, despite the aforementioned limitation. Second, the partial annotations in datasets like TimeBank were not selected uniformly at random from all possible edges. As described earlier, only salient and non-vague TempRels (which may often be those easy ones) are labeled in these datasets. Using TimeBank as P might potentially create some bias and we will need to keep this in mind when analyzing the results in Sec. 4. Recent advances in TempRel data annotation (Ning et al., 2018c) can be used in the future to collect both F and P more easily. Existing Datasets and Methods TimeBank (Pustejovsky et al., 2003) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeB"
S18-2018,N18-1077,1,0.766346,"ure of the problem being investigated here. At this point, TB-Dense is the only fully annotated dataset that can be adopted in this study, despite the aforementioned limitation. Second, the partial annotations in datasets like TimeBank were not selected uniformly at random from all possible edges. As described earlier, only salient and non-vague TempRels (which may often be those easy ones) are labeled in these datasets. Using TimeBank as P might potentially create some bias and we will need to keep this in mind when analyzing the results in Sec. 4. Recent advances in TempRel data annotation (Ning et al., 2018c) can be used in the future to collect both F and P more easily. Existing Datasets and Methods TimeBank (Pustejovsky et al., 2003) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeB"
S18-2018,P18-1122,1,0.831259,"ure of the problem being investigated here. At this point, TB-Dense is the only fully annotated dataset that can be adopted in this study, despite the aforementioned limitation. Second, the partial annotations in datasets like TimeBank were not selected uniformly at random from all possible edges. As described earlier, only salient and non-vague TempRels (which may often be those easy ones) are labeled in these datasets. Using TimeBank as P might potentially create some bias and we will need to keep this in mind when analyzing the results in Sec. 4. Recent advances in TempRel data annotation (Ning et al., 2018c) can be used in the future to collect both F and P more easily. Existing Datasets and Methods TimeBank (Pustejovsky et al., 2003) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeB"
S18-2018,D12-1062,1,0.949392,"to collect F. However, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences This work is a case study in exploring various usages of P in the TempRel extraction task. We empirically show that P is indeed useful withi"
S18-2018,W16-5706,0,0.129963,"Missing"
S18-2018,S13-2015,0,0.0247446,"ssidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annotated in this paper. The first system on TB3 Joint Learning on F and P In this work, we study two learning paradigms that make use of both F and P. In the first, we simply treat those edges that are annotated in P as edges in F so tha"
S18-2018,W04-2401,1,0.518292,"ocess can be performed on top of the union of F and P. This is the most straightforward approach to using F and P jointly and it is interesting to see if it already helps. In the second, we use bootstrapping: we use F as a starting point and learn a TempRel extraction system on it (denoted by SF ), and then fill those missing annotations in P based on SF (thus obtain ˜ finally, we treat P˜ as F and “fully” annotated P); learn from both. Algorithm 1 is a meta-algorithm of the above. 149 A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem(Roth and Yih, 2004) and enforce transitivity rules as constraints. Let R be the TempRel label set2 , Ir (ij) ∈ {0, 1} be the indicator function of (i, j) = r, and fr (ij) ∈ [0, 1] be the corresponding soft-max score obtained via SF+P . Then the ILP objective is formulated as ∑ ∑ Iˆ = argmax i&lt;j r∈R fr (ij)Ir (ij) (1) Algorithm 1: Joint learning from F and P by bootstrapping Input: F, P, Learn, Inference 1 SF = Learn(F) 2 Initialize SF+P = SF 3 while convergence criteria not satisfied do 4 P˜ = ∅ 5 foreach p ∈ P do ˆ = Inference(p; SF+P ) 6 y ˆ )} 7 P˜ = P˜ ∪ {(x, y ˜ 8 SF+P = Learn(F + P) 9 I s.t. return SF+P Σr"
S18-2018,P06-1095,0,0.565601,"is less labor intensive to collect P than to collect F. However, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences This work is a case study in exploring various usages of P in the TempRel extraction task. We empiric"
S18-2018,S13-2001,0,0.0623925,"t al., 2003) and AQUAINT (Graff, 2002) cover in total more than 250 documents. Since annotators are not required to label all the edges in these datasets, it is less labor intensive to collect P than to collect F. However, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edge"
S18-2018,S07-1014,0,0.0602043,"mple, TimeBank (Pustejovsky et al., 2003) and AQUAINT (Graff, 2002) cover in total more than 250 documents. Since annotators are not required to label all the edges in these datasets, it is less labor intensive to collect P than to collect F. However, existing TempRel extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two re"
S18-2018,C16-1007,0,0.188858,"extraction methods only work on one type of datasets (i.e., either F or P), without taking advantage of both. No one, as far as we know, has explored ways to combine both types of datasets in learning and whether it is helpful. Introduction Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017). A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Chambers et al., 2014; Mirza and Tonelli, 2016; Ning et al., 2017, 2018a,b). The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges – a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences This work is a case study in exploring various usages of P in the TempRel extraction task. We empirically show that P is indeed useful within a (constrained) bootstrapping type of learning"
S18-2018,C08-3012,0,0.0362613,"as been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note that there is a vague label in case the TempRel is not clear or does not exist). As a result, edges in TB-Dense are considered as fully annotated in this paper. The first system on TB3 Joint Learning on F and P In this"
S18-2018,W16-1007,0,0.0420682,"el dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Cassidy et al., 2014; Ning et al., 2017). Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (Graff, 2002), CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016). Most existing systems make use of P, including but not limited to, (Mani et al., 2006; Bramsen et al., 2006; Chambers et al., 2007; Bethard et al., 2007; Verhagen and Pustejovsky, 2008; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012); this applies also to the TempEval workshops systems, e.g., (Laokulrat et al., 2013; Bethard, 2013; Chambers, 2013). To address the missing annotation issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense. Edges are presented one-byone and the annotator has to choose a label for it (note t"
S18-2025,E12-1004,0,0.42751,"t, Wordnet, and test it on a broad array of open-domain hypernymy detection datasets. The results show the outstanding performance and strong generalization of the H YPER D EF model. Overall, our contributions are as follows: Related Work The main novelty of our H YPER D EF lies in the information resource that is employed to represent the terms. Prior work in exploring information resources can be put into two categories: understanding terms by the co-occurring context in raw text, or grounding the terms in open-domain objects. 2.1 Mining Distributional Context from Text Window-based Context Baroni et al. (2012b) build distributional semantic vectors for terms from a concatenation of three corpora: the British National Corpus, WackyPedia and ukWac. Each entry in the vector is the PMI-formulated score from co-occurrence counts. Dimension reduction is conducted by Singular Value Decomposition (SVD) before feeding representation vectors to a classifier. Dependency-based Context Roller and Erk (2016) compute a syntactic distributional space for terms by counting their dependency neighbors across the corpus. Shwartz et al. (2017) further compare (i) contexts being parent and daughter nodes in the depende"
S18-2025,N15-1098,0,0.0351235,"Missing"
S18-2025,W11-2501,0,0.0836449,"Missing"
S18-2025,P09-1113,0,0.0604692,"nces express (sense-specific) corpus-independent meanings of words, hence definition-driven approaches enable strong generalization – once trained, the model is expected to work well in opendomain testbeds; (ii) Global context from a large corpus and definitions provide complementary information for words. Consequently, our model, H YPER D EF, once trained on taskagnostic data, gets state-of-the-art results in multiple benchmarks1 . 1 Introduction Language understanding applications like textual entailment (Dagan et al., 2013), question answering (Saxena et al., 2007) and relation extraction (Mintz et al., 2009), benefit from the identification of lexical entailment relations. Lexical inference encompasses several semantic relations, with hypernymy being one of the prevalent (Roller et al., 2014; Shwartz et al., 2016), an i.e., “Is-A” relation that holds for a pair of terms2 (x, y) for specific terms’ senses. Two families of approaches have been studied for identifying term hypernymy. (i) Pattern match1 2 cogcomp.org/page/publication_view/836 This paper uses “term” to refer to any words or phrases. 203 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 203–21"
S18-2025,D15-1075,0,0.0197282,"ency in a given data set. Consequently, the task of identifying the relation between two terms is enhanced by the knowledge of the terms’ definitions. Our model can be applied to any new terms in any domain, given some context of the term usage and their domainagnostic definitions. Moreover, given our learning approach – we learn also the notion of lexical entailment between terms – we can generalize to any lexical relation between terms. Technically, we implement H YPER D EF by modifying the AttentiveConvNet (Yin and Sch¨utze, 2017), a top-performing system on a textual entailment benchmark (Bowman et al., 2015), to model the input (x, dx ; y, dy ), where di (i = x, y) is the definition of term i. In contrast to earlier work which mostly built separate representations for terms x and y, H YPER D EF instead directly models the representation for each pair in {(x, y), (x, dy ), (dx , y), (dx , dy )}, and then accumulates the four-way representations to form an overall representation for the input. In our experiments, we train H YPER D EF on a task-agnostic annotated dataset, Wordnet, and test it on a broad array of open-domain hypernymy detection datasets. The results show the outstanding performance a"
S18-2025,D16-1234,0,0.0243421,"Missing"
S18-2025,C14-1097,0,0.0515003,"Missing"
S18-2025,W15-4208,0,0.0454626,"Missing"
S18-2025,P14-1113,0,0.38622,"tection Wenpeng Yin and Dan Roth University of Pennsylvania {wenpeng,danroth}@seas.upenn.edu Abstract ing exploits patterns such as “animals such as cats” to indicate a hypernymy relation from “cat” to “animal” (Hearst, 1992; Snow et al., 2004). However, it requires the co-occurrence of the two terms in the same sentence, which limits the recall of this method; (ii) Term representation learning depends on a vector embedding of each term, where each entry in the vector expresses an explicit context feature (Baroni et al., 2012a; Roller and Erk, 2016; Shwartz et al., 2017) or a latent semantic (Fu et al., 2014; Vulic and Mrksic, 2017; Glavas and Ponzetto, 2017). Both approaches hinge on acquiring contextaware term meaning in a large corpus. The generalization of these corpus-based representation learning paradigms, however, is limited due to the domain specificity of the training data. For example, an IT corpus hardly mentions “apple” as a fruit. Furthermore, the surrounding context of a term may not convey subtle differences in term meaning – “he” and “she” have highly similar context that may not reveal the important difference between them. Moreover, rare words are poorly expressed by their spar"
S18-2025,D17-1185,0,0.0238262,"Missing"
S18-2025,P16-1226,0,0.20981,"ii) Global context from a large corpus and definitions provide complementary information for words. Consequently, our model, H YPER D EF, once trained on taskagnostic data, gets state-of-the-art results in multiple benchmarks1 . 1 Introduction Language understanding applications like textual entailment (Dagan et al., 2013), question answering (Saxena et al., 2007) and relation extraction (Mintz et al., 2009), benefit from the identification of lexical entailment relations. Lexical inference encompasses several semantic relations, with hypernymy being one of the prevalent (Roller et al., 2014; Shwartz et al., 2016), an i.e., “Is-A” relation that holds for a pair of terms2 (x, y) for specific terms’ senses. Two families of approaches have been studied for identifying term hypernymy. (i) Pattern match1 2 cogcomp.org/page/publication_view/836 This paper uses “term” to refer to any words or phrases. 203 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 203–213 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics 2 from their definitions. This paradigm has an important advantage in its powerful generalization, as definitions are agnostic to"
S18-2025,C92-2082,0,0.666615,"Missing"
S18-2025,K15-1018,0,0.0586666,"dominant features of fine-grained alignments across sentences. The reason we base our system on this model is two-fold: (i) AttentiveConvNet is one of the topperforming systems of modeling sentence pairs in textual entailment, and (ii) AttentiveConvNet implements the fine-grained cross-sentence alignments in the granularity of local windows; this makes it appropriate to reason between a definitional sentence and a term. Grounding Terms to Open-domain Objects Do and Roth (2012) build Wikipedia representations for input terms – representing the input terms by a set of relevant Wikipedia pages. Shwartz et al. (2015) represent each term pair as a set of paths which are extracted from different large-scale knowledge resources (DBPedia, Wikidata, Yago and WordNet), then train a classifier to determine whether the two terms satisfy a relation of interest given those path connections. Young et al. (2014) map terms to a set of images, then determine the directional inference by conditional probability over statistic of image intersection. Compared with mining of distributional context from text, these works switch the context from words to Wikipedia pages, KB paths or images. So, they share a similar mechanism"
S18-2025,Q16-1002,0,0.03231,"detection 4 For more details, please refer to (Yin and Sch¨utze, 2017). 206 H YPER D EF (F1 ) H YPER D EF (AP) w/o attention (F1) w/o definition (F1) LSTM+atten. (F1) all .905 .933 random split –pww –pwd –pdw .874 .896 .876 .902 .921 .905 .825 .734 .757 – pdd .881 .909 all .887 .900 –pww .875 .890 lexical split –pwd – pdw .870 .849 .883 .880 .743 .619 .685 –pdd .862 .877 Table 1: Tune HyperDef on wn dev • H YPER D EF employs definitions to provide richer information for the terms. But it does not generate an auxiliary term representation vector from the definitive sentence as the literature (Hill et al., 2016) did. Instead, H YPER D EF formulates a pair of input elements – each can be a distributional vector or a definition representation – into a cross-sentence attention mechanism, which directly yields a compact representation to the pair rather than two separate vectors for the two input elements. This is shown more effective to model the relationship of two pieces of text (Yin and Sch¨utze, 2017); (x, dx ; y, dy ) via concatenation: p = [pww , pwd , pdw , pdd ] (4) then p is fed to the final classifier. AttentiveConvNet over (x, y) resembles the conventional hypernymy classifiers which take two"
S18-2025,E17-1007,0,0.296767,"ain objects. 2.1 Mining Distributional Context from Text Window-based Context Baroni et al. (2012b) build distributional semantic vectors for terms from a concatenation of three corpora: the British National Corpus, WackyPedia and ukWac. Each entry in the vector is the PMI-formulated score from co-occurrence counts. Dimension reduction is conducted by Singular Value Decomposition (SVD) before feeding representation vectors to a classifier. Dependency-based Context Roller and Erk (2016) compute a syntactic distributional space for terms by counting their dependency neighbors across the corpus. Shwartz et al. (2017) further compare (i) contexts being parent and daughter nodes in the dependency tree, and (ii) contexts being the parentsister pairs in the dependency tree. Term Embeddings Unspecialized term embeddings are not informative signals for detecting specific lexico-semantic relations. Hence, community often explicitly build transformation functions from unspecialized embeddings to relationspecialized embeddings. Fu et al. (2014) first use the skip-gram model (Mikolov et al., 2013) to learn generic term embeddings from a large Chinese encyclopedia corpus, then learn a projection function from the ge"
S18-2025,N18-1103,0,0.0248681,"Missing"
S18-2025,C14-1212,0,0.0485235,"Missing"
S18-2025,Q14-1006,0,0.162499,"tence alignments in the granularity of local windows; this makes it appropriate to reason between a definitional sentence and a term. Grounding Terms to Open-domain Objects Do and Roth (2012) build Wikipedia representations for input terms – representing the input terms by a set of relevant Wikipedia pages. Shwartz et al. (2015) represent each term pair as a set of paths which are extracted from different large-scale knowledge resources (DBPedia, Wikidata, Yago and WordNet), then train a classifier to determine whether the two terms satisfy a relation of interest given those path connections. Young et al. (2014) map terms to a set of images, then determine the directional inference by conditional probability over statistic of image intersection. Compared with mining of distributional context from text, these works switch the context from words to Wikipedia pages, KB paths or images. So, they share a similar mechanism while differing in the categories of entries in distributional vectors. Our paradigm H YPER D EF shares the same inspiration with above distributional models. More importantly, It goes beyond the frame of distributional models by exploring a novel information resource – definitions – to"
S18-2031,Q15-1016,0,0.0357563,"I provide? Consider a word and two associated contexts, c1 and c2 , where the second context is significantly rarer. Further, imagine that the PMI of the word with either feature is the same. The word would have been seen in the rarer context only a few times, and this is more likely to have been a statistical fluke. In this case, the APMI with the more frequent term is higher: we reward the fact that the PMI is high despite its prevalence; this is less likely to be an artifact of chance. Note that the rearranged expression seen in the second line of Equation 3 is reminiscent of PPMI0.75 from Levy et al. (2015). The second log term in APMI is always negative, and we thus shift all values by a constant k (chosen based on practical considerations of data size: the smaller the k, the larger the size of the sparse matrices; based on experimenting with various values of k, it appears that expansion quality is not very sensitive to k). Clipping this shifted value at 0 produces Asymmetrical PPMI (APPMI): SimMatrix = M C→V W M V→C 4.3 E = M C→V WS M V→C S 4.2 4.4 Motivating Our Choice of W When expanding the set {taurus, cancer}—the set of star signs, or perhaps the constellations—we are faced with the pres"
S18-2031,D15-1200,0,0.0157615,"lysemy is intimately tied to the well-explored field of WSD so it is natural to expect techniques from WSD to be relevant. If WSD could neatly separate senses, the set expansion problem could be approached thus. Ford would split into, say, two senses: Ford-1 for the car, and Ford-2 for the president, and expanding {Ford, Nixon} could be translated to expanding {Ford-2, Nixon}. Such a representational approach is taken by many authors when they embed the different senses of words as distinct points in an embedding space (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Li and Jurafsky, 2015). Such approaches run into what we term the Fixed Inventory Problem. Either senses are obtained from a hand curated resource such as a dictionary, or are induced from the corpus directly by mapping contexts clusters to different senses. In either case, however, by the time the final representation (e.g., the embedding) is obtained, the number of different senses of each term has become fixed: all decisions have been made relating to how finely or coarsely to split senses. How to split senses is a hard problem: dictionaries such as NOAD list coarse senses and split these further into fine sense"
S18-2031,W16-2503,0,0.0266642,"Missing"
S18-2031,N13-1090,0,0.760773,"re web) is accessed on demand by making use of a search engine such as Google (Wang and Cohen, 2007, for example). Each family has its advantages and disadvantages. “Open web” techniques that piggyback on Google can have coverage deep into the tail. These typically rely on some form of Wrapper Induction, and tend to work better for sets whose instances show up in lists or other repeated structure on the web, and thus perform much better on sets of nouns than on sets of verbs or adjectives. By contrast, “packaged” techniques that work off a 266 ter than Word2Vec on some analogy classes used by Mikolov et al. (2013), despite being shown one fewer term. The approach is rather close to that used by Turney (2012) for a different problem: word compounds. Understanding what a dog house is can be phrased as “What is the house of a dog?”, with kennel being the correct answer. This is solved using the pair of similarity functions mentioned above. The evaluations provided in that paper are for ranking: which of five provided terms is a match. Here, we apply it to non-proportional analogies and evaluate for retrieval, where we are ranking over all words, a significantly more challenging problem. To our knowledge,"
S18-2031,L18-1405,0,0.0782741,"provided. We note, however, that Linzen (2016) briefly discusses this problem. preprocessed corpus are faster (no Google lookup needed) and can work well for any part of speech, but are of course limited to the corpus used. These typically use some form of distributional similarity, which can compute similarity between items that have never been seen together in the same document; approaches based on shared memberships in lists would need a sequence of overlapping lists to achieve this. Our work is in the “packaged” family, and we use sparse representations used for distributional similarity. Gyllensten and Sahlgren (2018) compares two subfamilies within the packaged family: centrality-based methods use a prototype of the seeds (say, the centroid) as a proxy for the entire seed set and classification-based methods (a strict superset), which produce a classifier by using the seeds. Our approach is classification-based. It is our goal to be able to expand nuanced categories. For example, we want our solution to expand the set {pluto, mickey}—both Disney characters—to other Disney characters. That is, the context mickey should determine what is considered ‘similar’ to pluto, rather than being biased by the more do"
S18-2031,D14-1113,0,0.169433,"guistics to a point in the vector space, or should, instead, each coarse sense map to a point? Many authors (Hofstadter and Sander, 2013, for example) discuss how the various dictionary senses of a term are not independent. Further, if context clusters map to senses, the word whale, which is seen both in mammal-like contexts (e.g., “whales have hair”) and water-animal contexts (“whales swim”), could get split into separate points. Thus, the different senses that terms are split into may instead be distinct facets. This is not an idle theoretical worry: such facet-based splitting is evident in Neelakantan et al. (2014, Table 3). Similarly, in the vectors they released, november splits into ten senses, likely based on facets. Once split, for subsequent processing, the points are independent. In contrast to such explicit, prior, splitting, in the Category Builder approach developed here, relevant contexts are chosen given the task at hand, and if multiple facets are relevant (as happens, for example, in {whale, dolphin, seal}, whose expansion should rank aquatic mammals highest), all these facets influence the expansion; if only one facet is of relevance (as happens in {whale, shark, seahorse}), the irreleva"
S18-2031,P12-1092,0,0.0605959,"ogether, get around explicitly solving WSD. Polysemy is intimately tied to the well-explored field of WSD so it is natural to expect techniques from WSD to be relevant. If WSD could neatly separate senses, the set expansion problem could be approached thus. Ford would split into, say, two senses: Ford-1 for the car, and Ford-2 for the president, and expanding {Ford, Nixon} could be translated to expanding {Ford-2, Nixon}. Such a representational approach is taken by many authors when they embed the different senses of words as distinct points in an embedding space (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Li and Jurafsky, 2015). Such approaches run into what we term the Fixed Inventory Problem. Either senses are obtained from a hand curated resource such as a dictionary, or are induced from the corpus directly by mapping contexts clusters to different senses. In either case, however, by the time the final representation (e.g., the embedding) is obtained, the number of different senses of each term has become fixed: all decisions have been made relating to how finely or coarsely to split senses. How to split senses is a hard problem: dictionaries such as NOAD list coa"
S18-2031,N10-1013,0,0.085419,"Missing"
S18-2031,W14-1618,0,0.533941,"n and corresponding algorithms that perform set expansion in Category Builder (CB). 4.1 Sparse Representations for Expansion PPMI(w, c) = max(0, PMI(w, c)) We use the traditional word representation that distributional similarity uses (Turney and Pantel, 2010), and that is commonly used in fields such as context sensitive spelling correction and grammatical correction (Golding and Roth, 1999; Rozovskaya and Roth, 2014); namely,words are associated with some ngrams that capture the contexts in which they occur – all contexts are represented in a sparse vector corresponding to a word. Following Levy and Goldberg (2014a), we call this representation explicit. Generating Representations. We start with web pages and extract words and phrases from these, as well as the contexts they appear in. An aggregation step then calculates the strengths of word to context and context to word associations. Vocabulary. The vocabulary is made up of words (nouns, verbs, adjectives, and adverbs) and some multi-word phrases. To go beyond words, we use a named entity recognizer to find multiword phrases such as New York. We also use one heuristic rule to add certain phrasal verbs (e.g., take shower), when a verb is directly fol"
S18-2031,Q14-1033,1,0.825229,"from various theoretical standpoints), but there are not, to our knowledge, unsupervised computational models of these phenomena. 4 Representations and Algorithms This section describes the representation and corresponding algorithms that perform set expansion in Category Builder (CB). 4.1 Sparse Representations for Expansion PPMI(w, c) = max(0, PMI(w, c)) We use the traditional word representation that distributional similarity uses (Turney and Pantel, 2010), and that is commonly used in fields such as context sensitive spelling correction and grammatical correction (Golding and Roth, 1999; Rozovskaya and Roth, 2014); namely,words are associated with some ngrams that capture the contexts in which they occur – all contexts are represented in a sparse vector corresponding to a word. Following Levy and Goldberg (2014a), we call this representation explicit. Generating Representations. We start with web pages and extract words and phrases from these, as well as the contexts they appear in. An aggregation step then calculates the strengths of word to context and context to word associations. Vocabulary. The vocabulary is made up of words (nouns, verbs, adjectives, and adverbs) and some multi-word phrases. To g"
S18-2031,P07-1030,0,\N,Missing
S19-1021,D18-1316,0,0.0143242,"SM) was first introduced by (Goodfellow et al., 2015) and was applied to sentence classification tasks through word embeddings by (Miyato et al., 2017). Gradient-based adversarial attacks have since been used to train models for various NLP tasks, such as relation extraction (Wu et al., 2017) and joint entity and relation extraction (Bekoulis et al., 2018). Our replacements of named entities can also be viewed as a way of generating adversarial examples for coreference systems; it is related to the earlier method proposed in (Khashabi et al., 2016) in the context of question answering and to (Alzantot et al., 2018), which provides a way of generating adversarial examples for simple classification tasks. 3 sents a mention and the arrows are directed from one mention to its possible antecedents. We now review the model architecture of (Lee et al., 2018) and describe how we apply the fastgradient-sign-method (FGSM) of (Miyato et al., 2017) to the model. Using GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings of each word and using learned character embeddings, the model computes contextualized representations {x1 , x2 , ..., xn } of each word xi in the input document using a bidirec"
S19-1021,D18-1307,0,0.0244267,"ich is limited in that it consists of only 30 documents. They then show that the addition of features representing linguistic information improves the performance of a coreference resolver on the out-of-domain dataset. The adversarial fast-gradient-sign-method (FGSM) was first introduced by (Goodfellow et al., 2015) and was applied to sentence classification tasks through word embeddings by (Miyato et al., 2017). Gradient-based adversarial attacks have since been used to train models for various NLP tasks, such as relation extraction (Wu et al., 2017) and joint entity and relation extraction (Bekoulis et al., 2018). Our replacements of named entities can also be viewed as a way of generating adversarial examples for coreference systems; it is related to the earlier method proposed in (Khashabi et al., 2016) in the context of question answering and to (Alzantot et al., 2018), which provides a way of generating adversarial examples for simple classification tasks. 3 sents a mention and the arrows are directed from one mention to its possible antecedents. We now review the model architecture of (Lee et al., 2018) and describe how we apply the fastgradient-sign-method (FGSM) of (Miyato et al., 2017) to the"
S19-1021,D13-1057,1,0.76648,"inneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics hibits state-of-the-art performance on the GAP dataset (Webster et al., 2018), a recently released dataset focusing on resolving pronouns to people’s names in excerpts from Wikipedia. The code and other relevant files for this project can be found via https://cogcomp.org/ page/publication_view/871. 2 Figure 1: For each mention, the model computes scores for each of the candidate antecedent mentions and chooses the candidate with the highest score to be the predicted antecedent. This image was created by the authors of (Chang et al., 2013). Related Work (Moosavi and Strube, 2017, 2018) also study generalization of neural coreference resolvers. However, they focus on transfer and indicate that the ranking of coreference resolvers (trained on the CoNLL training set) induced by their performance on the CoNLL test set is not preserved when the systems are evaluated on a different dataset. They use the Wikicoref dataset (Ghaddar and Langlais, 2016), which is limited in that it consists of only 30 documents. They then show that the addition of features representing linguistic information improves the performance of a coreference reso"
S19-1021,L16-1021,0,0.0528506,"the model computes scores for each of the candidate antecedent mentions and chooses the candidate with the highest score to be the predicted antecedent. This image was created by the authors of (Chang et al., 2013). Related Work (Moosavi and Strube, 2017, 2018) also study generalization of neural coreference resolvers. However, they focus on transfer and indicate that the ranking of coreference resolvers (trained on the CoNLL training set) induced by their performance on the CoNLL test set is not preserved when the systems are evaluated on a different dataset. They use the Wikicoref dataset (Ghaddar and Langlais, 2016), which is limited in that it consists of only 30 documents. They then show that the addition of features representing linguistic information improves the performance of a coreference resolver on the out-of-domain dataset. The adversarial fast-gradient-sign-method (FGSM) was first introduced by (Goodfellow et al., 2015) and was applied to sentence classification tasks through word embeddings by (Miyato et al., 2017). Gradient-based adversarial attacks have since been used to train models for various NLP tasks, such as relation extraction (Wu et al., 2017) and joint entity and relation extracti"
S19-1021,N18-1202,0,0.0480783,"Missing"
S19-1021,D15-1162,0,0.0818725,"Missing"
S19-1021,W12-4501,0,0.125437,"systems. Various regularization techniques have been proposed for improving the generalization capability of neural networks, including dropout (Srivastava et al., 2014) and adversarial training (Goodfellow et al., 2015; Miyato et al., 2017). The model of (Lee et al., 2018), like most neural approaches, uses dropout. In this work, we apply the adversarial fast-gradientsign-method (FGSM) described by (Miyato et al., 2017) to the model of (Lee et al., 2018), and show that this technique improves the model’s generalization even when applied on top of dropout. The CoNLL-2012 Shared Task dataset (Pradhan et al., 2012) has been the standard dataset used for both training and evaluating English coreference systems since the dataset was introduced. The dataset includes seven genres that span multiple writing styles and multiple nationalities. We demonstrate that the system of (Lee et al., 2018) retrained with adversarial training achieves state-of-the-art performance on the original CoNLL-2012 dataset (Pradhan et al., 2012) as well as the CoNLL-2012 dataset with changed named entities. Furthermore, the system trained with the adversarial method exIntroduction Through the use of neural networks, performance on"
S19-1021,D17-1018,0,0.389654,"al., 2018) embeddings of each word and using learned character embeddings, the model computes contextualized representations {x1 , x2 , ..., xn } of each word xi in the input document using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997). For candidate span i, which consists of the words at indices starti , starti + 1, ..., endi , the model constructs a span representation gi by concatenating xstarti , Pend 1 i xendi , Pendi j=starti βj xj , and φ(endi − j=starti βj starti ), where the βj ’s are learned scalar values and φ(·) is a learned embedding representing the width of the span (Lee et al., 2017). The span representations are then used as inputs to feedforward networks that compute mention scores for each span and that compute antecedent scores for pairs of spans. In Figure 1, the number associated with each arrow is the antecedent score for the associated pair of mentions. The coreference score for the pair of spans (i, j) is the sum of the mention score for span i, the mention score for span j, and the antecedent score for (i, j). For each span i, the antecedent span predicted by the model is the span j that maximizes the antecedent score for (i, j). Let g = {gi }N i=1 denote the se"
S19-1021,Q18-1042,0,0.184459,"he test set. We demonstrate that the performance of the (Lee et al., 2018) system, which is the current state-of-the-art, decreases when the named entities are replaced. An example of a replacement that causes the system to make an error is given in Table 1. Motivated by these issues of generalization, this paper aims to improve the training process of neu192 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 192–197 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics hibits state-of-the-art performance on the GAP dataset (Webster et al., 2018), a recently released dataset focusing on resolving pronouns to people’s names in excerpts from Wikipedia. The code and other relevant files for this project can be found via https://cogcomp.org/ page/publication_view/871. 2 Figure 1: For each mention, the model computes scores for each of the candidate antecedent mentions and chooses the candidate with the highest score to be the predicted antecedent. This image was created by the authors of (Chang et al., 2013). Related Work (Moosavi and Strube, 2017, 2018) also study generalization of neural coreference resolvers. However, they focus on tra"
S19-1021,N18-2108,0,0.181628,"Missing"
S19-1021,D17-1187,0,0.0267951,"use the Wikicoref dataset (Ghaddar and Langlais, 2016), which is limited in that it consists of only 30 documents. They then show that the addition of features representing linguistic information improves the performance of a coreference resolver on the out-of-domain dataset. The adversarial fast-gradient-sign-method (FGSM) was first introduced by (Goodfellow et al., 2015) and was applied to sentence classification tasks through word embeddings by (Miyato et al., 2017). Gradient-based adversarial attacks have since been used to train models for various NLP tasks, such as relation extraction (Wu et al., 2017) and joint entity and relation extraction (Bekoulis et al., 2018). Our replacements of named entities can also be viewed as a way of generating adversarial examples for coreference systems; it is related to the earlier method proposed in (Khashabi et al., 2016) in the context of question answering and to (Alzantot et al., 2018), which provides a way of generating adversarial examples for simple classification tasks. 3 sents a mention and the arrows are directed from one mention to its possible antecedents. We now review the model architecture of (Lee et al., 2018) and describe how we apply the"
S19-1021,P17-2003,0,0.212711,"sociation for Computational Linguistics hibits state-of-the-art performance on the GAP dataset (Webster et al., 2018), a recently released dataset focusing on resolving pronouns to people’s names in excerpts from Wikipedia. The code and other relevant files for this project can be found via https://cogcomp.org/ page/publication_view/871. 2 Figure 1: For each mention, the model computes scores for each of the candidate antecedent mentions and chooses the candidate with the highest score to be the predicted antecedent. This image was created by the authors of (Chang et al., 2013). Related Work (Moosavi and Strube, 2017, 2018) also study generalization of neural coreference resolvers. However, they focus on transfer and indicate that the ranking of coreference resolvers (trained on the CoNLL training set) induced by their performance on the CoNLL test set is not preserved when the systems are evaluated on a different dataset. They use the Wikicoref dataset (Ghaddar and Langlais, 2016), which is limited in that it consists of only 30 documents. They then show that the addition of features representing linguistic information improves the performance of a coreference resolver on the out-of-domain dataset. The a"
S19-1021,D18-1018,0,0.0863769,"d multiple nationalities. We demonstrate that the system of (Lee et al., 2018) retrained with adversarial training achieves state-of-the-art performance on the original CoNLL-2012 dataset (Pradhan et al., 2012) as well as the CoNLL-2012 dataset with changed named entities. Furthermore, the system trained with the adversarial method exIntroduction Through the use of neural networks, performance on the task of coreference resolution has increased significantly over the last few years. Still, neural systems trained on the standard coreference dataset have issues with generalization, as shown by (Moosavi and Strube, 2018). One way to improve the understanding of how a system overfits a dataset is to study the change in the system’s performance when the dataset is modified slightly in a focused and relevant manner. We take this approach by modifying the test set so that each PER and GPE (person and geopolitical entity) named entity is different from those seen in training. In other words, we ensure that there is no leakage of PER and GPE named entities from the training set into the test set. We demonstrate that the performance of the (Lee et al., 2018) system, which is the current state-of-the-art, decreases w"
S19-1021,D14-1162,0,0.0831998,"f named entities can also be viewed as a way of generating adversarial examples for coreference systems; it is related to the earlier method proposed in (Khashabi et al., 2016) in the context of question answering and to (Alzantot et al., 2018), which provides a way of generating adversarial examples for simple classification tasks. 3 sents a mention and the arrows are directed from one mention to its possible antecedents. We now review the model architecture of (Lee et al., 2018) and describe how we apply the fastgradient-sign-method (FGSM) of (Miyato et al., 2017) to the model. Using GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings of each word and using learned character embeddings, the model computes contextualized representations {x1 , x2 , ..., xn } of each word xi in the input document using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997). For candidate span i, which consists of the words at indices starti , starti + 1, ..., endi , the model constructs a span representation gi by concatenating xstarti , Pend 1 i xendi , Pendi j=starti βj xj , and φ(endi − j=starti βj starti ), where the βj ’s are learned scalar values and φ(·) is a learned embedding representing the"
W00-0701,J95-4004,0,0.144952,"Missing"
W00-0701,W95-0103,0,0.052262,"Missing"
W00-0701,W95-0104,0,0.100801,"Missing"
W00-0701,E95-1020,0,0.037154,"Missing"
W00-0701,P94-1013,0,0.0194941,"Missing"
W00-0701,P97-1056,0,0.0364302,"Missing"
W00-0701,W99-0621,1,\N,Missing
W00-0701,W97-0301,0,\N,Missing
W00-0701,C94-2195,0,\N,Missing
W00-0701,W99-0613,0,\N,Missing
W00-0721,P98-1034,0,0.0123524,"he context of shallow parsing. 1 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string O = < ol, 0 2 , . . . , On >, a phrase is a substring of consecutive input symbols oi, oi+l,...,oj. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases 2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Mufioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which 2 Markov Modeling H M M is a probabilistic finite state a u t o m a t o n used to model the probabilistic generation of sequential processes. The model consists of a finite set S of states, a set (9 of observations, an initial state distribution P1 (s), a statetransition distribution P(s[s') for s, # E S and an observation distribution P(o[s) for o E (9 and s 6 S. 3 In a supervised learning task, an observation sequence O - - < o l , o 2 , . . . On > is"
W00-0721,A88-1019,0,0.057726,"s under b o t h models and s t u d y them experimentally in the context of shallow parsing. 1 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string O = < ol, 0 2 , . . . , On >, a phrase is a substring of consecutive input symbols oi, oi+l,...,oj. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases 2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Mufioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which 2 Markov Modeling H M M is a probabilistic finite state a u t o m a t o n used to model the probabilistic generation of sequential processes. The model consists of a finite set S of states, a set (9 of observations, an initial state distribution P1 (s), a statetransition distribution P(s[s') for s, # E S and an observation distribution P(o[s) for o E (9 and s 6 S. 3 In a supervised"
W00-0721,J93-2004,0,0.0329342,"be used in further processing as P(slo ) (details omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (POS) tags only and POS with additional lexical information (words). The d a t a sets used are the standard data sets for this problem (Ramshaw and Maxcus, 1995; Argamon et al., 1999; Mufioz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different classifiers. The simple classifier corresponds to the standard HMM in which P(ols ) is estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult t h a n the NP task. This is consistent for all models and all features sets. W h e n comparing between different models and features sets, it is clear that the simple HMM formalism is not compe"
W00-0721,W99-0621,1,0.905691,"m experimentally in the context of shallow parsing. 1 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string O = < ol, 0 2 , . . . , On >, a phrase is a substring of consecutive input symbols oi, oi+l,...,oj. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases 2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Mufioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which 2 Markov Modeling H M M is a probabilistic finite state a u t o m a t o n used to model the probabilistic generation of sequential processes. The model consists of a finite set S of states, a set (9 of observations, an initial state distribution P1 (s), a statetransition distribution P(s[s') for s, # E S and an observation distribution P(o[s) for o E (9 and s 6 S. 3 In a supervised learning task, an observation sequence O - - <"
W00-0721,W95-0107,0,0.0150809,"h models and s t u d y them experimentally in the context of shallow parsing. 1 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string O = < ol, 0 2 , . . . , On >, a phrase is a substring of consecutive input symbols oi, oi+l,...,oj. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases 2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Mufioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which 2 Markov Modeling H M M is a probabilistic finite state a u t o m a t o n used to model the probabilistic generation of sequential processes. The model consists of a finite set S of states, a set (9 of observations, an initial state distribution P1 (s), a statetransition distribution P(s[s') for s, # E S and an observation distribution P(o[s) for o E (9 and s 6 S. 3 In a supervised learning task, an observat"
W00-0721,E99-1023,0,0.035701,"Missing"
W00-0721,C98-1034,0,\N,Missing
W01-0502,J97-3003,0,\N,Missing
W01-0502,W97-1016,0,\N,Missing
W01-0502,H90-1040,0,\N,Missing
W01-0502,J95-2001,0,\N,Missing
W01-0502,J95-4004,0,\N,Missing
W01-0502,P99-1005,0,\N,Missing
W01-0502,P94-1013,0,\N,Missing
W01-0502,P98-2186,1,\N,Missing
W01-0502,C98-2181,1,\N,Missing
W01-0706,P98-1010,0,0.0307956,"t shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such"
W01-0706,W99-0629,0,0.0333058,"ttern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale"
W01-0706,P98-1034,0,0.0108902,"formation can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and"
W01-0706,A88-1019,0,0.286334,"as been motivated by the observation that shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to us"
W01-0706,P96-1025,0,0.0577611,"c phrase with no nested sub-phrases. For example, in the parse tree, ( (S (NP (NP Pierre Vinken) , (ADJP (NP 61 years) old) ,) (VP will (VP join (NP the board) (PP as (NP a nonexecutive director)) (NP Nov. 29))) .)) Pierre Vinken, 61 years, the board, a nonexecutive director and Nov. 29 are atomic phrases while other higher-level phrases are not. That is, an atomic phrase denotes a tightly coupled message unit which is just above the level of single words. 2.1 Parsers We perform our comparison using two state-ofthe-art parsers. For the full parser, we use the one developed by Michael Collins (Collins, 1996; Collins, 1997) — one of the most accurate full parsers around. It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precis"
W01-0706,P97-1003,0,0.519148,"PP This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full"
W01-0706,W93-0113,0,0.0258616,"orthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform — identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney’s work (Abney, 1991), who has suggested to “chunk” sentences to base level phrases. For example, the sentence “He reckons the current account deficit will narrow to only $ 1.8 billion in September .” would be chunked as follows (Tjong Kim Sang and Buchholz, 2000):  [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this directi"
W01-0706,M95-1014,0,0.0184369,"nguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (Grishman, 1995; Appelt et al., 1993). Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis. This can be augmented later if more information is available. Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the nat"
W01-0706,W00-0726,0,0.689798,"ty texts. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney’s work (Abney, 1991), who has suggested to “chunk” sentences to base level phrases. For example, the sentence “He reckons the current account deficit will narrow to only $ 1.8 billion in September .” would be chunked as follows (Tjong Kim Sang and Buchholz, 2000):  [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP This research is supported by NSF grants IIS-9801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances"
W01-0706,J93-2004,0,0.0502552,"ws that it ranks among the top shallow parsers evaluated there 1 . Table 1: Rankings of Shallow Parsers in CoNLL-2000. See (Tjong Kim Sang and Buchholz, 2000) for details. Parsers  KM00  Hal00 CSCL *  TKS00  ZST00  Dej00  Koe00  Osb00  VB00 PMP00  Joh00  VD00 Baseline  Precision( ) 93.45 93.13 93.41 94.04 91.99 91.87 92.08 91.65 91.05 90.63 86.24 88.82 72.58 Recall( ) 93.51 93.51 92.64 91.00 92.25 91.31 91.86 92.23 92.03 89.65 88.25 82.91 82.14  (  ) 93.48 93.32 93.02 92.50 92.12 92.09 91.97 91.94 91.54 90.14 87.23 85.76 77.07 2.2 Data Training was done on the Penn Treebank (Marcus et al., 1993) Wall Street Journal data, sections 02-21. To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations. This is done using the “Chunklink” program provided for CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000). Testing was done on two types of data. For the first experiment, we used the WSJ section 00 (which contains about 45,000 tokens and 23,500 phrases). The goal here was simply to evaluate the full parser and the shallow parser on text that is similar to the one they were trained on. 1 We note that some of the variations"
W01-0706,W99-0621,1,0.893651,"d using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (NPs) and other syntactic seq"
W01-0706,W95-0107,0,0.0254033,"ted by the observation that shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint. First, it has been noted that in many natural language applications it is sufficient to use shallow parsing informat"
W01-0706,W97-0301,0,0.0335212,"801638, ITR-IIS-0085836 and an ONR MURI Award. to ] [NP only $ 1.8 billion ] [PP in ] [NP September] . While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information — by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997; Charniak, 1997a; Charniak, 1997b; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998; Munoz et al., 1999; Punyakanok and Roth, 2001; Buchholz et al., 1999; Tjong Kim Sang and Buchholz, 2000). Research on shallow parsing was inspired by psycholinguistics arguments (Gee and Grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence proc"
W01-0706,C98-1034,0,\N,Missing
W01-0706,C98-1010,0,\N,Missing
W03-0412,W99-0632,0,0.0249845,"xical semantic resources in computational linguistics and natural language processing. Many current syntactic theories make the common assumption that various aspects of syntactic alternation are predicable via the meaning of the predicate in the sentence (Fillmore, 1968; Jackendoff, 1990; Levin, 1993). With the resurgence of lexical semantics and corpus linguistics during the past two decades, this so-called linking regularity triggers a broad interest of using syntactic representations illustrated in corpora to classify lexical meaning (Baker et al., 1998; Levin, 1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin, 1998b; Pantel and Lin, 2002). FrameNet (Baker et al., 1998) produces a semantic dictionary that documents combinatorial properties of English lexical items in semantic and syntactic terms based on attestations in a very large corpus. In FrameNet, a frame is an intuitive structure that formalizes the links between semantics and syntax in the results of lexical analysis. (Fillmore et al., 2001) However, instead of derived via attested sentences from corpora automatically, each conceptual frame together with all its frame elements has to be constructed via slow and labor-intensive manual w"
W03-0412,P98-1013,0,0.00799162,"he related work on syntax, semantics interaction and lexical semantic resources in computational linguistics and natural language processing. Many current syntactic theories make the common assumption that various aspects of syntactic alternation are predicable via the meaning of the predicate in the sentence (Fillmore, 1968; Jackendoff, 1990; Levin, 1993). With the resurgence of lexical semantics and corpus linguistics during the past two decades, this so-called linking regularity triggers a broad interest of using syntactic representations illustrated in corpora to classify lexical meaning (Baker et al., 1998; Levin, 1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin, 1998b; Pantel and Lin, 2002). FrameNet (Baker et al., 1998) produces a semantic dictionary that documents combinatorial properties of English lexical items in semantic and syntactic terms based on attestations in a very large corpus. In FrameNet, a frame is an intuitive structure that formalizes the links between semantics and syntax in the results of lexical analysis. (Fillmore et al., 2001) However, instead of derived via attested sentences from corpora automatically, each conceptual frame together with all its frame elements h"
W03-0412,P01-1008,0,0.0137108,"rdNet to derive all lexical relations stored there. PN SN is modelled after the semantic concordance in (Landes et al., 1998). It takes a word token and an optional context as input, and returns the sense of the word in that context. Similarly to PN RL this function is implemented by appealing to WordNet senses and pruning the possible sense based on the wordlist determined for the given context. PN ST is not implemented at this point, but is designed to output a sentence that has same structure as the input context, but use different words. It is inspired by the work on reformulation, e.g., (Barzilay and McKeown, 2001). We can envision many ways users of PhraseNet can make use of the retrieved information. At this point in the life of PhraseNet we focus mostly on using PhraseNet as a way to acquire semantic features to aid learning based natural language applications. This determines our priorities in the implementation that we describe next. 3 Constructing PhraseNet Constructing PhraseNet involves three main stages: (1) extracting syntactic skeletons from corpora, (2) constructing the core element in PhraseNet: consets, and (3) developing access functions. The first stage makes use of fully parsed data. In"
W03-0412,C94-2195,0,0.0369416,"and is widely used in NLP tasks today. However, WordNet is organized at the word level, and at this level, English suffers ambiguities. Stand-alone words may have several meanings and take on relations (e.g., hypernyms, hyponyms) that depend on their meanings. Consequently, there are very few success stories of automatically using WordNet in natural language applications. In many cases, reported (and unreported) problems are due to the fact that WordNet enumerates all the senses of polysemous words; attempts to use this resource automatically often result in noisy and non-uniform information (Brill and Resnik, 1994; Krymolowski and Roth, 1998). PhraseNet is designed based on the assumption that, by and large, semantic ambiguity in English disappears when local context of words is taken into account. It makes use of WordNet as an important knowledge source and is generated automatically using WordNet and machine learning based processing of large English corpora. It enhances a WordNet synset with its contextual information and refines its relational structure, including relations such as hypernym, hyponym, antonym and synonym, by maintaining only those links that respect contextual constraints. However,"
W03-0412,P91-1034,0,0.150461,"f PhraseNet. Sec. 3 describes the construction of PhraseNet and the current stage of the implementation. An application that provides a preliminary experimental evaluation is described in Sec. 4. Sec. 5 discuses some related work on lexical semantics resources and Sec. 6 discusses future directions within PhraseNet. 2 The Design of PhraseNet Context is one important notion in PhraseNet. While the context may mean different things in natural language, many previous work in statistically natural language processing defined “context” as an n-word window around the target word (Gale et al., 1992; Brown et al., 1991; Roth, 1998). In PhraseNet, “context” has a more precise definition that depends on the grammatical structure of a sentence rather than simply counting surrounding words. We define “context” to be the syntactic structure of the sentence in which the word of interest occurs. Specifically, we define this notion at three abstraction levels. The highest level is the abstract syntactic skeleton of the sentence. That is, it is in the form of the different combinations of six syntactic components. Some components may be missing as long as the structure is from a legitimate English sentence. The most"
W03-0412,C02-1150,1,0.546275,"ases, the property seems not to depend on the specific word used in the sentence – that could be replaced without affecting this property – but rather on its ‘meaning’. In this section we show the benefit of using PhraseNet in doing that in the context of Question Classification. Question classification (QC) is the task of determining the semantic class of the answer of a given question. For example, given the question: “What Cuban dictator did Fidel Castro force out of power in 1958?” we would like to determine that its answer should be a name of a person. Our approach to QC follows that of (Li and Roth, 2002). The question classifier used is a multi-class classifier which can classify a question into one of 50 fine-grained classes. The baseline classifier makes use of syntactic features like the standard POS information and information extracted by a shallow parser in addition to the words in the sentence. The classifier is then augmented with standard WordNet or with PhraseNet information as follows. In all cases, words in the sentence are augmented with additional words that are supposed to be semantically related to them. The intuition, as described above, is that this provides a level of abstr"
W03-0412,W98-0712,0,0.0410376,"Missing"
W03-0412,P02-1014,0,0.0118559,"describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches. 1 Introduction Progress in natural language understanding research necessitates significant progress in lexical semantics and the development of lexical semantics resources. In a broad range of natural language applications, from ∗ Research supported by NSF grants IIS-99-84168, ITR-IIS-00-85836 and an ONR MURI award. Names of authors are listed alphabetically. prepositional phrase attachment (Pantel and Lin, 2000; Stetina and Nagao, 1997), co-reference resolution (Ng and Cardie, 2002) to text summarization (Saggion and Lapalme, 2002), semantic information is a necessary component in the inference, by providing a level of abstraction that is necessary for robust decisions. Inducing that the prepositional phrase in “They ate a cake with a fork” has the same grammatical function as that in “They ate a cake with a spoon”, for example, depends on the knowledge that “cutlery” and “tableware” are the hypernyms of both “fork” and “spoon”. However, the noun “fork” has five senses listed in WordNet and each of them has several different hypernyms. Choosing the correct one is a conte"
W03-0412,Y01-1001,0,0.0162745,"s so-called linking regularity triggers a broad interest of using syntactic representations illustrated in corpora to classify lexical meaning (Baker et al., 1998; Levin, 1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin, 1998b; Pantel and Lin, 2002). FrameNet (Baker et al., 1998) produces a semantic dictionary that documents combinatorial properties of English lexical items in semantic and syntactic terms based on attestations in a very large corpus. In FrameNet, a frame is an intuitive structure that formalizes the links between semantics and syntax in the results of lexical analysis. (Fillmore et al., 2001) However, instead of derived via attested sentences from corpora automatically, each conceptual frame together with all its frame elements has to be constructed via slow and labor-intensive manual work. FrameNet is not constructed automatically based on observed syntactic alternations. Though deep semantic analysis is built for each frame, lack of automatic derivation of the semantic roles from large corpora3 confines the usage of this network drastically. Levin’s classes (Levin, 1993) of verbs are based on the assumption that the semantics of a verb and its syntactic behavior are predictably"
W03-0412,P00-1014,0,0.0127065,"ord token and its context, to retrieve relevant semantic information. We describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches. 1 Introduction Progress in natural language understanding research necessitates significant progress in lexical semantics and the development of lexical semantics resources. In a broad range of natural language applications, from ∗ Research supported by NSF grants IIS-99-84168, ITR-IIS-00-85836 and an ONR MURI award. Names of authors are listed alphabetically. prepositional phrase attachment (Pantel and Lin, 2000; Stetina and Nagao, 1997), co-reference resolution (Ng and Cardie, 2002) to text summarization (Saggion and Lapalme, 2002), semantic information is a necessary component in the inference, by providing a level of abstraction that is necessary for robust decisions. Inducing that the prepositional phrase in “They ate a cake with a fork” has the same grammatical function as that in “They ate a cake with a spoon”, for example, depends on the knowledge that “cutlery” and “tableware” are the hypernyms of both “fork” and “spoon”. However, the noun “fork” has five senses listed in WordNet and each of"
W03-0412,J02-3001,0,0.0269979,"Missing"
W03-0412,J02-4005,0,0.0158668,"seNet and give preliminary experimental evidence to its usefulness for NLP researches. 1 Introduction Progress in natural language understanding research necessitates significant progress in lexical semantics and the development of lexical semantics resources. In a broad range of natural language applications, from ∗ Research supported by NSF grants IIS-99-84168, ITR-IIS-00-85836 and an ONR MURI award. Names of authors are listed alphabetically. prepositional phrase attachment (Pantel and Lin, 2000; Stetina and Nagao, 1997), co-reference resolution (Ng and Cardie, 2002) to text summarization (Saggion and Lapalme, 2002), semantic information is a necessary component in the inference, by providing a level of abstraction that is necessary for robust decisions. Inducing that the prepositional phrase in “They ate a cake with a fork” has the same grammatical function as that in “They ate a cake with a spoon”, for example, depends on the knowledge that “cutlery” and “tableware” are the hypernyms of both “fork” and “spoon”. However, the noun “fork” has five senses listed in WordNet and each of them has several different hypernyms. Choosing the correct one is a context sensitive decision. WordNet (Fellbaum, 1998), a"
W03-0412,W97-0109,0,0.0270021,"ext, to retrieve relevant semantic information. We describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches. 1 Introduction Progress in natural language understanding research necessitates significant progress in lexical semantics and the development of lexical semantics resources. In a broad range of natural language applications, from ∗ Research supported by NSF grants IIS-99-84168, ITR-IIS-00-85836 and an ONR MURI award. Names of authors are listed alphabetically. prepositional phrase attachment (Pantel and Lin, 2000; Stetina and Nagao, 1997), co-reference resolution (Ng and Cardie, 2002) to text summarization (Saggion and Lapalme, 2002), semantic information is a necessary component in the inference, by providing a level of abstraction that is necessary for robust decisions. Inducing that the prepositional phrase in “They ate a cake with a fork” has the same grammatical function as that in “They ate a cake with a spoon”, for example, depends on the knowledge that “cutlery” and “tableware” are the hypernyms of both “fork” and “spoon”. However, the noun “fork” has five senses listed in WordNet and each of them has several different"
W03-0412,W98-0717,1,0.850665,"P tasks today. However, WordNet is organized at the word level, and at this level, English suffers ambiguities. Stand-alone words may have several meanings and take on relations (e.g., hypernyms, hyponyms) that depend on their meanings. Consequently, there are very few success stories of automatically using WordNet in natural language applications. In many cases, reported (and unreported) problems are due to the fact that WordNet enumerates all the senses of polysemous words; attempts to use this resource automatically often result in noisy and non-uniform information (Brill and Resnik, 1994; Krymolowski and Roth, 1998). PhraseNet is designed based on the assumption that, by and large, semantic ambiguity in English disappears when local context of words is taken into account. It makes use of WordNet as an important knowledge source and is generated automatically using WordNet and machine learning based processing of large English corpora. It enhances a WordNet synset with its contextual information and refines its relational structure, including relations such as hypernym, hyponym, antonym and synonym, by maintaining only those links that respect contextual constraints. However, PhraseNet is not just a funct"
W03-0412,C98-1013,0,\N,Missing
W03-0412,W99-0501,0,\N,Missing
W04-2401,C02-1151,1,0.239201,"ion; (2) conjunctions of the features from the two arguments; (3) some patterns extracted from the sentence or between the two arguments. Some features in category (3) are “the number of words between arg1 and arg2 ”, “whether arg1 and arg2 are the same word”, or “arg1 is the beginning of the sentence and has words that consist of all capitalized characters”, where arg1 and arg2 represent the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes, in this case) are represented as linear functions over a common feature space. While SNoW can be used as a classifier and predicts using a winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors. It can be verified that the resulting"
W04-2401,W03-0419,0,0.102246,"Missing"
W04-2401,W03-0428,0,\N,Missing
W04-2421,C02-1151,1,0.815577,"designed to determine argument type, given a candidate phrase. Because phrases are considered as a whole, global properties of the candidates can be used to discover how likely it is that a phrase is of a given argument type. However, the set of possible role-labelings is restricted by structural and linguistic constraints. We encode these constraints using linear functions and use integer programming to ensure the final prediction is consistent (see Section 4). 2 SNoW Learning Architecture The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (phrase border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It incorporates several improvements over the basic Winnow update rule. In particular, a regularization term is added, which has the affect of trying to separate the data with a think separator (Grove and Roth, 2001; Hang et al., 2002). In the work presented here we use this regularization with a fixed"
W04-2421,W04-2401,1,0.738798,"actually constraints that can be evaluated on a per-phrase basis and thus can be applied to the individual phrases at any time. For efficiency sake, we eliminate these even before the second phase scoring is begun. Constraints 5, 8, and 9 are valid for only a subset of the arguments. These constraints are easy to transform into linear constraintsP (for example, for each class c, constraint 5 bei 2 comes M i=1 [S = c] ≤ 1) . Then the optimum solution of the cost function given in Equation 2 can be found by integer linear programming3. A similar method was used for entity/relation recognition (Roth and Yih, 2004). Almost all previous work on shallow parsing and phrase classification has used Constraint 4 to ensure that there are no overlapping phrases. By considering additional constraints, we show improved performance (see Table 1). 5 Results In this section, we present results. For the second phase, we evaluate the quality of the phrase predictor. The result first evaluates the phrase classifier, given the perfect phrase locations without using inference (i.e. F (P M ) = P M ). The second, adds inference to the phrase classification over the perfect classifiers (see Table 2). We evaluate the overall"
W04-3246,W02-0506,0,0.258822,"Missing"
W04-3246,W01-0502,1,0.889779,"Missing"
W04-3246,W02-2010,0,0.143443,"fier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including POS tagging, shallow parsing, information extraction tasks, etc., and compared favorably with other classifiers (Roth, 1998; Punyakanok and Roth, 2001; Florian, 2002). Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation values of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. 4.1 Feature types All the experiments we describe in this work share the same features and differ only in the target classifiers. The features that are used to characterize a word are both grammatical and statistical: • Location of letters (e.g., the thir"
W04-3246,P94-1025,0,0.284892,"Missing"
W04-3246,W03-0419,0,\N,Missing
W04-3246,W98-1007,0,\N,Missing
W05-0609,C04-1036,0,0.0153471,"+x2(2))| (b) K-Means with Euclidean (c) K-Means with a Linear Metric Figure 1: Different combinations of clustering algorithms with distance metrics. The 12 points, positioned in a twodimensional space &lt; X (1) , X (2) &gt;, are clustered into two groups containing solid and hollow points respectively. Moreover, it is not clear whether there exists any universal metric that is good for many different problems (or even different data sets for similar problems) and is appropriate for any clustering algorithm. For the word-based distributional similarity mentioned above, this point was discussed in (Geffet and Dagan, 2004) when it is shown that proximity metrics that are appropriate for class-based language models may not be appropriate for other tasks. We illustrate this critical point in Fig. 1. (a) and (b) show that even for the same data collection, different clustering algorithms with the same metric could generate different outcomes. (b) and (c) show that with the same clustering algorithm, different metrics could also produce different outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 over the classifier’s output. As expect"
W05-0609,W03-0405,0,0.0248206,"are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected t"
W05-0609,W04-3244,0,0.0265362,"fferent outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a metric directly in a clustering task but the learning p"
W05-0609,J92-4003,0,0.00706461,". Experiments in the context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem. 1 Introduction Clustering approaches have been widely applied to natural language processing (NLP) problems. Typically, natural language elements (words, phrases, sentences, etc.) are partitioned into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller g"
W05-0609,C04-1146,0,0.116276,"into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches t"
W05-0609,W99-0611,0,0.0366604,"was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting clusters based on the Euclidean distance. Clustering is typically called an unsupervised method"
W05-0625,W04-2412,0,0.189783,"Missing"
W05-0625,C04-1197,1,0.590283,"ed to classify the types of the arguments supplied by the argument identification stage. To reduce the excessive candidates mistakenly output by the previous stage, the classifier can also classify the argument as NULL (“not an argument”) to discard the argument. The features used here are the same as those used in the argument identification stage with the following additional features. most one argument of each type.” This knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions. We use the inference process introduced by (Punyakanok et al., 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier. The output is the optimal solution that maximizes the linear sum of the confidence scores (e.g., the conditional probabilities estimated by the argument classifier), subject to the constraints that encode the domain knowledge. Formally speaking, the argument classifier attempts to assign labels to a set of arguments, S 1:M , indexed from 1 to M . Each argument S i can take any label from a set of argument labels, P,"
W05-0625,C02-1151,1,0.737508,"ectively. In fact, these two parsers have noticeably different output. In evaluation, we run the system that was trained with Charniak’s parser 5 times with the top-5 parse trees output by Charniak’s parser1 . Together we have six different outputs per predicate. Per each parse tree output, we ran the first three stages, namely pruning, argument 1 The top parse tree were from the official output by CoNLL. The 2nd-5th parse trees were output by Charniak’s parser. 183 3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to"
W05-0625,W04-3212,0,0.407737,"conditional probability distribution, P rob(S i = ci ), then, given a sentence, the inference procedure seeks an global assignment that maximizes the following objective function, 1:M cˆ = argmax M X P rob(S i = ci ), c1:M ∈P M i=1 subject to linguistic and structural constraints. In other words, this objective function reflects the expected number of correct argument predictions, subject to the constraints. The constraints are encoded as the followings. • Syntactic frame describes the sequential pattern of the noun phrases and the predicate in the sentence. This is the feature introduced by (Xue and Palmer, 2004). • No overlapping or embedding arguments. • Propositional phrase head is the head of the first phrase after the preposition inside PP. • Exactly one V argument per predicate considered. • NEG and MOD feature indicate if the argument is a baseline for AM-NEG or AM-MOD. The rules of the NEG and MOD features are used in a baseline SRL system developed by Erik Tjong Kim Sang (Carreras and M`arquez, 2004). • NE indicates if the target argument is, embeds, overlaps, or is embedded in a named-entity along with its type. 1.4 Inference The purpose of this stage is to incorporate some prior linguistic"
W05-0625,J03-4003,0,\N,Missing
W05-0625,W05-0620,0,\N,Missing
W06-2926,P05-1013,0,0.0843985,"sentences. This is a multiclass classification task. The number of the dependency types for each language can be found in the organizer’s introduction paper of the shared task of CoNLL-X. In the phase of learning dependency types, the parent of the tokens, which was labeled in the first phase, will be used as features. The predicted actions can help us to make accurate predictions for dependency types. 1.3 Dealing with Crossing Edges The algorithm described in previous section is primarily designed for projective languages. To deal with non-projective languages, we use a similar approach of (Nivre and Nilsson, 2005) to map nonprojective trees to projective trees. Any single rooted projective dependency tree can be mapped into a projective tree by the Lift operation. The definition of Lift is as follows: Lift(wj → wk ) = parent(wj ) → wk , where a → b means that a is the parent of b, and parent is a function which returns the parent word of the given word. The procedure is as follows. First, the mapping algorithm examines if there is a crossing edge in the current tree. If there is a crossing edge, it will perform Lift and replace the edge until the tree becomes projective. 2 Local Search The advantage of"
W06-2926,W03-3023,0,0.712134,"ad local search that serves to make the local predictions more robust. As shown, the performance of the first generation of this algorithm is promising. 1 System Description 1.1 Parsing as a Pipeline Pipeline computation is a common computational strategy in natural language processing, where a task is decomposed into several stages that are solved sequentially. For example, a semantic role labeling program may start by using a part-of-speech tagger, than apply a shallow parser to chunk the sentence into phrases, and continue by identifying predicates and arguments and then classifying them. (Yamada and Matsumoto, 2003) proposed a bottom-up dependency parsing algorithm, where the local actions, chosen from among Shift, Left, Right, are used to generate a dependency tree using a shift-reduce parsing approach. Moreover, they used SVMs to learn the parsing decisions between pairs of consecutive words in the sentences 1 . This is a true pipeline approach in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the 1 A pair of words may become consecutive after the words between them become the children of these two words global structure."
W06-2926,dzeroski-etal-2006-towards,0,\N,Missing
W06-2926,W03-2405,0,\N,Missing
W06-2926,afonso-etal-2002-floresta,0,\N,Missing
W08-2111,kingsbury-palmer-2002-treebank,0,0.556763,"of test sentences classified with A0 first and A1 second (%A0A1). This labeling is a correct generalization for the novel ’A gorps B’ sentences, but is an overgeneralization for ’A and B gorp.’ containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-directed speech (CDS) training set consisted of about 2200 sentences, of which a majority had a single verb and two nouns to be labeled1 . We used the annotated CDS training data to train our Baby SRL, converting labeled phrases to labeled nouns in the manner described above. 3 3.1 The basic feature we propose is the noun pattern feature. We hypothesize that children use the number and order of nouns to represent argument structure. To encode this we created a feature (NPattern) that indicates how many nouns there are in the sentence and which noun the target is. For example, in ou"
W08-2111,W05-0639,0,0.318802,"for early sentence comprehension, but do not rule out the possibility that children’s early performance is based on a more complex underlying system. In this paper, we tested the consequences of our representational assumptions by performing experiments with a system for automatic semantic role labeling (SRL), whose knowledge of sentence structure is under our control. Computational models of semantic role labeling learn to identify, for each verb in a sentence, all constituents that fill a semantic role, and to determine their roles. We adopt the architecture proposed by Roth and colleagues (Punyakanok et al., 2005), limiting the classifier’s features to a set of lexical features and shallow structural features suggested by the structure-mapping account. Learning ability is measured by the level of SRL accuracy and, more importantly, the types of errors made by the system on sentences containing novel verbs. Testing these predictions on the automatic SRL provides us with a demonstration that it is possible to learn how to correctly assign semantic roles based only on these very simple cues. From an NLP perspective this feature study provides evidence for the efficacy of alternative, simpler syntactic rep"
W08-2111,W04-2412,0,0.120984,"Missing"
W09-1110,W02-1001,0,0.0324317,"Missing"
W09-1110,C92-2082,0,0.014843,"Missing"
W09-1110,N04-1043,0,0.0124554,"lake [Health Care Institution], an [Subsidiary] of Resurrection Health Care, [Locative Preposition] [Compass Direction] suburban Chicagoland. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66–74, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Deriving discriminative features from this representation often results in more informative features and a correspondingly simpler classification task. Although effective approaches along this vein have been shown to induce more accurate classifiers (Boggess et al., 1991; Miller et al., 2004; Li and Roth, 2005), naive approaches may instead result in higher sample complexity due to increased ambiguity introduced through these semantic resources. Features based upon SRWLs must therefore balance the tradeoff between descriptiveness and noise. This paper introduces the interactive feature space construction (IFSC) protocol, which facilitates coordination between a domain expert and learning algorithm to interactively define the feature space during training. This paper describes the particular instance of the IFSC protocol where semantic information is introduced through abstraction"
W09-1110,W04-2401,1,0.679846,"Missing"
W09-1110,W06-2809,0,0.0901477,"Missing"
W09-1110,D08-1004,0,0.0118176,"orms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where in73 stances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the feature space by abstracting the FGFs. 6 Conclu"
W09-1110,N07-1033,0,0.0266622,"nteractive protocol (Interactive) outperforms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where in73 stances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the f"
W09-1112,W04-2412,0,0.11405,"Missing"
W09-1112,W08-2111,1,0.550498,"predictions. 21-month-olds use the number of nouns to understand sentences containing new verbs (Yuan et al., 2007), generalize what they have learned about transitive word-order to new verbs (Gertner et al., 2006), and make the predicted error, treating intransitive sentences containing two nouns as if they were transitive (Gertner and Fisher, 2006). By 25 months, children have learned enough about English syntax to interpret conjoined-subject intransitives differently from transitives (Naigles, 1990). Our previous computational experiments with a system for automatic semantic role labeling (Connor et al., 2008) suggest that it is possible to learn to assign basic semantic roles based on the simple representations proposed by the structure-mapping view. The classifier’s features were limited to lexical information (nouns and verbs only) and the number and order of nouns in the sentence, and trained on a sample of child-directed speech annotated in PropBank (Kingsbury and Palmer, 2002) style. Given this training, our classifier learned to label the first of two nouns as an agent and the second as a patient. Even amid the variability of casual speech, simply representing the target word as the first or"
W09-1112,kingsbury-palmer-2002-treebank,0,0.094046,"ren have learned enough about English syntax to interpret conjoined-subject intransitives differently from transitives (Naigles, 1990). Our previous computational experiments with a system for automatic semantic role labeling (Connor et al., 2008) suggest that it is possible to learn to assign basic semantic roles based on the simple representations proposed by the structure-mapping view. The classifier’s features were limited to lexical information (nouns and verbs only) and the number and order of nouns in the sentence, and trained on a sample of child-directed speech annotated in PropBank (Kingsbury and Palmer, 2002) style. Given this training, our classifier learned to label the first of two nouns as an agent and the second as a patient. Even amid the variability of casual speech, simply representing the target word as the first or the second of two nouns significantly boosts SRL performance (relative to a lexical baseline) on transitive sentences 85 containing novel verbs. This result depends on key assumptions of the structure-mapping view, including abstract representations of semantic roles, and abstract but simple representations of sentence structure. Another approach was taken by (Alishahi and Ste"
W09-1112,J05-1004,0,0.0182893,"start of multiword sentence comprehension children can tell which words in a sentence are nouns (Waxman and Booth, 2001), and treat each noun as a candidate argument. We further simplify the SRL task such that classification is between two macro-roles: A0 (agent) and A1 (non-agent; all non-A0 arguments). We did so because we reason that this simplified feedback scheme can be primarily informative for a first stage of learning in which learners identify how their language identifies agents vs. non-agents in sentences. In addition, this level of role granularity is more consistent across verbs (Palmer et al., 2005). For argument classification we use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001). This learning algorithm provides a simple and general linear classifier that works well in other language tasks, and allows 86 us to inspect the weights of key features to determine their importance for classification. For the final predictions, the classifier uses predicate-level inference to ensure coherent argument assignments. In our task the only active constraints are that all nouns require a tag, and that they have unique labels, which for this restricted ca"
W09-1112,W05-0639,0,0.261497,"ve strong expectations about the capacities of animate and inanimate entities (Baillargeon et al., in press). Given the universal tendency for speakers to talk about animate action on less animate objects, many sentences will present useful training data to the SRL: In ordinary sentences such as ”You broke it,” feedback generated based on animacy will resemble gold-standard feedback. 2 Learning Model Our learning task is similar to the full SRL task (Carreras and M`arquez, 2004), except that we classify the roles of individual words rather than full phrases. A full automatic SRL system (e.g. (Punyakanok et al., 2005a)) typically involves multiple stages to 1) parse the input, 2) identify arguments, 3) classify those arguments, and then 4) run inference to make sure the final labeling for the full sentence does not violate any linguistic constraints. Our simplified BabySRL architecture essentially replaces the first two steps with developmentally plausible heuristics. Rather than identifying arguments via a learned classifier with access to a full syntactic parse, the BabySRL treats each noun in the sentence as a candidate argument and assigns a semantic role to it. A simple heuristic collapsed compound o"
W09-1119,P05-1001,0,0.0215723,"s have less context, this result is expected for the Webpages. However, we are unsure why MUC7 test set benefits from nonlocal features much less than MUC7 development set. Our key conclusion is that no single approach is better than the rest and that the approaches are complimentary- their combination is the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 1) 2) 3) 4) Component Baseline (1) + Gazetteer Match (1) + Word Class Model All External Knowledge CoNLL03 Test data 83.65 8"
W09-1119,J92-4003,0,0.115768,"ction, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 1) 2) 3) 4) Component Baseline (1) + Gazetteer Match (1) + Word Class Model All External Knowledge CoNLL03 Test data 83.65 87.22 86.82 88.55 CoNLL03 Dev data 89.25 91.61 90.85 92.49 MUC7 Dev 74.72 85.83 80.25 84.50 MUC7 Test 71.28 80.43 79.88 83.23 Web pages 71.41 74.46 72.26 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, producing a binary tree as in Figure 2. tem prediction on the previous word, four new features will be introduced which are concatenations of the previous pr"
W09-1119,W03-0422,0,0.041022,"Missing"
W09-1119,W03-0423,0,0.0216777,"labeled as LOC, and the second as ORG. We consider three approaches proposed in the literature in the following sections. Before continuing the discussion, we note that we found that adjacent documents in the CoNLL03 and the MUC7 datasets often discuss the same entities. Therefore, we ignore document boundaries and analyze global dependencies in 200 and 1000 token windows. These constants were selected by hand after trying a small number of values. We believe that this approach will also make our system more robust in cases when the document boundaries are not given. 5.1 Context aggregation (Chieu and Ng, 2003) used features that aggregate, for each document, the context tokens appear in. Sample features are: the longest capitilized sequence of words in the document which contains the current token and the token appears before a company marker such as ltd. elsewhere in text. In this work, we call this type of features context aggregation features. Manually designed context aggregation features clearly have low coverage, therefore we used the following approach. Recall that for each token instance xi , we use as features the tokens in the window of size two around it: ci = (xi−2 , xi−1 , xi , xi+1 ,"
W09-1119,W02-1001,0,0.0790866,"e counted it as a correct prediction ignoring the named entity type. 3 Design Challenges in NER Baseline system 83.29 83.38 83.38 83.71 Final System 90.57 90.67 90.67 N/A Table 1: Phrase-level F1 performance of different inference In this section we introduce the baseline NER system, and raise the fundamental questions underlying robust and efficient design. These questions define the outline of this paper. NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1 , . . . , xN ) be an input sequence and y = (y1 , . . . , yN ) be the output sequence. The sequential prediction problem is to estimate the probabilities P (yi |xi−k . . . xi+l , yi−m . . . yi−1 ), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes"
W09-1119,D07-1084,0,0.0264875,"ependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5. Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999). Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007). However, we are not aware of similar work in the NER settings. Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting. We focus instead on two most popular schemes– BIO and BILOU. The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments. The BILOU scheme suggests to learn classifiers that identify the Beginning, the Inside and the Last tokens of multi-token chunks as well as Unit-length chunks. The BILOU scheme allows to learn a more expressive model with only a small incr"
W09-1119,P05-1045,0,0.224864,". Next, we have compared the performance of our 1 http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz system to that of the Stanford NER tagger, across the datasets discussed above. We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data. We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). Our goal was to compare the performance of the taggers across several datasets. For the most realistic comparison, we have presented each system with a raw text, and relied on the system’s sentence splitter and tokenizer. When evaluating the systems, we matched against the gold tokenization ignoring punctuation marks. Table 6 summarizes the results. Note that due to differences in sentence splitting, tokenization and evaluation, these results are not identical to those reported in Table 5. Also note that in this experiment we have used token-level accuracy on the CoNLL dataset as well. Final"
W09-1119,W03-0425,0,0.0152682,"ing techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While these gazetteers have exc"
W09-1119,D07-1073,0,0.056263,"in the window c (7) conjunction of c and yi−1 . Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. 149 methods on CoNLL03 test data. Viterbi cannot be used in the end system due to non-local features. Key design decisions in an NER system. 1) How to represent text chunks in NER system? 2) What inference algorithm to use? 3) How to model non-local dependencies? 4) How to use external knowledge resources in NER? 4 Inference & Chunk Representa"
W09-1119,D07-1033,0,0.0705986,"in the window c (7) conjunction of c and yi−1 . Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. 149 methods on CoNLL03 test data. Viterbi cannot be used in the end system due to non-local features. Key design decisions in an NER system. 1) How to represent text chunks in NER system? 2) What inference algorithm to use? 3) How to model non-local dependencies? 4) How to use external knowledge resources in NER? 4 Inference & Chunk Representa"
W09-1119,P08-1068,0,0.0371211,"s the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 1) 2) 3) 4) Component Baseline (1) + Gazetteer Match (1) + Word Class Model All External Knowledge CoNLL03 Test data 83.65 87.22 86.82 88.55 CoNLL03 Dev data 89.25 91.61 90.85 92.49 MUC7 Dev 74.72 85.83 80.25 84.50 MUC7 Test 71.28 80.43 79.88 83.23 Web pages 71.41 74.46 72.26 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, produc"
W09-1119,P06-1141,0,0.0350191,"n All Non-local Features (1-4) CoNLL03 Test data 83.65 85.40 85.57 85.01 86.53 CoNLL03 Dev data 89.25 89.99 90.97 89.97 90.69 MUC7 Dev 74.72 79.16 78.56 75.48 81.41 MUC7 Test 71.28 71.53 74.27 72.16 73.61 Web pages 71.41 70.76 72.19 72.72 71.21 Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. No single technique outperformed the rest on all domains. The combination of all techniques is the most robust. 5.2 Two-stage prediction aggregation 5.4 Context aggregation as done above can lead to excessive number of features. (Krishnan and Manning, 2006) used the intuition that some instances of a token appear in easily-identifiable contexts. Therefore they apply a baseline NER system, and use the resulting predictions as features in a second level of inference. We call the technique two-stage prediction aggregation. We implemented the token-majority and the entity-majority features discussed in (Krishnan and Manning, 2006); however, instead of document and corpus majority tags, we used relative frequency of the tags in a 1000 token window. 5.3 Extended prediction history Both context aggregation and two-stage prediction aggregation treat all"
W09-1119,N04-1043,0,0.019561,"ve illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 1) 2) 3) 4) Component Baseline (1) + Gazetteer Match (1) + Word Class Model All External Knowledge CoNLL03 Test data 83.65 87.22 86.82 88.55 CoNLL03 Dev data 89.25 91.61 90.85 92.49 MUC7 Dev 74.72 85.83 80.25 84.50 MUC7 Test 71.28 80.43 79.88 83.23 Web pages 71.41 74.46 72.26 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, producing a binary tree as in Figure 2. tem prediction on the previous word,"
W09-1119,P98-2186,1,0.0999814,"be discussed later, therefore, we cannot use it in our end system. However, it has the appealing quality of finding the most likely assignment to a second-order model, and since the baseline features only have second order dependencies, we have tested it on the baseline configuration. Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later). Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (Toutanova et al., 2003; Roth and Zelenko, 1998). The implications are subtle. First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi. The reason is that with the BILOU encoding of four NE types, each token can take 21 states (O, B-PER, I-PER , U-PER, etc.). To tag a token, the greedy policy requires 21 comparisons, while the Viterbi requires 213 , and this analysis carries over to the number of classifier invocations. Furthermore, both beamsearch and Viterbi require transforming the predictions of the classiRep. Scheme BIO BILOU CoNLL03 Test Dev 89.15 93.61 90.57 93.28 MUC7 Dev Test 86.76 85."
W09-1119,P08-1076,0,0.0206937,"his result is expected for the Webpages. However, we are unsure why MUC7 test set benefits from nonlocal features much less than MUC7 development set. Our key conclusion is that no single approach is better than the rest and that the approaches are complimentary- their combination is the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 1) 2) 3) 4) Component Baseline (1) + Gazetteer Match (1) + Word Class Model All External Knowledge CoNLL03 Test data 83.65 87.22 86.82 88.55 CoNLL03 De"
W09-1119,W03-0419,0,0.639867,"Missing"
W09-1119,W06-2809,0,0.134164,"s whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While th"
W09-1119,N03-1033,0,0.0769347,"cal features which will be discussed later, therefore, we cannot use it in our end system. However, it has the appealing quality of finding the most likely assignment to a second-order model, and since the baseline features only have second order dependencies, we have tested it on the baseline configuration. Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later). Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (Toutanova et al., 2003; Roth and Zelenko, 1998). The implications are subtle. First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi. The reason is that with the BILOU encoding of four NE types, each token can take 21 states (O, B-PER, I-PER , U-PER, etc.). To tag a token, the greedy policy requires 21 comparisons, while the Viterbi requires 213 , and this analysis carries over to the number of classifier invocations. Furthermore, both beamsearch and Viterbi require transforming the predictions of the classiRep. Scheme BIO BILOU CoNLL03 Test Dev 89.15 93.61 90.57 93.2"
W09-1119,E99-1023,0,0.0254486,"by multiple “outside” tokens. This separation “breaks” the Viterbi decision process to independent maximization of assignment over short chunks, where the greedy policy performs well. On the other hand, dependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5. Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999). Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007). However, we are not aware of similar work in the NER settings. Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting. We focus instead on two most popular schemes– BIO and BILOU. The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments. The BILOU scheme suggests to learn c"
W09-1119,W03-0434,0,0.0293401,"ewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1 , . . . , xN ) be an input sequence and y = (y1 , . . . , yN ) be the output sequence. The sequential prediction problem is to estimate the probabilities P (yi |xi−k . . . xi+l , yi−m . . . yi−1 ), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes and suffixes of xi (5) tokens in the window c = (xi−2 , xi−1 , xi , xi+1 , xi+2 ) (6) capitalization pattern in the window c (7) conjunction of c and yi−1 . Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 beco"
W09-1119,C98-2181,1,\N,Missing
W10-1004,W07-1604,0,0.0858951,"Missing"
W10-1004,C08-1022,0,0.63065,"Missing"
W10-1004,I08-1059,0,0.152193,"us types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic"
W10-1004,P03-2026,0,0.445407,"ntroduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it"
W10-1004,P06-1031,0,0.047113,"d Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences writt"
W10-1004,N10-1018,1,0.493309,"ossibly, except for the Cambridge Learner Corpus http://www.cambridge.org/elt 28 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28–36, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of this size may not seem significant in many natural language applications, this is in fact a large corpus for this field, especially considering the effort to correct all mistakes, as opposed to focusing on one language phenomenon. This corpus was used in the experiments described in the companion paper (Rozovskaya and Roth, 2010). The annotation schema that we developed was motivated by our special interest in errors in article and preposition usage, but also includes errors in verbs, morphology, and noun number. The corpus contains 907 article corrections and 1309 preposition corrections, in addition to annotated mistakes of other types. While the focus of the present paper is on annotating ESL mistakes, we have several goals in mind. First, we present the annotation procedure for the task, including an error classification schema, annotation speed, and inter-annotator agreement. Second, we describe a computer progra"
W10-1004,W08-1205,0,0.732346,"he annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writ"
W10-1004,C08-1109,0,\N,Missing
W10-1004,izumi-etal-2004-overview,0,\N,Missing
W10-1004,W07-1600,0,\N,Missing
W10-1206,W09-1110,1,0.832848,"ing to 51 rank works assumed a fixed set of features, whereas, the feature set in object search depends on object domain. As we have shown, the effectiveness of the ranking function depends much on the set of features. Thus, an semi-automatic method to learn these was proposed in section 5. Our interactive learning protocol inherits features from existing works in Active Learning (see (Settles, 2009) for a survey). (Fails and Olsen, 2003) coined the term “interactive machine learning” and showed that a learner can take advantage of user interaction to quickly acquire necessary training data. (Roth and Small, 2009) proposed another interactive learning protocol that improves upon a relation extraction task by incremetally modifying the feature representation. Finally, this work is related to document retrieval mechanisms used for question answering tasks (Voorhees, 2001) where precise retrieval methods are necessary to find documents which contain specific information for answering factoids (Agichtein et al., 2001). 8 Conclusion We introduces the Object Search framework that searches the web for documents containing realworld objects. We formalized the problem as a learning to rank for IR problem and sh"
W10-2903,P09-1010,0,0.283359,"and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for their help with the Geoquery"
W10-2903,N10-1066,1,0.218067,"aining data and at a faster rate than D IRECT. Note that the performance of the D I RECT approach drops at the first iteration. We hypothesize this is due to imbalances in the binary feedback dataset (too many negative examples) in the first iteration. 7 It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 grounded world state. Our learning framework closely follows recent work on learning from indirect supervision. The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b). 90 Accuracy on Response 250 80 70 60 50 40 30 Initialization 20 Direct Approach 10 0 0 Aggressive Approach 1 2 3 4 5 6 7 Learning Iterations 8 Conclusions Figure 2: Accuracy on training set as number of learning In this paper we tackle one of the key bottlenecks in semantic parsing — providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a"
W10-2903,N06-1056,0,0.865919,"gical symbol candidates per word (on average 13 logical symbols per word). 1. Is it possible to learn a semantic parser without annotated logical forms? 24 Algorithm N O L EARN D IRECT AGGRESSIVE S UPERVISED R250 22.2 75.2 82.4 87.6 Q250 — 69.2 73.2 80.4 To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7 , we can see that with a similar number of logical forms for training our S UPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our S UPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. Table 1: Accuracy of learned models on R250 data and Q250 (testing) data. N O L EARN: using initialized weight vector, D IRECT : using feedback with the direct approach, AGGRESSIVE : using feedback with the aggressive approach, S UPERVISED: using gold 250 logical forms for training. Note"
W10-2903,P07-1121,0,0.92159,"the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm requires considerable amoun"
W10-2903,W05-0602,0,0.64828,"reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training d"
W10-2903,P06-1115,0,0.848912,"can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an"
W10-2903,D07-1071,0,0.658715,"ntic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm req"
W10-2903,W08-2105,0,0.095047,"aning Representation Sentence 2: “What is Texas’ capital?” Following previous work, we capture the semantics of the Geoquery domain using a subset of first-order logic consisting of typed constants and functions. There are two types: entities E in the domain and numeric values N . Functions describe a functional relationship over types (e.g., population : E → N ). A complete logical form is constructed through functional composition; in our formalism this is perThe ability to adapt to unseen inputs is one of the key challenges in semantic parsing. Several works (Zettlemoyer and Collins, 2007; Kate, 2008) have addressed this issue explicitly by manually defining syntactic transformation rules that can help the learned parser generalize better. Unfortunately these are only partial solutions as a 3 Mistake driven algorithms that do not enforce margin constraints may not be able to generalize using this protocol since they will repeat the same prediction at training time and therefore will not update the model. 4 This is true for all meaning representations designed to be executed by a computer system. 22 stituent c). formed by the substitution operator. For example, given the function next to(x)"
W10-2903,P09-1110,0,0.436956,"ng to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operat"
W10-2903,P09-1011,0,0.131626,"s. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for thei"
W10-2903,P06-2080,0,0.0713532,"better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty o"
W10-2903,W00-1317,0,0.0100761,"ntic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process. This approach allows the model to generalize better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then def"
W11-0327,W06-1615,0,0.171809,"Missing"
W11-0327,J92-4003,0,0.402499,"Missing"
W11-0327,W05-0620,0,0.0135737,"instances from the training domain. Consequently, it is expected that the exist229 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 229–237, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics ing model will make better predictions on them. All these predictions are then combined to choose the most probable and consistent prediction for the test sentence. ADUT is a general technique which can be applied to any natural language task. In this paper, we demonstrate its usefulness on the task of semantic role labeling (Carreras and Màrquez, 2005). Starting with a system that was trained on the news text and does not perform well on fiction, we show that ADUT provides significant improvement on fiction, and is competitive with the performance of algorithms that were re-trained on the test domain. The paper is organized as follows. Section 2 discusses two motivating examples. Section 3 gives a formal definition of our adaptation framework. Section 4 describes the transformation operators that we applied for this task. Section 5 presents our joint inference approach. Section 6 describes our semantic role labeling system and our experimen"
W11-0327,P05-1022,0,0.0154933,"s section, we discuss our experimental setup for the semantic role labeling system. Similar to the CoNLL 2005 shared tasks, we train our system using sections 02-21 of the Wall Street Journal portion of Penn TreeBank labeled with PropBank. We test our system on an annotated Brown corpus consisting of three sections (ck01 - ck03). Since we need to annotate new sentences with syntactic parse, POS tags and shallow parses, we do not use annotations in the CoNLL distribution; instead, we re-annotate the data using publicly available part of speech tagger and shallow parser1 , Charniak 2005 parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003). Our baseline SRL model is an implementation of (Punyakanok et al., 2008) which was the top performing system in CoNLL 2005 shared task. Due to space constraints, we omit the details of the system and refer readers to (Punyakanok et al., 2008). 7 Results Results for ADUT using only the top parse of Charniak and Stanford are shown in Table 2. The Baseline model using top Charniak parse (BaseLineCharniak) and top Stanford parse (BaseLineStanford) score respectively 76.4 and 73.3 on the 1 http://cogcomp.cs.illinois.edu/page/software WSJ test set. Sin"
W11-0327,P07-1033,0,0.116511,"Missing"
W11-0327,N09-1068,0,0.0517934,"Missing"
W11-0327,P09-1056,0,0.0371992,"Missing"
W11-0327,P10-1099,0,0.0701723,"joint model using multiple parse trees. In (Surdeanu et al., 2007), the authors experimented with several combination strategies. Their first combination strategy was similar to ours where they directly combined the outputs of different systems using constraints (denoted as Cons in Table 3). But their best result on Brown set was obtained by treating the combination of multiple systems as a meta-learning problem. 235 They trained a new model to score candidate arguments produced by individual systems before combining them through constraints (denoted as LBI in Table 3). We also compare with (Huang and Yates, 2010) where the authors retrained a SRL model using HMM features learned over unlabeled data of WSJ and Brown. System P R F1 Retrain (Punyakanok et al., 2008) 73.4 (Toutanova et al., 2008) NR 62.9 67.8 × NR 68.8 × (Surdeanu et al., 2007) (Cons) 78.2 (Surdeanu et al., 2007) (LBI) 81.8 62.1 69.2 × 61.3 70.1 ADUT-combined 74.3 × 67.0 70.5 × (Huang and Yates, 2010) 77.0 70.9 73.8 X Table 3: Comparison of the multi parse system on Brown. Table 3 shows that ADUT-Combined performs better than (Surdeanu et al., 2007) (Cons) when individual systems have been combined similarly. We believe that the technique"
W11-0327,P07-1034,0,0.0942588,"Missing"
W11-0327,N10-1004,0,0.0461497,"Missing"
W11-0327,D10-1075,1,0.831432,"Missing"
W11-0327,J08-2005,1,0.920753,"995). Note that multiple arguments with the same span can be generated from multiple transformed sentences. First, we take all arguments from S with distinct span and put them in S 0 . For each argument arg in S 0 , we calculate scores over possible labels as the sum over the probability distribution (over output labels) of all arguments in S that have the same span 234 as arg divided by the number of sentences in T that contained arg. This results in a set of arguments with distinct spans and for each argument, a set of scores over possible labels. Following the joint inference procedure in (Punyakanok et al., 2008), we want to select a label for each argument such that the total score is maximized subject to some constraints. Let us index the set S 0 as S 01:M where M = |S 0 |. Also assume that each argument can take a label from a set P . The set of arguments in S 01:M can take a set of labels c1:M ∈ P 1:M . Given some constraints, the resulting solution space is limited to a feasible set F; the inference task is: c1:M = arg maxc1:M ∈F (P 1:M ) PM 0i i i=1 score(S = c ). The constraints used are: 1) No overlapping or embedding argument. 2) No duplicate argument for core arguments A0-A5 and AA. 3) For C"
W11-0327,J08-2002,0,0.0492789,"Missing"
W11-0327,C10-2146,0,0.0388463,"Missing"
W11-0327,P08-1040,0,0.0283463,"8 66.8 Replacement of Quotes 71.0 63.4 67.0 Simplification 70.3 62.9 66.4 RuleTransformation 70.9 62.2 66.2 Sentence Split 70.8 62.1 66.2 Together 72.75 66.1 69.3 Table 4: Ablation Study for ADUT-Charniak Frequency Baseline Replacement of Predicate 0 64.2 67.8 less than 3 59.7 65.1 less than 7 58.9 64.8 all predicates 65.5 66.78 needed in our framework. In (Huang and Yates, 2010), the authors trained a HMM over the Brown test set and the WSJ unlabeled data. They derived features from Viterbi optimal states of single words and spans of words and retrained their models using these features. In (Vickrey and Koller, 2008), a large number of hand-written rules were used to simplify the parse trees and reduce syntactic variation to overcome feature sparsity. We have several types of transformations, and use less than 10 simplification heuristics, based on replacing larger phrases with smaller phrases and deleting unnecessary parse tree nodes. There are also some methods for unsupervised semantic role labeling (Swier and Stevenson, 2004), (Abend et al., 2009) that easily adapt across domains but their performances are not comparable to supervised systems. 9 Table 5: Performance on Infrequent Verbs for the Transfo"
W11-0327,W04-3213,0,\N,Missing
W11-0327,W04-3237,0,\N,Missing
W11-0327,P09-1004,0,\N,Missing
W11-0807,P05-1022,0,0.0876642,"Missing"
W11-0807,J90-1003,0,0.259251,"Missing"
W11-0807,W07-1106,0,0.395266,"early days (Jespersen, 1965; Butt, 2003; Kearns, 2002). Recent computational research on LVCs mainly focuses on type-based classification, i.e., statistically aggregated properties of LVCs. For example, many works are about direct measuring of the compositionality (Venkatapathy and Joshi, 2005), compatibility (Barrett and Davis, 2003), acceptability (North, 2005) and productivity (Stevenson et al., 2004) of LVCs. Other works, if related to tokenbased identification, i.e., identifying idiomatic expressions within context, only consider LVCs as one small subtype of other idiomatic expressions (Cook et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. On the other hand, some other works (Fazly and Ste"
W11-0807,E06-1043,0,0.0371288,"sen, 1965; Butt, 2003; Kearns, 2002). Recent computational research on LVCs mainly focuses on type-based classification, i.e., statistically aggregated properties of LVCs. For example, many works are about direct measuring of the compositionality (Venkatapathy and Joshi, 2005), compatibility (Barrett and Davis, 2003), acceptability (North, 2005) and productivity (Stevenson et al., 2004) of LVCs. Other works, if related to tokenbased identification, i.e., identifying idiomatic expressions within context, only consider LVCs as one small subtype of other idiomatic expressions (Cook et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. On the other hand, some other works (Fazly and Stevenson, 2007; Fazly and Stev"
W11-0807,W07-1102,0,0.694299,"et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. On the other hand, some other works (Fazly and Stevenson, 2007; Fazly and Stevenson, 2006; Stevenson et al., 2004), argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context. Tan et.al. (Tan et al., 2006) propose a learning approach to identify token-based LVCs. The method is only similar to ours in that it is a supervised framework. Our model uses a different data set annotated from BNC and the data set is larger and more balanced compared to the previous data set from WSJ. In addition, previous work assumes all verbs as potential LVCs while we intentionally exclude those"
W11-0807,W05-1005,0,0.0207672,"ing contexts. Noun Object: this is the noun head of the object noun phrase within the candidate LVC phrase. For example, for a verb phrase “take a quick look”, its noun head “look” is the active Noun Object feature. In our data set, there are 777 distinctive such nouns. LV-NounObj: this is the bigram of the light verb and the head of the noun phrase. This feature encodes the collocation information between the candidate light verb and the head noun of its object. Levin’s Class: it is observed that members within certain groups of verb classes are legitimate candidates to form acceptable LVCs (Fazly et al., 2005). For example, many sound emission verbs according to Levin (Levin, 1993), such as clap, whistle, and plop, can be used to generate legitimate LVCs. Phrases such as make a clap/plop/whistle are all highly acceptable LVCs by humans even though some of them, such as make a plop rarely occur within corpora. We formulate a vector for all the 256 Levin’s verb classes and turn the corresponding class-bits on when the verb usage of the head noun in a candidate LVC belongs to these classes. We add one extra class, other, to be mapped to those verbs which are not included in any one of these 256 Levin’"
W11-0807,W06-1203,0,0.0188353,"d to tokenbased identification, i.e., identifying idiomatic expressions within context, only consider LVCs as one small subtype of other idiomatic expressions (Cook et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. On the other hand, some other works (Fazly and Stevenson, 2007; Fazly and Stevenson, 2006; Stevenson et al., 2004), argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context. Tan et.al. (Tan et al., 2006) propose a learning approach to identify token-based LVCs. The method is only similar to ours in that it is a supervised framework. Our model uses a different data set annotated from BNC and the data set is larger"
W11-0807,W98-0604,0,0.0292661,"ample, in the candidate phrase “have a look”, “look” can directly be used as a verb while in the phrase “make a transmission”, “transmission” is derivationally related to the verb “transmit”. We use frequency counts gathered from British National Corpus (BNC) and then calculate the ratio since BNC encodes the lexeme for each word and is also tagged with parts of speech. In addition, it is a large corpus with 100 million words, thus, an ideal corpus to calculate the verb-noun usage for each candidate word in the object position. Two other lexical resources, WordNet (Fellbaum, 1998) and NomLex (Meyers et al., 1998), are used to identify words which can directly be used as a noun and a verb and those that are derivational related. Specifically, WordNet is used to identify the words which can be used as both a noun and a verb and NomLex is used to recognize those derivationally related words. And the verb usage counts of these nouns are the frequencies of their corresponding derivational verbs. For example, for the word “transmission”, its verb usage frequency is the count in BNC with its derivationally related verb “transmit”. Phrase Size: the third statistical feature is the actual size of the candidate"
W11-0807,rizzolo-roth-2010-learning,1,0.816728,"ion task as a supervised binary classification problem. For each target LVC candidate within a sentence, the classifier decides if it is a true LVC. Formally, given a set of n labeled examples {xi , yi }ni=1 , we learn a function f : X → Y where Y ∈ {−1, 1}. The learning algorithm we use is the classic soft-margin SVM with L2-loss which is among the best “off-the-shelf” supervised learning algorithms and in our experiments the algorithm indeed gives us the best performance with the shortest training time. The algorithm is implemented using a modeling language called Learning Based Java (LBJ) (Rizzolo and Roth, 2010) via the LIBSVM Java API (Chang and Lin, 2001). Previous research has suggested that both local contextual and statistical measures are informative in determining the class of an MWE token. However, it is not clear to what degree these two types of information overlap or interact. Do they contain similar knowledge or the knowledge they provide for LVC learning is different? Formulating a classification framework for identification enables us to integrate all contextual and statistical measures easily through features and test their effectiveness and interaction systematically. We focus on two"
W11-0807,W10-2108,0,0.138694,"Missing"
W11-0807,W04-0401,0,0.66739,"arning algorithm and statistical and contextual features in Sec. 3. We present our experiments and analysis in Sec. 4 and conclude our paper in Sec. 5. 2 Related Work LVCs have been well-studied in linguistics since early days (Jespersen, 1965; Butt, 2003; Kearns, 2002). Recent computational research on LVCs mainly focuses on type-based classification, i.e., statistically aggregated properties of LVCs. For example, many works are about direct measuring of the compositionality (Venkatapathy and Joshi, 2005), compatibility (Barrett and Davis, 2003), acceptability (North, 2005) and productivity (Stevenson et al., 2004) of LVCs. Other works, if related to tokenbased identification, i.e., identifying idiomatic expressions within context, only consider LVCs as one small subtype of other idiomatic expressions (Cook et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. F"
W11-0807,W06-2407,0,0.7997,"istical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally rely on or only emphasize on one of them. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. On the other hand, some other works (Fazly and Stevenson, 2007; Fazly and Stevenson, 2006; Stevenson et al., 2004), argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context. Tan et.al. (Tan et al., 2006) propose a learning approach to identify token-based LVCs. The method is only similar to ours in that it is a supervised framework. Our model uses a different data set annotated from BNC and the data set is larger and more balanced compared to the previous data set from WSJ. In addition, previous work assumes all verbs as potential LVCs while we intentionally exclude those verbs which linguistically never tested as light verbs, such as buy and sell in English and only focus on a half dozen of broadly documented English light verbs, such as have, take, give, do, get and make. The lack of common"
W11-0807,H05-1113,0,0.0207161,"lated work on LVCs in Sec. 2. Then we describe our 1 http://www.natcorp.ox.ac.uk/XMLedition/ 32 model including the learning algorithm and statistical and contextual features in Sec. 3. We present our experiments and analysis in Sec. 4 and conclude our paper in Sec. 5. 2 Related Work LVCs have been well-studied in linguistics since early days (Jespersen, 1965; Butt, 2003; Kearns, 2002). Recent computational research on LVCs mainly focuses on type-based classification, i.e., statistically aggregated properties of LVCs. For example, many works are about direct measuring of the compositionality (Venkatapathy and Joshi, 2005), compatibility (Barrett and Davis, 2003), acceptability (North, 2005) and productivity (Stevenson et al., 2004) of LVCs. Other works, if related to tokenbased identification, i.e., identifying idiomatic expressions within context, only consider LVCs as one small subtype of other idiomatic expressions (Cook et al., 2007; Fazly and Stevenson, 2006). Previous computational works on token-based identification differs from our work in one key aspect. Our work builds a learning system which systematically incorporates both informative statistical measures and specific local contexts and does indept"
W11-0807,J09-1005,0,\N,Missing
W11-0807,L08-1000,0,\N,Missing
W11-1904,D08-1031,1,0.660485,"ed reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each pair of mentions, generating a score that indicates their compatibility (Sec. 2.2). Next, at inference stage, a coreference decoder (Sec. 2.3) aggregates these scores into mention"
W11-1904,W11-1901,0,0.117627,"uction The coreference resolution task is challenging, requiring a human or automated reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each pair of mentions, generating a score that indicates their compatibility (Sec. 2.2). Next, at i"
W11-1904,rizzolo-roth-2010-learning,1,0.841039,", and discuss the challenges of resolving coreference for the OntoNotes-4.0 data set. 1 2 Introduction The coreference resolution task is challenging, requiring a human or automated reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each"
W11-2843,W11-2838,0,0.237806,"errors, and punctuation errors. Table 1 lists the error types that our system targets and shows sample errors from the pilot data1 . Introduction 2.1 The Text Correction task addresses the problem of detecting and correcting mistakes in text. This task is challenging, since many errors are not easy to detect, such as context-sensitive spelling mistakes that involve confusing valid words in a language (e.g. “there” and “their”). Recently, text correction has taken an interesting turn by focusing on contextsensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, wh"
W11-2843,N10-1019,0,0.253716,"nown to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). The methods use error statistics based only on a small amount of annotation. The preposition and article systems use these methods with additional improvements. An interesting distinction of the HOO data is that both the pilot and the test fragments are derived from the same set of papers. The size of the pilot data is not sufficient for training a competitive system, 3 http://cogcomp.c"
W11-2843,han-etal-2010-using,0,0.0576922,"ing algorithm is known to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). The methods use error statistics based only on a small amount of annotation. The preposition and article systems use these methods with additional improvements. An interesting distinction of the HOO data is that both the pilot and the test fragments are derived from the same set of papers. The size of the pilot data is not sufficient for training a competitive system, 3 ht"
W11-2843,J08-2005,1,0.678185,"involve articles. compared to a system trained on other data, but we observed only a small improvement when other data were added to the ACL Anthology corpus. The classifiers use features that are based on word n-grams, part-of-speech tags and phrase chunks. The systems use a discriminative learning framework and the regularized version of Averaged Perceptron in Learning Based Java3 (LBJ, (Rizzolo and Roth, 2007)). This linear learning algorithm is known to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozo"
W11-2843,W10-1004,1,0.645559,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,D10-1094,1,0.829333,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,N10-1018,1,0.707136,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,P11-1093,1,0.507326,"Missing"
W12-2032,P01-1005,0,0.0612509,"Missing"
W12-2032,P11-1092,0,0.0292491,"r correction, depending on whether the author’s original word choice is used in training as a feature. In the standard context-sensitive spelling correction paradigm, the decision of the classifier depends only on the context around the author’s word, e.g. article or preposition, and the author’s word itself is not taken into consideration in training. Mistakes made by non-native speakers obey certain regularities (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outperform systems that only look at context. We call the"
W12-2032,W11-2838,0,0.332638,"a and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarriff, 2011; Rozovskaya et al., 2011). We identify the unique characteristics of the error correction task and analyze the limitations of existing approaches to error correction that are due to these characteristics. Based on this analysis, we propose the error inflation method (Sect. 6.2). In this paper, we first briefly discuss the task (Sec272 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272–280, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics tion 2) and present our overall approach (Section 3. Next, we describe the spe"
W12-2032,W12-2006,0,0.33684,"tive mistakes that confuse valid English words and thus cannot be detected without considering the context around the word. Below we show examples of two common ESL mistakes considered in this paper: 1. “Nowadays ∅*/the Internet makes us closer and closer.” 2. “I can see at*/on the list a lot of interesting sports.” In (1), the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. This paper describes the University of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010"
W12-2032,N10-1019,0,0.371711,"etween two training paradigms in ESL error correction, depending on whether the author’s original word choice is used in training as a feature. In the standard context-sensitive spelling correction paradigm, the decision of the classifier depends only on the context around the author’s word, e.g. article or preposition, and the author’s word itself is not taken into consideration in training. Mistakes made by non-native speakers obey certain regularities (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outp"
W12-2032,han-etal-2010-using,0,0.12426,"Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outperform systems that only look at context. We call these models adapted. Training adapted models requires annotated data, since in native English data the source word is always correct and thus cannot be used by the classifier. In this work, we use two methods of adapting a model to typical errors that have been proposed earlier. Both methods were originally developed for models trained on native English data: they use a small amount of annotated ESL data to generate error statistics"
W12-2032,rizzolo-roth-2010-learning,1,0.56539,"he FCE data a held-out set for development. The results in Sections 7 and 8 give performance on this held-out set, where we use the HOO data (1000 files) for training. The actual performance in the task (Section 9) reflects the system trained on the whole set of 1244 documents. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). All article systems are trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). Our preposition systems combine the AP algorithm with the Na¨ıve Bayes (NB) classifier with prior parameters adapted to the learner data (see Section 5). The AP systems are trained using the inflation method (see Section 6.2). We submitted 10 runs. All of our runs achieved comparable performance. Sections 7 and 8 describe our modules. 3 4 2 Task Description System Overview Our system consists of two components that address individually article2 and preposition errors and use the same pre-processing. 1 In addition, the participating teams were allowed to use for training the remaining 244 fil"
W12-2032,W10-1004,1,0.715056,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,D10-1094,1,0.775814,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,N10-1018,1,0.522843,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,P11-1093,1,0.894649,"use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarri"
W12-2032,W11-2843,1,0.434192,"skaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarriff, 2011; Rozovskaya et al., 2011). We identify the unique characteristics of the error correction task and analyze the limitations of existing approaches to error correction that are due to these characteristics. Based on this analysis, we propose the error inflation method (Sect. 6.2). In this paper, we first briefly discuss the task (Sec272 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272–280, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics tion 2) and present our overall approach (Section 3. Next, we describe the spelling correction module (S"
W12-2032,P11-1019,0,0.0484201,"d the preposition error correction module (Section 8). In Section 9, we present the performance results of our system in the competition. We conclude with a brief discussion (Section 10). The HOO 2012 shared task focuses on correcting determiner and preposition errors made by nonnative speakers of English. These errors are some of the most common and also some of the most difficult for ESL learners (Leacock et al., 2010); even very advanced learners make these mistakes (Rozovskaya and Roth, 2010a). The training data released by the task organizers comes from the publicly available FCE corpus (Yannakoudakis et al., 2011). The original FCE data set contains 1244 essays written by non-native English speakers and is corrected and error-tagged using a detailed error classification schema. The HOO training data contains 1000 of those files.1 The test data for the task consists of an additional set of 100 student essays, different from the 1244 above. Since the HOO task focuses on determiner and preposition mistakes, only annotations marking preposition and determiner mistakes were kept. Note that while the other error annotations were removed, the errors still remain in the HOO data. More details can be found in D"
W12-4513,W11-1904,1,0.870986,"1 Introduction Coreference resolution has been a popular topic of study in recent years. In the task, a system requires to identify denotative phrases (“mentions”) and to cluster the mentions into equivalence classes, so that the mentions in the same class refer to the same entity in the real world. Coreference resolution is a central task in the Natural Language Processing research. Both the CoNLL-2011 (Pradhan et al., 2011) and CoNLL2012 (Pradhan et al., 2012) shared tasks focus on resolving coreference on the OntoNotes corpus. We also participated in the CoNLL-2011 shared task. Our system (Chang et al., 2011) ranked first in two out of four scoring metrics (BCUB and BLANC), and ranked third in the average score. This year, we further improve the system in several respects. In Sec. 2, we describe the Illinois-Coref system for the CoNLL-2011 shared task, which we take as the baseline. Then, we discuss the improvements on mention detection (Sec. 3.1), pronoun resolution (Sec. 3.2), and learning algorithm (Sec. 3.3). 113 Section 4 shows experimental results and Section 5 offers a brief discussion. 2 Baseline System We use the Illinois-Coref system from CoNLL-2011 as the basis for our current system an"
W12-4513,W11-1901,0,0.0900002,"Missing"
W12-4513,W12-4501,0,0.0892953,"Missing"
W12-4513,D08-1031,1,\N,Missing
W13-3602,P08-1021,0,0.116744,"Missing"
W13-3602,P11-1121,0,0.0795709,"Missing"
W13-3602,W13-3601,0,0.365904,"Missing"
W13-3602,rizzolo-roth-2010-learning,1,0.760565,"system on the training data using 5-fold cross-validation (hereafter, “5fold CV”) and in Section 5 we show the results we obtained on test. We close with a discussion focused on error analysis (Section 6) and our conclusions (Section 7). 2 form and subject-verb agreement errors. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). The AP system is trained using the inflation method (Rozovskaya et al., 2012). Our preposition system is a Na¨ıve Bayes (NB) classifier trained on the Google corpus and with prior parameters adapted to the learner data. The other modules – those that correct noun and verb errors – are all NB models trained on the Google corpus. All components take as input the corpus documents preprocessed with a part-of-speech tagger2 and shallow parser3 (Punyakanok and Roth, 2001). Note that the shared task data already contains comparable pre-processing information, in addition to other information, inclu"
W13-3602,W10-1004,1,0.706637,"Missing"
W13-3602,W12-2032,1,0.819811,"CV”) and in Section 5 we show the results we obtained on test. We close with a discussion focused on error analysis (Section 6) and our conclusions (Section 7). 2 form and subject-verb agreement errors. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). The AP system is trained using the inflation method (Rozovskaya et al., 2012). Our preposition system is a Na¨ıve Bayes (NB) classifier trained on the Google corpus and with prior parameters adapted to the learner data. The other modules – those that correct noun and verb errors – are all NB models trained on the Google corpus. All components take as input the corpus documents preprocessed with a part-of-speech tagger2 and shallow parser3 (Punyakanok and Roth, 2001). Note that the shared task data already contains comparable pre-processing information, in addition to other information, including dependency parse and constituency parse, but we chose to run our own pre-p"
W13-3602,W13-1703,0,0.333024,"Missing"
W13-3602,N10-1019,0,0.343943,"Missing"
W13-3602,N10-1018,1,\N,Missing
W13-3602,P11-1093,1,\N,Missing
W13-3602,D10-1094,1,\N,Missing
W13-3602,W11-2843,1,\N,Missing
W14-1704,N12-1067,0,0.0945153,"Missing"
W14-1704,W13-1703,0,0.553096,"Missing"
W14-1704,W11-2838,0,0.437403,"Missing"
W14-1704,W12-2006,0,0.416041,"Missing"
W14-1704,P08-1118,0,0.0667098,"Missing"
W14-1704,W14-1701,0,0.309089,"Missing"
W14-1704,P11-1093,1,0.64212,"tion on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system – noun number, verb agreement, verb form, and preposition – are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Na¨ıve Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and “Hence, the environmental *factor/factors also *contributes/contribute to various difficulties, *included/including problems in nuclear technology.” Error type Confusion set Noun number {factor, factors} Verb Agreement {contribute, contributes} {included, including, Verb Form includes, include} Table 2: Sample confusion sets for noun number, verb agreement, and verb form. 3 The Baseline System In this section, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to"
W14-1704,D13-1074,1,0.92303,"competition covers all errors occurring in the data. Errors outside the target group were present in the task corpora last year as well, but were not evaluated. Our system extends the one developed by the University of Illinois (Rozovskaya et al., 2013) that placed first in the CoNLL-2013 competition. For this year’s shared task, the system has been extended and improved in several respects: we extended the set of errors addressed by the system, developed a general approach for improving the error-specific models, and added a joint inference component to address interaction among errors. See Rozovskaya and Roth (2013) for more detail. We briefly discuss the task (Section 2) and give an overview of the baseline Illinois system (Section 3). Section 4 presents the novel aspects of the system. In Section 5, we evaluate the complete system on the development data and show the results obtained on test. We offer error analysis and a brief discussion in Section 6. Section 7 concludes. The CoNLL-2014 shared task is an extension of last year’s shared task and focuses on correcting grammatical errors in essays written by non-native learners of English. In this paper, we describe the Illinois-Columbia system that part"
W14-1704,W12-2032,1,0.892017,"Missing"
W14-1704,E14-1038,1,0.918558,"sample confusion sets for noun, agreement, and form errors. Each classifier takes as input the corpus documents preprocessed with a part-of-speech tag2 http://cogcomp.cs.illinois.edu/page/ software view/POS 3 http://cogcomp.cs.illinois.edu/page/ software view/Chunker 1 ∅ denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). correcting verb errors, we refer the reader to Rozovskaya et al. (2014). 4 The Mec error category includes errors in spelling, context-sensitive spelling, capitalization, and punctuation. Our system addresses punctuation errors and capitalization errors. To correct capitalization errors, we collected words that are always capitalized in the training and development data when not occurring sentence-initially. The punctuation classifier includes two modules: a learned component targets missing and extraneous comma usage and is an AP classifier trained on the learner data with error inflation. A second, pattern-based component, complements the AP model: it inserts m"
W14-1704,W13-3602,1,\N,Missing
W14-1704,W13-3601,0,\N,Missing
W16-1001,P13-1122,0,0.0273775,"cument must contain a reference to its news-peg, as it is the most noteworthy among all events in the document. Drawing on this intuition, we can use the presence/absence of a newspeg as a alternative measure of quality of a summarization. Moreover, extractive summarization (Carbonell and Goldstein, 1998) approaches can prune out sentences which do not contain (or refer to) the news-peg, thereby reducing the search space of sentences from which the summary is constructed. NLP applications such as Event Timeline Construction (Do et al., 2012) and headline generation (Woodsend and Lapata, 2012; Alfonseca et al., 2013) can benefit similarly from news-pegs. 3 Related Work 4 Attempts to distinguish foreground and background regions in text date back to the 1980s. Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events. These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph1 , and drew heavily on the syntactic correlation between grounding and information content. We analyze the performance of these rules for news-peg identification in our experiments (§6). The study of domi"
W16-1001,P98-1013,0,0.2031,"omain-independence. 1 Also known as nut-paragraph. 3 News-Peg Definition In this section we define what constitutes an event and then formally describe the criteria to determine if an event is a news-peg. We use the example sentences marked with events and news-pegs in Table 1 to help us illustrate our definition. 4.1 Event Extraction Definition 1 An event is a predicate-argument structure, whose predicate (verbal or nominal) describes a single occurrence (eg. died, married) or an aggregate of occurrences (eg. elections, shootings etc.). We adopt a event extraction approach based on FrameNet (Baker et al., 1998). We automatically generate a set of acceptable frames from FrameNet which are associated with events of our interest. This list dictates the frames of the occurrences that we will consider events. For each predicateargument structure, we identify the frame evoked by the predicate, and accept it as an event if the triggered frame belongs to the set of acceptable frames. 4.2 News-Peg Definition Team of French archaeologists work at piece-by-piece reconstruction of ancient Baphuon temple in Siem Reap, Cambodia. Kuwait’s Interior Ministry says young Kuwaiti man who fled to Saudi Arabia after terr"
W16-1001,P11-1144,0,0.0873137,"corpus contains around 650k documents annotated with human-generated summaries. We only work with the “World News” section of the corpus from 2003 to 2007. We randomly sampled 100 articles to be manually annotated and used for evaluation. We generated the list of acceptable frames by identifying the frame evoked by each event trigger in ACE (NIST, 2004) and ERE (ERE, 2013). We use this list of frames to determine whether a predicateargument structure qualifies as an event. For identifying the frame evoked by a given predicate, we use the state-of-the-art frame-identifier packaged in SEMAFOR (Das and Smith, 2011). For our event 5 extraction, all the documents were annotated using Illinois-SRL (Punyakanok et al., 2008), a state-ofthe-art SRL system, to identify nominal and verbal predicates. Note that we do not take into account semantic role assignments, as we believe this does not have any consequence on event extraction. To extract features, we use the constituency and dependency parser from Stanford CoreNLP (Manning et al., 2014) to identify the clause structure of sentences. Any clause labeled “SBAR” is considered subordinate, and a subordinate clause starting with a Wh- word is considered relativ"
W16-1001,P85-1039,0,0.304998,"e of a newspeg as a alternative measure of quality of a summarization. Moreover, extractive summarization (Carbonell and Goldstein, 1998) approaches can prune out sentences which do not contain (or refer to) the news-peg, thereby reducing the search space of sentences from which the summary is constructed. NLP applications such as Event Timeline Construction (Do et al., 2012) and headline generation (Woodsend and Lapata, 2012; Alfonseca et al., 2013) can benefit similarly from news-pegs. 3 Related Work 4 Attempts to distinguish foreground and background regions in text date back to the 1980s. Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events. These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph1 , and drew heavily on the syntactic correlation between grounding and information content. We analyze the performance of these rules for news-peg identification in our experiments (§6). The study of dominant elements of discourse has been formally studied in linguistics as a part of centering theory (Grosz et al., 1995), a broader theory of attention and coher"
W16-1001,D11-1027,1,0.864716,"th discourse. Document-level importance of entities (which include events) was explored by Gamon et al. (2013). The authors use the term salience to denote entity importance and graded entities into 3 categories – most salient, less salient, not salient. They extracted supervision from web-search logs to semiautomatically obtain noisy salience judgments for a large web corpus. Salient entities in a web document were then identified using graph centrality measures. Our event extraction approach (§4.1) closely resembles the Open-IE event extraction approach (Fader et al., 2011; Hu et al., 2013; Do et al., 2011) which views events as sentence-level relations. Events are extracted via syntactic and lexical constraints, which are imposed on sentence level structure, such as dependency parse. For example, Sun et al. (2015) use the nsubj and dobj relations to identify relation pairs, which are then merged if they share the same predicate to form a (Subj,Pred,Obj) tuple expressing an event. Unlike traditional event paradigms like ACE (NIST, 2004) and ERE (ERE, 2013), the Open-IE event paradigm enjoys portability and domain-independence. 1 Also known as nut-paragraph. 3 News-Peg Definition In this section"
W16-1001,D12-1062,1,0.806148,"can prove useful to a summarization system. Any good summary for a document must contain a reference to its news-peg, as it is the most noteworthy among all events in the document. Drawing on this intuition, we can use the presence/absence of a newspeg as a alternative measure of quality of a summarization. Moreover, extractive summarization (Carbonell and Goldstein, 1998) approaches can prune out sentences which do not contain (or refer to) the news-peg, thereby reducing the search space of sentences from which the summary is constructed. NLP applications such as Event Timeline Construction (Do et al., 2012) and headline generation (Woodsend and Lapata, 2012; Alfonseca et al., 2013) can benefit similarly from news-pegs. 3 Related Work 4 Attempts to distinguish foreground and background regions in text date back to the 1980s. Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events. These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph1 , and drew heavily on the syntactic correlation between grounding and information content. We analyze the performance of these"
W16-1001,J95-2003,0,0.392925,"ground regions in text date back to the 1980s. Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events. These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph1 , and drew heavily on the syntactic correlation between grounding and information content. We analyze the performance of these rules for news-peg identification in our experiments (§6). The study of dominant elements of discourse has been formally studied in linguistics as a part of centering theory (Grosz et al., 1995), a broader theory of attention and coherence in discourse, both of which were analyzed on a document-level basis (i.e. local discourse). The authors suggested the use of centering constructs to keep track of the key entities, which change with discourse. Document-level importance of entities (which include events) was explored by Gamon et al. (2013). The authors use the term salience to denote entity importance and graded entities into 3 categories – most salient, less salient, not salient. They extracted supervision from web-search logs to semiautomatically obtain noisy salience judgments fo"
W16-1001,D13-1036,0,0.0238942,", which change with discourse. Document-level importance of entities (which include events) was explored by Gamon et al. (2013). The authors use the term salience to denote entity importance and graded entities into 3 categories – most salient, less salient, not salient. They extracted supervision from web-search logs to semiautomatically obtain noisy salience judgments for a large web corpus. Salient entities in a web document were then identified using graph centrality measures. Our event extraction approach (§4.1) closely resembles the Open-IE event extraction approach (Fader et al., 2011; Hu et al., 2013; Do et al., 2011) which views events as sentence-level relations. Events are extracted via syntactic and lexical constraints, which are imposed on sentence level structure, such as dependency parse. For example, Sun et al. (2015) use the nsubj and dobj relations to identify relation pairs, which are then merged if they share the same predicate to form a (Subj,Pred,Obj) tuple expressing an event. Unlike traditional event paradigms like ACE (NIST, 2004) and ERE (ERE, 2013), the Open-IE event paradigm enjoys portability and domain-independence. 1 Also known as nut-paragraph. 3 News-Peg Definitio"
W16-1001,P14-5010,0,0.00281819,"predicateargument structure qualifies as an event. For identifying the frame evoked by a given predicate, we use the state-of-the-art frame-identifier packaged in SEMAFOR (Das and Smith, 2011). For our event 5 extraction, all the documents were annotated using Illinois-SRL (Punyakanok et al., 2008), a state-ofthe-art SRL system, to identify nominal and verbal predicates. Note that we do not take into account semantic role assignments, as we believe this does not have any consequence on event extraction. To extract features, we use the constituency and dependency parser from Stanford CoreNLP (Manning et al., 2014) to identify the clause structure of sentences. Any clause labeled “SBAR” is considered subordinate, and a subordinate clause starting with a Wh- word is considered relative2 . All remaining clauses are considered main. To identify appositions, we used the Illinois-Comma-SRL (Arivazhagan et al., 2016) package. 6.1 Annotation Setup We obtained news-peg judgments using the Brat annotation tool (Stenetorp et al., 2012) from two annotators3 . Each annotator was shown the human generated summary from New York Times Corpus, where event predicates detected by our event extraction procedure (§4.1) wer"
W16-1001,C02-1139,0,0.0400329,"the task of entity coreference; instead of looking for intra-document event mentions however, we look across documents. During the lifetime of an event, it is a news-peg for a short duration near its origin, as it is likely that several news sources deem it newsworthy at that time. Therefore, if an event is a news-peg, it is unlikely that it will refer to an earlier event instance (in a different article). It has been shown that detecting whether an entity is non-anaphoric benefits entitylevel coreference resolution (Peng et al., 2015; de Marneffe et al., 2015; Wiseman et al., 2015; Ng, 2004; Ng and Cardie, 2002) – we expect crossdocument event coreference to benefit in a similar way from news-peg identification. First Story Detection (FSD) Current approaches to FSD (Petrovi´c et al., 2010; Petrovi´c et al., 2012) use similarity metrics like Latent Semantic Hashing (Salakhutdinov and Hinton, 2009) and use an inverted index to compare it with O(1) (≈ 1000) of the most recent documents. A shortcoming of this problem formulation is that they treat the entire document as a event and do not account for multiple events in the same document. This formulation works well with tweets (assuming most tweets descr"
W16-1001,P04-1020,0,0.0163023,"tities in the task of entity coreference; instead of looking for intra-document event mentions however, we look across documents. During the lifetime of an event, it is a news-peg for a short duration near its origin, as it is likely that several news sources deem it newsworthy at that time. Therefore, if an event is a news-peg, it is unlikely that it will refer to an earlier event instance (in a different article). It has been shown that detecting whether an entity is non-anaphoric benefits entitylevel coreference resolution (Peng et al., 2015; de Marneffe et al., 2015; Wiseman et al., 2015; Ng, 2004; Ng and Cardie, 2002) – we expect crossdocument event coreference to benefit in a similar way from news-peg identification. First Story Detection (FSD) Current approaches to FSD (Petrovi´c et al., 2010; Petrovi´c et al., 2012) use similarity metrics like Latent Semantic Hashing (Salakhutdinov and Hinton, 2009) and use an inverted index to compare it with O(1) (≈ 1000) of the most recent documents. A shortcoming of this problem formulation is that they treat the entire document as a event and do not account for multiple events in the same document. This formulation works well with tweets (assu"
W16-1001,P12-2045,0,0.0161976,"(on average), the number of comparisons will be O(n2 ). A news-peg, on the other hand, identifies the reported event and thus allows us to focus on the most noteworthy event in the article, bringing the number of comparisons back to O(1). Using news-pegs we can also perform a heuristic pruning of this search space, by allowing the system to ignore documents which do not have a similar event as their news-peg. For instance, if a newly arrived article describes a bombing event, we can prune out all articles from the 1000 most recent articles whose news-peg was not a bombing event. Event Linking Nothman et al. (2012) introduced event linking as the task of grounding a event mention (referent) to a article in a news archive that first reports it (anchor article). They noted that annotating a large corpus was impractical because of the under-specification of “newsworthiness” and because the same article can be the anchor for some events and not for others. Annotation effort can be significantly reduced by knowing what is being reported for the first time in a document as it narrows the set of possible events the document can be an anchor for. News-pegs can help in segmenting a document into events which are"
W16-1001,K15-1002,1,0.75412,"otion of a news-peg is closely related to that of a nonanaphoric entities in the task of entity coreference; instead of looking for intra-document event mentions however, we look across documents. During the lifetime of an event, it is a news-peg for a short duration near its origin, as it is likely that several news sources deem it newsworthy at that time. Therefore, if an event is a news-peg, it is unlikely that it will refer to an earlier event instance (in a different article). It has been shown that detecting whether an entity is non-anaphoric benefits entitylevel coreference resolution (Peng et al., 2015; de Marneffe et al., 2015; Wiseman et al., 2015; Ng, 2004; Ng and Cardie, 2002) – we expect crossdocument event coreference to benefit in a similar way from news-peg identification. First Story Detection (FSD) Current approaches to FSD (Petrovi´c et al., 2010; Petrovi´c et al., 2012) use similarity metrics like Latent Semantic Hashing (Salakhutdinov and Hinton, 2009) and use an inverted index to compare it with O(1) (≈ 1000) of the most recent documents. A shortcoming of this problem formulation is that they treat the entire document as a event and do not account for multiple events in the sa"
W16-1001,N10-1021,0,0.0540702,"Missing"
W16-1001,N12-1034,0,0.0662173,"Missing"
W16-1001,J08-2005,1,0.638493,"“World News” section of the corpus from 2003 to 2007. We randomly sampled 100 articles to be manually annotated and used for evaluation. We generated the list of acceptable frames by identifying the frame evoked by each event trigger in ACE (NIST, 2004) and ERE (ERE, 2013). We use this list of frames to determine whether a predicateargument structure qualifies as an event. For identifying the frame evoked by a given predicate, we use the state-of-the-art frame-identifier packaged in SEMAFOR (Das and Smith, 2011). For our event 5 extraction, all the documents were annotated using Illinois-SRL (Punyakanok et al., 2008), a state-ofthe-art SRL system, to identify nominal and verbal predicates. Note that we do not take into account semantic role assignments, as we believe this does not have any consequence on event extraction. To extract features, we use the constituency and dependency parser from Stanford CoreNLP (Manning et al., 2014) to identify the clause structure of sentences. Any clause labeled “SBAR” is considered subordinate, and a subordinate clause starting with a Wh- word is considered relative2 . All remaining clauses are considered main. To identify appositions, we used the Illinois-Comma-SRL (Ar"
W16-1001,E12-2021,0,0.052933,"Missing"
W16-1001,P15-1045,0,0.0196433,"– most salient, less salient, not salient. They extracted supervision from web-search logs to semiautomatically obtain noisy salience judgments for a large web corpus. Salient entities in a web document were then identified using graph centrality measures. Our event extraction approach (§4.1) closely resembles the Open-IE event extraction approach (Fader et al., 2011; Hu et al., 2013; Do et al., 2011) which views events as sentence-level relations. Events are extracted via syntactic and lexical constraints, which are imposed on sentence level structure, such as dependency parse. For example, Sun et al. (2015) use the nsubj and dobj relations to identify relation pairs, which are then merged if they share the same predicate to form a (Subj,Pred,Obj) tuple expressing an event. Unlike traditional event paradigms like ACE (NIST, 2004) and ERE (ERE, 2013), the Open-IE event paradigm enjoys portability and domain-independence. 1 Also known as nut-paragraph. 3 News-Peg Definition In this section we define what constitutes an event and then formally describe the criteria to determine if an event is a news-peg. We use the example sentences marked with events and news-pegs in Table 1 to help us illustrate o"
W16-1001,P15-1137,0,0.0162204,"t of a nonanaphoric entities in the task of entity coreference; instead of looking for intra-document event mentions however, we look across documents. During the lifetime of an event, it is a news-peg for a short duration near its origin, as it is likely that several news sources deem it newsworthy at that time. Therefore, if an event is a news-peg, it is unlikely that it will refer to an earlier event instance (in a different article). It has been shown that detecting whether an entity is non-anaphoric benefits entitylevel coreference resolution (Peng et al., 2015; de Marneffe et al., 2015; Wiseman et al., 2015; Ng, 2004; Ng and Cardie, 2002) – we expect crossdocument event coreference to benefit in a similar way from news-peg identification. First Story Detection (FSD) Current approaches to FSD (Petrovi´c et al., 2010; Petrovi´c et al., 2012) use similarity metrics like Latent Semantic Hashing (Salakhutdinov and Hinton, 2009) and use an inverted index to compare it with O(1) (≈ 1000) of the most recent documents. A shortcoming of this problem formulation is that they treat the entire document as a event and do not account for multiple events in the same document. This formulation works well with tw"
W16-1001,D12-1022,0,0.0212113,". Any good summary for a document must contain a reference to its news-peg, as it is the most noteworthy among all events in the document. Drawing on this intuition, we can use the presence/absence of a newspeg as a alternative measure of quality of a summarization. Moreover, extractive summarization (Carbonell and Goldstein, 1998) approaches can prune out sentences which do not contain (or refer to) the news-peg, thereby reducing the search space of sentences from which the summary is constructed. NLP applications such as Event Timeline Construction (Do et al., 2012) and headline generation (Woodsend and Lapata, 2012; Alfonseca et al., 2013) can benefit similarly from news-pegs. 3 Related Work 4 Attempts to distinguish foreground and background regions in text date back to the 1980s. Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events. These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph1 , and drew heavily on the syntactic correlation between grounding and information content. We analyze the performance of these rules for news-peg identification in our experiment"
W16-1001,C98-1013,0,\N,Missing
W16-1001,D11-1142,0,\N,Missing
W16-1906,W12-1913,1,0.844806,"nts happen in batch mode. This assumption could be relaxed in future, since there already exist incremental models of word category assignment (Parisien et al., 2008; Fountain and Lapata, 2011). Here, as with the original work, we chose not to focus on this earlier stage of language acquisition, and instead assume that learning distributional facts about words proceeds largely independently for some time, until a few nouns are known – at which point syntax guides interpretation of the distributional classes. However, we know that category learning itself is influenced by syntactic properties (Christodoulopoulos et al., 2012). As such, in future work we plan to integrate the syntactic category learning with the verb and noun prediction stage to improve the accuracy of both. 4 3 The storing of both states 57 and 58 as potential oneargument verbs in the example may seem to conflict with the assumption that there is only one verb per sentence. It is true that at this stage, the model will lose information relevant to the true number of arguments of each verb, since potential arguments may be wrongly identified as verb candidates. However, the statistical stability of verb argument-taking behaviour, as well as the inc"
W16-1906,P10-1101,1,0.843459,"ersity of Illinois at Urbana-Champaign {christod,danr,clfishe}@illinois.edu * Abstract characterise the syntactic structure of sentences. One mechanism for resolving this issue is Structure Mapping (Fisher et al., 2010), which hypothesises that, assuming an innate one-to-one mapping between nouns and semantic arguments in an utterance, children are able to use this information to first identify verbs and their arguments, and then assign semantic roles to those arguments. In this paper we provide a computational model for this account of syntactic bootstrapping. We use a system called BabySRL (Connor et al., 2010; Connor et al., 2012) that assigns semantic roles to arguments in an utterance – a simplified version of the Semantic Role Labeling Task (SRL; (Palmer et al., 2011)). Here, we focus on the preliminary task of identifying nouns and verbs from sentences in a corpus of child-directed speech (the Brown corpus (Brown, 1973), a subset of the CHILDES database (MacWhinney, 2000)). Previous work (Connor et al., 2010) presented a model which could identifying noun and verb clusters with minimal supervision (a few seed nouns). However, this model had two substantial limitations: the first was training w"
W16-1906,W08-2112,0,0.0134436,"oom’, and one known seed noun, ‘you’, leaving ‘eating’ as the only allowable verb candidate, and correctly predicting the argument histogram count (2) for its state (73). Using this toy example, we can see how it will not take long for both the noun and verb heuristics to reach the prediction level of the batch mode via an incremental process. Note that while noun and verb prediction is truly incremental, the preliminary HMM learning and state assignments happen in batch mode. This assumption could be relaxed in future, since there already exist incremental models of word category assignment (Parisien et al., 2008; Fountain and Lapata, 2011). Here, as with the original work, we chose not to focus on this earlier stage of language acquisition, and instead assume that learning distributional facts about words proceeds largely independently for some time, until a few nouns are known – at which point syntax guides interpretation of the distributional classes. However, we know that category learning itself is influenced by syntactic properties (Christodoulopoulos et al., 2012). As such, in future work we plan to integrate the syntactic category learning with the verb and noun prediction stage to improve the"
W17-0902,J14-2004,0,0.0128507,"etween predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-ali"
W17-0902,araki-etal-2014-detecting,0,0.0144858,"tracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between pre"
W17-0902,D08-1031,1,0.548822,"reras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (O"
W17-0902,P10-1124,1,0.91175,"ic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+"
W17-0902,J12-1003,1,0.927138,"rbal performance (accuracy of 0.73 vs. 0.25). Finally, argument identification was hard mainly because of inconsistencies in verbal versus nominal predicate-argument structure in dependency trees.4 The low performance in predicate coreference compared to entity coreference can be explained by the higher variability of predicate terms. The argument co-reference task becomes easy given gold predicate-argument structures, as most arguments are singletons (i.e. composed of a single element). Finally, while the performance of the predicate entailment component reflects the current stateof-the-art (Berant et al., 2012; Han and Sun, 2016), the performance on entity entailment is much worse than the current state-of-the-art in this task as measured on common lexical inference test sets. We conjecture that this stems from the nature of the entities in our dataset, consisting of both named entities and common nouns, many of which are multi-word expressions, whereas most work in entity entailment is focused on single word common nouns. Furthermore, it is worth noting that our annotations are of naturally occurring texts, and represent lexical entailment in real world coreference chains, as opposed to synthetica"
W17-0902,W05-0620,0,0.162639,"Missing"
W17-0902,W99-0201,0,0.0935993,"on Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference"
W17-0902,W09-4303,0,0.0115663,") is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining en"
W17-0902,W13-2322,0,0.107609,"le texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al.,"
W17-0902,P15-1136,0,0.0225471,"Missing"
W17-0902,E12-1004,0,0.0217269,"tractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art"
W17-0902,cybulska-vossen-2014-using,0,0.0126286,"achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can h"
W17-0902,P99-1071,0,0.536493,"on. We can expect the use of OKR structures in MDS to shift the research efforts in this task to other components, e.g. generation, and eventually contribute to improving state of the art on this task. Similarly, an algorithm creating the ECKG structure can benefit from building upon a consolidated structure such as OKR, rather than working directly on free text. systematic solution, and the burden of integrating information across multiple texts is delegated to downstream applications, leading to partial solutions which are geared to specific applications. Multi-Document Summarization (MDS) (Barzilay et al., 1999) is a task whose goal is to produce a concise summary from a set of related text documents, such that it includes the most important information in a non-redundant manner. While extractive summarization selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a sing"
W17-0902,P13-2080,1,0.82203,"ment annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of"
W17-0902,bejan-harabagiu-2008-linguistic,0,0.0326074,"1; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align"
W17-0902,duclaye-etal-2002-using,0,0.0512409,"implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 20"
W17-0902,D12-1045,0,0.0269834,"2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing t"
W17-0902,D11-1142,0,0.0614886,"ference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments a"
W17-0902,D14-1168,0,0.0154703,"selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entitie"
W17-0902,C16-1273,0,0.0306302,"Missing"
W17-0902,liu-etal-2014-supervised,0,0.0231139,"Missing"
W17-0902,D15-1076,0,0.0606389,"on in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use"
W17-0902,N15-1114,0,0.026058,"ng each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on coreference. 7 Conclusions In this paper we advocate the development of representation frameworks for the consolidated information expressed in a set of texts. The key ingredients of our approach are the extraction of pr"
W17-0902,C92-2082,0,0.446376,"wo candidate sentences for the summary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Ent"
W17-0902,W97-1311,0,0.0722333,"rior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to"
W17-0902,N15-1050,1,0.790932,"event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored"
W17-0902,P13-1048,0,0.0297432,"zation generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on corefere"
W17-0902,W16-5304,1,0.885624,"Missing"
W17-0902,K15-1018,1,0.833569,"m for improvement. These bottle-necks are bound to hinder the performance of a pipeline end-to-end system. Future research into OKR should first target these areas; either as a pipeline or in a joint learning framework. 3 For Argument Mention detection we attach the components (entities and propositions) as arguments of predicates when the components are syntactically dependent on them. Argument Coreference is simply predicted by marking coreference if and only if the arguments are both mentions of the same entity co-reference chain. For Entity Entailment purposes we used knowledge resources (Shwartz et al., 2015) and a pretrained model for HypeNET (Shwartz et al., 2016) to obtain a score for all pairs of Wikipedia common words (unigrams, bigrams, and trigrams). A threshold for the binary entailment decision was then calibrated on a held out development set. Finally, for Predicate Entailment we used the entailment rules extracted by Berant et al. (2012). 5.1 Results and Error Analysis Using the same metrics used for measuring interannotator agreement, we evaluated how well the presented models were able to recover the different facets of the OKR gold annotations. The performance on the different subtas"
W17-0902,P15-1146,0,0.0257952,"Missing"
W17-0902,P16-1226,1,0.873666,"Missing"
W17-0902,K15-1002,1,0.834714,"ile the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al.,"
W17-0902,D16-1038,1,0.86973,"Missing"
W17-0902,P11-1080,0,0.0191905,"ddle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankar"
W17-0902,S12-1030,0,0.0281997,"ms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee e"
W17-0902,J01-4004,0,0.174444,"s like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolutio"
W17-0902,P16-1119,1,0.813405,"cly available tools and simple baselines which approximate the current state-of-the-art in each of these subtasks. For brevity sake, in the rest of the section we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agr"
W17-0902,E14-4008,0,0.0143178,"mmary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Entities: E1 = {Turkey}, E2 = {Syrian}, E3"
W17-0902,P15-2050,1,0.81398,"test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are rep"
W17-0902,D10-1106,0,0.0195202,"ub.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity c"
W17-0902,P12-3013,1,0.834165,"ss document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of Open IE, we would like the representation to be open, while relying only on the"
W17-0902,W04-3206,1,0.412965,"ailable at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions,"
W17-0902,C16-1183,1,0.835662,"on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual"
W17-0902,M95-1005,0,0.294299,"ction we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agreement scores for the two annotators are shown in Table 1, and overall show high levels of agreement. A qualitative analysis of the more common disa"
W17-0902,C14-1212,0,0.0125443,"t Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task"
W17-0902,P13-2012,0,0.0402841,"Missing"
W17-0902,N15-1002,0,0.0522403,"Missing"
W17-0902,P02-1014,0,\N,Missing
W17-0902,P15-1034,0,\N,Missing
W17-2812,W10-2903,1,0.74867,"e language in the context of that task (i.e. to map between utterances and meaning representations the problem solving components of the agent can act on in a particular situation). The agent may also need to initiate clarification requests when communication fails, and to learn new domain (or conversation) specific vocabulary and its meaning. This kind of symmetric, grounded communication with a problem-solving agent goes significantly beyond the one-step, single direction understanding tasks considered in standard semantic parsing (e.g. Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010) or even short, simple instructions to robots (e.g. Tellex et al., 2011). In order to focus on these concept learning and communication issues, we deliberately limit ourselves here to a simple, simulated environment. 95 Proceedings of the First Workshop on Language Grounding for Robotics, pages 95–103, c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics Figure 1: A complex shape, which can be viewed as conjunction of simpler known shapes: a row (dark, width = 4) and a square (light, size = 3). 2 Domain and Problem Setup We consider a two-dimensional (2"
W17-2812,P16-1004,0,0.0294175,"= h{hsi , idi , ∧k di i}i∈S , ∧j∈[S×S] fj i, where In this section, we describe the current implementations of the different modules (language comprehension, memory, problem solving, language production, and dialogue mediation) in COG, noting that the architecture is flexible and allows for us to plug-in other implementations as needed. 4.1 Language Comprehension The LC module consists of semantic parsing and language grounding components. 4.1.1 Language Grounding Semantic Parsing Our semantic parser is implemented as a neural sequence-to-sequence model with attention (Bahdanau et al., 2014; Dong and Lapata, 2016; Jia and Liang, 2016). The model consists of two LSTMs. The first LSTM (the encoder) processes the input sentence x = (x1 , . . . , xm ) token-by-token, producing a sequence of hidden states hs = (hs1 , . . . , hsm ) as output. The second LSTM (the decoder) models a distribution P (yi |y1 , . . . , yi−1 ; hs ) at each time step over output tokens as a function of the encoder hidden states and the previous outputs. The final parse y = (y1 , . . . , yn ) is obtained by selecting the token at each time step that maximizes this probability and feeding a learned embedding for it into (k) S is the"
W17-2812,W03-2316,0,0.0595628,"expect to see a large number of examples. We will consider the use of probabilistic logic models which can handle both issues by explicitly including the trade-off in the optimization function (Odom et al., 2015). A final challenge is the application of our agent to new domains. Currently, the memory module contains all the knowledge required to plan and produce comprehensible responses. This declarative approach should generalize well to some simple enough domains, but will need to be extended to deal with more involved tasks and domains. veloped a grammar-based realizer inspired by OpenCCG (White and Baldridge, 2003; White, 2006) that operates over the first-order semantic representations used by our agent. We plan to augment the realizer’s semantic lexicon with the learned definitions of predicates for new shapes, allowing our system to generate natural language instructions describing the new configurations. One of the key challenges of the scenario we envision (and a fundamental problem in language acquisition) is the necessity to generalize across situations. This is required in order to learn general concepts from a few specific instances. At this point, our agent is able to generalize from a single"
W17-2812,P16-1154,0,0.0142285,"ere the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are used by LC and LP."
W17-2812,P16-1014,0,0.0162333,"entence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are u"
W17-2812,P16-1002,0,0.050247,"nputs are processed sentence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquire"
W19-2801,P98-1012,0,0.626464,"he number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links betw"
W19-2801,I13-1193,0,0.017275,"any name or are linked to a wrong name, the results would not be useful for downstream tasks. Standard coreference metrics do not incorporate these aspects and hence give high performance for results unsuitable for further use. We also show that the existing metrics are not sensitive to finding any mention to an entity at all. They give higher performance for systems that do not find a large number of entities but do good coreference resolution on the subset of entities they find. This problem of coreference chains without any named mentions being unsuitable has previously been discussed in (Chen and Ng, 2013). The authors argued that a name is more informative than a nominal, which is more informative than a pronoun so they assign different weights to co-reference links (mention-antecedent pairs) in a chain depending on the type of mentions the link contains. They assign a higher weight to In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the German chancellor, etc.). It is therefore important that coreference resolution systems"
W19-2801,D14-1221,0,0.0185875,"ntribution 1 Proceedings of the 2nd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2019), pages 1–7, c Minneapolis, USA, June 7, 2019. 2019 Association for Computational Linguistics B-cubed B 3 works on the mention level. It iterates over all gold-standard mentions of an entity, averaging the recall of its gold cluster in its predicted cluster. It computes precision by reversing the role of gold and predicted clusters. a link having a name than one that doesn’t and also higher weight to a link having a nominal than a link that contains just pronouns. Similarly, (Martschat and Strube, 2014) perform an error analysis for co-reference by choosing an antecedent that is a name or a nominal in this order because they are more informative than a pronoun. However, we argue that we should view the coreference chains as a whole instead of individual links when evaluating systems for downstream application. If a chain contains even one named mention, it should be sufficient for using it in applications and we need not consider the mention type in each link within the chain. We introduce metrics focused on Named Entity Coreference (NEC) which separate the identification of entities and res"
W19-2801,P16-1061,0,0.050434,"Missing"
W19-2801,P16-1060,0,0.0187315,"ed clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links between mentions. Results for each entity are weighted by its num"
W19-2801,P14-2006,0,0.102739,"ki ∈K |ki ∩rj |2 |ki | |ki | CEAF CEAF first maps each gold cluster to a predicted cluster. It then computes recall as the number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |"
W19-2801,D15-1162,0,0.0265647,"eural end-to-end systems of (Lee et al., 2017) and (Lee et al., 2018) on traditional and NEC metrics. These general coreference systems find coreferring expressions of any type and produce coreference chains for all mentioned entities. In NEC, the goal is to find all mentions to an entity that has been referred to by name at least once in the document. The output of off-the-shelf coreference systems has to be filtered to keep only chains that contain at least one mention noun phrase with a syntactic head that is a entity’s name.4 For our evaluation, we use the spaCy dependency parsing system (Honnibal and Johnson, 2015) to detect whether a name is the head of a mention, by checking that no other word in the mention is an ancestor of the name in the dependency parse tree. In evaluation, we use gold NER tags to determine if the head is a name. Note that the dependency parsing and gold NER are not given to the systems but are used to process their output. Many system NEC chains did not have any Evaluation of Systems We make use of the relevant part of OntoNotes coreference corpus (Pradhan et al., 2007) and gold-standard annotations for named entities on the same data to quantify the patterns in coreference of d"
W19-2801,P11-1115,0,0.0821185,"Missing"
W19-2801,W11-1902,0,0.398855,"Missing"
W19-2801,D10-1048,0,0.104489,"Missing"
W19-2801,W09-1119,1,0.669316,"Missing"
W19-2801,D17-1018,0,0.0863558,"Missing"
W19-2801,P11-1138,1,0.814955,"Missing"
W19-2801,N18-2108,0,0.0883171,"Missing"
W19-2801,N13-1071,0,0.0599811,"Missing"
W19-2801,H05-1004,0,0.124694,"by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links between mentions. Results"
W19-2801,P09-1074,0,0.0937083,"Missing"
W19-2801,W02-1111,0,0.170753,"ty Coreference Oshin Agarwal ∗ University of Pennsylvania oagarwal@seas.upenn.edu Sanjay Subramanian ∗ University of Pennsylvania subs@seas.upenn.edu Ani Nenkova University of Pennsylvania nenkova@seas.upenn.edu Dan Roth University of Pennsylvania danroth@seas.upenn.edu Abstract information extraction (Ji and Grishman, 2011), biography summarization (Zhou et al., 2004) and knowledge base completion tasks (West et al., 2014). More relevant information can be extracted for these tasks if we also know which pronouns and nominals refer to the entity. Similarly, creation of proper noun ontologies (Mann, 2002) can use patterns other than (proper noun–common noun) if other references to the entity are known. Recent work (Webster et al., 2018) has shown that standard coreference datasets are biased and high performance on these need not mean high performance in downstream tasks. We argue that the standard coreference metrics are not suitable either from the perspective of downstream applications. Since applications require information about entities and entities are usually identified by their names, the evaluation metrics should focus on the resolution of mentions to the correct name. If all the pro"
W19-2801,M95-1005,0,0.901397,"t then computes recall as the number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction o"
W19-2801,A97-1030,0,0.644032,"Missing"
W19-2801,Q18-1042,0,0.0602823,"vania subs@seas.upenn.edu Ani Nenkova University of Pennsylvania nenkova@seas.upenn.edu Dan Roth University of Pennsylvania danroth@seas.upenn.edu Abstract information extraction (Ji and Grishman, 2011), biography summarization (Zhou et al., 2004) and knowledge base completion tasks (West et al., 2014). More relevant information can be extracted for these tasks if we also know which pronouns and nominals refer to the entity. Similarly, creation of proper noun ontologies (Mann, 2002) can use patterns other than (proper noun–common noun) if other references to the entity are known. Recent work (Webster et al., 2018) has shown that standard coreference datasets are biased and high performance on these need not mean high performance in downstream tasks. We argue that the standard coreference metrics are not suitable either from the perspective of downstream applications. Since applications require information about entities and entities are usually identified by their names, the evaluation metrics should focus on the resolution of mentions to the correct name. If all the pronouns referring to an entity are resolved correctly to each other but are not linked to any name or are linked to a wrong name, the re"
W19-2801,W04-3256,0,0.341417,"Missing"
W19-3710,P14-5010,0,0.00317602,"om the 2019 task in that training data was not provided to participants. Approaches submitted to this task included a model based on parallel projection (Mayfield et al., 2017) and a model with language-specific features trained on found data (Marci´nczuk et al., 2017). There has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018). Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years. Early models proposed were averaged perceptron (Ratinov and Roth, 2009), and conditional random field (Manning et al., 2014). In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016). A further increase in performance has come with contextual embeddings (Devlin et al., 2019; Peters et al., 2018; Akbik et al., 2018), which are based on large language models trained over massive corpora. Of particular interest is the multilingual BERT model (Devlin et al., 2019), which is trained over the concatenation of the Wikipedias in over 100 languages.1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have em"
W19-3710,C18-1139,0,0.0277876,"ere has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018). Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years. Early models proposed were averaged perceptron (Ratinov and Roth, 2009), and conditional random field (Manning et al., 2014). In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016). A further increase in performance has come with contextual embeddings (Devlin et al., 2019; Peters et al., 2018; Akbik et al., 2018), which are based on large language models trained over massive corpora. Of particular interest is the multilingual BERT model (Devlin et al., 2019), which is trained over the concatenation of the Wikipedias in over 100 languages.1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have emergent cross-lingual properties, as well as language identification capabilities (Wu and Dredze, 2019). Several models have been proposed for multisource learning, in which multiple languages are used to train a model, including for machine translation (Zoph and Knight,"
W19-3710,Q16-1026,0,0.020847,"d a model based on parallel projection (Mayfield et al., 2017) and a model with language-specific features trained on found data (Marci´nczuk et al., 2017). There has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018). Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years. Early models proposed were averaged perceptron (Ratinov and Roth, 2009), and conditional random field (Manning et al., 2014). In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016). A further increase in performance has come with contextual embeddings (Devlin et al., 2019; Peters et al., 2018; Akbik et al., 2018), which are based on large language models trained over massive corpora. Of particular interest is the multilingual BERT model (Devlin et al., 2019), which is trained over the concatenation of the Wikipedias in over 100 languages.1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have emergent cross-lingual properties, as well as language identification capabilities (Wu and Dredze, 2019). Several mod"
W19-3710,W17-1413,0,0.0654005,"Missing"
W19-3710,D18-1327,0,0.0246059,"anguage models trained over massive corpora. Of particular interest is the multilingual BERT model (Devlin et al., 2019), which is trained over the concatenation of the Wikipedias in over 100 languages.1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have emergent cross-lingual properties, as well as language identification capabilities (Wu and Dredze, 2019). Several models have been proposed for multisource learning, in which multiple languages are used to train a model, including for machine translation (Zoph and Knight, 2016; Johnson et al., 2017; Currey and Heafield, 2018), and NER (T¨ackstr¨om, 2012; Tsai et al., 2016; Mayhew et al., 2017; Rahimi et al., 2019). 3 Tokens English (CoNLL) 964 203,621 Bulgarian (BG) Czech (CS) Polish (PL) Russian (RU) 699 373 586 271 226,728 84,636 237,333 67,495 Table 1: Training data sizes in CoNLL and BSNLP19 datasets. Of the BSNLP19 sets, the largest (Polish) is nearly 3 times the size of the smallest (Russian). Tag Total Unique Ratio PER LOC ORG EVT PRO 9986 9563 8520 2601 1699 2851 1540 1923 235 739 3.5 6.2 4.4 11.0 2.3 Table 2: Entity distribution statistics across all languages in the BSNLP19 training set, where the “Ratio"
W19-3710,W17-1414,0,0.0174978,"hallenging), and differing domains – the training and test sets are composed of newswire documents collected around 75 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 75–82, c Florence, Italy, 2 August 2019. 2019 Association for Computational Linguistics 2 Related Work Lang. The first shared task in Balto-slavic NLP was held in 2017, and reported in Piskorski et al. (2017). The task was somewhat different from the 2019 task in that training data was not provided to participants. Approaches submitted to this task included a model based on parallel projection (Mayfield et al., 2017) and a model with language-specific features trained on found data (Marci´nczuk et al., 2017). There has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018). Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years. Early models proposed were averaged perceptron (Ratinov and Roth, 2009), and conditional random field (Manning et al., 2014). In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016). A further increa"
W19-3710,N19-1423,0,0.565809,"f the multilingual named entity recognition task on Slavic languages organized for the BSNLP workshop. A similar shared task was previously held in 2017 (BSNLP2017), and was composed of the same subtasks, but was evaluated on seven Slavic languages. It had a slightly different format, in that training data was not provided to the participants, so the majority of the submissions relied on cross-lingual or rule-based approaches. Our overarching research goal for this project was to experiment with multisource neural NER transfer, leveraging recent advances in multilingual contextual embeddings (Devlin et al., 2019). Ultimately, we aimed to maximize parametersharing by training a single model on the concatenation of training data from sources (languages). Such multi-source systems have seen success in machine translation (Zoph and Knight, 2016), and to some extent in non-neural NER systems (Mayhew et al., 2017), and neural systems (Rahimi et al., 2019). Given that training data is available in this iteration of the shared task, we purposefully chose to not include rule-based components into our model in order to focus on getting the most out of the given training data. Our results on the official test da"
W19-3710,D17-1269,1,0.888861,"Missing"
W19-3710,W18-2501,0,0.0184618,"buted to this success: Factor 1. The large overlap in the tagset distributions. PER, LOC, and ORG tags made up the majority of annotations in all datasets. Thus, most information required to learn a model is present in the training data regardless of tagset. Furthermore, PRO and EVT entities are rare enough in the test data that even small scores shouldn’t hurt the micro-average. In fact, Table 4 shows that when going from AllTrain, which uses only the BSNLP19 tagset, to AllLangsEng, which includes Model For our model, we use a standard BiLSTMCRF (Lample et al., 2016) implemented in AllenNLP (Gardner et al., 2018). The model used character embeddings with a single layer Convolutional Neural Network (CNN) with 128 filters, and word embeddings from multilingual BERT (Devlin et al., 2019). We used the bertbase-multilingual-cased model from huggingface5 which uses a shared wordpiece vocabulary among all languages, meaning that we can share models even across Cyrillic and Latin scripts. We did not fine-tune BERT during training, but learned a scalar mix of the 12 layers. For each word, we use the first wordpiece to be representative of the entire word, as done in Devlin et al. (2019). 4 fasttext.cc/docs/en/"
W19-3710,N18-1202,0,0.0227231,"zuk et al., 2017). There has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018). Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years. Early models proposed were averaged perceptron (Ratinov and Roth, 2009), and conditional random field (Manning et al., 2014). In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016). A further increase in performance has come with contextual embeddings (Devlin et al., 2019; Peters et al., 2018; Akbik et al., 2018), which are based on large language models trained over massive corpora. Of particular interest is the multilingual BERT model (Devlin et al., 2019), which is trained over the concatenation of the Wikipedias in over 100 languages.1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have emergent cross-lingual properties, as well as language identification capabilities (Wu and Dredze, 2019). Several models have been proposed for multisource learning, in which multiple languages are used to train a model, including for machine translati"
W19-3710,W19-3709,0,0.105108,"Missing"
W19-3710,D18-1330,0,0.0301923,"Missing"
W98-0717,C94-2195,0,0.0387323,"ence gathered for the class. In this section we attempt to isolate the semantic content of the classes from their disjunctive meaning. Random classes, which mimic in different aspects the structure of the semantic CL classes, were 4.4 Comparison with o t h e r works This section presents a comparison of our work with other works on the P P A task. In order to obtain a fair comparison we have tested our system on the complete data set, including the preposition of (cf. Sec. 2). The results are compared with a maximum-entropy method (Ratnaparkhi et al.,1994), transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the backoff estimation (Collins and Brooks, 1995) and a memory-based method (Zavrel et al., 1997). All these works have used the same train and test data set. Table 5 presents the comparison. In all cases, the quoted figures axe the best results obtained by the authors; with the exception of the Brill and Resnik (1994) result, which was obtained by Zavrel et al. (1997) using the same method. Originally,T B L was evaluated by Brilland Resnik (1994) nouns. [CL126:] 126 classes uniformly distributed over CL nouns. Here the number of classes in CL is maintained. [CL-PERM:] A"
W98-0717,W95-0103,0,0.0301268,"k, PPA and the SNOW architecture and algorithm. In section 4 we describe the classes and present the main experiments with the semantic and random classes. Section 5 concludes. 2 Prepositional phrase attachment The PPA problem is to decide whether the prepositional phrase (PP) attaches to the direct object NP as in Buy the car with the steering wheel (nattachment) or to the verb phrase buy, as in Buy the car with his money (v-attachment). P P A is • a c o m m o n cause of structural ambiguity in natural language. Earlier works on this problem (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Zavrel et al., 1997) represented an example by the 4-tuple &lt;v, nl, p, n2&gt; containing the V P head, the direct object N P head, the preposition, and the indirect object N P head respectively. The first example in the previous paragraph is thus represented by &lt;buy, car, with, wheel&gt;. The experiments reported here were done using data extracted by Ratnaparkhi et al. (1994) from the Penn Treebank (Marcus et al., 1993) WSJ corpus. It consists of 20801 training examples and 3097 separate test examples. The preposition of turns out to be a very strong indicator for noun attachment. Among the 3097 t"
W98-0717,W97-0306,1,0.495905,"g (1) what types of knowledge sources can be used for performance improvement, and at what granularity level and (2) which computational mechanisms can make the best use of these sources. In particular, the effect of noun-class information on learning Prepositional Phrase Attachment (PPA, cf. Sec. 2) is studied. This problem is studied within SNO IF, a sparse architecture utilizing an on-line learning algorithm based on Winnow (Littlestone, 1988). That algorithm has been applied for natural language disambiguation tasks and related problems and perform remarkably well (Golding and Roth, 1996; Dagan et al., 1997; Roth and Zelenko, 1998). The noun-class data was derived from the WordNet database (Miller, 1990) which was compiled for general linguistic purposes, irrespective of the PPA problem. We derived the classes at different granularities. At the highest level, nouns are classified according to their synsets. The lower levels are obtained by successively using the hypernym relation defined in WordNet. In addition, we use the Corelex database (Buitelaar, 1998). Consisting of 126 coarse-grained semantic types covering around 40,000 nouns, Corelex defines a large number of systematic polysemous class"
W98-0717,J93-2004,0,0.0237234,"Missing"
W98-0717,H94-1048,0,0.0356642,"ganized as follows: we start by presenting the task, PPA and the SNOW architecture and algorithm. In section 4 we describe the classes and present the main experiments with the semantic and random classes. Section 5 concludes. 2 Prepositional phrase attachment The PPA problem is to decide whether the prepositional phrase (PP) attaches to the direct object NP as in Buy the car with the steering wheel (nattachment) or to the verb phrase buy, as in Buy the car with his money (v-attachment). P P A is • a c o m m o n cause of structural ambiguity in natural language. Earlier works on this problem (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Zavrel et al., 1997) represented an example by the 4-tuple &lt;v, nl, p, n2&gt; containing the V P head, the direct object N P head, the preposition, and the indirect object N P head respectively. The first example in the previous paragraph is thus represented by &lt;buy, car, with, wheel&gt;. The experiments reported here were done using data extracted by Ratnaparkhi et al. (1994) from the Penn Treebank (Marcus et al., 1993) WSJ corpus. It consists of 20801 training examples and 3097 separate test examples. The preposition of turns out to be a very stro"
W98-0717,P92-1053,0,0.0776201,"a proper noun, which clearly gives a very crude approximation. 4.2 E x p e r i m e n t a l R e s u l t s In this section we present results of incorporating various semantic data and their combinations. Since the classes were not compiled specifically for the PPA problem, some of the class information may be irrelevant or even slightly misleading. The results provide an assesment of the relative relevance of each knowledge source. When a noun belongs to a class, one may replace the explicit noun feature by its classes. Using the classes in addition to the original noun (Brill and R~nik, 1994; Resnik, 1992; Resnik, 1995)seems, however, a better strategy. Consider, for example, the feature &lt;prep,indirect-object=n2&gt;. Suppose the noun n2 belongs to two classes c l and c2. The class information will be incorporated by creating two additional features: &lt;prep,indirect-object=c 1&gt; and &lt;prep,indirect-object=c2&gt;, thereby enhancing the feature set without losing the original information. As mentioned above, giving up the original feature yielded degraded results. The results of adding features from a single knowledge source, presented in Table 2, show that FF have yielded small improvements over the l e"
W98-0717,W95-0105,0,0.0139184,"which clearly gives a very crude approximation. 4.2 E x p e r i m e n t a l R e s u l t s In this section we present results of incorporating various semantic data and their combinations. Since the classes were not compiled specifically for the PPA problem, some of the class information may be irrelevant or even slightly misleading. The results provide an assesment of the relative relevance of each knowledge source. When a noun belongs to a class, one may replace the explicit noun feature by its classes. Using the classes in addition to the original noun (Brill and R~nik, 1994; Resnik, 1992; Resnik, 1995)seems, however, a better strategy. Consider, for example, the feature &lt;prep,indirect-object=n2&gt;. Suppose the noun n2 belongs to two classes c l and c2. The class information will be incorporated by creating two additional features: &lt;prep,indirect-object=c 1&gt; and &lt;prep,indirect-object=c2&gt;, thereby enhancing the feature set without losing the original information. As mentioned above, giving up the original feature yielded degraded results. The results of adding features from a single knowledge source, presented in Table 2, show that FF have yielded small improvements over the l e n a set, within"
W98-0717,P98-2186,1,0.810881,"knowledge sources can be used for performance improvement, and at what granularity level and (2) which computational mechanisms can make the best use of these sources. In particular, the effect of noun-class information on learning Prepositional Phrase Attachment (PPA, cf. Sec. 2) is studied. This problem is studied within SNO IF, a sparse architecture utilizing an on-line learning algorithm based on Winnow (Littlestone, 1988). That algorithm has been applied for natural language disambiguation tasks and related problems and perform remarkably well (Golding and Roth, 1996; Dagan et al., 1997; Roth and Zelenko, 1998). The noun-class data was derived from the WordNet database (Miller, 1990) which was compiled for general linguistic purposes, irrespective of the PPA problem. We derived the classes at different granularities. At the highest level, nouns are classified according to their synsets. The lower levels are obtained by successively using the hypernym relation defined in WordNet. In addition, we use the Corelex database (Buitelaar, 1998). Consisting of 126 coarse-grained semantic types covering around 40,000 nouns, Corelex defines a large number of systematic polysemous classes that are derived from"
W98-0717,W97-1016,0,0.036995,"ecture and algorithm. In section 4 we describe the classes and present the main experiments with the semantic and random classes. Section 5 concludes. 2 Prepositional phrase attachment The PPA problem is to decide whether the prepositional phrase (PP) attaches to the direct object NP as in Buy the car with the steering wheel (nattachment) or to the verb phrase buy, as in Buy the car with his money (v-attachment). P P A is • a c o m m o n cause of structural ambiguity in natural language. Earlier works on this problem (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Zavrel et al., 1997) represented an example by the 4-tuple &lt;v, nl, p, n2&gt; containing the V P head, the direct object N P head, the preposition, and the indirect object N P head respectively. The first example in the previous paragraph is thus represented by &lt;buy, car, with, wheel&gt;. The experiments reported here were done using data extracted by Ratnaparkhi et al. (1994) from the Penn Treebank (Marcus et al., 1993) WSJ corpus. It consists of 20801 training examples and 3097 separate test examples. The preposition of turns out to be a very strong indicator for noun attachment. Among the 3097 test examples, 925 cont"
W98-0717,W96-0104,0,\N,Missing
W98-0717,C98-2181,1,\N,Missing
W99-0621,P98-1010,0,0.505894,"rsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part-of-speech information - has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998). * Research supported by NSF grants IIS-9801638 and SBR-9873450. t Research supported by NSF grant CCR-9502540. 168 This paper presents a general learning approach for identifying syntactic patterns, based on the S N o W learning architecture (Roth, 1998; Roth, 1999). The S N o W learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. S N o W is specificallytailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large - of which N"
W99-0621,J95-4004,0,0.399471,"Missing"
W99-0621,A88-1019,0,0.937073,", 1991; Greffenstette, 1993). Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part-of-speech information - has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998). * Research supported by NSF grants IIS-9801638 and SBR-9873450. t Research supported by NSF grant CCR-9502540. 168 This paper presents a general learning approach for identifying syntactic patterns, based on the S N o W learning architecture (Roth, 1998; Roth, 1999). The S N o W learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. S N o W is specificallytailored for learning in domains in which the potential number of information sources (features) taki"
W99-0621,W93-0113,0,0.0157692,"r Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors. 1 Introduction Shallow parsing is studied as an alternative to full-sentence parsers. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part-of-speech information - has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Ma"
W99-0621,W95-0107,0,0.047593,"nstette, 1993). Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part-of-speech information - has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998). * Research supported by NSF grants IIS-9801638 and SBR-9873450. t Research supported by NSF grant CCR-9502540. 168 This paper presents a general learning approach for identifying syntactic patterns, based on the S N o W learning architecture (Roth, 1998; Roth, 1999). The S N o W learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. S N o W is specificallytailored for learning in domains in which the potential number of information sources (features) taking part in decisions is ve"
W99-0621,P98-2186,1,0.55262,"rning approach for identifying syntactic patterns, based on the S N o W learning architecture (Roth, 1998; Roth, 1999). The S N o W learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. S N o W is specificallytailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large - of which N L P is a principal example. Preliminary versions of it have already been used successfully on several tasks in natural language processing (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998). In particular, SNoW's sparse architecture supports well chaining and combining predictors to produce a coherent inference. This property of the architecture is the base for the learning approach studied here in the context of shallow parsing. Shallow parsing tasks often involve the identification of syntactic phrases or of words that participate in a syntactic relationship. Computationally, each decision of this sort involves multiple predictions that interact in some way. For example, in identifying a phrase, one can identify the beginning and end of the phrase while also making sure they a"
W99-0621,J93-2004,0,\N,Missing
W99-0621,C98-1010,0,\N,Missing
W99-0621,C98-2181,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,W09-1119,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,N06-2015,0,\N,Missing
wu-etal-2014-illinoiscloudnlp,W03-0419,0,\N,Missing
wu-etal-2014-illinoiscloudnlp,D13-1184,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,P13-4004,0,\N,Missing
wu-etal-2014-illinoiscloudnlp,clarke-etal-2012-nlp,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,P98-2186,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,C98-2181,1,\N,Missing
wu-etal-2014-illinoiscloudnlp,rizzolo-roth-2010-learning,1,\N,Missing
